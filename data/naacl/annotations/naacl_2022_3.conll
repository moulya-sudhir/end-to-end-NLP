-DOCSTART- O
Learning O
Natural O
Language O
Generation O
with O
Truncated B-MethodName
Reinforcement I-MethodName
Learning I-MethodName

Learning B-MethodName
for I-MethodName
Language I-MethodName
( I-MethodName
TrufLL I-MethodName
) I-MethodName
, O
an O
original O
approach O
to O
train O
conditional O
language O
models O
without O
a O
supervised O
learning O
phase O
, O
by O
only O
using O
reinforcement O
learning O
( O
RL O
) O
. O
As O
RL O
methods O
unsuccessfully O
scale O
to O
large O
action O
spaces O
, O
we O
dynamically O
truncate O
the O
vocabulary O
space O
using O
a O
generic O
language O
model. O
TrufLL B-MethodName
thus O
enables O
to O
train O
a O
language O
agent O
by O
solely O
interacting O
with O
its O
environment O
without O
any O
task-specific O
prior O
knowledge O
; O
it O
is O
only O
guided O
with O
a O
task-agnostic O
language O
model. O
Interestingly O
, O
this O
approach O
avoids O
the O
dependency O
to O
labelled O
datasets O
and O
inherently O
reduces O
pretrained O
policy O
flaws O
such O
as O
language O
or O
exposure O
biases. O
We O
evaluate O
TrufLL B-MethodName
on O
two O
visual O
question O
generation O
tasks O
, O
for O
which O
we O
report O
positive O
results O
over O
performance O
and O
language O
metrics O
, O
which O
we O
then O
corroborate O
with O
a O
human O
evaluation. O
To O
our O
knowledge O
, O
it O
is O
the O
first O
approach O
that O
successfully O
learns O
a O
language O
generation O
policy O
without O
pre-training O
, O
using O
only O
reinforcement O
learning. O
1 O

Introduction O
Since O
the O
development O
of O
generic O
language B-MethodName
models I-MethodName
trained O
on O
massive O
unlabelled O
text O
corpora O
Brown O
et O
al. O
, O
2020 O
) O
, O
state-of-the O
art O
language O
processing O
systems O
rely O
on O
sequential B-MethodName
transfer I-MethodName
learning I-MethodName
( O
Ruder O
, O
2019 O
) O
. O
The O
pretrained O
Language B-MethodName
Model I-MethodName
( O
LM B-MethodName
) O
is O
fine-tuned O
on O
the O
downstream O
task O
using O
a O
standard B-MethodName
supervised I-MethodName
learning I-MethodName
( I-MethodName
SL I-MethodName
) I-MethodName
1 O
Code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
AMDonati O
/ O
RL-NLP O
VQG O
, O
TrufLL B-MethodName
truncates O
the O
vocabulary O
space O
by O
using O
a O
language O
model. O
Here O
, O
'run O
, O
' O
and O
'the O
' O
are O
syntactically O
incorrect O
and O
thus O
truncated. O
Yet O
, O
'car O
' O
is O
not O
trimmed O
as O
the O
LM B-MethodName
is O
not O
visually O
grounded. O
( O
right O
) O
In O
a O
VQG B-MethodName
training O
loop O
, O
the O
agent O
generates O
a O
question O
given O
an O
image-answer O
pair O
, O
which O
is O
then O
fed O
to O
a O
VQA B-MethodName
model O
predicting O
an O
expected O
answer. O
If O
both O
answers O
match O
, O
the O
agent O
is O
rewarded O
. O

objective O
Peters O
et O
al. O
, O
2019 O
) O
. O
Yet O
, O
such O
an O
approach O
suffers O
from O
several O
issues O
: O
( O
i O
) O
catastrophic O
forgetting O
when O
a O
model O
forgets O
previously O
learned O
knowledge O
and O
overfits O
to O
target O
domains O
, O
( O
ii O
) O
computational O
inefficiency O
from O
fine-tuning O
billion-parameters O
networks O
, O
and O
( O
iii O
) O
the O
need O
of O
supervised O
datasets. O
Moreover O
, O
task-specific O
language B-MethodName
models I-MethodName
learned O
with O
SL B-MethodName
suffer O
from O
well-studied O
text B-TaskName
degeneration I-TaskName
issues O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
such O
as O
the O
exposure O
bias O
, O
language O
biases O
( O
Saleh O
et O
al. O
, O
2020 O
; O
, O
or O
a O
lack O
of O
diversity O
( O
Li O
et O
al. O
, O
2015 O
) O
. O

On O
the O
other O
hand O
, O
text B-TaskName
generation I-TaskName
can O
be O
naturally O
framed O
as O
a O
sequential B-TaskName
decision I-TaskName
making I-TaskName
problem O
, O
with O
the O
sequence O
of O
words O
seen O
as O
successive O
actions O
over O
a O
vocabulary. O
Thus O
, O
some O
researchers O
have O
recently O
focused O
on O
learning O
language O
models O
using O
instead O
Reinforcement B-MethodName
Learning I-MethodName
( I-MethodName
RL I-MethodName
) I-MethodName
Das O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
. O
RL B-MethodName
methods O
allow O
acquiring O
language O
through O
interactions O
within O
rich O
and O
diverse O
environments O
( O
Luketina O
et O
al. O
, O
2019 O
) O
, O
help O
understanding O
language O
acquisition O
and O
language O
pragmatics O
( O
Lazaridou O
et O
al. O
, O
2016 O
; O
Bisk O
et O
al. O
, O
2020 O
) O
. O
" O
Reward O
is O
enough O
" O
( O
Silver O
et O
al. O
, O
2021 O
) O
highlights O
the O
necessity O
of O
using O
RL B-MethodName
for O
AI O
systems O
to O
acquire O
language O
in O
its O
full O
richness. O
Indeed O
, O
( O
i O
) O
language O
may O
be O
intertwined O
with O
other O
modalities O
of O
action O
and O
observation O
, O
( O
ii O
) O
the O
utility O
of O
language O
varies O
according O
to O
situations O
and O
behaviours O
, O
( O
iii O
) O
it O
is O
consequential O
and O
purposeful O
, O
and O
( O
iv O
) O
some O
linguistic O
problems O
are O
better O
solved O
dynamically O
, O
through O
experience O
( O
such O
as O
using O
a O
diplomatic O
tone O
in O
a O
speech. O
) O
In O
addition O
, O
RL B-MethodName
allows O
optimizing O
a O
non-differentiable O
learning O
signal O
, O
hence O
handles O
more O
diverse O
objective O
functions O
, O
and O
also O
avoids O
some O
of O
the O
text B-TaskName
degeneration I-TaskName
issues O
previously O
mentioned. O
So O
far O
, O
RL-based B-MethodName
text-generation B-TaskName
tasks O
have O
relied O
on O
a O
pre-training O
phase O
to O
ease O
learning O
: O
the O
policy O
language O
model O
is O
trained O
with O
SL B-MethodName
on O
the O
task O
dataset O
, O
before O
being O
fine-tuned O
with O
policy O
gradient O
methods O
( O
Sutton O
et O
al. O
, O
1999 O
) O
on O
the O
task O
at O
hand. O
Those O
approaches O
often O
require O
human-labelled O
datasets. O
Besides O
, O
combining O
pre-training O
and O
fine-tuning O
phases O
either O
barely O
change O
the O
policy O
distribution O
, O
or O
induces O
language O
drift O
( O
Lazaridou O
et O
al. O
, O
2020 O
; O
Lu O
et O
al. O
, O
2020b O
) O
, O
i.e O
the O
generated O
language O
drifts O
semantically O
or O
syntactically O
from O
natural O
language O
. O

In O
this O
paper O
, O
we O
aim O
at O
learning O
a O
conditional O
language O
model O
using O
RL B-MethodName
without O
a O
pre-training O
phase O
, O
so O
that O
( O
i O
) O
we O
get O
free O
from O
datasets O
with O
human O
annotations O
, O
and O
( O
ii O
) O
we O
avoid O
the O
text B-TaskName
generation I-TaskName
flaws O
induced O
by O
the O
common O
methods. O
While O
appealing O
, O
such O
an O
approach O
requires O
overcoming O
the O
hurdle O
of O
the O
combinatorial O
language O
action O
space O
, O
a O
vocabulary O
usually O
containing O
more O
than O
10,000 O
words. O
Yet O
, O
while O
large O
and O
discrete O
, O
a O
language O
action O
space O
contains O
a O
specific O
structure O
, O
made O
of O
all O
the O
syntactical O
and O
semantics O
rules O
of O
a O
given O
language. O
TrufLL B-MethodName
leverages O
such O
structure O
to O
drive O
the O
exploration O
of O
the O
RL-based B-MethodName
language O
agent O
during O
training. O
At O
each O
time O
step O
of O
the O
text B-TaskName
generation I-TaskName
process O
, O
TrufLL B-MethodName
truncates O
its O
effective O
action O
space O
to O
a O
small O
subset O
of O
words O
provided O
by O
a O
pretrained O
task-agnostic O
language O
model. O
Such O
an O
approach O
injects O
a O
generic O
prior O
linguistic O
knowledge O
into O
the O
RL B-MethodName
algorithm O
, O
is O
usable O
on O
tasks O
lacking O
in-domain O
labeled O
data O
, O
and O
can O
be O
easily O
transferred O
to O
new O
RL-based B-MethodName
text O
generation O
tasks. O
Thus O
, O
TrufLL B-MethodName
can O
be O
applied O
to O
any O
language O
generation O
task O
given O
a O
generic O
LM O
and O
a O
reward. O
We O
here O
evaluate O
it O
on O
two O
Visual O
Question O
Generation O
( O
VQG O
) O
tasks O
, O
the O
synthetic O
CLEVR B-DatasetName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
, O
and O
the O
natural O
language O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
. O
Unlike O
alternative O
RL B-MethodName
without O
pre-training O
approaches O
, O
TrufLL B-MethodName
manages O
to O
ask O
meaningful O
and O
valid O
questions O
on O
large O
vocabularies O
, O
exhibiting O
success B-MetricName
rate I-MetricName
and O
language B-MetricName
metrics I-MetricName
close O
to O
pretrain O
models O
with O
labeled O
data O
, O
while O
producing O
more O
original O
language O
. O

TrufLL B-MethodName
We O
here O
aim O
at O
making O
RL B-MethodName
methods O
feasible O
in O
the O
language O
setting O
by O
dynamically O
reducing O
the O
action O
space O
, O
i.e. O
, O
by O
restricting O
the O
language O
agent O
to O
select O
a O
word O
within O
a O
subset O
of O
the O
vocabulary O
at O
each O
time O
step. O
We O
detail O
below O
the O
action O
space O
's O
truncation O
model O
and O
the O
associated O
RL B-MethodName
algorithm O
to O
learn O
the O
language O
agent O
. O

Dynamic B-MethodName
Vocabulary I-MethodName
Truncation I-MethodName
TrufLL I-MethodName
combines O
two O
distinct O
language O
models O
, O
which O
share O
the O
same O
vocabulary O
V O
: O
a O
RL O
language O
agent O
π O
θ O
and O
a O
pretrained O
language O
model O
f O
LM O
. O
At O
each O
timestep O
t O
, O
TrufLL B-MethodName
restricts O
the O
vocabulary O
space O
of O
the O
RL B-MethodName
language O
agent O
with O
: O

V O
− O
t O
= O
{ O
w|w O
∈V O
, O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
} O
, O

where O
g O
trunc O
is O
a O
truncation O
function O
based O
on O
f O
LM O
which O
either O
associates O
0 O
or O
1 O
with O
each O
word O
in O
the O
vocabulary O
given O
the O
past O
words O
w O
< O
t O
. O
From O
a O
language O
modelling O
perspective O
, O
the O
vocabulary O
space O
of O
the O
language O
agent O
is O
reduced O
from O
V O
to O
V O
− O
where O
|V O
− O
|≪|V| O
, O
with O
|•| O
the O
cardinal O
of O
a O
finite O
set. O
From O
a O
RL B-MethodName
perspective O
, O
the O
RL B-MethodName
agent O
follows O
a O
truncated O
policy O
π O
− O
θ O
which O
only O
samples O
actions O
over O
the O
subset O
V O
− O
. O
In O
practice O
, O
such O
a O
policy O
is O
computed O
using O
a O
masked O
softmax O
function O
over O
the O
truncated O
vocabulary O
V O
− O
t O
: O
π O
− O
θ O
( O
.|w O
< O
t O
, O
c O
) O
= O
softmax O
( O
m O
* O
logits O
π O
θ O
( O
w O
< O
t O
, O
c O
) O
) O
where O
m=1 O
when O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
otherwise O
m=−∞ O
. O

Truncation O
Functions O
We O
here O
list O
the O
different O
truncation O
functions O
g O
trunc O
explored O
through O
the O
paper O
. O

Top-k O
words O
: O
This O
function O
selects O
the O
k O
words O
with O
the O
highest O
probability O
given O
by O
f O
LM O
( O
.|w O
< O
t O
) O
: O

g O
top O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈top O
( O
k O
) O
( O
f O
LM O
( O
.|w O
< O
t O
) O
) O
. O

Probability O
threshold O
( O
α O
) O
: O
This O
function O
only O
keeps O
words O
having O
a O
probability O
f O
LM O
( O
.|w O
< O
t O
) O
greater O
than O
α O
: O

g O
p O
th O
( O
α O
) O
( O
w O
t O
|w O
< O
t O
; O
α O
) O
=1 O
f O
LM O
( O
wt|w O
< O
t O
) O
> O
α O
. O

Top-p B-HyperparameterName
: O
This O
function O
is O
based O
on O
nucleus O
sampling O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
and O
it O
keeps O
the O
most O
likely O
words O
contained O
in O
a O
probability O
mass O
p O
of O
f O
LM O
( O
.|w O
< O
t O
) O
. O
Formally O
, O
we O
define O
V O
p O
t O
as O
: O

V O
p O
t O
= O

g O
sample O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈ O
{ O
w O
i O
∼f O
LM O
( O
.|w O
< O
t O
) O
i∈ O
1 O
, O
... O
, O
k O
} O
. O

Only O
top B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
provides O
a O
fixed O
number O
of O
words O
at O
each O
time O
step. O
p B-HyperparameterName
th I-HyperparameterName
( I-HyperparameterName
α I-HyperparameterName
) I-HyperparameterName
, O
top B-HyperparameterName
( I-HyperparameterName
p I-HyperparameterName
) I-HyperparameterName
, O
and O
sample B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
have O
a O
dynamic O
truncation O
, O
whose O
size O
at O
t O
depends O
on O
the O
language O
model O
entropy O
. O

Task-Specific O
vs. O
Generic O
LM O
We O
benchmark O
two O
types O
of O
language O
models O
for O
truncation. O
On O
the O
one O
hand O
, O
we O
use O
an O
external O
language O
model O
pretrained O
on O
a O
large O
task-agnostic O
language O
corpora. O
Such O
a O
model O
provides O
a O
generic O
linguistic O
prior O
to O
the O
RL B-MethodName
agent O
exploration O
process O
, O
solely O
encoding O
syntactic O
and O
semantic O
information. O
On O
the O
other O
hand O
, O
we O
use O
a O
task-related O
language O
model O
pretrained O
on O
the O
supervised O
dataset O
associated O
with O
the O
task. O
Such O
a O
model O
provides O
a O
task-specific O
linguistic O
prior O
to O
the O
RL B-MethodName
language O
agent O
, O
and O
captures O
language O
pragmatics. O
We O
emphasize O
that O
this O
paper O
aims O
at O
leveraging O
taskagnostic O
language O
models O
as O
they O
discard O
the O
need O
for O
task-specific O
data. O
For O
the O
sake O
of O
completeness O
, O
we O
also O
study O
the O
truncation O
with O
the O
task-related O
LM O
as O
an O
additional O
benchmark O
to O
assess O
our O
approach O
. O

Experimental O
Setting O
We O
here O
list O
the O
experimental O
setting O
and O
detail O
the O
network O
and O
hyperparameters O
in O
Appendix O
A.4 O
. O

Visual O
Question O
Generation O
We O
showcase O
TrufLL B-MethodName
on O
the O
task O
of O
Visual B-MethodName
Question I-MethodName
Generation I-MethodName
( O
VQG B-MethodName
) O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
, O
which O
is O
a O
form O
of O
Visual O
Jeopardy O
! O
™ O
( O
Ferrucci O
, O
2012 O
) O
. O
There O
, O
the O
language O
agent O
observes O
an O
image-answer O
pair O
and O
has O
to O
generate O
a O
question O
that O
results O
in O
a O
similar O
answer O
, O
as O
illustrated O
in O
Figure O
1. O
Such O
a O
task O
presents O
multiple O
advantages. O
First O
, O
by O
combining O
vision O
, O
scene O
understanding O
and O
language O
generation O
, O
it O
requires O
high-level O
reasoning O
and O
exhibits O
a O
large O
spectrum O
of O
language O
difficulties. O
Secondly O
, O
the O
success O
criterion O
is O
naturally O
non-differentiable O
, O
hence O
a O
natural O
fit O
for O
RL B-MethodName
methods. O
Such O
a O
criterion O
, O
unlike O
metrics O
based O
on O
ground-truth O
sentences O
, O
allows O
generating O
diverse O
grounded O
questions O
given O
an O
image-answer O
pair O
. O

Formally O
, O
the O
initial O
context O
c O
is O
composed O
of O
the O
image-answer O
pair O
( O
I O
, O
A O
) O
. O
The O
RL B-MethodName
agent O
then O
generates O
a O
sequence O
of O
words O
w O
< O
t O
of O
maximum O
length O
T O
. O
We O
then O
provide O
the O
generated O
question O
to O
a O
pretrained O
VQA B-MethodName
model. O
This O
model O
takes O
as O
inputs O
the O
image O
I O
, O
the O
generated O
question O
w O
< O
t O
and O
outputs O
a O
predicted O
answerÂ. O
Finally O
, O
the O
agent O
receives O
a O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
based O
on O
A O
andÂ O
. O

Datasets O
We O
evaluate O
TrufLL B-MethodName
on O
the O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
datasets O
to O
simulate O
large-scale O
VQG B-DatasetName
datasets. O
The O
two O
datasets O
have O
been O
originally O
created O
for O
the O
task O
of O
Visual B-MethodName
Question I-MethodName
Answering I-MethodName
( I-MethodName
VQA I-MethodName
) I-MethodName
, O
i.e. O
for O
multi-modal O
classification O
algorithms O
predicting O
an O
answer O
given O
an O
image-question O
pair O
. O

CLEVR B-DatasetName
The O
CLEVR B-DatasetName
VQA B-MethodName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
is O
made O
of O
template O
questions O
on O
synthetic O
images O
, O
which O
contain O
simple O
objects O
with O
four O
distinct O
properties O
( O
shape O
, O
material O
, O
color O
, O
size O
) O
. O
The O
vocabulary O
contains O
86 O
words O
and O
28 O
potential O
answers O
, O
making O
it O
a O
valuable O
proof O
of O
concept O
for O
assessing O
TrufLL. B-MethodName
Both O
language O
models O
are O
single-layer O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
512 O
units O
, O
and O
512 O
word O
embedding O
dimension. O
The O
task-specific O
LM O
is O
trained O
over O
the O
full O
train O
dataset O
of O
CLEVR B-DatasetName
questions. O
The O
external O
language O
model O
is O
trained O
on O
the O
mixture O
of O
CLOSURE B-DatasetName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
and O
CLEVR-Dialog B-DatasetName
( O
Kottur O
et O
al. O
, O
2019 O
) O
datasets. O
Although O
those O
two O
datasets O
share O
the O
CLEVR B-DatasetName
vocabulary O
, O
their O
language O
distribution O
differs O
from O
vanilla B-DatasetName
CLEVR. I-DatasetName
Finally O
, O
we O
use O
a O
pretrained O
GT-Vector-NMN B-MethodName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
to O
compute O
the O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
A=Â O
, O
t=T O
−1 O
, O
where O
1 O
is O
the O
indicator O
function O
. O

VQAv2 B-DatasetName
The O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
is O
made O
of O
natural O
language O
and O
open-formed O
questions O
on O
images O
from O
the O
MS-Coco B-DatasetName
Dataset O
( O
Lin O
et O
al. O
, O
2014 O
) O
. O
It O
has O
a O
vocabulary O
of O
14,810 O
words O
and O
3,149 O
answers. O
The O
task-specific O
language O
model O
is O
a O
one-layer B-MethodName
LSTM I-MethodName
with O
512 O
units O
and O
a O
512 O
word O
embedding O
dimension O
, O
pretrained O
over O
the O
full O
training O
dataset O
of O
VQAv2 B-DatasetName
questions. O
The O
External O
Language O
Model O
is O
Open-AI B-MethodName
's I-MethodName
GPT-2 I-MethodName
. O
The O
original O
language O
model O
outputs O
a O
probability O
distribution O
over O
50,257 O
tokens O
, O
but O
we O
use O
a O
masked O
softmax O
function O
to O
restrict O
the O
probability O
distribution O
to O
the O
14,810 O
tokens O
of O
the O
VQAv2 B-DatasetName
dataset. O
Unlike O
most O
NLP B-MethodName
tasks O
relying O
on O
pretrained O
generic O
language O
models O
, O
we O
do O
not O
fine-tune O
it O
on O
the O
task O
dataset. O
Instead O
, O
we O
leverage O
the O
few-shot O
generalization O
capabilities O
of O
GPT-2 B-MethodName
, O
by O
feeding O
the O
language O
model O
with O
the O
prompt O
" O
Here O
are O
a O
few O
examples O
: O
" O
followed O
by O
100 O
random O
questions O
q O
< O
100 O
from O
the O
dataset. O
The O
truncation O
is O
then O
based O
on O
the O
probability O
distribution O
f O
gpt2 O
LM O
( O
.|q O
< O
100 O
, O
w O
< O
t O
) O
. O
Finally O
, O
we O
used O
a O
pretrained O
Vil-BERT B-MethodName
to O
compute O
the O
reward O
( O
Lu O
et O
al. O
, O
2020a O
) O
. O
Given O
the O
large O
number O
of O
answers O
, O
we O
use O
as O
reward O
a O
decreasing O
function O
of O
the O
rank O
of O
the O
reference O
answer O
rk O
( O
A O
) O
: O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
as O
further O
explained O
in O
Appendix O
A.5 O
. O

In O
these O
two O
settings O
, O
we O
acknowledge O
that O
the O
task O
dataset O
is O
still O
used O
to O
train O
the O
VQA B-MethodName
models. O
Please O
note O
that O
the O
VQA B-MethodName
modules O
are O
only O
used O
to O
model O
the O
environment O
, O
i.e. O
to O
provide O
a O
positive O
/ O
negative O
feedback O
to O
the O
agent. O
In O
other O
settings O
, O
TrufLL B-MethodName
would O
still O
work O
if O
we O
replace O
the O
VQA B-MethodName
model O
by O
any O
language O
interface O
: O
text-game O
( O
e.g. O
Zork O
) O
, O
expert-systems O
, O
or O
humans. O
Here O
, O
we O
only O
use O
the O
VQG O
framework O
as O
a O
proof O
of O
concept O
that O
natural O
language O
can O
be O
learned O
through O
pure O
interaction O
given O
any O
task O
reward. O
Other O
language O
generation O
applications O
are O
discussed O
in O
Section O
5.3 O
. O

Baselines O
In O
this O
paper O
, O
we O
aim O
to O
show O
that O
a O
RL B-MethodName
language O
agent O
can O
be O
trained O
from O
scratch O
, O
i.e. O
without O
the O
usual O
pre-training O
phase O
by O
solely O
interacting O
with O
another O
language O
system O
, O
the O
VQA B-MethodName
model O
, O
when O
supported O
by O
truncation O
methods. O
The O
truncation O
with O
the O
task-related O
LM O
is O
referred O
to O
as O
TrufLL B-MethodName
( O
Task-LM B-MethodName
) O
, O
while O
the O
one O
with O
the O
External O
LM O
is O
referred O
as O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
. O
We O
first O
emphasize O
the O
difficulty O
of O
training O
an O
RL B-MethodName
language O
agent O
without O
a O
supervised O
pre-training O
phase O
through O
two O
baselines. O
We O
trained O
a O
simple O
on-policy O
PPO O
algorithm O
without O
any O
action O
space O
pruning O
, O
and O
refer O
to O
it O
as O
scratch. O
Then O
, O
we O
added O
a O
Kullback-Leibler B-HyperparameterName
( I-HyperparameterName
KL I-HyperparameterName
) I-HyperparameterName
regularization I-HyperparameterName
term O
to O
the O
loss O
, O
λ O
KL B-HyperparameterName
KL B-HyperparameterName
( O
π O
θ O
||f O
LM O
) O
, O
with O
λ O
KL B-HyperparameterName
> O
0 O
, O
to O
incorporate O
language O
prior O
to O
the O
agent O
as O
in O
( O
Jaques O
et O
al. O
, O
2017 O
( O
Jaques O
et O
al. O
, O
, O
2019. O
We O
refer O
to O
it O
as O
scratch O
+ O
KL-task B-HyperparameterName
when O
distilling O
the O
task-specific O
language O
model O
, O
and O
scratch O
+ O
KL-ext B-HyperparameterName
with O
the O
external O
language O
model. O
Finally O
, O
we O
include O
two O
baselines O
with O
a O
pre-training O
phase. O
We O
trained O
a O
language O
agent O
on O
the O
task-dataset O
with O
a O
log-likelihood O
objective O
, O
and O
refer O
to O
it O
as O
pretrain. O
Then O
, O
we O
fine-tune O
the O
pretrained O
language O
agent O
with O
PPO O
without O
truncation O
, O
and O
refer O
to O
it O
as O
pretrain O
+ O
RL B-MethodName
fine-tune. O
These O
two O
baselines O
should O
be O
viewed O
as O
gold O
standards O
as O
they O
rely O
on O
task-related O
data O
; O
additionally O
, O
pretrain O
+ O
RL B-MethodName
fine-tune O
is O
today O
the O
state-of-the-art O
method O
for O
learning O
RL-based B-MethodName
LM O
. O

Metrics O
and O
Evaluation O
Methods O
Evaluating O
text O
generation O
is O
an O
open-research O
problem O
in O
language O
literature. O
We O
decompose O
automatic O
language O
evaluation O
into O
three O
categories O
to O
assess O
different O
facets O
of O
language O
, O
and O
perform O
as O
well O
a O
human O
evaluation O
study O
. O

Performance O
metrics. O
We O
measure O
the O
taskcompletion B-MetricName
score I-MetricName
or O
recall B-MetricName
@ O
1 B-MetricValue
which O
states O
whether O
the O
target O
answer O
A O
is O
the O
top O
answer O
of O
the O
VQA B-MethodName
models O
, O
and O
the O
recall B-MetricName
@ O
5 B-MetricValue
( O
R O
@ O
5 O
) O
, O
which O
assesses O
whether O
A O
is O
in O
the O
5 O
top O
answers. O
These O
scores O
measure O
the O
task-solving O
abilities O
of O
the O
agent O
, O
but O
they O
are O
also O
conditioned O
by O
the O
VQA B-MethodName
model O
abilities O
. O

Language O
Metrics. O
First O
, O
we O
used O
n-grams B-MetricName
metrics O
, O
BLEU B-MetricName
( O
Papineni O
et O
al. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
and O
CIDEr B-MetricName
( O
Vedantam O
et O
al. O
, O
2015 O
) O
, O
to O
measure O
the O
similarity O
between O
the O
generated O
question O
and O
the O
reference O
questions O
in O
the O
evaluation O
set. O
While O
those O
scores O
can O
capture O
syntactic O
and O
semantic O
properties O
of O
language O
, O
they O
also O
fall O
short O
when O
dealing O
with O
open-form O
language O
, O
e.g. O
an O
identical O
answer O
may O
arise O
from O
two O
non-overlapping O
but O
syntactically O
correct O
questions. O
Thus O
, O
we O
also O
compute O
two O
metrics O
assessing O
the O
quality O
of O
the O
language O
independently O
of O
reference O
questions O
, O
the O
perplexity B-MetricName
of I-MetricName
the I-MetricName
question I-MetricName
given I-MetricName
an I-MetricName
external I-MetricName
LM I-MetricName
( I-MetricName
ppl-e I-MetricName
) I-MetricName
, O
and O
its O
perplexity B-MetricName
given I-MetricName
the I-MetricName
task-related I-MetricName
LM I-MetricName
( I-MetricName
ppl-t I-MetricName
) I-MetricName
. O

Diversity O
Metrics. O
We O
here O
estimate O
a O
self-BLEU B-MetricName
( I-MetricName
sBLEU I-MetricName
) I-MetricName
score O
( O
Zhang O
et O
al. O
, O
2017 O
) O
over O
10 O
questions O
generated O
on O
the O
same O
image-answer O
pair. O
Although O
such O
score O
detects O
potential O
mode O
collapse O
, O
i.e. O
, O
when O
the O
language O
utters O
identical O
sequences O
of O
words O
, O
it O
also O
values O
babbling O
, O
i.e. O
, O
outputting O
random O
words. O
We O
thus O
also O
measure O
the O
probability O
mass O
of O
the O
ten O
most O
frequent O
words O
( O
Choshen O
et O
al. O
, O
2020 O
) O
, O
and O
refer O
to O
it O
as O
peakiness B-MetricName
( I-MetricName
peak I-MetricName
) I-MetricName
. O

Human O
Evaluation. O
On O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
performed O
human O
evaluation O
by O
surveying O
53 O
participants O
on O
the O
first O
50 O
questions O
produced O
by O
some O
of O
the O
models O
at O
test O
time. O
The O
study O
( O
further O
detailed O
in O
Appendix O
C O
) O
is O
based O
on O
pairwise O
comparison O
of O
question O
samples O
produced O
by O
the O
concurrent O
algorithms O
according O
to O
four O
criteria. O
First O
, O
we O
evaluated O
the O
language O
quality O
of O
the O
question O
samples O
, O
by O
asking O
the O
participants O
to O
select O
the O
most O
syntactically O
and O
semantically O
correct O
question O
among O
the O
two O
samples O
of O
the O
questions O
pair. O
Secondly O
, O
we O
evaluated O
language O
grounding O
, O
i.e O
adequacy O
of O
the O
sample O
to O
the O
image-answer O
pair O
, O
by O
asking O
the O
participants O
to O
select O
the O
question O
most O
suitable O
given O
the O
two O
elements. O
Thirdly O
, O
we O
evaluated O
the O
language O
originality O
and O
diversity O
, O
by O
asking O
participants O
to O
select O
the O
question O
the O
most O
different O
from O
the O
dataset O
reference O
question. O
Finally O
, O
we O
evaluated O
the O
number O
of O
syntax O
errors O
by O
asking O
participants O
to O
tick O
the O
question O
if O
it O
is O
grammatically O
incorrect. O
Examples O
of O
questions O
asked O
during O
the O
study O
are O
included O
in O
the O
Appendix O
C O
. O

Sampling O
methods O
for O
text B-TaskName
generation I-TaskName
When O
generating O
text O
from O
a O
trained O
language O
model O
, O
the O
quality O
and O
diversity O
of O
samples O
depend O
on O
the O
decoding O
algorithm O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O
We O
consider O
three O
text B-TaskName
generation I-TaskName
methods. O
greedy O
uses O
the O
argmax O
of O
the O
policy O
, O
while O
sampling O
uses O
the O
multinomial O
distribution. O
Finally O
, O
we O
sampled O
ten O
text O
sequences O
from O
the O
policy O
, O
and O
selected O
the O
one O
with O
the O
lowest O
perplexity B-MetricName
according O
to O
the O
external O
language O
model O
, O
and O
refer O
to O
it O
as O
lm-ranking. O
This O
process O
has O
been O
used O
recently O
in O
Text-to-Image B-TaskName
Generation O
tasks O
( O
Ramesh O
et O
al. O
, O
2021 O
5 O
Results O

CLEVR B-DatasetName
results O
Quantitative O
performance O
: O
In O
Table O
1 O
, O
vanilla O
RL B-MethodName
from O
scratch O
fails O
to O
have O
a O
decent O
performance O
even O
with O
synthetic O
language. O
Besides O
, O
adding O
a O
KL B-HyperparameterName
regularisation I-HyperparameterName
term O
does O
kick-start O
the O
learning O
process. O
Yet O
, O
as O
soon O
as O
we O
apply O
the O
dynamic O
truncation O
, O
TrufLL B-MethodName
matches O
the O
pretrained O
baselines O
performance O
when O
using O
the O
external O
LM O
, O
and O
even O
outperforms O
them O
with O
the O
task-specific O
LM. O
In O
this O
synthetic O
VQG B-MethodName
setting O
, O
TrufLL B-MethodName
seems O
to O
be O
a O
viable O
and O
promising O
procedure O
to O
learn O
a O
RL B-MethodName
language O
agent O
without O
a O
supervised O
training O
phase. O
Pretrained O
baselines O
have O
high O
language O
scores O
when O
assessed O
with O
datasetbased O
metrics O
, O
e.g O
BLEU B-MetricName
or O
task-perplexity. B-MetricName
Yet O
, O
they O
also O
remain O
close O
to O
the O
original O
dataset O
distribution O
with O
a O
medium O
external O
perplexity. O
Noticeably O
, O
TrufLL B-MethodName
with O
the O
task-specific O
LM O
follows O
the O
same O
pattern. O
On O
the O
other O
hand O
, O
TrufLL B-MethodName
with O
the O
external O
LM O
reports O
poor O
dataset-based O
language O
scores O
, O
while O
maintaining O
a O
low O
external O
perplexity. O
Therefore O
, O
TrufLL B-MethodName
seems O
to O
correctly O
capture O
the O
language O
distribution O
of O
the O
initial O
LM. O
As O
the O
performance O
score O
is O
high O
when O
using O
an O
external O
LM O
, O
it O
suggests O
that O
our O
approach O
can O
learn O
a O
policy O
on O
a O
language O
task O
with-out O
the O
need O
of O
a O
task-related O
dataset. O
Less O
positively O
, O
TrufLL B-MethodName
diversity O
metrics O
suggest O
potential O
mode O
collapse O
, O
with O
a O
high O
peakiness B-MetricName
and O
self-BLEU B-MetricName
score O
. O

Qualitative O
performance O
: O
We O
display O
qualitative O
samples O
in O
Figure O
2 O
and O
Appendix O
D. O
On O
the O
one O
hand O
, O
the O
pretrained O
baselines O
generate O
either O
a O
question O
inconsistent O
with O
the O
visual O
context O
, O
or O
which O
fails O
to O
answer O
the O
expected O
answer. O
action O
space O
, O
while O
having O
a O
lower O
performance O
, O
yields O
to O
the O
most O
correct O
and O
diverse O
language O
, O
with O
higher O
language B-MetricName
scores I-MetricName
and O
a O
lower O
self-BLEU. B-MetricName
A O
stochastic O
action O
space O
might O
be O
harder O
to O
explore O
efficiently O
for O
reaching O
good O
task-solving O
abilities O
, O
but O
might O
strengthen O
the O
agent O
language O
generation O
properties O
. O

VQAv2 B-DatasetName
task O
In O
CLEVR B-DatasetName
, O
we O
observe O
that O
TrufLL B-MethodName
seems O
a O
promising O
approach O
to O
learn O
a O
language O
policy O
without O
a O
supervised O
training O
phase O
, O
by O
solely O
interacting O
with O
another O
language O
system. O
We O
scale O
our O
approach O
to O
natural O
language O
with O
large O
vocabulary O
( O
15k O
tokens O
) O
through O
the O
VQAv2 B-DatasetName
dataset O
. O

Quantitative O
performance O
: O
Table O
3 O
reports O
the O
VQAv2 B-DatasetName
results O
, O
for O
which O
TrufLL B-MethodName
and O
the O
baselines O
present O
a O
similar O
trend O
than O
on O
CLEVR. B-DatasetName
First O
, O
the O
scratch O
baselines O
keep O
failing O
to O
learn O
a O
valuable O
policy O
, O
with O
performance B-MetricName
scores I-MetricName
and O
n-grams B-MetricName
metrics O
close O
to O
zero. O
Although O
TrufLL B-MethodName
does O
not O
outperform O
the O
performance O
of O
the O
pretrained O
baselines O
anymore O
, O
it O
still O
leads O
to O
similar O
performances O
, O
and O
satisfactory O
language B-MetricName
scores. I-MetricName
The O
similarity O
between O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
results O
suggests O
that O
the O
truncation O
approach O
is O
viable O
when O
using O
a O
generic O
LM O
whose O
original O
vocabulary O
distribution O
differs O
from O
the O
task. O
Interestingly O
, O
TrufLL B-MethodName
displays O
a O
self-BLEU B-MetricName
score O
similar O
to O
the O
pretrained O
baselines. O
This O
suggests O
that O
the O
poor O
diversity O
behavior O
observed O
on O
CLEVR B-DatasetName
is O
likely O
attributable O
to O
the O
small O
vocabulary O
and O
synthetic O
language O
distribution O
. O

Qualitative O
performance O
: O
In O
Figure O
2 O
and O
Appendix O
D O
, O
we O
display O
question O
samples O
for O
all O
models O
. O

TrufLL B-MethodName
and O
the O
pretrained O
baselines O
successfully O
generate O
a O
question O
giving O
the O
expected O
answer O
( O
" O
Black O
" O
) O
, O
while O
the O
RL B-MethodName
from O
scratch O
baselines O
fail O
, O
and O
even O
showcase O
degenerated O
language. O
Pretrained O
baselines O
tend O
to O
output O
a O
question O
closer O
to O
the O
reference O
question O
whereas O
TrufLL B-MethodName
outputs O
original O
questions O
which O
differs O
from O
the O
VQA B-MethodName
distribution O
, O
yet O
consistent O
with O
the O
context O
. O

Human O
Evaluation O
: O
Figure O
3 O
details O
the O
Human O
Evaluation O
results. O
Among O
the O
RL B-MethodName
from O
scratch O
baselines O
, O
we O
selected O
scratch+KL-task O
as O
the O
only O
model O
producing O
sometimes O
meaningful O
questions. O
Yet O
, O
it O
fails O
to O
generate O
correct O
and O
grounded O
language O
; O
it O
is O
thus O
not O
a O
viable O
approach O
despite O
its O
diverse O
output. O
In O
line O
with O
the O
automatic O
metrics O
, O
the O
supervised O
baselines O
produce O
the O
best O
language O
, O
while O
being O
accurately O
grounded. O
Yet O
, O
they O
exhibit O
significantly O
less O
diversity O
with O
the O
reference O
language O
; O
this O
suggests O
in O
particular O
that O
pretrain+RL O
fails O
to O
go O
beyond O
the O
initial O
task-data O
distribution. O
Finally O
, O
unlike O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
which O
suffers O
from O
syntactic O
errors O
, O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
produces O
language O
that O
qualitatively O
competes O
with O
pretrain O
models O
( O
53 B-MetricValue
% I-MetricValue
) O
, O
with O
a O
similar O
ratio O
of O
syntactic O
uncorrect O
samples. O
Although O
its O
questions O
are O
less O
grounded O
, O
they O
are O
diverse O
, O
which O
suggests O
that O
they O
follow O
a O
different O
distribution O
from O
the O
initial O
VQA B-MethodName
dataset. O
It O
confirms O
that O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
could O
be O
an O
alternative O
approach O
as O
it O
has O
an O
excellent O
trade-off O
between O
language O
quality O
, O
diversity O
, O
and O
grounding O
. O

Decoding O
procedure O
: O
In O
Table O
4 O
, O
we O
evaluate O
the O
text O
sampling O
procedures O
described O
in O
Section O
4.5. O
While O
greedy O
decoding O
produces O
the O
best O
outcome O
for O
pretrained O
models O
, O
lm-ranking O
provides O
an O
excellent O
trade-off O
between O
task O
performance O
and O
language O
quality O
with O
RL-based B-MethodName
methods. O
As O
PG O
solely O
optimizes O
the O
task B-MetricName
success I-MetricName
ratio I-MetricName
, O
this O
may O
reduce O
overall O
language O
quality O
, O
the O
re-ranking O
thus O
retrieves O
the O
best O
syntactically O
sentences O
a O
posteriori O
. O

Discussion O
Removing O
the O
truncation O
at O
evaluation O
with O
offpolicy O
RL. B-MethodName
So O
far O
, O
TrufLL B-MethodName
directly O
learns O
the O
truncated O
policy O
over O
the O
truncated O
vocabulary O
V O
− O
each O
cell O
displays O
the O
proportion O
of O
questions O
chosen O
for O
the O
models O
in O
the O
row O
( O
bold O
) O
when O
compared O
to O
the O
concurrent O
model O
in O
the O
column. O
The O
table O
at O
the O
bottom O
displays O
the O
proportion O
of O
incorrect O
questions O
coming O
from O
each O
model O
among O
all O
incorrect O
samples. O
In O
all O
figures O
, O
bracket O
numbers O
indicates O
the O
model O
rank O
per O
criteria O
, O
from O
1= O
" O
best O
" O
to O
5= O
" O
worst O
" O
. O
2012 O
) O
. O
Formally O
, O
the O
off-policy O
PPO B-MetricName
loss I-MetricName
is O
defined O
by O
: O

L B-MetricName
off I-MetricName
ppo I-MetricName
( I-MetricName
θ I-MetricName
) I-MetricName
=E O
π O
− O
θ O
min O
( O
ρ O
θ O
t O
A O
t O
, O
clip O
( O
1−ϵ O
, O
ρ O
θ O
t O
,1+ϵ O
) O
A O
t O
) O
, O
whereρ O
θ O
t O
= O
π O
θ O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
− O
θ O
old O

( O
at|st O
) O
is O
the O
new O
ratio. O
4 O
Table O
5 O
displays O
the O
on-policy O
and O
off-policy O
results O
on O
both O
VQG B-MethodName
tasks O
for O
TrufLL B-MethodName
( O
task-LM O
) O
, O
and O
is O
further O
detailed O
in O
Appendix O
B.3. O
We O
also O
monitor O
the O
probability B-MetricName
mass I-MetricName
of O
the O
policy O
attributed O
to O
the O
truncated O
action O
space O
( O
sumVA B-MetricName
) O
. O
The O
policy O
only O
samples O
words O
within O
the O
truncated O
action O
space O
when O
sumVA B-MetricName
= O
1 O
, O
without O
needing O
the O
truncation. O
On O
CLEVR B-DatasetName
, O
the O
TrufLL B-MethodName
off O
has O
lower O
-yet O
close O
-performance O
on O
language O
and O
task O
scores O
than O
TrufLL. B-MethodName
As O
its O
sumVA B-MetricName
ratios O
are O
very O
close O
to O
1 O
, O
the O
agent O
has O
learned O
to O
generalize O
over O
the O
full O
vocabulary. O
However O
, O
the O
approach O
does O
not O
manage O
to O
sufficiently O
scale O
to O
VQAv2. B-DatasetName
It O
could O
be O
improved O
with O
regularisation O
techniques O
and O
the O
use O
of O
TruFLL B-MethodName
within O
state-of-the-art O
off-policy O
RL B-MethodName
algorithms. O
We O
leave O
such O
possibilities O
to O
future O
works. O
Additional O
experiments. O
We O
sweep O
over O
truncation O
hyper-parameters O
in O
Table O
6 O
of O
Appendix O
B. O
In O
Table O
8 O
, O
we O
observe O
that O
rewarding O
an O
agent O
with O
a O
BLEU B-MetricName
score O
is O
sub-optimal O
in O
both O
language O
and O
task O
scores O
on O
CLEVR. B-DatasetName
In O
VQA B-MethodName
, O
we O
apply O
temperature O
scheduling O
on O
the O
LM O
to O
perform O
fine-grained O
truncations O
in O
Table O
9 O
of O
B.2. O
Finally O
, O
we O
explore O
TrufLL B-MethodName
with O
a O
pre-training O
phase O
in O
Table O
10 O
. O

Generalization O
of O
the O
approach. O
TrufLL B-MethodName
learns O
conditional O
language O
models O
able O
to O
solve O
specific O
Natural B-MethodName
Language I-MethodName
Generation I-MethodName
tasks O
given O
a O
context O
c. O
For O
solving O
such O
tasks O
, O
it O
only O
requires O
the O
context O
, O
a O
reward O
function O
that O
scores O
the O
language O
generated O
by O
the O
RL B-MethodName
agent O
with O
respect O
to O
the O
task O
, O
and O
eventually O
a O
few O
natural O
language O
demonstrations O
fed O
as O
input O
prompt O
to O
the O
generic O
language O
model O
used O
in O
the O
truncation O
algorithm. O
Hence O
, O
the O
method O
is O
transferable O
to O
a O
wide O
variety O
of O
NLG B-MethodName
tasks O
, O
without O
requiring O
upfront O
large-scale O
labelled O
datasets. O
Additionally O
, O
the O
RL B-MethodName
framework O
allows O
to O
optimize O
non-differentiable O
objectives O
, O
making O
TrufLL B-MethodName
a O
natural O
choice O
to O
learn O
end-to-end O
task-oriented O
dialogs O
, O
such O
as O
Das O
et O
al. O
, O
2017 O
) O
. O
Other O
interesting O
tasks O
for O
TrufLL B-MethodName
include O
the O
ones O
typically O
found O
in O
Vision O
and O
Language O
Representation O
Learning O
( O
Lu O
et O
al. O
, O
2020a O
) O
, O
such O
as O
Image B-TaskName
Captioning I-TaskName
, O
Grounding B-TaskName
Referring I-TaskName
Expressions I-TaskName
( O
generation O
of O
a O
referring O
expression O
over O
a O
specific O
bounding O
box O
of O
an O
image O
) O
, O
Captionbased B-TaskName
Image I-TaskName
Retrieval I-TaskName
( O
generation O
of O
a O
caption O
that O
discriminates O
an O
image O
between O
a O
set O
of O
images O
) O
. O

Reward O
functions O
for O
such O
tasks O
can O
be O
based O
on O
similarity O
scores O
between O
the O
generated O
language O
and O
the O
associated O
image O
or O
image O
region O
, O
which O
can O
be O
computed O
using O
pretrained O
language O
representations O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
or O
multi-modal O
pretrained O
systems O
such O
as O
ViLBERT B-MethodName
( O
Lu O
et O
al. O
, O
2019 O
) O
. O
The O
context O
can O
be O
any O
kind O
of O
data O
structure O
( O
natural O
language O
, O
database O
, O
video O
, O
etc O
) O
: O
if O
it O
is O
a O
linguistic O
input O
, O
TrufLL B-MethodName
can O
be O
applied O
for O
instance O
to O
text O
summarization O
, O
paraphrase O
generation O
( O
with O
reward O
functions O
based O
on O
similarity O
scores O
between O
the O
context O
and O
the O
generated O
language O
) O
or O
text-based O
games O
( O
Ammanabrolu O
and O
Riedl O
, O
2018 O
) O
. O

6 O
Related O
work O
RL B-MethodName
and O
NLP B-MethodName
Tasks. O
Following O
( O
Singh O
et O
al. O
, O
2002 O
; O
Lemon O
and O
Pietquin O
, O
2007 O
) O
, O
recent O
RL-based O
taskoriented O
dialogues O
Das O
et O
al. O
, O
2017 O
; O
Lewis O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
have O
been O
developed O
, O
where O
the O
policy O
language O
model O
is O
generally O
pretrained O
with O
SL B-MethodName
followed O
RL B-MethodName
( O
Yao O
et O
al. O
, O
2020 O
) O
combines O
a O
pretrained O
language O
model O
to O
prune O
the O
action O
space O
with O
a O
Deep-Q B-MethodName
network O
, O
aka O
DRNN O
) O
. O
Yet O
, O
its O
truncation O
language O
model O
remains O
fine-tuned O
on O
the O
RL B-MethodName
dataset. O
Besides O
, O
CALM B-MethodName
is O
only O
evaluated O
on O
a O
vocabulary O
of O
697 O
tokens O
, O
and O
on O
4-words O
action O
sequences O
. O

Learning O
Language O
Models O
from O
scratch. O
( O
Ziegler O
et O
al. O
, O
2019 O
; O
Garg O
et O
al. O
, O
2021 O
) O
finetune O
pretrained O
GPT-2 B-MethodName
models O
with O
RL B-MethodName
for O
language O
generation O
tasks O
without O
task-related O
data O
, O
only O
using O
reward O
signals. O
Yet O
, O
they O
still O
face O
optimization O
and O
computational O
challenges O
( O
Parisotto O
et O
al. O
, O
2020 O
) O
. O

Conclusion O
We O
proposed O
TrufLL B-MethodName
, O
an O
original O
approach O
to O
learn O
a O
natural B-MethodName
language I-MethodName
generation I-MethodName
( O
NLG B-MethodName
) O
task O
using O
RL B-MethodName
, O
without O
the O
usual O
pre-training O
phase O
requiring O
supervised O
datasets. O
To O
our O
knowledge O
, O
this O
is O
the O
first O
RL-based B-MethodName
algorithm O
dedicated O
to O
learning O
a O
word-based O
text-generation O
task O
, O
which O
does O
not O
rely O
on O
a O
pre-training O
phase O
while O
scaling O
to O
large O
vocabularies. O
Although O
it O
comes O
with O
its O
limitations O
, O
the O
truncated O
RL B-MethodName
algorithm O
provided O
by O
TrufLL B-MethodName
gets O
free O
from O
labelled O
data O
in O
task-oriented O
language O
models O
, O
presents O
interesting O
language O
generation O
properties O
, O
and O
provides O
a O
generic O
and O
transferable O
method O
to O
learn O
any O
NLG B-MethodName
problem O
. O

A O
Dataset O
and O
training O
details O
A.1 O
Evaluation O
Metrics O
For O
the O
BLEU B-MetricName
and O
METEOR B-MetricName
scores O
, O
we O
used O
the O
NLTK B-MethodName
5 I-MethodName
implementations O
with O
the O
smoothing O
function O
number O
2 O
for O
the O
BLEU B-MetricName
score. O
For O
the O
CIDEr B-MetricName
score O
, O
we O
used O
the O
nlg-eval B-MethodName
implementation O
6 O
. O

A.2 O
Answer O
filtering O
For O
each O
dataset O
, O
we O
remove O
yes O
and O
no O
question-answer O
pairs O
which O
frequency O
largely O
exceeds O
other O
answers O
, O
to O
avoid O
any O
bias O
in O
the O
question O
generation O
process O
, O
as O
usually O
done O
in O
the O
VQG B-MethodName
litterature O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
. O

A.3 O
Dataset O
split O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
the O
RL B-MethodName
language O
agent O
is O
trained O
for O
50k O
( O
resp. O
100k O
) O
episodes O
over O
the O
first O
20k O
images O
( O
resp. O
all O
the O
images O
) O
of O
the O
training O
dataset O
, O
and O
is O
then O
evaluated O
on O
the O
first O
5k O
( O
resp. O
20k O
) O
images O
of O
the O
validation O
set. O
Besides O
, O
we O
uniformly O
sample O
the O
answer O
in O
the O
set O
of O
reference O
answers O
for O
each O
image O
to O
reduce O
the O
bias O
in O
the O
distribution O
of O
answers. O
Finally O
, O
questions O
are O
limited O
to O
20 O
( O
resp. O
10 O
) O
words O
. O

A.4 O
Language O
Agent O
Networks O
and O
Training O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
we O
used O
a O
single-layer O
LSTM B-MethodName
with O
64 B-HyperparameterValue
( O
resp. O
256 O
) O
units O
for O
the O
policy B-HyperparameterName
network. I-HyperparameterName
At O
every O
time O
step O
, O
the O
LSTM B-MethodName
input O
is O
then O
the O
concatenation O
of O
the O
word B-HyperparameterName
embedding I-HyperparameterName
of I-HyperparameterName
dimension I-HyperparameterName
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
the O
answer B-HyperparameterName
embedding I-HyperparameterName
of O
dimension O
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
and O
the O
image O
representation. O
For O
CLEVR B-DatasetName
, O
the O
image O
representation O
is O
extracted O
from O
a O
pretrained O
ResNet50 B-MethodName
and O
projected O
into O
a O
tensor B-HyperparameterName
of O
size O
( O
32,7,7 B-HyperparameterValue
) O
before O
being O
flattened. O
For O
VQAv2 B-DatasetName
, O
the O
image O
representation O
is O
the O
average O
of O
200 O
bounding O
box O
features O
of O
dimension O
1048 B-HyperparameterValue
, O
extracted O
from O
a O
faster O
R- B-MethodName
CNN I-MethodName
( O
Ren O
et O
al. O
, O
2015 O
) O
. O
We O
optimize O
the O
full O
loss B-MetricName
L=L O
P O
P O
O O
+αL O
V O
F O
+βL O
E O
with O
α=0.5 B-HyperparameterName
, O
β B-HyperparameterName
=0.01 B-HyperparameterValue
and O
a O
PPO B-HyperparameterName
clipping I-HyperparameterName
ratio I-HyperparameterName
ϵ=0.02 I-HyperparameterName
( O
resp. O
0.01 O
) O
for O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
. O
We O
use O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
) O
of O
10 B-MetricName
−3 I-MetricName
for O
TrufLL B-MethodName
and O
the O
scratch O
baseline O
, O
10 B-HyperparameterValue
−5 I-HyperparameterValue
( O
resp. O
10 O
−6 O
) O
for O
RL B-MethodName
algorithms O
with O
a O
pre-training O
phase O
on O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
and O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
models O
including O
a O
KL B-MethodName
regularization I-MethodName
term. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
( O
bs O
) O
of O
128 B-HyperparameterValue
for O
all O
models O
except O
the O
ones O
with O
KL O
regularization O
, O
for O
which O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64. B-HyperparameterValue
Finally O
, O
for O
the O
RL B-MethodName
from O
scratch O
baselines O
, O
we O
perform O
gradient B-HyperparameterName
clipping I-HyperparameterName
( O
gladclip B-HyperparameterName
) O
of O
1 B-HyperparameterValue
( O
resp. O
5 O
) O
for O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
. O

Such O
hyper-parameters O
were O
selected O
, O
after O
conducting O
an O
extensive O
hyper-parameter O
search. O
The O
following O
values O
were O
tested O
: O
β B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue
} O
, O
ϵ B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue
, I-HyperparameterValue
0.5 I-HyperparameterValue
, I-HyperparameterValue
0.9 I-HyperparameterValue
} O
, O
lr B-HyperparameterName
∈ O
{ O
10 B-HyperparameterValue
−6 I-HyperparameterValue
,10 I-HyperparameterValue
−5 I-HyperparameterValue
,10 I-HyperparameterValue
−4 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
,10 I-HyperparameterValue
−3 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−3 I-HyperparameterValue
,10 I-HyperparameterValue
−2 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−2 I-HyperparameterValue
} O
, O
gradclip B-HyperparameterName
∈ O
{ O
None,1,5,10,100 B-HyperparameterValue
} O
, O
bs B-HyperparameterName
∈ O
{ O
32,64,128 B-HyperparameterValue
} O
. O

Additionally O
, O
we O
also O
tested O
for O
VQAv2 B-DatasetName
policy B-HyperparameterName
networks I-HyperparameterName
with O
64 B-HyperparameterValue
, O
256 B-HyperparameterValue
and O
1024 B-HyperparameterValue
units O
, O
with O
respectively O
32 B-HyperparameterValue
, O
128 B-HyperparameterValue
and O
512 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimensions. I-HyperparameterName
We O
kept O
the O
network B-HyperparameterName
size I-HyperparameterName
giving O
the O
best O
performances O
, O
i.e. O
policy B-HyperparameterName
network I-HyperparameterName
of O
256 B-HyperparameterValue
units O
and O
128 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimension I-HyperparameterName
. O

A.5 O
Reward O
formula O
for O
VQAv2 B-DatasetName
In O
this O
section O
, O
we O
detail O
the O
reward O
function O
used O
for O
the O
VQAv2 B-DatasetName
task. O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
=1 O
rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
with O
rk O
( O
A O
) O
the O
rank O
of O
the O
ground-truth O
answer O
given O
by O
the O
VQA B-MethodName
model O
, O
when O
predicting O
the O
actual O
answer O
from O
the O
terminal O
state O
( O
c O
, O
w O
< O
T O
) O
. O
Formally O
, O
it O
is O
defined O
as O
: O

rk O
( O
A O
) O
=rank O
( O
VQA O
( O
c O
, O
w O
< O
T O
) O
[ O
A O
] O
) O
, O

with O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
the O
probability O
distribution O
given O
by O
the O
VQA B-MethodName
model O
over O
the O
set O
of O
answers O
, O
and O
rank O
the O
function O
which O
ranks O
the O
probability O
of O
answer O
A O
within O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
probability O
distribution O
. O

B O
Additional O
experiments O
B.1 O
CLEVR B-DatasetName
Table O
6 O
displays O
the O
complete O
ablation O
on O
the O
truncation O
functions O
with O
parameters O
sweep. O
The O
'sizeVA O
' O
variable O
indicates O
the O
average O
size O
of O
the O
truncated O
action O
space O
for O
each O
truncation O
function. O
Table O
7 O
displays O
the O
ablation O
over O
the O
three O
decoding O
procedures O
defined O
in O
Section O
4.5. O
Such O
an O
ablation O
presents O
a O
similar O
pattern O
than O
VQAv2 B-DatasetName
results O
described O
in O
section O
5.2 O
. O

Finally O
, O
Table O
8 O
reports O
CLEVR B-DatasetName
metrics O
when O
using O
the O
BLEU B-MetricName
score O
as O
the O
reward. O
While O
on O
such O
a O
task O
TrufLL B-MethodName
still O
exhibits O
promising O
language B-MetricName
scores I-MetricName
, O
the O
n-grams B-MetricName
metrics O
remain O
lower O
than O
the O
pretrained O
baselines. O
This O
illustrates O
that O
using O
a O
language O
similarity B-MetricName
score I-MetricName
as O
a O
reward O
signal O
is O
much O
less O
interesting O
than O
a O
reward O
based O
on O
a O
task O
completion O
score O
. O

B.2 O
VQAv2 B-DatasetName
Temperature O
scheduling O
: O
On O
the O
CLEVR B-DatasetName
task O
, O
we O
observed O
that O
dynamic O
truncations O
outperform O
static O
ones O
such O
as O
top B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
: O
indeed O
, O
they O
better O
take O
into O
account O
the O
inherent O
variability O
of O
the O
language O
structure O
at O
the O
sentence-level. O
When O
scaling O
up O
to O
the O
15k O
words O
of O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
dynamically O
decrease O
the O
truncation O
size O
through O
training O
, O
by O
applying O
a O
decreasing O
temperature O
schedule O
on O
the O
language O
model. O
While O
temperature O
scaling O
( O
Bahdanau O
et O
al. O
, O
2015 O
) O
is O
usually O
used O
at O
test O
time O
to O
control O
the O
smoothness O
of O
the O
language O
model O
distribution O
, O
temperature O
schedules O
during O
training O
of O
language O
models O
have O
been O
used O
in O
several O
settings O
( O
Jang O
et O
al. O
, O
2016 O
; O
Zhang O
et O
al. O
, O
2018 O
; O
Wang O
et O
al. O
, O
2020 O
) O
. O
Formally O
, O
f O
LM O
( O
w O
i O
|w O
< O
t O
) O
distribution O
is O
computed O
as O
softmax B-MetricName
( I-MetricName
x I-MetricName
i I-MetricName
) I-MetricName
=e O
−x O
i O
/ O
τ O
/ O
j O
e O
−x O
j O
/ O
τ O
, O
with O
x O
j O
the O
LM O
logits O
and O
τ O
the O
temperature O
, O
which O
decreases O
from O
τ O
max O
to O
τ O
min O
by O
a O
factor O
T O
F O
every O
T O
u O
training O
step. O
In O
Table O
9 O
, O
both O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
benefit O
slightly O
from O
truncation O
with O
a O
temperature O
schedule O
compared O
to O
a O
vanilla O
truncation. O
The O
former O
displays O
the O
best O
performance B-MetricName
/ I-MetricName
language I-MetricName
scores I-MetricName
trade-off O
for O
the O
schedule O
" O
τ O
: O
3 O
> O
1. O
& O
T O
u O
=5,000 O
" O
, O
while O
the O
latter O
has O
the O
best O
metrics O
trade-off O
for O
" O
τ O
: O
1.5 O
> O
1. O
& O
T O
u O
=5,000 O
" O
. O
Finally O
, O
Figure O
4 O
displays O
the O
evolution O
of O
the O
training O
return O
for O
TrufLL B-MethodName
and O
the O
baselines. O
As O
expected O
, O
the O
pretrain+RL O
fine-tune O
baseline O
return O
does O
not O
evolve O
much O
, O
confirming O
that O
the O
policy O
distribution O
almost O
does O
not O
shift O
through O
the O
fine-tuning O
phase. O
The O
training O
curves O
of O
TrufLL B-MethodName
present O
a O
steady O
increase O
in O
the O
return O
until O
reaching O
convergence O
, O
confirming O
that O
our O
approach O
, O
by O
guiding O
the O
exploration O
of O
the O
action O
space O
, O
provides O
a O
sufficient O
learning O
signal. O
On O
the O
other O
hand O
, O
the O
scratch+KL O
baselines O
stay O
stuck O
to O
a O
low O
training O
return. O
This O
suggests O
that O
the O
KL B-MethodName
regularization I-MethodName
term O
, O
while O
encouraging O
the O
policy O
distribution O
to O
resemble O
the O
language O
model O
distribution O
, O
fails O
to O
capture O
the O
task O
pragmatics O
, O
which O
requires O
generating O
a O
language O
that O
is O
visually O
grounded O
. O


-DOCSTART- O
Language B-MethodName
Model I-MethodName
Augmented I-MethodName
Monotonic I-MethodName
Attention I-MethodName
for I-MethodName
Simultaneous I-MethodName
Translation I-MethodName

The B-MethodName
state-of-the-art I-MethodName
adaptive I-MethodName
policies I-MethodName
for I-MethodName
simultaneous I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
use O
monotonic O
attention O
to O
perform O
read O
/ O
write O
decisions O
based O
on O
the O
partial O
source O
and O
target O
sequences. O
The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic O
attention O
to O
take O
poor O
read O
/ O
write O
decisions O
, O
which O
in O
turn O
negatively O
affects O
the O
performance O
of O
the O
SNMT B-MethodName
model. O
On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read O
/ O
write O
decisions O
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge. O
In O
this O
work O
, O
we O
propose O
a O
framework O
to O
aid O
monotonic B-MethodName
attention I-MethodName
with O
an O
external O
language O
model O
to O
improve O
its O
decisions. O
Experiments O
on O
MuST-C B-DatasetName
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state-of-the-art O
monotonic O
multi-head O
attention O
model O
further O
. O

Introduction O
A O
typical O
application O
of O
simultaneous O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( I-TaskName
SNMT I-TaskName
) I-TaskName
is O
conversational O
speech O
or O
live O
video O
caption O
translation. O
In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT B-TaskName
model O
alternates O
between O
performing O
read O
from O
source O
sequence O
and O
write O
to O
target O
sequence. O
For O
a O
model O
to O
decide O
whether O
to O
read O
or O
write O
at O
certain O
moment O
, O
either O
a O
fixed O
or O
an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O

Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al. O
( O
2019a O
) O
and O
Dalvi O
et O
al. O
( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
and O
write O
after O
the O
waiting O
period O
of O
k O
tokens. O
To O
alleviate O
possible O
long O
delay O
of O
fixed O
polices O
, O
recent O
works O
such O
as O
monotonic B-MethodName
infinite I-MethodName
lookback I-MethodName
attention I-MethodName
( I-MethodName
MILk I-MethodName
) I-MethodName
( O
Arivazhagan O
et O
al. O
, O
2019 O
) O
, O
and O
monotonic B-MethodName
multihead I-MethodName
attention I-MethodName
( I-MethodName
MMA I-MethodName
) I-MethodName
( O
Ma O
et O
al. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic B-MethodName
attention I-MethodName
( O
Raffel O
et O
al. O
, O
2017 O
) O
. O
* O
⋆ O
Work O
done O
while O
at O
Samsung O
Research O
† O

Equal O
contribution O
Figure O
1 O
: O
The O
finetuned O
XLM-RoBERTa B-MethodName
language O
model O
predicts O
German O
words O
using O
the O
prefix O
as O
input. O
( O
Green O
: O
Correct O
, O
Red O
: O
Incorrect O
, O
Black O
: O
Neutral O
) O
. O

While O
these O
monotonic B-MethodName
attention I-MethodName
anticipates O
target O
words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguistic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra-linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O
Inspired O
by O
human B-MethodName
translation I-MethodName
experts O
, O
we O
aim O
to O
augment O
monotonic B-MethodName
attention I-MethodName
with O
future O
information O
using O
language O
models O
( O
LM O
) O
( O
Devlin O
et O
al. O
, O
2019 O
; O
Conneau O
et O
al. O
, O
2019 O
) O
. O

Integrating O
the O
external O
information O
effectively O
into O
text-to-text B-TaskName
machine I-TaskName
translation I-TaskName
( B-MethodName
MT I-MethodName
) I-MethodName
systems I-MethodName
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al. O
, O
2020 O
; O
Gulcehre O
et O
al. O
, O
2015Gulcehre O
et O
al. O
, O
, O
2017Stahlberg O
et O
al. O
, O
2018 O
) O
. O
Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT B-MethodName
system O
during O
training O
is O
explored O
in O
Wu O
et O
al. O
( O
2020 O
) O
by O
simultaneously O
training O
different O
wait-k O
SNMT B-MethodName
systems. O
However O
, O
no O
previous O
works O
make O
use O
of O
explicit O
future O
information O
both O
during O
training O
and O
inference. O
To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
integrate O
future O
information O
from O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O
to O
explore O
a O
tighter O
integration O
of O
the O
LM B-MethodName
information O
into O
SNMT B-MethodName
model O
. O

In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future O
information O
from O
LM O
during O
training O
by O
transforming O
the O
monotonic O
attention O
mechanism. O
As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM O
takes O
the O
prefix O
target O
( O
and O
source O
, O
for O
cross-lingual O
LM O
) O
sequence O
and O
predicts O
the O
probable O
future O
information. O
We O
hypothesize O
that O
aiding O
the O
monotonic B-TaskName
attention I-TaskName
with O
this O
future O
information O
can O
improve O
MMA B-MethodName
model O
's O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay. O
Several O
experiments O
on O
MuST-C O
( O
Di O
Gangi O
et O
al. O
, O
2019 O
) O
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency-quality B-MetricName
trade-offs O
over O
the O
state-of-the-art O
MMA B-MethodName
models O
. O

2 O
Monotonic B-MethodName
Attention I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
Model I-MethodName

Monotonic B-MethodName
Attention I-MethodName
In O
simultaneous O
machine B-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
models I-MethodName
, O
the O
probability O
of O
predicting O
the O
target O
token O
y O
i O
∈ O
y O
depends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x O
≤j O
∈ O
x O
, O
y O
< O
i O
∈ O
y O
) O
. O
In O
sequence-tosequence B-MethodName
based I-MethodName
SNMT I-MethodName
model I-MethodName
, O
each O
target O
token O
y O
i O
is O
generated O
as O
follows O
: O

h O
j O
= O
E O
( O
x O
≤j O
) O
( O
1 O
) O

s O
i O
= O
D O
( O
y O
< O
i O
, O
c O
i O
= O
A O
( O
s O
i−1 O
, O
h O
≤j O
) O
) O
( O
2 O
) O

y O
i O
= O
Output O
( O
s O
i O
) O
( O
3 O
) O

where O
E O
( O
. O
) O
and O
D O
( O
. O
) O
are O
the O
encoder O
and O
decoder O
layers O
, O
and O
c O
i O
is O
a O
context O
vector. O
In O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
SNMT I-MethodName
, O
the O
context O
vector O
is O
computed O
as O
follows O
: O

e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i−1 O
, O
h O
j O
) O
( O
4 O
) O
p O
i O
, O
j O
= O
Sigmoid O
( O
e O
i O
, O
j O
) O
( O
5 O
) O

z O
i O
, O
j O
∼ O
Bernoulli O
( O
p O
i O
, O
j O
) O
( O
6 O
) O

When O
generating O
a O
target O
token O
y O
i O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
p O
i O
, O
j O
. O
When O
z O
i O
, O
j O
= O
1 O
( O
write O
) O
, O
model O
sets O
t O
i O
= O
j O
, O
c O
i O
= O
h O
j O
and O
generates O
the O
target O
token O
y O
i O
. O
For O
z O
i O
, O
j O
= O
0 O
( O
read O
) O
, O
it O
sets O
t O
i O
= O
j O
+ O
1 O
and O
repeats O
Eq. O
4 O
to O
6. O
Here O
t O
i O
refers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
i O
th O
target O
token. O
Instead O
of O
hard O
alignment O
of O
c O
i O
= O
h O
j O
, O
Raffel O
et O
al. O
( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O
manner O
and O
propose O
a O
closed-form O
parallel O
solution. O
Arivazhagan O
et O
al. O
( O
2019 O
) O
adopt O
monotonic B-MethodName
attention I-MethodName
into O
SNMT B-MethodName
and O
later O
, O
Ma O
et O
al. O
( O
2019c O
) O
extend O
it O
to O
MMA B-MethodName
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O

Monotonic B-MethodName
Attention I-MethodName
with B-MethodName
Future I-MethodName
Information I-MethodName
The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information. O
To O
augment O
this O
anticipation O
process O
using O
future O
information O
extracted O
using O
LMs B-MethodName
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic B-MethodName
attention I-MethodName
. O

Future O
Representation O
Layer O
: O
At O
every O
decoding O
step O
i O
, O
the O
previous O
target O
token O
y O
i−1 O
is O
equipped O
with O
a O
plausible O
future O
tokenŷ O
i O
as O
shown O
in O
the O
Figure O
2. O
Since O
the O
tokenŷ O
i O
comes O
from O
an O
LM B-MethodName
possibly O
with O
a O
different O
tokenizer O
and O
vocabulary O
set O
, O
applying O
the O
model O
's O
tokenizer O
and O
vocabulary O
might O
split O
the O
tokenŷ O
i O
further O
into O
multiple O
sub-tokens O

{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
. O

To O
get O
a O
single O
future O
token O
representations O
i O
∈ O
R O
d O
from O
all O
the O
sub-tokens O
, O
we O
apply O
a O
sub-token O
summary O
layer O
: O

s O
i O
= O
Γ O
( O
{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
) O
( O
7 O
) O

The O
Γ B-HyperparameterName
represents O
a O
general O
sequence O
representation O
layer O
such O
as O
a O
Transformer B-HyperparameterName
encoder I-HyperparameterName
layer I-HyperparameterName
or O
a O
simple O
normalized O
sum O
of O
sub-token O
representations O
. O

We O
enrichs O
i O
at O
every O
layer B-HyperparameterName
l B-HyperparameterName
of O
the O
decoder O
block O
by O
applying O
a O
residual O
feed-forward O
network O
. O

s O
l O
i O
= O
F O
F O
N O
( O
ỹ O
l−1 O
i O
) O
( O
8 O
) O

Monotonic B-MethodName
Energy I-MethodName
Layer I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plausible O
future O
information O
to O
the O
output O
layer O
( O
Appendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
representation O
y O
i−1 O
, O
the O
MMA B-MethodName
read O
/ O
write O
decisions O
happen O
in O
Eq. O
4. O
Therefore O
, O
we O
integrates O
i O
into O
the O
Eq. O
4 O
instead O
. O

The O
integration O
is O
carried O
out O
by O
modifying O
Eq. O
4 O
-Eq. O
5. O
We O
compute O
the O
monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representations O
i O
available O
at O
each O
layer O
: O

e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i O
, O
h O
j O
) O
( O
9 O
) O

We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq. O
5 O
as O
follows O
: O

p O
i O
, O
j O
= O
Sigmoid O
( O
e O
i O
, O
j O
+ẽ O
i O
, O
j O
) O
( O
10 O
) O

After O
computingp O
i O
, O
j O
, O
we O
compute O
c O
i O
similar O
to O
MMA O
model. O
This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM O
output O
usage O
on O
the O
input O
sequence. O
The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM O
output O
by O
varying O
theẽ O
i O
, O
j O
. O
In O
case O
of O
insufficient O
source O
information O
in O
the O
low O
latency B-MetricName
regime O
, O
we O
expect O
the O
model O
's O
decision O
policy O
to O
rely O
more O
onẽ O
i O
, O
j O
. O

Inference O
: O
During O
inference O
, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information. O
After O
predicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
y O
i O
, O
we O
invoke O
the O
LM O
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq. O
10 O
. O

Experiments O
and O
Results O

Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experiments O
on O
the O
MuST-C B-DatasetName
English O
( O
En O
) O
-German O
( O
De O
) O
and O
English O
( O
En O
) O
-French O
( O
Fr O
) O
speech-to-text B-TaskName
( I-TaskName
ST I-TaskName
) I-TaskName
translation I-TaskName
task. O
The O
speech O
sequence O
is O
represented O
using O
80-dimensional O
log-mel O
filter O
bank O
features. O
The O
target O
sequence O
is O
represented O
as O
subwords O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary O
of O
size O
10,000. O
We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency B-MetricName
and O
quality B-MetricName
aspects. O
We O
use O
Average B-MetricName
Lagging I-MetricName
( I-MetricName
AL I-MetricName
) I-MetricName
as O
our O
latency B-MetricName
metric O
and O
case-sensitive O
detokenized O
SacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality B-MetricName
, O
similar O
to O
( O
Ma O
et O
al. O
, O
2020 O
) O
. O
The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST-C B-DatasetName
test O
( O
tst-COMMON O
) O
sets O
. O

Language O
Models O
We O
use O
two O
language O
models O
to O
train O
our O
proposed O
modified O
MMA B-MethodName
model. O
Firstly O
, O
we O
use O
the O
pretrained O
XLM-RoBERTa B-MethodName
( O
Conneau O
et O
al. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers O
1 O
model O
repository. O
Since O
the O
LM O
output O
can O
be O
very O
open-ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O
dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST-C B-DatasetName
target O
text O
data O
for O
each O
task O
. O

We O
also O
train O
a O
smaller B-MethodName
language I-MethodName
model I-MethodName
( O
SLM B-MethodName
) O
, O
which O
contains O
6 B-HyperparameterValue
Transformer B-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
512 B-HyperparameterValue
hidden-states B-HyperparameterName
and O
24M O
parameters. O
We O
use O
the O
MuST-C B-DatasetName
data O
along O
with O
additional O
data O
augmentation O
to O
reduce O
overfitting. O
The O
SLM B-MethodName
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O

Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al. O
( O
2020 O
) O
. O
We O
use O
a O
predecision B-MetricName
ratio I-MetricName
of O
7 B-MetricValue
, O
which O
means O
that O
the O
simultaneous O
read O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states. O
We O
use O
λ B-HyperparameterName
or O
λ B-HyperparameterName
latency B-HyperparameterName
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted O
average O
( O
λ B-HyperparameterName
avg I-HyperparameterName
) O
in O
MMA. B-MethodName
The O
values O
of O
this O
hyperparameter O
λ B-HyperparameterName
are O
chosen O
from O
the O
set O
{ O
0.01 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
} O
. O
The O
Γ O
layer O
in O
Eq. O
7 O
computes O
the O
normalized O
sum O
of O
the O
sub-token O
representations. O
For O
SLM B-MethodName
, O
it O
simply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set. O
All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
update_f B-MetricName
req I-MetricName
set O
to O
8 B-MetricValue
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait-K O
( O
Ma O
et O
al. O
, O
2019b O
) O
, O
we O
choose O
monotonic B-MethodName
attention I-MethodName
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al. O
, O
2019 O
; O
Ma O
et O
al. O
, O
2019c O
) O
. O
We O
train O
a O
baseline O
based O
on O
Ma O
et O
al. O
( O
2020 O
) O
work O
, O
called O
as O
MMA B-MethodName
model. O
The O
MMA B-MethodName
model O
encoder O
and O
decoder O
embedding B-HyperparameterName
dimensions I-HyperparameterName
are O
set O
to O
392 B-HyperparameterValue
, O
whereas O
our O
proposed O
model O
's O
encoder B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
embeddings I-HyperparameterName
are O
set O
to O
256 B-HyperparameterValue
to O
have O
similar O
parameters O
( O
≈ O
39M O
) O
for O
a O
fair O
comparison. O
We O
train O
two O
models O
using O
the O

Results O
We O
first O
analyze O
how O
the O
LM O
predictions O
are O
being O
utilized O
by O
the O
our O
model. O
In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
's O
internal O
states O
versus O
the O
predictions O
from O
the O
LM O
, O
we O
compare O
the O
norm B-MetricName
of I-MetricName
the I-MetricName
monotonic I-MetricName
energies I-MetricName
corresponding O
to O
the O
LM O
predictions O
e O
pred O
( O
Eq. O
9 O
) O
and O
the O
previous O
output O
tokens O
e O
output O
( O
Eq. O
4 O
) O
. O
Let O
us O
define O
LM O
prediction O
weight O
as O
follows O
: O

LM B-MethodName
pw B-HyperparameterName
= O
∥e O
pred O
∥ O
∥e O
output O
∥ O
( O
11 O
) O

In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LM O
pw B-HyperparameterName
( O
averaged O
) O
vs. O
λ. B-HyperparameterName
We O
use O
two O
additional O
values O
of O
λ B-HyperparameterName
∈ O
{ O
0.005 B-HyperparameterValue
, O
0.001 B-HyperparameterValue
} O
to O
obtain O
this O
plot. O
We O
can O
observe O
that O
as O
the O
latency B-MetricName
requirements O
become O
more O
and O
more O
strict O
, O
the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM. O
This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
information O
coming O
from O
LM O
predictions O
based O
on O
latency B-MetricName
requirements. O
Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach. O
By O
varying O
the O
λ B-HyperparameterName
, O
we O
train O
separate O
models O
for O
different O
latency B-MetricName
regimes. O
Moreover O
, O
the O
quality B-MetricName
and O
latency B-MetricName
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
controlling O
the O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
the O
inference. O
Speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
or O
step B-HyperparameterName
size I-HyperparameterName
refers O
to O
the O
duration O
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O
read O
decision. O
We O
vary O
these O
hyperparameters O
for O
all O
the O
three O
models O
, O
namely O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
. O

The O
BLEU-AL B-MetricName
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU-AL B-MetricName
numbers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference. O
We O
vary O
the O
step B-HyperparameterName
sizes I-HyperparameterName
in O
intervals O
of O
80ms B-HyperparameterValue
from O
120 B-HyperparameterValue
ms I-HyperparameterValue
to O
520 B-HyperparameterValue
ms I-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency B-MetricName
regimes. O
We O
can O
observe O
that O
the O
LM-based O
models O
using O
both O
XLM B-MethodName
and O
SLM B-MethodName
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA O
model. O
We O
observe O
improvements O
in O
the O
range O
of O
1-2 B-MetricValue
BLEU B-MetricName
scores O
consistently O
across O
all O
the O
latency O
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.01 B-HyperparameterValue
) O
. O
The O
MMA B-MethodName
using O
SLM B-MethodName
language O
model O
performs O
slightly O
better O
than O
MMA B-MethodName
using O
XLM B-MethodName
language O
model. O
This O
is O
due O
to O
SLM B-MethodName
's I-MethodName
higher O
accuracy B-MetricName
on O
the O
next O
token O
prediction O
task O
as O
compared O
to O
XLM B-MethodName
, O
30.15 B-MetricValue
% O
vs. O
A O
LM B-MethodName
at O
MMA B-MethodName
Output O
Layer O

We O
explored O
a O
naive O
approach O
of O
integrating O
LM O
information O
into O
the O
MMA. B-MethodName
In O
this O
approach O
, O
we O
integrate O
the O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
We O
refer O
to O
this O
experiment O
as O
'LM B-MethodName
Rescoring I-MethodName
( I-MethodName
LMR I-MethodName
) I-MethodName
' O
, O
and O
the O
corresponding O
model O
is O
called O
MMA-LMR. B-MethodName
As O
observed O
in O
Figure O
5 O
, O
MMA-LMR B-MethodName
has O
inferior O
performance O
compared O
to O
the O
MMA B-MethodName
model. O
Since O
the O
LM O
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA O
model O
can O
not O
easily O
discard O
the O
incorrect O
information O
from O
LM. O
This O
motivates O
us O
to O
tightly O
integrate O
the O
LM O
information O
into O
the O
simultaneous O
model O
. O

B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
language O
models O
( O
LMs O
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
Simultaneous I-MethodName
models I-MethodName
. O

B.1 O
XLM-Roberta B-MethodName
( I-MethodName
XLM-R I-MethodName
) I-MethodName
XLM-R B-MethodName
Large I-MethodName
model I-MethodName
2 O
was O
trained O
on O
the O
100 O
languages O
CommonCrawl B-DatasetName
corpora O
total O
size O
of O
2.5TB O
with O
550M O
parameters O
from O
24 B-HyperparameterValue
layers B-HyperparameterName
, O
1024 B-HyperparameterValue
hidden B-HyperparameterName
states I-HyperparameterName
, O
4096 B-HyperparameterValue
feed-forward B-HyperparameterName
hidden-states I-HyperparameterName
, O
and O
16 B-HyperparameterValue
heads. B-HyperparameterName
Total O
number O
of O
parameters O
is O
558M. O
We O
finetune O
the O
head O
of O
the O
XLM-R O
LM O
model O
using O
the O
Masked O
Language O
Modeling O
objective O
which O
accounts O
for O
0.23 O
% O
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3M O
parameters O
. O

B.2 O
Smaller O
Language O
Model O
Since O
the O
LM B-MethodName
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
2 O
https O
: O
/ O
/ O
huggingface.co O
/ O
xlm-roberta-large O
LM O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency B-MetricName
requirements. O
To O
reduce O
the O
LM O
computation B-MetricName
time I-MetricName
, O
we O
train O
a O
smaller B-MethodName
Language I-MethodName
Model I-MethodName
( I-MethodName
SLM I-MethodName
) I-MethodName
from O
scratch O
using O
the O
Causal B-MethodName
Language I-MethodName
Modeling I-MethodName
objective. O
SLM B-MethodName
is O
composed O
of O
6 O
Transformer O
decoder O
blocks O
, O
512 O
hidden-states O
, O
2048 O
feed-forward O
hidden-states O
& O
8 O
attention O
heads. O
It O
alleviates O
the O
need O
for O
the O
sub-token O
summary O
layer O
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models. O
The O
train O
examples O
are O
at O
the O
sentence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multiple O
sentences O
( O
which O
is O
the O
usual O
case O
for O
Language O
Models O
) O
. O

Since O
the O
target O
texts O
contain O
lesser O
than O
250k O
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data. O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST-C B-DatasetName
target O
text. O
Details O
have O
been O
provided O
in O
B.2.1 O
. O

B.2.1 O
Data O
Augmentation O
Up-Sampling O
: O
To O
boost O
the O
LM O
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST-C B-DatasetName
target O
text O
data O
by O
substituting O
and O
inserting O
words O
based O
on O
LM O
predictions. O
We O
use O
the O
NLPAUG O
3 O
package O
to O
get O
similar O
words O
based O
on O
contextual O
embeddings. O
From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
We O
observe O
that O
both O
upsampling O
and O
data O
augmentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST-C B-DatasetName
dev O
set O
. O

B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O
the O
LM O
prediction O
is O
obtained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM O
model. O
These O
predictions O
are O
pre-computed O
for O
training O
and O
validation O
sets. O
This O
ensures O
parallelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM O
simultaneously O
during O
the O
training O
process. O
During O
inference O
, O
the O
LM O
model O
is O
called O
every O
time O
a O
new O
output O
token O
is O
written O
. O

C O
Dataset O
The O
MuST-C B-DatasetName
dataset O
comprises O
of O
English O
TED O
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level. O
Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O

D O
Effect O
of O
LM B-HyperparameterName
Size I-HyperparameterName
on O
Latency-Quality B-MetricName
We O
train O
several O
SLM O
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 B-MetricName
accuracy. I-MetricName
As O
we O
increase O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
model O
from O
2 B-HyperparameterValue
to O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
layers O
, O
the O
SLM B-MethodName
and O
the O
proposed O
MMA B-MethodName
with O
future O
information O
models O
have O
shown O
performance O
improvements. O
However O
, O
increasing O
the O
number O
of O
layers O
greater O
than O
6 B-HyperparameterValue
does O
not O
yield O
any O
performance O
improvements. O
We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM B-MethodName
model O
while O
varying O
the O
number B-HyperparameterName
of I-HyperparameterName
hidden I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
head O
. O

E O
Training O
Details O
We O
follow O
the O
training O
process O
similar O
to O
Ma O
et O
al. O
( O
2020 O
) O
training O
process. O
We O
train O
an O
English O
ASR B-MethodName
model O
using O
the O
source O
speech O
data. O
Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency B-HyperparameterName
loss I-HyperparameterName
( O
setting O
λ B-HyperparameterName
latency B-HyperparameterName
= O
0 B-HyperparameterValue
) O
after O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model. O
After O
this O
step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs. B-HyperparameterName
This O
training O
process O
is O
repeated O
for O
all O
the O
reported O
models O
and O
for O
each O
task. O
The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O

F O
BLEU-AL O
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O
main O
paper O
, O
we O
vary O
the O
latency O
weight O
hyperparameter O
( O
λ O
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes. O
We O
also O
vary O
the O
step-size B-HyperparameterName
/ O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
inference. O
In O
total O
, O
we O
obtain O
18 O
different O
data O
points O
corresponding O
to O
each O
model. O
In O
Table O
3 O
, O
we O
compare O
the O
results O
obtained O
using O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
under O
similar O
hyperparameter O
settings. O
It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach O
. O
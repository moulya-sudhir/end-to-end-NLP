-DOCSTART- O
TSTR B-MethodName
: O
Too B-MethodName
Short I-MethodName
to I-MethodName
Represent I-MethodName
, O
Summarize O
with O
Details O
! O
Intro-Guided O
Extended O
Summary O
Generation O

Many O
scientific O
papers O
such O
as O
those O
in O
arXiv O
and O
PubMed O
data O
collections O
have O
abstracts O
with O
varying O
lengths O
of O
50-1000 O
words O
and O
average O
length O
of O
approximately O
200 O
words O
, O
where O
longer O
abstracts O
typically O
convey O
more O
information O
about O
the O
source O
paper. O
Up O
to O
recently O
, O
scientific O
summarization O
research O
has O
typically O
focused O
on O
generating O
short O
, O
abstractlike O
summaries O
following O
the O
existing O
datasets O
used O
for O
scientific O
summarization. O
In O
domains O
where O
the O
source O
text O
is O
relatively O
long-form O
, O
such O
as O
in O
scientific O
documents O
, O
such O
summary O
is O
not O
able O
to O
go O
beyond O
the O
general O
and O
coarse O
overview O
and O
provide O
salient O
information O
from O
the O
source O
document. O
The O
recent O
interest O
to O
tackle O
this O
problem O
motivated O
curation O
of O
scientific O
datasets O
, O
arXiv-Long O
and O
PubMed-Long O
, O
containing O
human-written O
summaries O
of O
400-600 O
words O
, O
hence O
, O
providing O
a O
venue O
for O
research O
in O
generating O
long O
/ O
extended O
summaries. O
Extended O
summaries O
facilitate O
a O
faster O
read O
while O
providing O
details O
beyond O
coarse O
information. O
In O
this O
paper O
, O
we O
propose O
TSTR B-MethodName
, O
an O
extractive O
summarizer O
that O
utilizes O
the O
introductory O
information O
of O
documents O
as O
pointers O
to O
their O
salient O
information. O
The O
evaluations O
on O
two O
existing O
large-scale O
extended O
summarization O
datasets O
indicate O
statistically O
significant O
improvement O
in O
terms O
of O
ROUGE B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( I-MetricName
F1 I-MetricName
) I-MetricName
scores O
( O
except O
in O
one O
case O
) O
as O
compared O
to O
strong O
baselines O
and O
state-of-theart. O
Comprehensive O
human O
evaluations O
favor O
our O
generated O
extended O
summaries O
in O
terms O
of O
cohesion O
and O
completeness O
. O

Introduction O
Over O
the O
past O
few O
years O
, O
summarization O
task O
has O
witnessed O
a O
huge O
deal O
of O
progress O
in O
extractive B-TaskName
( O
Nallapati O
et O
al. O
, O
2017 O
; O
Liu O
and O
Lapata O
, O
2019 O
; O
Yuan O
et O
al. O
, O
2020 O
; O
Cui O
et O
al. O
, O
2020 O
; O
Jia O
et O
al. O
, O
2020 O
; O
Feng O
et O
al. O
, O
2018 O
) O
and O
abstractive B-TaskName
( O
See O
et O
al. O
, O
2017 O
; O
Gehrmann O
et O
al. O
, O
2018 O
; O
Zhang O
et O
al. O
, O
2019 O
; O
Tian O
et O
al. O
, O
2019 O
; O
Zou O
et O
al. O
, O
2020 O
) O
[ O
Introductory O
] O
Neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
@ O
xcite O
) O
, O
directly O
applying O
a O
single O
neural O
network O
to O
transform O
the O
source O
sentence O
into O
the O
target O
sentence O
, O
has O
now O
reached O
impressive O
performance O
( O
@ O
xcite O
[ O
… O
] O
Motivated O
by O
recent O
success O
in O
unsupervised O
cross-lingual O
embeddings O
( O
@ O
xcite O
) O
, O
the O
models O
proposed O
for O
unsupervised O
NMT B-TaskName
often O
assume O
that O
a O
pair O
of O
sentences O
from O
two O
different O
languages O
can O
be O
mapped O
to O
a O
same O
latent O
representation O
in O
a O
shared-latent O
space O
( O
@ O
xcite O
) O
[ O
… O
] O
Although O
the O
shared O
encoder O
is O
vital O
for O
mapping O
sentences O
from O
different O
languages O
into O
the O
shared-latent O
space O
, O
it O
is O
weak O
in O
keeping O
the O
uniqueness O
and O
internal O
characteristics O
of O
each O
language O
, O
such O
as O
the O
style O
, O
terminology O
and O
sentence O
structure. O
[ O
… O
] O
For O
each O
language O
, O
the O
encoder O
and O
its O
corresponding O
decoder O
perform O
an O
AE O
, O
where O
the O
encoder O
generates O
the O
latent O
representations O
from O
the O
perturbed O
input O
sentences O
and O
the O
decoder O
reconstructs O
the O
sentences O
from O
the O
latent O
representations. O
Experimental O
results O
show O
that O
the O
proposed O
approach O
consistently O
achieves O
great O
success O
. O

Related O
Work O
Summarizing O
scientific O
documents O
has O
gained O
a O
huge O
deal O
of O
attention O
from O
researchers O
, O
although O
it O
has O
been O
studied O
for O
decades. O
Neural O
efforts O
in O
scientific O
text O
have O
used O
specific O
characteristics O
of O
papers O
such O
as O
discourse O
structure O
Xiao O
and O
Carenini O
, O
2019 O
) O
and O
citation O
information O
( O
Qazvinian O
and O
Radev O
, O
2008 O
; O
Goharian O
, O
2015 O
, O
2018 O
) O
to O
aid O
summarization O
model. O
While O
prior O
work O
has O
mostly O
covered O
the O
generation O
of O
shorter-form O
summaries O
( O
approx. O
200 O
terms O
) O
, O
generating O
extended O
summaries O
of O
roughly O
600 O
terms O
for O
long-form O
source O
documents O
such O
as O
scientific O
papers O
has O
been O
motivated O
very O
recently O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
. O
The O
proposed O
models O
for O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task I-TaskName
include O
jointly O
learning O
to O
predict O
sentence O
importance O
and O
sentence O
section O
to O
extract O
top O
sentences O
( O
Sotudeh O
et O
al. O
, O
2020 O
) O
; O
utilizing O
section-contribution O
computations O
to O
pick O
sentences O
from O
important O
section O
for O
forming O
the O
final O
summary O
( O
Ghosh O
Roy O
et O
al. O
, O
2020 O
) O
; O
identifying O
salient O
sections O
for O
generating B-TaskName
abstractive I-TaskName
summaries I-TaskName
( O
Gidiotis O
et O
al. O
, O
2020 O
) O
; O
ensembling O
of O
extraction O
and O
abstraction O
models O
to O
form O
final O
summary O
( O
Ying O
et O
al. O
, O
2021 O
) O
; O
an O
extractive O
model O
with O
TextRank O
algorithm O
equipped O
with O
BM25 O
as O
similarity O
function O
( O
Kaushik O
et O
al. O
, O
2021 O
) O
; O
and O
incorporating O
sentences O
embeddings O
into O
graph-based O
extractive O
summarizer O
in O
an O
unsupervised O
manner O
( O
Ramirez-Orta O
and O
Milios O
, O
2021 O
) O
. O
Unlike O
these O
works O
, O
we O
do O
not O
exploit O
any O
sectional O
nor O
citation O
information O
in O
this O
work. O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
at O
proposing O
the O
novel O
method O
of O
utilizing O
introductory O
information O
of O
the O
scientific O
paper O
to O
guide O
the O
model O
to O
learn O
to O
generate O
summary O
from O
the O
salient O
and O
related O
information O
. O

3 O
Background O
: O
Contextualized O
language O
models O
for O
summarization O

Contextualized O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
and O
ROBERTA B-MethodName
have O
achieved O
state-of-the-art O
performance O
on O
a O
variety O
of O
downstream O
NLP O
tasks O
including O
text O
summarization. O
Liu O
and O
Lapata O
( O
2019 O
) O
were O
the O
first O
to O
fine-tune O
a O
contextualized O
language O
model O
( O
i.e. O
, O
BERT B-MethodName
) O
for O
the O
summarization O
task. O
They O
proposed O
BERTSUM B-MethodName
-a O
fine-tuning O
scheme O
for O
text O
summarization-that O
outputs O
the O
sentence O
representations O
of O
the O
source O
document O
( O
we O
use O
the O
term O
source O
and O
source O
document O
interchangeably O
, O
referring O
to O
the O
entire O
document O
) O
. O
The O
BERT-SUMEXT B-MethodName
model O
, O
which O
is O
built O
based O
on O
BERT-SUM B-MethodName
, O
was O
proposed O
for O
the O
extractive B-TaskName
summarization I-TaskName
task. O
It O
utilizes O
the O
representations O
produced O
by O
BERTSUM B-MethodName
, O
passes O
them O
through O
Transformers O
encoder O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
, O
and O
finally O
uses O
a O
linear O
layer O
with O
Sigmoid O
function O
to O
compute O
copying O
probabilities O
for O
each O
input O
sentence. O
Formally O
, O
let O
l O
1 O
, O
l O
2 O
, O
... O
, O
l O
n O
be O
the O
binary O
tags O
over O
the O
source O
sentences O
x O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
n O
} O
of O
a O
long O
document O
, O
in O
which O
n O
is O
the O
number O
of O
sentences O
in O
the O
paper. O
The O
BERTSUMEXT B-MethodName
network O
runs O
over O
the O
source O
documents O
as O
follows O
( O
Eq. O
1 O
) O
, O

h O
b O
= O
BertSum B-MethodName
( O
x O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
o O
h O
+ O
b O
o O
) O
( O
1 O
) O

where O
h O
b O
and O
h O
are O
the O
representations O
of O
source O
sentences O
encoded O
by O
BERTSUM B-MethodName
and O
Trasformers O
encoder O
, O
respectively. O
W O
o O
and O
b O
o O
are O
trainable O
parameters O
, O
and O
p O
is O
the O
probability O
distribution O
over O
the O
source O
sentences O
, O
signifying O
extraction O
copy O
likelihood. O
The O
goal O
of O
this O
network O
is O
to O
train O
a O
network O
that O
can O
identify O
the O
positive O
sets O
of O
sentences O
as O
the O
summary. O
To O
prevent O
the O
network O
from O
selecting O
redundant O
sentences O
, O
BERTSUM B-MethodName
uses O
Trigram O
Blocking O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
for O
sentence O
selection O
in O
inference O
time. O
We O
refer O
the O
reader O
to O
the O
main O
paper O
for O
more O
details O
. O

TSTR B-MethodName
: O
Intro-guided O
Summarization O
In O
this O
section O
, O
we O
describe O
our O
methodology O
to O
tackle O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task. O
Our O
approach O
exploits O
the O
introductory O
information O
3 O
. O

3 O
Introductory O
information O
is O
defined O
in O
Section O
1 O
of O
the O
paper O
as O
pointers O
to O
salient O
sentences O
within O
it O
, O
as O
shown O
in O
Figure O
2. O
It O
is O
ultimately O
expected O
that O
the O
extractive O
summarizer O
is O
guided O
to O
pick O
salient O
sentences O
across O
the O
entire O
paper O
. O

The O
detailed O
illustration O
of O
our O
model O
is O
shown O
in O
Figure O
3. O
To O
aid O
the O
extractive B-TaskName
summarization I-TaskName
model O
( O
i.e. O
, O
right-hand O
box O
in O
Figure O
3 O
) O
which O
takes O
in O
source O
sentences O
of O
a O
scientific O
paper O
, O
we O
utilize O
an O
additional O
BERTSUM B-MethodName
encoder O
called O
Introductory O
encoder O
( O
left-hand O
box O
in O
Fig. O
3 O
) O
that O
receives O
x O
intro O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
m O
} O
, O
with O
m O
being O
the O
number O
of O
sentences O
in O
introductory O
section. O
The O
aim O
of O
adding O
second O
encoder O
in O
this O
framework O
is O
to O
identify O
the O
clues O
in O
the O
introductory O
section O
which O
point O
to O
the O
salient O
supplementary O
sentences O
4 O
. O
The O
BERTSUM B-MethodName
network O
computes O
the O
extraction O
probabilities O
for O
introductory O
sentences O
as O
follow O
( O
same O
way O
as O
in O
Eq. O
1 O
) O
, O

h O
b O
= O
BertSum B-MethodName
( O
x O
intro O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
jh O
+ O
b O
j O
) O
( O
2 O
) O

in O
whichh O
b O
, O
andh O
are O
the O
introductory O
sentence O
representations O
by O
BERTSUM B-MethodName
, O
Transformers O
encoder O
, O
respectively.p O
is O
the O
introductory O
sentence O
extraction O
probabilities. O
W O
j O
and O
b O
j O
are O
trainable O
matrices O
. O

After O
identifying O
salient O
introductory O
sentences O
, O
the O
representations O
associated O
with O
them O
are O
retrieved O
using O
a O
pooling O
function O
and O
further O
used O
to O
guide O
the O
first O
task O
( O
i.e. O
, O
right-hand O
side O
in O
Figure O
3 O
) O
as O
follows O
, O
where O
Select O
( O
• O
) O
is O
a O
function O
that O
takes O
in O
all O
introductory O
sentence O
representations O
( O
i.e. O
, O
h O
) O
, O
and O
introductory O
sentence O
probabilitiesp. O
It O
then O
outputs O
the O
representations O
associated O
with O
top O
k O
introductory O
sentences O
, O
sorted O
byp. O
To O
extract O
top O
introductory O
sentences O
, O
we O
first O
sorth O
vectors O
based O
on O
their O
computed O
probabilitiesp O
and O
then O
we O
pick O
up O
top B-HyperparameterName
k I-HyperparameterName
hidden I-HyperparameterName
vectors I-HyperparameterName
( O
i.e. O
, O
h B-HyperparameterName
top I-HyperparameterName
) O
that O
has O
the O
highest O
probability. O
MLP O
1 O
is O
a O
multi-layer O
perceptron O
that O
takes O
in O
concatenated O
vector O
of O
top O
introductory O
sentences O
and O
projects O
it O
into O
a O
new O
vector O
calledĥ O
. O

h B-HyperparameterName
top I-HyperparameterName
= O
Select O
( O
h O
, O
p O
, O
k O
) O
h O
= O
MLP O
1 O
( O
h O
top O
) O
( O
3 O
) O

At O
the O
final O
stage O
, O
we O
concatenate O
the O
transformed O
introductory O
top O
sentence O
representations O
( O
i.e. O
, O
ĥ O
) O
with O
each O
source O
sentence O
representations O
from O
Eq. O
1 O
( O
i.e. O
, O
h O
i O
where O
i O
shows O
the O
ith O
paper O
sentence O
) O
and O
process O
them O
to O
produce O
a O
resulting O
vector O
r O
which O
is O
intro-aware O
source O
sentence O
hidden O
representations. O
After O
processing O
the O
resulting O
vector O
through O
a O
linear O
output O
layer O
( O
with O
W O
z O
and O
b O
z O
as O
trainable O
parameters O
) O
, O
we O
obtain O
final O
introaware O
sentence O
extraction O
probabilities O
( O
i.e. O
, O
p O
) O
as O
follows O
, O

r O
= O
MLP O
2 O
( O
h O
i O
; O
ĥ O
) O
p O
= O
σ O
( O
W O
z O
r O
+ O
b O
z O
) O
( O
4 O
) O

in O
which O
MLP O
2 O
is O
a O
multi-layer O
perceptron O
, O
influencing O
the O
knowledge O
from O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O
t O
2 O
) O
into O
the O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O
t O
1 O
) O
. O
We O
train O
both O
tasks O
through O
our O
end-to-end O
system O
jointly O
as O
follows O
, O

ℓ O
total O
= O
( O
α O
) O
ℓ O
t O
1 O
+ O
( O
1 O
− O
α O
) O
ℓ O
t O
2 O
( O
5 O
) O

where O
ℓ B-MetricName
t I-MetricName
1 I-MetricName
, O
and O
ℓ B-MetricName
t I-MetricName
2 I-MetricName
are O
the O
losses B-MetricName
computed O
for O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
and O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
tasks O
, O
α O
is O
the O
regularizing O
parameter O
that O
balances O
the O
learning O
process O
between O
two O
tasks O
, O
and O
ℓ B-MetricName
total I-MetricName
is O
the O
total O
computed O
loss B-MetricName
that O
is O
optimized O
during O
the O
training O
. O

Experimental O
Setup O
In O
this O
section O
, O
we O
explain O
the O
datasets O
, O
baselines O
, O
and O
preprocessing O
and O
training O
parameters O
. O

Dataset O
We O
use O
two O
publicly O
available O
scientific O
extended O
summarization O
datasets O
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
. O

-arXiv-Long O
: O

A O
set O
of O
arXiv O
scientific O
papers O
containing O
papers O
from O
various O
scientific O
domains O
such O
as O
physics O
, O
mathematics O
, O
computer O
science O
, O
quantitative O
biology. O
arXiv-Long B-DatasetName
is O
intended O
for O
extended B-TaskName
summarization I-TaskName
task O
and O
was O
filtered O
from O
a O
larger O
dataset O
i.e. O
, O
arXiv O
for O
the O
summaries O
of O
more O
than O
350 O
tokens. O
The O
ground-truth O
summaries O
( O
i.e. O
, O
abstract O
) O
are O
long O
, O
with O
the O
average O
length O
of O
574 O
tokens. O
It O
contains O
7816 O
( O
train O
) O
, O
1381 O
( O
validation O
) O
, O
and O
1952 O
( O
test O
) O
papers O
. O

-PubMed-Long B-DatasetName
: O
A O
set O
of O
biomedical O
scientific O
papers O
from O
PubMed O
with O
average O
summary O
length O
of O
403 O
tokens. O
This O
dataset O
contains O
79893 O
( O
train O
) O
, O
4406 O
( O
validation O
) O
, O
and O
4402 O
( O
test O
) O
scientific O
papers O
. O

Baselines O
We O
compare O
our O
model O
with O
two O
strong O
non-neural O
systems O
, O
and O
four O
state-of-the-art O
neural O
summarizers. O
We O
use O
all O
of O
these O
baselines O
for O
the O
purpose O
of O
extended O
summary O
generation O
whose O
documents O
hold O
different O
characteristics O
in O
length O
, O
writing O
style O
, O
and O
discourse O
structure O
as O
compared O
to O
documents O
in O
the O
other O
domains O
of O
summarization O
. O

-LSA B-MethodName
( O
Steinberger O
and O
Jezek O
, O
2004 O
) O
: O
an O
extractive O
vector-based O
model O
that O
utilizes O
Singular B-MethodName
Value I-MethodName
Decomposition I-MethodName
( O
SVD B-MethodName
) O
to O
find O
the O
semantically O
important O
sentences O
. O

-LEXRANK B-MethodName
( O
Erkan O
and O
Radev O
, O
2004 O
) O
: O
a O
widely O
adopted O
extractive O
summarization O
baseline O
that O
utilizes O
a O
graph-based O
approach O
based O
on O
eigenvector O
centrality O
to O
identify O
the O
most O
salient O
sentences O
. O

-BERTSUMEXT B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
: O
a O
contextualized O
summarizer O
fine-tuned O
for O
summarization O
task O
, O
which O
encodes O
input O
sentence O
representations O
, O
and O
then O
processes O
them O
through O
a O
multi-layer O
Transformers O
encoder O
to O
obtain O
document-level O
sentence O
representation. O
Finally O
, O
a O
linear O
output O
layer O
with O
Sigmoid O
activation O
function O
outputs O
a O
probability O
distribution O
over O
each O
input O
sentence O
, O
denoting O
the O
extent O
to O
which O
they O
are O
probable O
to O
be O
extracted O
. O

-BERTSUMEXT-INTRO B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
: O
a O
BERTSUMEXT B-MethodName
model O
that O
only O
runs O
on O
the O
introductory O
sentences O
as O
the O
input O
, O
and O
extracts O
the O
salient O
introductory O
sentences O
as O
the O
summary O
. O

-BERTSUMEXTMULTI B-MethodName
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
: O
an O
extension O
of O
the O
BERTSUMEXT B-MethodName
model O
that O
incorporates O
an O
additional O
linear O
layer O
with O
Sigmoid O
classifier O
to O
output O
a O
probability O
distribution O
over O
a O
fixed O
number O
of O
pre-defined O
sections O
that O
an O
input O
sentence O
might O
belong O
to. O
The O
additional O
network O
is O
expected O
to O
predict O
a O
single O
section O
for O
an O
input O
sentence O
and O
is O
trained O
jointly O
with O
BERTSUMEXT B-MethodName
module O
( O
i.e. O
, O
sentence O
extractor O
) O
. O

-BART B-MethodName
( O
Lewis O
et O
al. O
, O
2020 O
) O
: O
a O
state-of-the-art O
abstractive O
summarization O
model O
that O
makes O
use O
of O
pretrained O
encoder O
and O
decoder. O
BART B-MethodName
can O
be O
thought O
of O
as O
an O
extension O
of O
BERTSUM B-MethodName
in O
which O
merely O
encoder O
is O
pre-trained O
, O
but O
decoder O
is O
trained O
from O
scratch. O
While O
our O
model O
is O
an O
extractive O
one O
, O
at O
the O
same O
time O
, O
we O
find O
it O
of O
value O
to O
measure O
the O
abstractive O
model O
performance O
in O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task O
. O

Preprocessing O
, O
parameters O
, O
labeling O
, O
and O
implementation O
details O
We O
used O
the O
open O
implementation O
of O
BERT-SUMEXT B-MethodName
with O
default O
parameters O
5 B-HyperparameterValue
. O
To O
implement O
the O
non-neural O
baseline O
models O
, O
we O
utilized O
Sumy O
python O
package O
6 O
. O
Longformer B-MethodName
model O
( O
Beltagy O
et O
al. O
, O
2020 O
) O
is O
utilized O
as O
our O
contextualized O
language O
model O
for O
running O
all O
the O
models O
due O
to O
its O
efficacy O
at O
processing O
long O
documents. O
For O
our O
model O
, O
the O
cross-entropy O
loss O
function O
is O
set O
for O
two O
tasks O
( O
i.e. O
, O
t O
1 O
: O
source O
sentence O
extraction O
and O
t O
2 O
: O
introductory O
sentences O
extraction O
in O
Figure O
3 O
) O
and O
the O
model O
is O
optimized O
through O
multi-tasking O
approach O
as O
discussed O
in O
Section O
3. O
The O
model O
with O
the O
highest O
ROUGE-2 B-MetricName
on O
validation O
set O
is O
selected O
for O
inference. O
The O
validation O
is O
performed O
every O
2k B-HyperparameterValue
training B-HyperparameterName
steps. I-HyperparameterName
α B-HyperparameterName
( O
in O
Eq. O
5 O
) O
is O
set O
to O
be O
0.5 B-HyperparameterValue
( O
empirically O
determined O
) O
. O
Our O
model O
includes O
474M B-HyperparameterValue
trainable B-HyperparameterName
parameters I-HyperparameterName
, O
trained O
on O
dual O
GeForce O
GTX O
1080Ti O
GPUs O
for O
approximately O
a O
week. O
We O
use O
k B-HyperparameterName
= O
5 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
k B-HyperparameterName
= O
8 B-HyperparameterValue
for O
PubMed-Long B-DatasetName
datasets O
( O
Eq. O
3 O
) O
. O
We O
make O
our O
model O
implementation O
as O
well O
as O
sample O
summaries O
publicly O
available O
to O
expedite O
ongoing O
research O
in O
this O
direction O
7 O
. O
A O
two-stage O
labeling O
approach O
was O
employed O
to O
identify O
ground-truth O
introductory O
and O
nonintroductory O
sentences. O
In O
the O
first O
stage O
, O
we O
used O
a O
greedy O
labeling O
approach O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
to O
label O
sentences O
within O
the O
first O
section O
of O
a O
given O
paper O
( O
i.e. O
, O
labeling O
introductory O
sentences O
) O
with O
respect O
to O
their O
ROUGE B-MetricName
overlap O
8 O
with O
the O
groundtruth O
summary O
( O
i.e. O
, O
abstract O
) O
. O
In O
the O
second O
stage O
, O
the O
same O
greedy O
approach O
was O
exploited O
over O
the O
rest O
of O
sentences O
( O
i.e. O
, O
non-introductory O
) O
9 O
with O
regard O
to O
their O
ROUGE B-MetricName
overlap O
with O
the O
identified O
introductory O
sentences O
in O
the O
first O
stage. O
Our O
choice O
of O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
is O
based O
on O
the O
fact O
that O
these O
express O
higher O
similarity O
with O
human O
judgments O
( O
Cohan O
and O
Goharian O
, O
2016 O
) O
. O
We O
continued O
the O
second O
stage O
until O
a O
fixed O
length O
of O
the O
summary O
was O
reached. O
Specifically O
, O
the O
fixed B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
labels I-HyperparameterName
is O
set O
to O
be O
15 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
and O
20 B-HyperparameterValue
for O
PubMed-Long B-DatasetName
datasets O
as O
these O
achieved O
the O
highest O
oracle O
ROUGE B-MetricName
scores O
in O
our O
experiments O
. O

Results O

Experimental O
evaluation O
The O
recent O
effort O
in O
extended O
summarization O
and O
its O
shared O
task O
of O
LongSumm O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
used O
average O
ROUGE B-MetricName
( O
F1 B-MetricName
) O
to O
rank O
the O
participating O
systems O
, O
in O
addition O
to O
commonly-used O
ROUGE-N B-MetricName
scores. O
Table O
2 O
shows O
the O
performance O
of O
the O
participated O
systems O
on O
the O
blind O
test O
set. O
As O
shown O
, O
BERTSUMEXTMULTI B-MethodName
model O
outperforms O
other O
models O
by O
a O
large O
margin O
( O
i.e. O
, O
with O
relative O
improvements O
of O
6 B-MetricValue
% I-MetricValue
and O
3 B-MetricValue
% I-MetricValue
on O
ROUGE-1 B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( O
F1 B-MetricName
) O
, O
respectively O
) O
; O
hence O
, O
we O
use O
the O
best-performing O
in O
terms O
of O
F1 B-MetricName
( O
i.e. O
, O
BERTSUMEXTMULTI B-MethodName
model O
) O
in O
our O
experiments. O
Tables. O
1 O
presents O
our O
results O
on O
the O
test O
sets O
of O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
datasets O
, O
respectively. O
As O
observed O
, O
our O
model O
statistically O
significantly O
outperforms O
the O
state-of-the-art O
systems O
on O
both O
datasets O
across O
most O
of O
the O
ROUGE B-MetricName
vari-ants O
, O
except O
ROUGE-L B-MetricName
on O
PubMed-Long. B-DatasetName
The O
improvements O
gained O
by O
our O
model O
validates O
our O
hypothesis O
that O
incorporating O
the O
salient O
introductory O
sentence O
representations O
into O
the O
extractive O
summarizer O
yields O
a O
promising O
improvement. O
Two O
nonneural O
models O
( O
i.e. O
, O
LSA B-MethodName
and O
LEXRANK B-MethodName
) O
underperform O
the O
neural O
models O
, O
as O
expected. O
Comparing O
the O
abstractive O
model O
( O
i.e. O
, O
BART B-MethodName
) O
with O
extractive O
neural O
ones O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
, O
we O
see O
that O
while O
there O
is O
relatively O
a O
smaller O
gap O
in O
terms O
of O
ROUGE-1 B-MetricName
, O
the O
gap O
is O
larger O
for O
ROUGE-2 B-MetricName
, O
and O
ROUGE-L. B-MetricName
Interestingly O
, O
in O
the O
case O
of O
BART B-MethodName
, O
we O
found O
that O
generating O
extended O
summaries O
is O
rather O
challenging O
for O
abstractive O
summarizers. O
Current O
abstractive O
summarizers O
including O
BART B-MethodName
have O
difficulty O
in O
abstracting O
very O
detailed O
information O
, O
such O
as O
numbers O
, O
and O
quantities O
, O
which O
hurts O
the O
faithfulness O
of O
the O
generated O
summaries O
to O
the O
source. O
This O
behavior O
has O
a O
detrimental O
effect O
, O
specifically O
, O
on O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
as O
their O
high O
correlation O
with O
human O
judgments O
in O
terms O
of O
faithfulness O
has O
been O
shown O
( O
Pagnoni O
et O
al. O
, O
2021 O
) O
. O
Comparing O
the O
extractive O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
models O
, O
while O
BERTSUMMULTIEXT B-MethodName
is O
expected O
to O
outperfom O
BERTSUMEXT B-MethodName
, O
it O
is O
observed O
that O
they O
perform O
almost O
similarly O
, O
with O
small O
( O
i.e. O
, O
insignificant O
) O
improved O
metrics. O
This O
might O
be O
due O
to O
the O
fact O
that O
BERTSUMEXTMULTI B-MethodName
works O
out-of-the-box O
when O
a O
handful O
amount O
of O
sentences O
are O
sampled O
from O
diverse O
sections O
to O
form O
the O
oracle O
summary O
as O
also O
reported O
by O
its O
authors. O
However O
, O
when O
labeling O
oracle O
sentences O
in O
our O
framework O
( O
i.e. O
, O
Intro-guided O
labeling O
) O
, O
there O
is O
no O
guarantee O
that O
the O
final O
set O
of O
oracle O
sentences O
are O
labeled O
from O
diverse O
sections. O
Overall O
, O
our O
model O
achieves O
about O
1.4 B-MetricValue
% I-MetricValue
, O
2.4 B-MetricValue
% I-MetricValue
, O
3.5 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
and O
1.0 B-MetricValue
% I-MetricValue
, O
2.5 B-MetricValue
% I-MetricValue
, O
1.3 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
across O
ROUGE B-MetricName
score O
variants O
; O
and O
2.2 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
1.4 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
over O
F1 B-MetricName
, O
compared O
to O
the O
neural O
baselines O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
. O
While O
comparing O
our O
model O
with O
BERTSUMEXT-INTRO B-MethodName
, O
we O
see O
the O
vital O
effect O
of O
adding O
second O
encoder O
at O
finding O
supplementary O
sentences O
across O
non-introductory O
sections O
, O
where O
our O
model O
gains O
relative O
improvements O
of O
9.62 B-MetricValue
% I-MetricValue
-26.26 B-MetricValue
% I-MetricValue
-16.09 B-MetricValue
% I-MetricValue
and O
9.40 B-MetricValue
% I-MetricValue
-5.27 B-MetricValue
% I-MetricValue
-9.99 B-MetricValue
% I-MetricValue
for O
ROUGE-1 B-MetricName
, O
ROUGE-2 B-MetricName
, O
ROUGE-L B-MetricName
on O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
, O
respectively. O
In O
fact O
, O
the O
sentences O
that O
are O
picked O
as O
summary O
from O
the O
in- O
troduction O
section O
are O
not O
comprehensive O
as O
such O
they O
are O
clues O
to O
the O
main O
points O
of O
the O
paper. O
The O
other O
important O
sentences O
are O
picked O
from O
the O
supplementary O
parts O
( O
i.e. O
, O
non-introductory O
) O
of O
the O
paper O
. O

Human O
evaluation O
While O
our O
model O
statistically O
significantly O
improves O
upon O
the O
state-of-the-art O
baselines O
in O
terms O
of O
ROUGE B-MetricName
scores O
, O
a O
few O
works O
have O
reported O
the O
low O
correlation O
of O
ROUGE B-MetricName
with O
human O
judgments O
( O
Liu O
and O
Liu O
, O
2008 O
; O
Cohan O
and O
Goharian O
, O
2016 O
; O
Fabbri O
et O
al. O
, O
2021 O
) O
. O
In O
order O
to O
provide O
insights O
into O
why O
and O
how O
our O
model O
outperforms O
the O
bestperforming O
baselines O
, O
we O
perform O
a O
manual O
analysis O
of O
our O
system O
's O
generated O
summaries O
, O
BERT-SUMEXT B-MethodName
's I-MethodName
, O
and O
BERTSUMEXTMULTI's. B-MethodName
For O
the O
sake O
of O
evaluation O
, O
two O
annotators O
were O
asked O
to O
manually O
evaluate O
two O
sets O
of O
40 O
papers O
' O
groundtruth O
abstracts O
( O
40 O
for O
arXiv-Long B-DatasetName
, O
and O
40 O
for O
PubMed-Long B-DatasetName
) O
with O
their O
generated B-TaskName
extended I-TaskName
summaries I-TaskName
( O
baselines O
' O
and O
ours O
) O
to O
gain O
insights O
into O
qualities O
of O
each O
model. O
Annotators O
were O
Electrical O
Engineering O
and O
Computer O
Science O
PhD O
students O
and O
familiar O
with O
principles O
of O
reading O
scientific O
papers. O
Samples O
were O
randomly O
selected O
from O
the O
test O
set O
, O
one O
from O
each O
40 O
evenly-spaced O
bins O
sorted O
by O
the O
difference O
of O
ROUGE-L B-MetricName
between O
two O
experimented O
systems O
. O

The O
evaluations O
were O
performed O
according O
to O
two O
metrics O
: O
( O
1 O
) O
Cohesion B-MetricName
: O
whether O
the O
ordering O
of O
sentences O
in O
summary O
is O
cohesive O
, O
namely O
sentences O
entail O
each O
other. O
( O
2 O
) O
Completeness B-MetricName
: O
whether O
the O
summary O
covers O
all O
salient O
information O
provided O
in O
the O
ground-truth O
summary. O
To O
prevent O
bias O
in O
selecting O
summaries O
, O
the O
ordering O
of O
systemgenerated O
summaries O
were O
shuffled O
such O
that O
it O
could O
not O
be O
guessed O
by O
the O
annotators. O
Annotators O
were O
asked O
to O
specify O
if O
the O
first O
system-generated O
summary O
wins O
/ O
loses O
or O
ties O
with O
the O
second O
systemgenerated O
summary O
in O
terms O
of O
qualitative O
metrics. O
It O
has O
to O
be O
mentioned O
that O
since O
our O
model O
is O
purely O
extractive O
, O
it O
does O
not O
introduce O
any O
fact O
that O
is O
unfaithful O
to O
the O
source O
. O

Our O
human O
evaluation O
results O
along O
with O
Cohen O
's O
kappa O
( O
Cohen O
, O
1960 O
) O
inter-rater O
agreements O
are O
shown O
in O
Table O
3 O
( O
agr. O
column O
) O
. O
As O
shown O
, O
our O
system O
's O
generated O
summaries O
improve O
completeness B-MetricName
and O
cohesion B-MetricName
in O
over O
40 B-MetricValue
% I-MetricValue
for O
most O
of O
the O
cases O
( O
6 O
out O
of O
8 O
for O
win O
cases O
10 O
) O
. O
Specifically O
, O
when O
comparing O
with O
BERTSUMEXT B-MethodName
, O
we O
see O
that O
68 B-MetricValue
% I-MetricValue
, O
80 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
60 B-MetricValue
% I-MetricValue
, O
66 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
sampled O
summaries O
are O
at O
least O
as O
good O
as O
or O
better O
than O
the O
corresponding O
baseline O
's O
generated O
summaries O
in O
terms O
of O
cohesion O
and O
completeness O
, O
respectively. O
Overall O
, O
across O
two O
metrics O
for O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
, O
we O
gain O
relative O
improvements O
over O
the O
baselines O
: O
25.6 B-MetricValue
% I-MetricValue
, O
19.0 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
56.5 B-MetricValue
% I-MetricValue
, O
[ O
Introductory O
] O
The O
objective O
of O
the O
work O
presented O
here O
is O
to O
study O
the O
mechanism O
of O
radiative O
line O
driving O
and O
the O
corresponding O
properties O
of O
the O
winds O
of O
possible O
generations O
of O
very O
massive O
stars O
at O
extremely O
low O
metallicities O
and O
to O
investigate O
the O
principal O
influence O
of O
these O
winds O
on O
ionizing O
fluxes O
and O
observable O
ultraviolet O
spectra O
. O

[ O
" O
# O
] O
The O
basic O
new O
element O
of O
this O
approach O
, O
needed O
in O
the O
domain O
of O
The O
purpose O
of O
this O
first O
study O
is O
to O
provide O
an O
estimate O
about O
the O
strengths O
of O
stellar O
winds O
at O
very O
low O
metallicity O
for O
very O
massive O
hot O
stars O
in O
a O
mass O
range O
roughly O
between O
100 O
to O
300 O
m O
@ O
xmath3 O
. O

[ O
" O
) O
] O
With O
our O
new O
approach O
to O
describe O
line O
driven O
stellar O
winds O
at O
extremely O
low O
metallicity O
we O
were O
able O
to O
make O
first O
predictions O
of O
stellar O
wind O
properties O
, O
ionizing O
fluxes O
and O
synthetic O
spectra O
of O
a O
possible O
population O
of O
very O
massive O
stars O
in O
this O
range O
of O
metallicity. O
46.7 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName
) O
on O
arXiv-Long B-DatasetName
; O
and O
23.1 B-MetricValue
% I-MetricValue
, O
13.5 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
27.7 B-MetricValue
% I-MetricValue
, O
21.9 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName
) O
on O
PubMed-Long. B-DatasetName
11 O
These O
improvements O
, O
qualitatively O
evaluated O
by O
the O
human O
annotators O
, O
show O
the O
promising O
capability O
of O
our O
purposed O
model O
in O
generating O
improved O
extended O
summaries O
which O
are O
more O
preferable O
than O
the O
baselines'. O
We O
observe O
a O
similar O
improvement O
trend O
when O
comparing O
our O
summaries O
with O
BERTSUMEXTMULTI B-MethodName
, O
where O
66 B-MetricValue
% I-MetricValue
, O
77 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
58 B-MetricValue
% I-MetricValue
, O
58 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
our O
summaries O
are O
as O
good O
as O
or O
better O
than O
the O
baseline O
's O
in O
terms O
of O
cohesion O
and O
completeness. O
Looking O
at O
the O
Cohen O
's O
inter-rater O
agreement O
, O
the O
correlation B-MetricName
scores O
fall O
into O
" B-MetricValue
moder- I-MetricValue
11 I-MetricValue
Relative O
improvement O
of O
win O
rate O
over O
lose O
rate O
. O

ate O
" O
agreement O
range O
according O
to O
the O
interpretation O
of O
Cohen O
's O
kappa O
range O
( O
McHugh O
, O
2012 O
) O
. O


-DOCSTART- O
Enhancing B-MethodName
Self-Attention I-MethodName
with I-MethodName
Knowledge-Assisted I-MethodName
Attention I-MethodName
Maps I-MethodName

Large-scale B-MethodName
pre-trained I-MethodName
language I-MethodName
models I-MethodName
have O
attracted O
extensive O
attentions O
in O
the O
research O
community O
and O
shown O
promising O
results O
on O
various O
tasks O
of O
natural B-TaskName
language I-TaskName
processing. I-TaskName
However O
, O
the O
attention O
maps O
, O
which O
record O
the O
attention B-MetricName
scores I-MetricName
between O
tokens O
in O
self-attention B-MethodName
mechanism I-MethodName
, O
are O
sometimes O
ineffective O
as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantic O
knowledge. O
Thus O
, O
we O
aim O
to O
infuse O
explicit O
external O
knowledge O
into O
pretrained B-MethodName
language I-MethodName
models I-MethodName
to O
further O
boost O
their O
performance. O
Existing O
works O
of O
knowledge O
infusion O
largely O
depend O
on O
multi-task B-MethodName
learning I-MethodName
frameworks O
, O
which O
are O
inefficient O
and O
require O
large-scale O
re-training O
when O
new O
knowledge O
is O
considered. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
solution O
, O
KAM-BERT B-MethodName
, O
which O
directly O
incorporates O
knowledge-generated B-MethodName
attention I-MethodName
maps I-MethodName
into O
the O
self-attention B-MethodName
mechanism. I-MethodName
It O
requires O
only O
a O
few O
extra O
parameters O
and O
supports O
efficient O
fine-tuning B-TaskName
once O
new O
knowledge O
is O
added. O
KAM-BERT B-MethodName
achieves O
consistent O
improvements O
on O
various O
academic O
datasets O
for O
natural B-TaskName
language I-TaskName
understanding. I-TaskName
It O
also O
outperforms O
other O
state-of-the-art O
methods O
which O
conduct O
knowledge B-TaskName
infusion I-TaskName
into O
transformerbased O
architectures. O
Moreover O
, O
we O
apply O
our O
model O
to O
an O
industry-scale O
ad O
relevance O
application O
and O
show O
its O
advantages O
in O
the O
real-world O
scenario O
. O

Introduction O
Language O
models O
pre-trained B-TaskName
by O
a O
large O
text O
corpus O
have O
shown O
superior O
performances O
on O
a O
wide O
range O
of O
natural B-TaskName
language I-TaskName
processing I-TaskName
tasks. O
Many O
advanced O
models O
based O
on O
the O
transformer O
architectures O
achieve O
state-of-the-art O
results O
on O
various O
NLP B-TaskName
benchmarks. O
Existing O
literature O
( O
Jawahar O
et O
al. O
, O
2019 O
; O
Hewitt O
and O
Manning O
, O
2019 O
) O
shows O
that O
pre-training B-TaskName
enables O
a O
model O
to O
capture O
syntactic O
and O
semantic O
information O
in O
the O
self-attention B-MethodName
mechanism. I-MethodName
However O
, O
the O
attention O
maps O
, O
which O
* O
The O
work O
was O
done O
when O
the O
author O
visited O
Microsoft. O
record O
the O
attention O
scores O
between O
tokens O
in O
a O
selfattention O
mechanism O
, O
are O
sometimes O
ineffective O
as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantics O
( O
Jain O
and O
Wallace O
, O
2019 O
) O
. O
If O
the O
knowledge O
can O
be O
leveraged O
in O
a O
reasonable O
way O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
we O
have O
a O
good O
chance O
to O
improve O
the O
quality O
of O
attention B-MetricName
scores I-MetricName
as O
well O
as O
the O
performance O
of O
downstream O
applications O
. O

Recently O
, O
there O
have O
been O
multiple O
attempts O
for O
incorporating O
knowledge O
into O
transformer O
architectures. O
ERNIE B-MethodName
( O
Zhang O
et O
al. O
, O
2019 O
) O
and O
KE-PLER B-MethodName
utilize O
both O
large-scale O
textual O
corpora O
and O
knowledge O
graphs O
to O
train O
a O
representation O
model O
in O
a O
multi-task B-MethodName
learning I-MethodName
framework. I-MethodName
They O
need O
to O
be O
retrained O
from O
scratch O
when O
injecting O
new O
knowledge O
, O
which O
is O
inefficient O
and O
can O
not O
benefit O
from O
existing O
pre-trained O
checkpoints. O
K-Adapter B-MethodName
( O
Wang O
et O
al. O
, O
2020 O
) O
integrates O
additional O
neural O
models O
to O
capture O
different O
kinds O
of O
knowledge. O
It O
enables O
adaptation O
based O
on O
pretrained B-TaskName
language B-MethodName
models. I-MethodName
However O
, O
it O
does O
not O
instruct O
the O
self-attention B-MethodName
mechanism I-MethodName
directly O
and O
introduces O
a O
relatively O
large O
number O
of O
parameters O
to O
the O
original O
model O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
self-attention B-MethodName
mechanism I-MethodName
enhanced O
by O
explicit O
knowledge O
to O
address O
problems O
mentioned O
above. O
First O
, O
we O
show O
a O
failure O
case O
of O
query-ad B-TaskName
matching I-TaskName
, O
which O
motivates O
us O
to O
inject O
explicit O
knowledge O
into O
self-attention B-MethodName
mechanism. I-MethodName
In O
Figure O
1 O
, O
the O
attention O
map O
of O
a O
query-ad O
pair O
is O
visualized O
, O
and O
the O
goal O
is O
to O
judge O
if O
the O
query O
and O
ad O
text O
are O
semantically O
relevant. O
As O
shown O
in O
the O
figure O
, O
BERT B-MethodName
misclassifies O
this O
pair O
as O
irrelevant O
, O
probably O
because O
it O
does O
not O
understand O
the O
query O
word O
" O
glipizide O
" O
, O
which O
rarely O
appears O
in O
the O
pre-training O
corpus. O
In O
fact O
, O
" O
glipizide O
" O
is O
a O
kind O
of O
medicine O
and O
highly O
related O
to O
the O
word O
" O
Pharmacy O
" O
in O
ad O
text O
, O
so O
this O
case O
should O
be O
classified O
as O
relevant. O
In O
this O
case O
, O
if O
we O
know O
" O
glipizide O
" O
is O
semantically O
correlated O
to O
" O
Pharmacy O
" O
as O
prior O
knowledge O
, O
we O
can O
enrich O
the O
attention O
maps O
accordingly. O
In O
addition O
, O
terms O
" O
Fred O
" O
and O
" O
Meyer O
" O
are O
from O
the O
same O
entity O
, O
so O
the O
attention B-MetricName
scores I-MetricName
between O
these O
two O
terms O
should O
be O
relatively O
high. O
Based O
on O
this O
fact O
, O
we O
believe O
that O
simply O
using O
language O
models O
pre-trained B-TaskName
by O
a O
general O
corpus O
is O
not O
enough O
to O
meet O
the O
satisfaction O
of O
a O
specific O
application. O
Thus O
, O
our O
motivation O
is O
to O
inject O
explicit O
knowledge O
into O
the O
transformer O
architecture O
, O
which O
guides O
the O
pre-trained B-TaskName
language O
model O
to O
perform O
better O
adaptation O
in O
an O
efficient O
fine-tuning B-TaskName
procedure O
. O

To O
address O
the O
above O
motivation O
, O
we O
propose O
a O
novel O
architecture O
, O
namely O
KAM-BERT B-MethodName
( O
Knowledge-assisted B-MethodName
Attention I-MethodName
Maps I-MethodName
for I-MethodName
BERT I-MethodName
) O
. O
First O
, O
it O
constructs O
semantic O
attention O
maps O
based O
on O
corresponding O
relevance O
functions O
defined O
by O
various O
kinds O
of O
semantic O
knowledge. O
Specifically O
, O
we O
consider O
three O
kinds O
of O
semantic O
knowledge O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
i.e. O
, O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O
and O
term B-TaskName
correlation. I-TaskName
Entity O
and O
phrase B-TaskName
segmentation I-TaskName
indicate O
the O
cohesion O
of O
continuous O
terms O
, O
while O
term B-TaskName
correlation I-TaskName
can O
help O
to O
enrich O
the O
semantic O
representation O
of O
a O
sentence. O
Then O
, O
the O
knowledge B-TaskName
infusion I-TaskName
procedure O
is O
completed O
by O
concatenating O
these O
attention O
maps O
with O
vanilla O
self-attention O
and O
then O
performing O
2Dconvolution O
for O
integration. O
Finally O
, O
the O
result O
attention O
maps O
are O
served O
as O
inputs O
for O
value O
projection O
, O
and O
the O
rest O
part O
is O
the O
same O
as O
a O
standard O
transformer. O
The O
KAM-BERT B-MethodName
model O
can O
be O
fine-tuned O
on O
existing O
pre-trained B-TaskName
checkpoints O
in O
a O
plug O
and O
play O
mode O
, O
which O
is O
highly O
efficient O
in O
practice O
. O

As O
illustrated O
in O
Section O
4 O
, O
we O
compare O
KAM-BERT B-MethodName
with O
BERT B-MethodName
and O
other O
knowledge-enhanced O
SOTAs O
on O
various O
natural B-TaskName
language I-TaskName
understanding I-TaskName
tasks O
, O
where O
KAM-BERT B-MethodName
shows O
consistent O
superiority. O
Especially O
, O
we O
lift O
the O
average O
score O
of O
BERT-Base B-MetricName
from O
77.5 B-MetricValue
to O
78.7 B-MetricValue
on O
the O
GLUE B-MetricName
benchmark. O
We O
also O
demonstrate O
the O
advantage O
of O
KAM-BERT B-MethodName
for O
knowledge B-TaskName
injection I-TaskName
on O
LAMA B-DatasetName
, O
a O
probing O
benchmark O
to O
analyze O
the O
factual O
and O
commonsense O
knowledge O
contained O
in O
a O
model. O
Furthermore O
, O
KAM-BERT B-MethodName
is O
successfully O
applied O
to O
the O
query-ad B-TaskName
relevance I-TaskName
scenario O
in O
a O
commercial O
search O
engine O
and O
shows O
significant O
lift O
in O
AUC B-MetricName
score O
. O

The O
major O
contributions O
of O
this O
paper O
are O
summarized O
as O
follows O
: O

• O
First O
, O
we O
propose O
a O
novel O
self-attention O
mechanism O
enhanced O
by O
semantic O
attention O
maps O
, O
which O
incorporates O
knowledge O
from O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O
and O
term B-TaskName
correlation. I-TaskName
Ablation O
study O
will O
demonstrate O
the O
effectiveness O
of O
these O
kinds O
of O
semantic O
attention O
maps O
. O

• O
Second O
, O
KAM-BERT B-MethodName
requires O
little O
extra O
memory O
and O
computation O
cost O
compared O
to O
vanilla O
BERT B-MethodName
and O
other O
SOTAs. O
It O
can O
be O
finetuned B-TaskName
efficiently O
on O
existing O
language O
models O
for O
a O
given O
application. O
We O
have O
successfully O
applied O
it O
to O
improve O
the O
performance O
of O
query-ad B-TaskName
relevance I-TaskName
in O
a O
commercial O
search O
engine. O
• O
Last O
but O
not O
least O
, O
the O
proposed O
framework O
is O
generic O
and O
flexible O
for O
infusing O
various O
kinds O
of O
knowledge O
into O
the O
transformer O
architecture. O
Except O
for O
the O
three O
kinds O
of O
knowledge O
considered O
in O
this O
paper O
, O
we O
will O
also O
showcase O
how O
to O
incorporate O
other O
kinds O
of O
information O
, O
such O
as O
a O
knowledge O
graph. O
It O
opens O
up O
new O
opportunities O
for O
further O
exploration O
. O

KAM-BERT B-MethodName
As O
illustrated O
in O
Figure O
2 O
, O
KAM-BERT B-MethodName
injects O
multiple O
kinds O
of O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models I-MethodName
in O
the O
form O
of O
multi-channel O
semantic O
attention O
maps. O
Different O
kinds O
of O
knowledge O
can O
be O
extracted O
independently O
and O
infused O
together O
into O
one O
self-attention O
map O
in O
the O
transformer O
architecture. O
KAM-BERT B-MethodName
can O
be O
fine-tuned B-TaskName
directly O
from O
an O
existing O
checkpoint O
of O
BERT B-MethodName
, O
while O
additional O
parameters O
are O
initialized O
randomly O
and O
learned O
in O
the O
fine-tuning B-TaskName
stage. O
Thus O
, O
it O
is O
quite O
efficient O
and O
flexible O
to O
incorporate O
new O
kinds O
of O
knowledge O
into O
the O
model O
. O

Below O
we O
first O
describe O
the O
standard O
selfattention B-MethodName
mechanism. I-MethodName
Then O
, O
we O
will O
introduce O
the O
generic O
definition O
of O
semantic O
attention O
maps O
, O
as O
well O
as O
the O
methodology O
of O
multi-channel B-MethodName
knowledge I-MethodName
infusion I-MethodName
which O
integrates O
semantic O
attention O
maps O
into O
transformer O
models. O
At O
last O
, O
the O
generation O
of O
different O
kinds O
of O
semantic O
attention O
maps O
will O
be O
presented. O
Note O
that O
the O
time O
complexity O
of O
KAM-BERT B-MethodName
is O
on-par O
with O
a O
vanilla O
BERT. B-MethodName
A O
detailed O
analysis O
can O
be O
found O
in O
the O
supplementary O
material O
. O

Self-Attention B-MethodName
The O
representation O
of O
a O
text O
sequence O
can O
be O
written O
as O
X O
∈ O
R O
N O
×C O
, O
where O
N O
denotes O
the O
sequence O
length O
and O
C O
is O
the O
word O
embedding O
dimension O
size. O
A O
standard O
Transformer O
block O
is O
composed O
of O
a O
self-attention O
layer O
and O
a O
position-wise O
feedforward O
layer O
, O
while O
each O
attention O
map O
is O
generated O
by O
a O
self-attention O
layer O
without O
any O
other O
prior O
knowledge O
introduced O
. O

The O
self-attention B-MethodName
mechanism I-MethodName
plays O
an O
important O
role O
in O
the O
transformer-based O
model. O
In O
a O
vanilla O
Transformer O
, O
the O
self-attention O
map O
A O
i O
sa O
of O
layer O
i O
is O
calculated O
by O
the O
dimension-normalized O
dotproduct O
operation O
. O

A O
i O
sa O
=Self-Attention B-MethodName
( O
X O
) O
= O
QK O
⊤ O
√ O
d O
( O
1 O
) O

where O
d O
is O
the O
dimension O
of O
representation O
vectors O
. O

In O
a O
vanilla O
transformer O
, O
A O
i O
sa O
is O
then O
normalized O
by O
softmax O
and O
fed O
into O
position-wise O
feed-forward O
layers. O
In O
KAM-BERT B-MethodName
, O
the O
self-attention O
map O
A O
i O
sa O
is O
infused O
with O
semantic O
attention O
maps O
to O
calculate O
the O
final O
attention O
matrix O
, O
which O
will O
be O
described O
in O
the O
following O
sub-sections O
. O

Semantic O
Attention O
Maps O
Given O
a O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
, O
the O
semantic O
attention O
map O
can O
be O
defined O
in O
a O
generic O
form O
M O
∈ O
R O
n×n O
, O
where O
M O
i O
, O
j O
∈ O
[ O
0 O
, O
1 O
] O
denotes O
the O
attention B-MetricName
score I-MetricName
from O
token O
i O
to O
token O
j O
, O
and O
n O
is O
the O
number O
of O
tokens O
in O
the O
current O
sentence. O
Then O
, O
for O
a O
specific O
kind O
of O
knowledge O
, O
we O
need O
a O
corresponding O
relevance O
function O
to O
calculate O
the O
attention B-MetricName
score I-MetricName
, O
i.e. O
, O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
x O
i O
, O
x O
j O
) O
. O
Note O
that O
if O
x O
i O
denotes O
a O
sub-word O
as O
in O
the O
BERT B-MethodName
model O
, O
we O
define O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
W O
( O
x O
i O
) O
, O
W O
( O
x O
j O
) O
) O
, O
where O
W O
( O
x O
i O
) O
denotes O
the O
entire O
word O
which O
contains O
the O
sub-word O
x O
i O
. O
For O
example O
, O
BERT B-MethodName
will O
convert O
a O
sequence O
" O
I O
like O
tacos O
! O
" O
into O
a O
sequence O
of O
sub-words O
, O
i.e. O
, O
{ O
I O
, O
like O
, O
ta O
, O
# O
# O
cos O
, O
! O
} O
, O
where O
" O
ta O
" O
and O
" O
# O
# O
cos O
" O
are O
sub-words O
from O
" O
tacos O
" O
, O
so O
both O
W O
( O
ta O
) O
and O
W O
( O
# O
# O
cos O
) O
denote O
the O
word O
" O
tacos O
" O
. O
In O
Section O
3.4 O
, O
we O
will O
introduce O
three O
kinds O
of O
semantic O
attention O
maps O
considered O
in O
this O
paper O
and O
the O
generation O
method O
for O
other O
knowledgeassisted O
attention O
maps O
. O

Multi-Channel B-MethodName
Knowledge I-MethodName
Infusion I-MethodName
In O
order O
to O
incorporate O
external O
knowledge O
into O
self-attention B-MethodName
, O
we O
concatenate O
semantic O
attention O
maps O
with O
vanilla O
self-attention B-MethodName
, O
and O
then O
infuse O
them O
into O
a O
single O
multi-head O
attention O
map O
using O
multi-channel O
2D-convolutions. O
Applying O
2Dconvolution O
to O
a O
self-attention O
map O
is O
first O
pro- O
posed O
by O
( O
Wang O
et O
al. O
, O
2021 O
) O
and O
shows O
advantages O
in O
both O
NLP B-TaskName
and O
CV B-TaskName
tasks. O
Here O
we O
use O
2D-convolution O
to O
infuse O
different O
kinds O
of O
knowledge O
. O

Q O
K O
V O
multi-channel O
convolution O
block O
0 O
X O
X O
X O
X O
X O
input O
tokens O
feed-forward O
concatenate O
Knowledge-assisted O
attention O
maps O
Q O
K O
V O
multi-channel O
convolution O
block O
1 O
X O
X O
X O
X O
X O
feed-forward O
linear O
fusion O

First O
, O
we O
perform O
Channel B-MethodName
Wise I-MethodName
Concatenation I-MethodName
( O
CWC B-MethodName
) O
: O
the O
vanilla O
self-attention O
map O
A O
i O
sa O
will O
be O
concatenated O
with O
each O
semantic O
attention O
map O
M O
i O
separately O
along O
the O
channel O
dimension. O
Then O
, O
a O
multi-channel O
2D-convolution O
is O
applied O
to O
generate O
an O
knowledge-infused O
attention O
map O
, O
denoted O
by O
A O
i O
sem O
. O
The O
entire O
process O
can O
be O
formulated O
as O
below O
. O

A O
i O
sem O
= O
Conv O
( O
CW O
C O
( O
A O
i O
sa O
|M O
1..k O
) O
) O

( O
2 O
) O
where O
M O
1..k O
is O
a O
set O
of O
semantic O
attention O
maps O
obtained O
by O
k O
different O
kinds O
of O
knowledge O
, O
including O
but O
not O
limited O
to O
the O
three O
ones O
considered O
in O
this O
paper O
; O
To O
infuse O
different O
types O
of O
knowledge O
, O
we O
apply O
a O
standard O
2D O
convolution O
operation O
, O
the O
output O
dimension O
of O
which O
is O
the O
same O
as O
that O
of O
A O
sa O
. O
If O
A O
sa O
has O
m O
attention O
heads O
, O
then O
A O
sem O
will O
also O
has O
m O
attention O
heads. O
We O
adopt O
3 O
× O
3 O
kernel O
for O
the O
convolution O
empirically O
as O
it O
performs O
better O
than O
1 O
× O
1 O
and O
5 O
× O
5 O
kernels O
according O
to O
( O
Wang O
et O
al. O
, O
2021 O
) O
. O

At O
last O
, O
we O
adopt O
a O
hyper-parameter O
α B-HyperparameterName
to O
balance O
the O
importance O
of O
A O
i O
sa O
and O
A O
i O
sem O
. O

A O
i O
= O
Sof O
tmax O
( O
α B-HyperparameterName
• O
A O
i O
sem O
+ O
( O
1 O
− O
α B-HyperparameterName
) O
• O
A O
i O
sa O
) O

( O
3 O
) O
After O
softmax O
activation O
, O
we O
get O
the O
final O
selfattention O
map O
A O
i O
with O
m O
heads O
for O
the O
i-th O
layer O
. O

Given O
the O
self-attention B-MethodName
map O
, O
the O
rest O
components O
are O
identical O
to O
a O
vanilla O
Transformer O
, O
which O
can O
be O
calculated O
as O

h O
i O
j O
= O
A O
i O
j O
V O
i O
, O
H O
i O
= O
( O
m O
j=1 O
h O
j O
) O
W O
O O
, O
j O
∈ O
m. O
( O
4 O
) O

In O
detail O
, O
we O
use O
the O
obtained O
attention O
map O
A O
i O
to O
multiply O
the O
value O
matrix O
V O
in O
the O
attention B-MethodName
mechanism I-MethodName
to O
get O
the O
representation O
h O
j O
of O
the O
j-th O
attention O
head. O
Next O
, O
the O
outputs O
of O
all O
attention O
heads O
from O
each O
layer O
will O
be O
concatenated O
along O
the O
embedding O
dimension. O
Finally O
, O
we O
multiply O
this O
result O
with O
a O
linear O
transformation O
matrix O
W O
O O
to O
get O
the O
output O
representation O
of O
the O
i-th O
KAM-BERT B-MethodName
layer. O
Besides O
, O
we O
add O
a O
skip O
connection O
from O
the O
result O
attention O
map O
in O
the O
i-th O
layer O
to O
the O
self-attention O
map O
of O
the O
i O
+ O
1 O
layer O
to O
enhance O
the O
flow O
of O
information O
between O
layers O
. O

Generation O
of O
Semantic O
Attention O
Maps O
The O
knowledge O
we O
inject O
into O
KAM-BERT B-MethodName
includes O
entity B-TaskName
information I-TaskName
, O
phrase B-TaskName
segmentation I-TaskName
information I-TaskName
, O
and O
term B-TaskName
correlation I-TaskName
information. I-TaskName
We O
consider O
these O
three O
types O
of O
knowledge O
because O
they O
reflect O
language O
semantics O
from O
different O
perspectives. O
Entity O
and O
phrase O
represent O
coherence O
between O
adjacent O
words O
while O
term O
correlations O
build O
a O
semantic O
bridge O
for O
related O
words O
which O
may O
be O
far O
away O
or O
even O
unseen O
in O
the O
current O
sentence. O
The O
first O
two O
kinds O
of O
knowledge O
are O
presented O
as O
labeled O
sequences O
, O
and O
the O
last O
one O
is O
presented O
as O
relationship O
between O
tokens. O
As O
defined O
in O
Section O
3.2 O
, O
a O
specific O
kind O
of O
knowledge O
can O
be O
transferred O
to O
semantic O
attention O
maps O
once O
the O
corresponding O
Relevance O
function O
is O
defined O
. O

In O
the O
following O
paragraphs O
, O
we O
will O
demonstrate O
how O
to O
define O
Relevance O
functions O
for O
the O
three O
types O
of O
knowledge O
used O
in O
this O
paper. O
Also O
, O
we O
need O
to O
emphasize O
that O
the O
proposed O
framework O
is O
generic O
and O
is O
feasible O
to O
incorporate O
other O
semantic O
information O
like O
a O
knowledge O
graph. O
Thus O
, O
we O
will O
discuss O
how O
to O
generate O
other O
knowledgeassisted O
attention O
maps O
as O
our O
future O
work. O
Entity B-TaskName
Attention I-TaskName
Map I-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
( O
Nadeau O
and O
Sekine O
, O
2007 O
) O
is O
a O
standard O
task O
for O
natural B-TaskName
language I-TaskName
processing I-TaskName
which O
has O
been O
studied O
for O
years. O
Mathematically O
, O
a O
entity O
extractor O
transforms O
the O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
into O
a O
sequence O
of O
labels O
{ O
label O
0 O
, O
label O
1 O
, O
... O
, O
label O
n−1 O
} O
, O
where O
label O
i O
falls O
into O
one O
of O
three O
classes O
, O
denoting O
non-entity O
words O
, O
starting O
words O
in O
entities O
and O
other O
words O
in O
entities. O
Based O
on O
the O
labeled O
sequence O
, O
the O
Relevance O
function O
for O
entity O
attention O
map O
can O
be O
defined O
as O

Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
E O
W O
j O
0 O
otherwise O
( O
5 O
) O

where O
A O
≡ O
E O
B O
denotes O
A O
and O
B O
belong O
to O
the O
same O
entity O
. O

Phrase B-TaskName
Segmentation I-TaskName
Attention O
Map O
Similar O
to O
the O
entity O
attention O
map O
, O
one O
can O
highlight O
the O
term O
correlations O
within O
the O
same O
phrase O
segmentation O
to O
emphasize O
the O
locality O
inductive O
bias. O
Syntax O
tree O
is O
a O
generic O
source O
to O
extract O
phrases O
in O
different O
semantic O
levels O
, O
which O
can O
be O
generated O
by O
a O
trained O
syntax O
parser. O
In O
a O
syntax O
tree O
, O
each O
internal O
node O
represents O
a O
phrase O
segment O
for O
a O
specific O
level. O
For O
example O
, O
we O
can O
select O
the O
parents O
of O
leaf O
nodes O
in O
the O
syntax O
tree O
as O
the O
root O
of O
each O
sub-tree O
which O
represents O
a O
phrase. O
We O
define O
the O
distance O
of O
an O
internal O
node O
i O
to O
the O
leaf O
node O
as O
level O
( O
i O
) O
. O
Thus O
, O
the O
relevance O
function O
can O
be O
computed O
by O

Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
T O
W O
j O
0 O
otherwise O
( O
6 O
) O

where O
A O
≡ O
T O
B O
denotes O
that O
A O
and O
B O
belong O
to O
the O
same O
sub-tree O
at O
level O
( O
i O
) O
. O

Term B-TaskName
Correlation I-TaskName
Attention O
Map O
In O
computational O
linguistics O
, O
Pointwise B-MetricName
Mutual I-MetricName
Information I-MetricName
( O
PMI B-MetricName
) O
has O
been O
widely O
used O
for O
finding O
associations O
between O
words O
( O
Arora O
et O
al. O
, O
2016 O
) O
. O
In O
our O
work O
, O
we O
adopt O
PMI B-MetricName
to O
measure O
the O
semantic O
correlations O
between O
terms. O
The O
PMI B-MetricName
of O
a O
pair O
( O
x O
, O
y O
) O
from O
discrete O
random O
variables O
( O
X O
, O
Y O
) O
quantifies O
the O
discrepancy O
between O
the O
probability O
of O
their O
coincidence O
given O
joint O
distributions O
and O
individual O
distributions. O
We O
define O
the O
PMI-based B-MetricName
relevance I-MetricName
function O
as O

P O
M O
I O
( O
x O
; O
y O
) O
= O
log O
p O
( O
x O
, O
y O
) O
p O
( O
x O
) O
p O
( O
y O
) O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
=P O
M O
I O
( O
W O
i O
; O
W O
j O
) O
/ O
Z O
( O
7 O
) O

where O
Z O
denotes O
the O
normalized O
factor O
of O
PMI B-MetricName
matrix. O
In O
our O
experiments O
, O
PMI B-MetricName
is O
calculated O
using O
a O
large O
web O
corpus. O
We O
calculate O
the O
probability O
p O
( O
x O
, O
y O
) O
of O
a O
word O
pair O
appearing O
jointly O
, O
and O
the O
probability O
of O
single O
word O
appearance O
is O
denoted O
as O
p O
( O
x O
) O
and O
p O
( O
y O
) O
. O
Finally O
, O
we O
use O
log O
p O
( O
x O
, O
y O
) O
− O
log O
p O
( O
x O
) O
− O
log O
p O
( O
y O
) O
to O
compute O
the O
PMI B-MetricName
score O
. O

To O
better O
incorporate O
semantic O
knowledge O
into O
attention O
maps O
, O
we O
further O
enrich O
each O
attention O
map O
by O
adding O
top O
k O
terms O
which O
do O
not O
appear O
in O
the O
current O
sentence O
but O
hold O
the O
highest O
average O
PMI B-MetricName
scores O
with O
terms O
in O
the O
original O
sentence. O
Note O
that O
we O
should O
expand O
the O
selected O
k O
words O
to O
K O
subwords O
for O
BERT. B-MethodName
Then O
the O
subwords O
will O
be O
appended O
to O
the O
original O
sentence. O
After O
augmentation O
, O
the O
input O
sentence O
has O
N O
+ O
K O
words O
and O
the O
shape O
of O
an O
attention O
map O
becomes O
( O
N O
+ O
K O
) O
× O
( O
N O
+ O
K O
) O
, O
where O
N O
and O
K O
denote O
the O
number O
of O
original O
terms O
and O
auxiliary O
terms O
respectively O
( O
see O
an O
example O
in O
Fig. O
2 O
( O
d O
) O
) O
. O
In O
order O
to O
align O
the O
shapes O
of O
different O
attention O
maps O
( O
including O
the O
vanilla O
self-attention O
map O
) O
, O
we O
add O
zero-padding O
for O
smaller O
ones. O
After O
passing O
one O
transformer O
layer O
, O
the O
output O
sequence O
length O
is O
still O
N O
+ O
K. O
Note O
that O
the O
auxiliary O
words O
are O
only O
utilized O
to O
enrich O
the O
semantics O
of O
original O
word O
representations O
, O
which O
is O
done O
within O
each O
transformer O
layer. O
Thus O
, O
we O
trim O
the O
output O
sequence O
length O
to O
N O
before O
taking O
it O
as O
input O
to O
the O
next O
transformer O
layer O
( O
while O
a O
new O
round O
of O
augmentation O
will O
be O
done O
in O
the O
next O
layer O
) O
. O

Other O
Knowledge-assisted O
Attention O
Maps. O
The O
KAM-BERT B-MethodName
framework O
is O
generic O
and O
can O
be O
extended O
to O
other O
kinds O
of O
knowledge O
in O
future O
works. O
For O
each O
semantic O
type O
, O
we O
can O
define O
a O
specific O
Relevance B-MetricName
function O
to O
transfer O
the O
corresponding O
information O
into O
semantic O
attention O
maps. O
For O
example O
, O
we O
can O
define O
the O
relevance O
function O
for O
a O
Knowledge O
Graph O
( O
KG O
) O
as O
: O

Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
E O
( O
W O
i O
) O
≡ O
KG O
E O
( O
W O
j O
) O
0 O
otherwise O
( O
8 O

) O

where O
E O
( O
W O
i O
) O
is O
the O
corresponding O
entity O
of O
word O
or O
sub-word O
W O
i O
in O
a O
KG O
, O
and O
A O
≡ O
KG O
B O
represents O
that O
both O
A O
and O
B O
exist O
and O
are O
adjacent O
in O
a O
KG O
. O

Experiments O
We O
briefly O
introduced O
the O
extraction O
of O
semantic O
information O
in O
Section O
4.1. O
Then O
we O
report O
experimental O
results O
on O
natural B-TaskName
language I-TaskName
understand I-TaskName
and O
question B-TaskName
answering I-TaskName
tasks O
in O
Section O
4.2 O
and O
4.3 O
respectively. O
In O
Section O
4.4 O
, O
we O
show O
evaluation O
on O
LAMA B-DatasetName
, O
a O
benchmark O
especially O
designed O
to O
study O
how O
much O
semantic O
knowledge O
is O
contained O
in O
a O
language O
model. O
Experiments O
on O
query-ad B-TaskName
relevance I-TaskName
is O
described O
in O
Section O
4.5. O
At O
last O
, O
we O
present O
ablation O
study O
in O
Section O
4.6 O
. O

Semantic O
Information O
Extraction O
We O
use O
Stanza O
library O
( O
Qi O
et O
al. O
, O
2020 O
) O
to O
extract O
NER B-TaskName
information O
and O
syntax O
information. O
Stanza O
NER B-TaskName
takes O
one O
sentence O
as O
input O
and O
returns O
the O
start O
and O
end O
indices O
of O
the O
corresponding O
named O
entity O
in O
the O
sentence. O
While O
Stanza O
Parser O
can O
extract O
the O
corresponding O
syntax O
tree O
for O
each O
sentence. O
We O
use O
query-ad O
logs O
from O
a O
commercial O
search O
engine O
to O
calculate O
PMI B-MetricName
matrix. O
These O
steps O
gain O
the O
knowledge O
required O
to O
generate O
the O
semantic O
attention O
maps O
mentioned O
in O
Section O
3.4 O
. O

GLUE B-MetricName
Benchmark O
The O
GLUE B-MetricName
benchmark O
offers O
a O
collection O
of O
tools O
for O
evaluating O
the O
performance O
of O
models O
across O
a O
diverse O
set O
of O
NLP B-TaskName
applications. O
It O
contains O
singlesentence B-TaskName
classification I-TaskName
tasks O
( O
CoLA B-TaskName
and O
SST-2 B-TaskName
) O
, O
similarity O
and O
paraphrase O
tasks O
( O
MRPC B-TaskName
, O
QQP B-TaskName
and O
STS-B B-TaskName
) O
and O
pairwise O
inference O
tasks O
( O
MNLI B-TaskName
, O
RTE B-TaskName
and O
QNLI B-TaskName
) O
. O
We O
use O
the O
default O
train O
/ O
dev O
/ O
test O
split O
for O
each O
dataset. O
The O
hyper-parameters O
are O
chosen O
based O
on O
the O
validation O
set O
( O
refer O
to O
appendix O
for O
details O
) O
. O
After O
the O
model O
is O
trained O
, O
we O
make O
predictions O
on O
the O
test O
data O
and O
send O
the O
results O
to O
GLUE B-MetricName
online O
evaluation O
service O
1 O
to O
get O
testing O
scores. O
Note O
that O
the O
original O
WNLI B-DatasetName
dataset O
in O
the O
GLUE B-MetricValue
benchmark O
is O
problematic O
, O
which O
causes O
the O
evaluation O
results O
to O
be O
65.1. B-MetricValue
In O
order O
to O
make O
a O
fair O
comparison O
, O
most O
papers O
( O
Devlin O
et O
al. O
, O
2019 O
; O
choose O
to O
ignore O
the O
results O
of O
WNLI B-DatasetName
when O
calculating O
GLUE B-MetricName
average B-MetricName
score I-MetricName
. O

The O
scores O
on O
all O
datasets O
in O
GLUE B-MetricName
benchmark O
are O
listed O
in O
Table O
1. O
We O
report O
test O
scores O
on O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
, O
RoBERTa B-MethodName
related O
models O
and O
their O
corresponding O
enhanced O
models. O
The O
performances O
of O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
and O
1 O
https O
: O
/ O
/ O
gluebenchmark.com O
RoBERTa-Large B-MethodName
are O
reproduced O
using O
the O
official O
checkpoints O
provided O
by O
corresponding O
authors O
. O

As O
shown O
in O
the O
table O
, O
our O
models O
outperform O
all O
corresponding O
baselines. O
KAM-BERT-Base B-MethodName
achieves O
an O
average O
GLUE B-MetricName
score O
of O
78.7 B-MetricValue
, O
lifting O
1.2 B-MetricValue
scores O
from O
standard O
BERT-Base B-MethodName
model O
with O
only O
a O
few O
extra O
parameters O
introduced O
to O
the O
baseline O
model. O
Particularly O
, O
the O
improvements O
on O
CoLA B-DatasetName
datasets O
are O
fairly O
large O
, O
showing O
that O
our O
knowledge O
integration O
method O
has O
good O
generalization O
performance O
for O
natural B-TaskName
language I-TaskName
inference I-TaskName
and O
understanding. O
ERNIE B-MethodName
have O
also O
added O
external O
information O
such O
as O
entity O
and O
knowledge O
graph O
, O
but O
it O
needs O
much O
more O
time O
for O
a O
joint O
re-training. O
As O
for O
BERT-Large B-MethodName
and O
its O
counterpart O
KAM-BERT-Large B-MethodName
, O
the O
average O
improvement O
on O
GLUE B-MetricName
benchmark O
is O
0.9. B-MetricValue
We O
can O
see O
that O
the O
improvement O
becomes O
smaller O
when O
the O
model O
grows O
larger O
, O
because O
larger O
models O
often O
capture O
more O
semantic O
knowledge O
in O
the O
pre-training B-TaskName
phrase. O
But O
incorporating O
explicit O
knowledge O
is O
still O
indispensable O
for O
achieving O
a O
superior O
performance O
. O

Question B-TaskName
Answering I-TaskName
We O
conduct O
experiments O
on O
two O
kinds O
of O
question O
answering O
tasks O
, O
i.e. O
, O
commonsense B-TaskName
QA I-TaskName
and O
open-domain B-TaskName
QA. I-TaskName
Commonsense B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
with O
commonsense. O
We O
adopt O
CosmosQA B-DatasetName
for O
evaluation O
, O
which O
requires O
commonsense-based O
reading O
comprehension O
formulated O
as O
multiple O
answer O
selection. O
Opendomain B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
using O
external O
resources O
such O
as O
collections O
of O
documents O
and O
webpages. O
We O
consider O
two O
public O
datasets O
for O
this O
task O
, O
i.e. O
, O
Quasar-T B-DatasetName
and O
SearchQA B-DatasetName
. O

The O
results O
of O
CosmosQA B-DatasetName
are O
shown O
in O
Table O
2. O
Compared O
with O
BERT-Large B-MethodName
, O
KAM-BERT-Large B-MethodName
achieves O
10.6 B-MetricValue
% O
improvement O
in O
accuracy. O
KAM-RoBERTa-Large B-MethodName
further O
improves O
the O
accuracy O
of O
RoBERTa-Large B-MethodName
by O
5.4 B-MetricValue
% O
, O
which O
indicates O
that O
our O
models O
has O
better O
knowledge O
inference O
ability. O
For O
open-domain B-DatasetName
QA I-DatasetName
, O
our O
model O
also O
achieves O
better O
results O
compared O
to O
corresponding O
baselines. O
This O
because O
that O
KAM-based B-MethodName
models I-MethodName
can O
make O
full O
use O
of O
the O
infused O
knowledge. O
At O
the O
same O
time O
, O
one O
can O
notice O
that O
KAM-based B-MethodName
models I-MethodName
have O
fewer O
parameters O
than O
K-Adaptor B-MethodName
, O
demonstrating O
its O
effectiveness O
for O
knowledge O
infusion. O
WKLM B-MethodName
( O
Xiong O
et O
al. O
, O
2019 O
) O
forces O
the O
pre-trained O
language O
model O
to O
incorporate O
knowledge O
from O
a O
knowledge O
graph. O
This O
makes O
WKLM B-MethodName
to O
achieve O
a O
better O
score O
on O
QA B-TaskName
tasks O
, O
but O
KAM-BERT B-MethodName
performs O
even O
better O
than O
WKLM B-MethodName
. O

LAMA B-DatasetName
Benchmark O
To O
further O
verify O
whether O
KAM-BERT B-MethodName
better O
integrate O
internal O
knowledge O
into O
pre-trained B-MetricName
language O
models O
, O
we O
conduct O
experiments O
on O
LAMA B-DatasetName
, O
a O
widely O
used O
benchmark O
for O
knowledge B-TaskName
probing I-TaskName
. O

LAMA B-DatasetName
examines O
models O
' O
abilities O
on O
recalling O
relational O
facts O
by O
cloze-style O
questions. O
The O
first O
place O
micro-averaged B-MetricName
accuracy I-MetricName
is O
used O
as O
evaluation O
metrics. O
The O
evaluation O
results O
are O
shown O
in O
Table O
3. O
KAM-BERT B-MethodName
consistently O
outperforms O
corresponding O
baselines O
on O
all O
tasks. O
It O
indicates O
that O
KAM-BERT B-MethodName
can O
generate O
better O
attention O
maps O
with O
semantic O
guidance O
. O

Query-Ad B-TaskName
Relevance I-TaskName
Query-ad O
relevance O
measures O
how O
relevant O
a O
search O
ad O
matches O
with O
a O
user O
's O
search O
query. O
Very O
often O
queries O
and O
ads O
have O
words O
with O
special O
meanings O
, O
which O
are O
not O
easily O
understood O
well O
by O
traditional O
NLP B-TaskName
techniques O
but O
can O
benefit O
from O
the O
knowledge-assisted O
mechanism O
proposed O
in O
this O
work. O
Besides O
, O
user O
queries O
and O
ads O
text O
often O
contain O
noises O
, O
so O
evaluation O
on O
query-ad O
relevance O
task O
would O
test O
our O
model O
's O
robustness O
and O
resistance O
of O
noise. O
We O
compare O
BERT B-MethodName
and O
KAM-BERT B-MethodName
on O
a O
large-scale O
internal O
dataset O
of O
a O
commercial O
search O
engine. O
As O
shown O
in O
Table O
5 O
, O
our O
model O
outperforms O
corresponding O
baselines O
by O
a O
large O
margin O
, O
which O
is O
statistically O
significant O
under O
95 B-HyperparameterValue
% I-HyperparameterValue
confidence B-HyperparameterName
interval. O
One O
thing O
to O
call O
out O
is O
that O
, O
although O
NER B-TaskName
and O
syntax B-TaskName
parsing I-TaskName
results O
are O
nosier O
comparing O
to O
the O
ones O
in O
academic O
datasets O
, O
we O
still O
have O
good O
improvements O
on O
this O
dataset. O
This O
indicates O
the O
way O
we O
combine O
those O
knowledge O
together O
makes O
our O
model O
more O
robust O
to O
noisy O
inputs O
. O

Model O
Analysis O
In O
this O
section O
, O
we O
explore O
the O
sensitivity O
of O
hyperparameter O
α B-HyperparameterName
, O
and O
then O
conduct O
ablation O
experiments O
on O
three O
types O
of O
added O
knowledge O
. O

Hyper-parameter O
Analysis O
The O
optimal O
α B-HyperparameterName
value O
after O
grid O
search O
is O
0.2 B-HyperparameterValue
, O
which O
means O
that O
the O
original O
attention O
maps O
still O
dominate O
the O
token O
relationships. O
We O
chose O
three O
tasks O
from O
different O
fields O
to O
do O
ablation O
study O
for O
α. B-HyperparameterName
Our O
model O
is O
KAM-BERT-Base B-MethodName
, O
and O
its O
performance O
is O
shown O
in O
Table O
4. O
In O
three O
different O
tasks O
, O
setting O
α B-HyperparameterName
to O
0.2 B-HyperparameterValue
achieves O
the O
best O
results. O
An O
intuitive O
understanding O
is O
that O
when O
α B-HyperparameterName
is O
small O
, O
external O
knowledge O
plays O
an O
unimportant O
role O
and O
can O
not O
participate O
in O
the O
entire O
training O
process O
well. O
With O
the O
gradual O
increase O
of O
α B-HyperparameterName
, O
the O
intervention O
of O
external O
knowledge O
on O
the O
attention O
map O
will O
increase O
, O
and O
the O
attention O
relationship O
in O
the O
original O
sequence O
will O
be O
gradually O
lost O
, O
resulting O
in O
the O
decline O
of O
model O
performance O
. O

Ablation O
Study O
For O
a O
comprehensive O
understanding O
of O
our O
model O
design O
, O
we O
conduct O
ablation O
study O
with O
the O
following O
settings O
in O
Table O
6. O
The O
average O
scores B-MetricValue
of O
all O
ablation O
experiments O
are O
better O
than O
BERT B-MethodName
, O
but O
are O
relatively O
worse O
than O
KAM-BERT B-MethodName
, O
demonstrating O
all O
the O
components O
are O
beneficial O
for O
the O
final O
performance. O
At O
the O
same O
time O
, O
we O
observe O
that O
after O
deleting O
the O
entity O
attention O
map O
, O
the O
score B-MetricName
of O
KAM-BERT B-MethodName
drops O
drastically O
from O
78.7 B-MetricValue
to O
77.7. B-MetricValue
This O
shows O
that O
the O
gain O
brought O
by O
entity O
information O
is O
the O
greatest. O
In O
addition O
, O
the O
convolution O
layer O
is O
indispensable O
for O
achieving O
a O
superior O
performance O
. O

Case O
Study O
In O
Figure O
3 O
, O
we O
visualize O
an O
example O
of O
query-ad B-TaskName
relevance I-TaskName
, O
where O
the O
query O
is O
" O
buy O
glipizide O
" O
and O
ad O
text O
is O
" O
Fred O
Meyer O
Pharmacy O
Near O
Me O
" O
. O
The O
darker O
color O
in O
the O
figure O
represents O
a O
higher O
attention B-MetricName
score. O
Figure O
3 O
( O
a O
) O
is O
the O
attention O
map O
of O
vanilla O
BERT B-MethodName
without O
adding O
explicit O
knowledge. O
When O
encountering O
rare O
words O
like O
" O
glipizide O
" O
, O
the O
self-attention O
mechanism O
can O
not O
do O
a O
good O
job O
to O
decide O
which O
terms O
should O
" O
glipizide O
" O
attend O
to. O
But O
in O
Figure O
3 O
( O
b O
) O
, O
the O
attention O
map O
of O
KAM-BERT B-MethodName
uses O
term O
correlations O
to O
learn O
that O
" O
glipizide O
" O
is O
a O
medicine O
, O
so O
it O
focuses O
on O
the O
medicinerelated O
tokens O
like O
" O
Pharmacy O
" O
and O
" O
antidiabetic O
" O
. O

Conclusion O
In O
this O
paper O
, O
we O
proposed O
KAM-BERT B-MethodName
, O
a O
flexible O
and O
efficient O
approach O
to O
inject O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models. I-MethodName
Extensive O
experiments O
on O
GLUE B-MetricName
and O
LAMA B-DatasetName
benchmark O
show O
that O
our O
approach O
outperforms O
all O
BERT-Style B-MethodName
baselines O
and O
achieves O
new O
SOTA O
on O
QA B-TaskName
tasks O
, O
suggesting O
that O
our O
models O
indeed O
integrate O
knowledge O
in O
an O
effective O
manner O
and O
have O
good O
generalization O
ability. O
In O
future O
work O
, O
we O
hope O
to O
investigate O
more O
types O
of O
knowledge O
which O
can O
be O
effectively O
integrated O
in O
our O
framework O
. O
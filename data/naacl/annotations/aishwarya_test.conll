-DOCSTART- O
Fine-tuning O
Pre-trained O
Language O
Models O
for O
Few-shot O
Intent O
Detection O
: O
Supervised O
Pre-training O
and O
Isotropization O
It O
is O
challenging O
to O
train O
a O
good O
intent O
classifier O
for O
a O
task-oriented O
dialogue O
system O
with O
only O
a O
few O
annotations. O
Recent O
studies O
have O
shown O
that O
fine-tuning O
pre-trained O
language O
models O
with O
a O
small O
amount O
of O
labeled O
utterances O
from O

public O
benchmarks O
in O
a O
supervised O
manner O
is O
extremely O
helpful. O
However O
, O
we O
find O
that O
supervised O
pre-training O
yields O
an O
anisotropic O
feature O
space O
, O
which O
may O
suppress O
the O
expressive O
power O
of O
the O
semantic O
representations. O
Inspired O
by O
recent O
research O
in O
isotropization O
, O
we O
propose O
to O
improve O
supervised O
pretraining O
by O
regularizing O
the O
feature O
space O

towards O
isotropy. B-MetricName
We O
propose O
two O
regularizers O
based O
on O
contrastive B-HyperparameterName
learning I-HyperparameterName
and O
correlation B-HyperparameterName
matrix I-HyperparameterName
respectively O
, O
and O
demonstrate O
their O
effectiveness O
through O
extensive O
experiments. O
Our O
main O
finding O
is O
that O
it O
is O
promising O
to O
regularize O
supervised O
pre-training O
with O
isotropization O
to O
further O
improve O
the O
performance O
of O
few-shot O
intent O
detection. O
The O
source O
code O
can O
be O

found O
at O
https O
: O
/ O
/ O
github.com O
/ O
fanolabs O
/ O
isoIntentBert-main O
. O
Introduction B-MethodName
Intent I-MethodName
detection I-MethodName
is O
a O
core O
module O
of O
task-oriented O
dialogue O
systems. O
Training O
a O
well-performing O
intent O
classifier O
with O
only O
a O
few O
annotations O
, O
i.e. O
, O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
is O
of O
great O
practical O
value. O
Recently O
, O
this O
problem O
has O

attracted O
considerable O
attention O
( O
Vulić O
et O
al. O
, O
2021 O
; O
Zhang O
et O
al. O
, O
b O
; O
Dopierre O
et O
al. O
, O
b O
) O
but O
remains O
a O
challenge O
. O
To O
tackle O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
earlier O
works O
employ O
induction O
network O
( O
Geng O
et O
al. O
, O
2019 O
) O
, O
generation-based O
methods O
( O
Xia O

et O
al. O
, O
a O
) O
, O
metric O
learning O
( O
Nguyen O
et O
al. O
, O
2020 O
) O
, O
and O
selftraining O
( O
Dopierre O
et O
al. O
, O
b O
) O
, O
to O
design O
sophisticated O
algorithms. O
Recently O
, O
pre-trained B-MethodName
language I-MethodName
models I-MethodName
( O
PLMs B-MethodName
) O
have O
emerged O
as O
a O
simple O
yet O
promising O
solution O
to O
a O
wide O
spectrum O

of O
natural B-MethodName
language I-MethodName
processing I-MethodName
( O
NLP B-MethodName
) O
tasks O
, O
triggering O
the O
surge O
of O
PLM- B-MethodName
* O
Corresponding O
author O
. O
based O
solutions O
for O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
Zhang O
et O
al. O
, O
a O
, O
b O
; O
Vulić O
et O
al. O
, O
2021 O
; O
Zhang O
et O
al. O
, O
b O
) O
, O
which O
typically O
fine-tune O
PLMs B-MethodName
on O

conversation O
data O
. O
A O
PLM-based B-MethodName
fine-tuning O
method O
( O
Zhang O
et O
al. O
, O
a O
) O
, O
called O
IntentBERT B-MethodName
, O
utilizes O
a O
small O
amount O
of O
labeled O
utterances O
from O
public O
intent O
datasets O
to O
fine-tune O
PLMs B-MethodName
with O
a O
standard O
classification O
task O
, O
which O
is O
referred O
to O
as O
supervised O
pre-training. O
Despite O
its O
simplicity O
, O
supervised O

pre-training O
has O
been O
shown O
extremely O
useful O
for O
few-shot O
intent O
detection O
even O
when O
the O
target O
data O
and O
the O
data O
used O
for O
fine-tuning O
are O
very O
different O
in O
semantics. O
However O
, O
as O
will O
be O
shown O
in O
Section O
3.2 O
, O
IntentBERT B-MethodName
suffers O
from O
severe O
anisotropy O
, O
an O
undesirable O
property O
of O
PLMs B-MethodName
Ethayarajh O
, O
2019 O

; O
Li O
et O
al. O
, O
2020 O
) O
. O
Anisotropy O
is O
a O
geometric O
property O
that O
semantic O
vectors O
fall O
into O
a O
narrow O
cone. O
It O
has O
been O
identified O
as O
a O
crucial O
factor O
for O
the O
sub-optimal O
performance O
of O
PLMs B-MethodName
on O
a O
variety O
of O
downstream O
tasks O
Arora O
et O
al. O
, O
b O
; O
Cai O
et O
al. O

, O
2020 O
; O
Ethayarajh O
, O
2019 O
) O
, O
which O
is O
also O
known O
as O
the O
representation O
degeneration O
problem O
( O
Gao O
et O
al. O
, O
a O
) O
. O
Fortunately O
, O
isotropization O
techniques O
can O
be O
applied O
to O
adjust O
the O
embedding O
space O
and O
yield O
significant O
performance O
improvement O
in O
many O
tasks O
Rajaee O
and O
Pilehvar O
, O
2021a O

) O
. O
Hence O
, O
this O
paper O
aims O
to O
answer O
the O
question O
: O
• O
Can O
we O
improve O
supervised O
pre-training O
via O
isotropization O
for O
few-shot O
intent O
detection O
? O
Many O
isotropization O
techniques O
have O
been O
developed O
based O
on O
transformation O
Huang O
et O
al. O
, O
2021 O
) O
, O
contrastive O
learning O
( O
Gao O
et O
al. O
, O
b O
) O

, O
and O
top O
principal O
components O
elimination O
( O
Mu O
and O
Viswanath O
, O
2018 O
) O
. O
However O
, O
these O
methods O
are O
designed O
for O
off-the-shelf O
PLMs. B-MethodName
When O
applied O
on O
PLMs B-MethodName
that O
have O
been O
fine-tuned O
on O
some O
NLP B-MethodName
task O
such O
as O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
or O
intent B-TaskName
classification I-TaskName
, O
they O
may O
introduce O
an O
adverse O
effect O

, O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
are O
designed O
to O
regularize O
SPT B-MethodName
and O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
, O
which O
leads O
to O
better O
performance O
on O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
. O
as O
observed O
in O
Rajaee O
and O
Pilehvar O
( O
2021c O
) O
and O
our O
pilot O
experiments. O
In O
this O
work O
, O
we O
propose O
to O
regularize O
supervised O

pre-training O
with O
isotropic O
regularizers. O
As O
shown O
in O
Fig. O
1 O
, O
we O
devise O
two O
regularizers O
, O
a O
contrastive-learning-based B-HyperparameterName
regularizer I-HyperparameterName
( O
CL-Reg B-HyperparameterName
) O
and O
a O
correlation-matrix-based B-HyperparameterName
regularizer I-HyperparameterName
( O
Cor-Reg B-HyperparameterName
) O
, O
each O
of O
which O
can O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
during O
supervised O
training. O
Our O
empirical O
study O
shows O
that O
the O
regularizers O

can O
significantly O
improve O
the O
performance O
of O
standard O
supervised O
training O
, O
and O
better O
performance O
can O
often O
be O
achieved O
when O
they O
are O
combined O
. O
The O
contributions O
of O
this O
work O
are O
three-fold O
: O
• O
We O
present O
the O
first O
study O
on O
the O
isotropy B-MetricName
property O
of O
PLMs B-MethodName
for O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
shedding O
light O
on O

the O
interaction O
of O
supervised O
pre-training O
and O
isotropization O
. O
• O
We O
improve O
supervised O
pre-training O
by O
devising O
two O
simple O
yet O
effective O
regularizers O
to O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
. O
• O
We O
conduct O
a O
comprehensive O
evaluation O
and O
analysis O
to O
validate O
the O
effectiveness O
of O
the O
proposed O
approach O
. O
2 O
Related O
Works O
Few-shot B-TaskName

Intent I-TaskName
Detection I-TaskName
With O
a O
surge O
of O
interest O
in O
few-shot O
learning O
( O
Finn O
et O
al. O
, O
2017 O
; O
Vinyals O
et O
al. O
, O
2016 O
; O
Snell O
et O
al. O
, O
2017 O
) O
, O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
has O
started O
to O
receive O
attention. O
Earlier O
works O
mainly O
focus O
on O
model O
design O
, O
using O
capsule O
network O
( O

Geng O
et O
al. O
, O
2019 O
) O
, O
variational O
autoencoder O
( O
Xia O
et O
al. O
, O
a O
) O
, O
or O
metric O
functions O
( O
Yu O
et O
al. O
, O
2018 O
; O
Nguyen O
et O
al. O
, O
2020 O
) O
. O
Recently O
, O
PLMs-based B-MethodName
methods O
have O
shown O
promising O
performance O
in O
a O
variety O
of O
NLP B-MethodName
tasks O
and O
become O

the O
model O
of O
choice O
for O
few-shot B-TaskName
intent I-TaskName
detection. I-TaskName
Zhang O
et O
al. O
( O
c O
) O
cast O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
into O
a O
natural B-MethodName
language I-MethodName
inference I-MethodName
( O
NLI B-MethodName
) O
problem O
and O
fine-tune O
PLMs B-MethodName
on O
NLI B-MethodName
datasets. O
Zhang O
et O
al. O
( O
b O
) O
propose O
to O
fine-tune O
PLMs B-MethodName
on O
unlabeled O
utterances O
by O
contrastive O
learning. O
Zhang O

et O
al. O
( O
a O
) O
leverage O
a O
small O
set O
of O
public O
annotated O
intent O
detection O
benchmarks O
to O
fine-tune O
PLMs B-MethodName
with O
standard O
supervised O
training O
and O
observe O
promising O
perfor-mance O
on O
cross-domain O
few-shot O
intent O
detection O
. O
Meanwhile O
, O
the O
study O
of O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
has O
been O
extended O
to O
other O
settings O
including O
semisupervised O
learning O
( O

Dopierre O
et O
al. O
, O
b O
, O
a O
) O
, O
generalized O
setting O
( O
Nguyen O
et O
al. O
, O
2020 O
) O
, O
multi-label O
classification O
( O
Hou O
et O
al. O
, O
2021 O
) O
, O
and O
incremental O
learning O
( O
Xia O
et O
al. O
, O
b O
) O
. O
In O
this O
work O
, O
we O
consider O
standard O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName

, O
following O
the O
setup O
of O
Zhang O
et O
al. O
( O
a O
) O
and O
aiming O
to O
improve O
supervised O
pre-training O
with O
isotropization O
. O
Further O
Pre-training O
PLMs B-MethodName
with O
Dialogue B-DatasetName
Corpora O
Recent O
works O
have O
shown O
that O
further O
pre-training O
off-the-shelf O
PLMs B-MethodName
using O
dialogue B-DatasetName
corpora O
( O
Henderson O
et O
al. O
, O
b O
; O
Peng O
et O
al. O
, O
2020Peng O

et O
al. O
, O
, O
2021 O
are O
beneficial O
for O
task-oriented O
downstream O
tasks O
such O
as O
intent O
detection. O
Specifically O
, O
TOD-BERT B-MethodName
conducts O
self-supervised O
learning O
on O
diverse O
task-oriented O
dialogue B-DatasetName
corpora. O
ConvBERT B-MethodName
( O
Mehri O
et O
al. O
, O
2020 O
) O
is O
pre-trained O
on O
a O
700 O
million O
open-domain O
dialogue B-DatasetName
corpus. O
Vulić O
et O
al. O
( O
2021 O
) O
propose O

a O
two-stage O
procedure O
: O
adaptive B-TaskName
conversational I-TaskName
fine-tuning I-TaskName
followed O
by O
task-tailored B-TaskName
conversational I-TaskName
fine-tuning. I-TaskName
In O
this O
work O
, O
we O
follow O
Zhang O
et O
al. O
( O
a O
) O
to O
further O
pre-train O
PLMs B-MethodName
using O
a O
small O
amount O
of O
labeled O
utterances O
from O
public O
intent O
detection O
benchmarks O
. O
Measuring O
isotropy B-MetricName
Following O
Mu O
and O
Viswanath O
( O
2018 O
) O

; O
Biś O
et O
al. O
( O
2021 O
) O
, O
we O
adopt O
the O
following O
measurement O
of O
isotropy B-MetricName
: O
I B-MetricName
( I-MetricName
V I-MetricName
) I-MetricName
= O
min O
c O
∈ O
C O
Z O
( O
c O
, O
V O
) O
max O
c O
∈ O
C O
Z O
( O
c O
, O
V O
) O
, O
( O
1 O
) O
where O
V O
∈ O
R O
N O

×d O
is O
the O
matrix O
of O
stacked O
embeddings O
of O
N O
utterances O
( O
note O
that O
the O
embeddings O
have O
zero O
mean O
) O
, O
C O
is O
the O
set O
of O
unit O
eigenvectors O
of O
V O
⊤ O
V O
, O
and O
Z O
( O
c O
, O
V O
) O
is O
the O
partition O
function O
( O
Arora O
et O
al. O
, O
b O
) O

defined O
as O
: O
Z O
( O
c O
, O
V O
) O
= O
N O
i=1 O
exp O
c O
⊤ O
v O
i O
, O
( O
2 O
) O
where O
v O
i O
is O
the O
i O
th O
row O
of O
V. O
I O
( O
V O
) O
∈ O
[ O
0 O
, O
1 O
] O
, O
and O
1 O
indicates O
perfect O
isotropy B-MetricName
. O
Fine-tuning O
Leads O

to O
Anisotropy O
To O
observe O
the O
impact O
of O
fine-tuning O
on O
isotropy B-MetricName
, O
we O
follow O
IntentBERT B-MethodName
( O
Zhang O
et O
al. O
, O
a O
) O
to O
fine-tune O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
with O
standard O
supervised O
training O
on O
a O
small O
set O
of O
an O
intent O
detection O
benchmark O
OOS O
( O
Larson O
et O
al. O
, O

2019 O
) O
( O
details O
are O
given O
in O
Section O
4.1 O
) O
. O
We O
then O
compare O
the O
isotropy B-MetricName
of O
the O
original O
embedding O
space O
( O
BERT B-MethodName
) O
and O
the O
embedding O
space O
after O
fine-tuning O
( O
IntentBERT B-MethodName
) O
on O
target O
datasets. O
As O
shown O
in O
Table O
1 O
, O
after O
finetuning O
, O
the O
isotropy B-MetricName
of O
the O
embedding O

space O
is O
notably O
decreased O
on O
all O
datasets. O
Hence O
, O
it O
can O
be O
seen O
that O
fine-tuning O
may O
render O
the O
feature O
space O
more O
anisotropic O
. O
Isotropization O
after O
Fine-tuning O
May O
Have O
an O
Adverse O
Effect O
To O
examine O
the O
effect O
of O
isotropization O
on O
a O
finetuned O
model O
, O
we O
apply O
two O
strong O
isotropization O
techniques O
to O
IntentBERT B-MethodName

: O
dropout-based O
contrastive O
learning O
( O
Gao O
et O
al. O
, O
b O
) O
and O
whitening O
transformation O
. O
The O
former O
fine-tunes O
PLMs B-MethodName
in O
a O
contrastive O
learning O
manner O
1 O
, O
while O
the O
latter O
transforms O
the O
semantic O
feature O
space O
into O
an O
isotropic O
space O
via O
matrix O
transformation. O
These O
methods O
have O
been O
demonstrated O
highly O
effective O
( O
Gao O

et O
al. O
, O
b O
; O
when O
applied O
to O
off-the-shelf O
PLMs B-MethodName
, O
but O
things O
are O
different O
when O
they O
are O
applied O
to O
fine-tuned O
models. O
As O
shown O
in O
Fig. O
2 O
, O
contrastive O
learning O
improves O
isotropy B-MetricName
, O
but O
it O
significantly O
lowers O
the O
performance O
on O
two O
benchmarks. O
As O
for O
whitening O
transformation O
, O
it O
has O
inconsistent O

effects O
on O
the O
two O
datasets O
, O
as O
shown O
in O
Fig. O
3. O
It O
hurts O
the O
performance O
on O
HWU64 B-DatasetName
( O
Fig. O
3a O
) O
but O
yields O
better O
results O
on O
BANKING77 B-DatasetName
( O
Fig. O
3b O
) O
, O
while O
producing O
nearly O
perfect O
isotropy B-MetricName
on O
both. O
The O
above O
observations O
indicate O
that O
isotropization O
may O
hurt O
fine-tuned O
models O
, O

which O
echoes O
the O
recent O
finding O
of O
Rajaee O
and O
Pilehvar O
. O
Method O
The O
pilot O
experiments O
reveal O
the O
anisotropy O
of O
a O
PLM B-MethodName
fine-tuned O
on O
intent O
detection O
tasks O
and O
the O
1 O
We O
refer O
the O
reader O
to O
the O
original O
paper O
for O
details. O
Whitening O
transformation O
leads O
to O
perfect O
isotropy B-MetricName
but O
has O
inconsistent O
effects O
on O
the O

performance O
. O
challenge O
of O
applying O
isotropization O
techiniques O
on O
the O
fine-tuned O
model. O
In O
this O
section O
, O
we O
propose O
a O
joint O
fine-tuning O
and O
isotropization O
framework. O
Specifically O
, O
we O
propose O
two O
regularizers O
to O
make O
the O
feature O
space O
more O
isotropic O
during O
fine-tuning. O
Before O
presenting O
our O
method O
, O
we O
first O
introduce O
supervised O
pre-training O
. O
Supervised O

Pre-training O
for O
Few-shot B-TaskName
Intent I-TaskName
Detection I-TaskName
Few-shot B-TaskName
intent I-TaskName
detection I-TaskName
targets O
to O
train O
a O
good O
intent O
classifier O
with O
only O
a O
few O
labeled O
data O
D O
target O
= O
{ O
( O
x O
i O
, O
y O
i O
) O
} O
Nt O
, O
where O
N B-HyperparameterName
t O
is O
the O
number B-HyperparameterName
of I-HyperparameterName
labeled I-HyperparameterName
samples I-HyperparameterName
in O
the O
target O
dataset O
, O
x B-HyperparameterName

i I-HyperparameterName
denotes O
the O
i B-HyperparameterName
th I-HyperparameterName
utterance I-HyperparameterName
, O
and O
y O
i O
is O
the O
label. O
x B-HyperparameterName
i I-HyperparameterName
is O
the O
i O
th O
utterance O
in O
a O
batch B-HyperparameterName
of O
size O
3. B-HyperparameterValue
In O
( O
a O
) O
, O
x B-HyperparameterName
i I-HyperparameterName
is O
fed O
to O
the O
PLM B-MethodName
twice O
with O
built-in O
dropout O
to O
produce O
two O
different O
representations O
of O
x B-HyperparameterName
i I-HyperparameterName

: O
h B-HyperparameterName
i I-HyperparameterName
and O
h B-HyperparameterName
+ I-HyperparameterName
i I-HyperparameterName
. O
Positive O
and O
negative O
pairs O
are O
then O
constructed O
for O
each O
x B-MethodName
i I-MethodName
. O
For O
example O
, O
h O
1 O
and O
h O
+ O
1 O
form O
a O
positive O
pair O
for O
x O
1 O
, O
while O
h O
1 O
and O
h O
+ O
2 O
, O
and O
h O
1 O
and O
h O

+ O
3 O
, O
form O
negative O
pairs O
for O
x O
1 O
. O
In O
( O
b O
) O
, O
the O
correlation O
matrix O
is O
estimated O
from O
h O
i O
, O
feature O
vectors O
generated O
by O
the O
PLM B-MethodName
, O
and O
is O
regularized O
towards O
the O
identity O
matrix. O
can O
work O
well O
when O
the O
label O
spaces O
of O
D O
source O
and O
D O

target O
are O
disjoint O
. O
Specifically O
, O
the O
pre-training O
is O
conducted O
by O
attaching O
a O
linear O
layer O
( O
as O
the O
classifier O
) O
on O
top O
of O
the O
utterance O
representation O
generated O
by O
the O
PLM B-MethodName
: O
p O
( O
y|h O
i O
) O
= O
softmax O
( O
Wh O
i O
+ O
b O
) O
∈ O
R O
L O
, O
( O
3 O

) O
where O
h O
i O
∈ O
R O
d O
is O
Regularizing O
Supervised O
Pre-training O
with O
Isotropization O
To O
mitigate O
the O
anisotropy O
of O
the O
PLM B-MethodName
fine-tuned O
by O
supervised O
pre-training O
, O
we O
propose O
a O
joint O
training O
objective O
by O
adding O
a O
regularization O
term O
L B-MetricName
reg I-MetricName
for O
isotropization O
: O
L O
= O
L O
ce O
( O
D O
source O
; O
θ O

) O
+ O
λL B-MetricName
reg I-MetricName
( O
D O
source O
; O
θ O
) O
, O
( O
5 O
) O
where O
λ B-HyperparameterName
is O
a O
weight B-HyperparameterName
parameter. I-HyperparameterName
The O
aim O
is O
to O
learn O
intent O
detection O
skills O
while O
maintaining O
an O
appropriate O
degree O
of O
isotropy. B-MetricName
We O
devise O
two O
different O
regularizers O
introduced O
as O
follows O
. O
Contrastive-learning-based B-TaskName
Regularizer. I-TaskName
Inspired O
by O
the O
recent O

success O
of O
contrastive O
learning O
in O
mitigating O
anisotropy O
( O
Yan O
et O
al. O
, O
2021 O
; O
Gao O
et O
al. O
, O
b O
) O
, O
we O
employ O
the O
dropout-based O
contrastive O
learning O
loss O
used O
in O
Gao O
et O
al. O
( O
b O
) O
as O
the O
regularizer O
: O
L B-MetricName
reg I-MetricName
= O
− O
1 O
N O
b O
N O
b O
i O

log O
e O
sim O
( O
h O
i O
, O
h O
+ O
i O
) O
/ O
τ O
N O
b O
j=1 O
e O
sim O
( O
h O
i O
, O
h O
+ O
j O
) O
/ O
τ O
. O
( O
6 O
) O
In O
particular O
, O
h O
i O
∈ O
R O
d O
and O
h O
+ O
i O
∈ O
R O
d O
are O
two O
different O

representations O
of O
utterance O
x O
i O
generated O
by O
the O
PLM B-MethodName
with O
built-in O
standard O
dropout O
( O
Srivastava O
et O
al. O
, O
2014 O
) O
, O
i.e. O
, O
x B-MetricName
i I-MetricName
is O
passed O
to O
the O
PLM B-MethodName
twice O
with O
different O
dropout O
masks O
to O
produce O
h O
i O
and O
h O
+ O
i O
. O
sim O
( O
h O
1 O
, O
h O

2 O
) O
denotes O
the O
cosine O
similarity O
between O
h O
1 O
and O
h O
2 O
. O
τ O
is O
the O
temperature O
parameter. O
N O
b O
is O
the O
batch O
size. O
Since O
h O
i O
and O
h O
+ O
i O
represent O
the O
same O
utterance O
, O
they O
form O
a O
positive O
pair. O
Similarly O
, O
h O
i O
and O
h O
+ O
j O
form O

a O
negative O
pair O
, O
since O
they O
represent O
different O
utterances. O
An O
example O
is O
given O
in O
Fig. O
4a. O
By O
minimizing O
the O
contrastive O
loss O
, O
positive O
pairs O
are O
pulled O
together O
while O
negative O
pairs O
are O
pushed O
away O
, O
which O
in O
theory O
enforces O
an O
isotropic O
feature O
space O
( O
Gao O
et O
al. O
, O
b O
) O
. O

In O
Gao O
et O
al. O
( O
b O
) O
, O
the O
contrastive O
loss O
is O
used O
as O
the O
single O
objective O
to O
fine-tune O
off-the-shelf O
PLMs O
in O
an O
unsupervised O
manner O
, O
while O
in O
this O
work O
we O
use O
it O
jointly O
with O
supervised O
pre-training O
to O
fine-tune O
PLMs B-MethodName
for O
fewshot O
learning O
. O
Correlation-matrix-based B-TaskName
Regularizer. I-TaskName
The O
above O
regularizer O
enforces O

isotropization O
implicitly O
. O
Here O
, O
we O
propose O
a O
new O
regularizer O
that O
explicitly O
enforces O
isotropization. O
The O
perfect O
isotropy B-MetricName
is O
characterized O
by O
zero O
covariance O
and O
uniform O
variance O
Zhou O
et O
al. O
, O
2021 O
) O
, O
i.e. O
, O
a O
covariance O
matrix O
with O
uniform O
diagonal O
elements O
and O
zero O
non-diagonal O
elements. O
Isotropization O
can O
be O
achieved O
by O

endowing O
the O
feature O
space O
with O
such O
statistical O
property. O
However O
, O
as O
will O
be O
shown O
in O
Section O
5.3 O
, O
it O
is O
difficult O
to O
determine O
the O
appropriate O
scale O
of O
variance. O
Therefore O
, O
we O
base O
the O
regularizer O
on O
correlation O
matrix O
: O
L B-MetricName
reg I-MetricName
= O
∥Σ O
− O
I∥ O
, O
( O
7 O
) O
where O
∥•∥ O

denotes O
Frobenius O
norm O
, O
I O
∈ O
R O
d×d O
is O
the O
identity O
matrix O
, O
Σ O
∈ O
R O
d×d O
is O
the O
correlation O
matrix O
with O
Σ O
ij O
being O
the O
Pearson O
correlation O
coefficient O
between O
the O
i O
th O
dimension O
and O
the O
j O
th O
dimension O
. O
As O
shown O
in O
Fig. O
4b O
, O
Σ O
is O
estimated O
with O

utterances O
in O
the O
current O
batch. O
By O
pushing O
the O
correlation O
matrix O
towards O
the O
identity O
matrix O
during O
training O
, O
we O
can O
learn O
a O
more O
isotropic O
feature O
space. O
Moreover O
, O
the O
proposed O
two O
regularizers O
can O
be O
used O
together O
as O
follows O
: O
L O
= O
L O
ce O
( O
D O
source O
; O
θ O
) O
+ O
λ O

1 O
L O
cl O
( O
D O
source O
; O
θ O
) O
+λ O
2 O
L O
cor O
( O
D O
source O
; O
θ O
) O
, O
( O
8 O
) O
where O
λ B-HyperparameterName
1 I-HyperparameterName
and O
λ B-HyperparameterName
2 I-HyperparameterName
are O
the O
weight B-HyperparameterName
parameters I-HyperparameterName
, O
and O
L O
cl O
and O
L O
cor O
denote O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
, O
respectively. O
Our O
experiments O
show O
that O

better O
performance O
is O
often O
observed O
when O
they O
are O
used O
together O
. O
Experimental O
Setup O
Datasets. O
Gaming O
" O
. O
HWU64 B-DatasetName
( O
Liu O
et O
al. O
, O
2019a O
) O
is O
a O
largescale O
dataset O
containing O
21 O
domains. O
Dataset O
statistics O
are O
summarized O
in O
Table O
3 O
. O
Our O
Method. O
Our O
method O
can O
be O
applied O
to O
fine-tune O
any O

PLM. B-MethodName
We O
conduct O
experiments O
on O
two O
popular O
PLMs B-MethodName
, O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al. O
, O
2019b O
) O
. O
For O
both O
of O
them O
, O
the O
embedding O
of O
[ O
CLS O
] O
is O
used O
as O
the O
utterance O
representation O
in O
Eq. O
3. O
We O
employ O
logistic O

regression O
as O
the O
classifier. O
We O
select O
the O
hyperparameters O
λ B-HyperparameterName
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
and O
τ B-HyperparameterName
by O
validation. O
The O
best O
hyperparameters O
are O
provided O
in O
Table O
4. O
Baselines. O
We O
compare O
our O
method O
to O
the O
following O
baselines O
. O
First O
, O
for O
BERT-based B-MethodName
methods O
, O
BERT-Freeze B-MethodName
freezes O
BERT B-MethodName
; O
CON-VBERT B-MethodName
( O

Mehri O
et O
al. O
, O
2020 O
) O
, O
TOD-BERT B-MethodName
, O
and O
DNNC-BERT B-MethodName
( O
Zhang O
et O
al. O
, O
c O
) O
further O
pre-train O
BERT B-MethodName
on O
conversational O
corpus O
or O
natural B-MethodName
language I-MethodName
inference I-MethodName
tasks. O
USE-ConveRT B-MethodName
Casanueva O
et O
al. O
, O
2020 O
) O
5 O
: O
5-way O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
using O
BERT. B-MethodName
We O
report O
the O
mean O
and O
standard O

deviation O
of O
our O
methods O
and O
IntentBERT B-MethodName
variants. O
CL-Reg B-HyperparameterName
, O
Cor-Reg B-HyperparameterName
, O
and O
CL-Reg B-HyperparameterName
+ O
CorReg B-HyperparameterName
denote O
supervised O
pre-training O
regularized O
by O
the O
corresponding O
regularizer. O
The O
top O
3 O
results O
are O
highlighted. O
¶ O
denotes O
results O
from O
( O
Zhang O
et O
al. O
, O
a O
Finally O
, O
to O
show O
the O
superiority O
of O
the O
joint O
finetuning O
and O

isotropization O
, O
we O
compare O
our O
method O
against O
whitening O
transformation O
. O
BERT-White B-MethodName
and O
RoBERTa-White B-MethodName
apply O
the O
transformation O
to O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
respectively. O
IntentBERT-White B-MethodName
and O
IntentRoBERTa-White B-MethodName
apply O
the O
transformation O
to O
IntentBERT-ReImp B-MethodName
and O
IntentRoBERTa B-MethodName
, O
respectively. O
All O
baselines O
use O
logistic O
regression O
as O
classifier O
except O
DNNC-BERT B-MethodName
and O
DNNC-RoBERTa B-MethodName
, O
wherein O
we O
follow O
the O

original O
work O
2 O
to O
train O
a O
pairwise O
encoder O
for O
nearest O
neighbor O
classification O
. O
Training O
Details. O
We O
use O
PyTorch O
library O
and O
Python O
to O
build O
our O
model. O
We O
employ O
Hugging O
Face O
implementation O
3 O
of O
bert-base-uncased B-MethodName
and O
roberta-base. O
We O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
as O
the O
optimizer O
with O
learning B-HyperparameterName

rate I-HyperparameterName
of O
2e B-HyperparameterValue
− I-HyperparameterValue
05 I-HyperparameterValue
and O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e B-HyperparameterValue
− I-HyperparameterValue
03. I-HyperparameterValue
The O
model O
is O
trained O
with O
Nvidia O
RTX O
3090 O
GPUs. O
The O
training O
is O
early O
stopped O
if O
no O
improvement O
in O
validation O
accuracy B-MetricName
is O
observed O
for O
100 B-HyperparameterValue
steps. B-HyperparameterName
The O
same O
set O
of O
random O
seeds B-HyperparameterName
, O
{ B-HyperparameterValue
1 I-HyperparameterValue
, I-HyperparameterValue
2 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue

, I-HyperparameterValue
4 I-HyperparameterValue
, I-HyperparameterValue
5 I-HyperparameterValue
} I-HyperparameterValue
, O
is O
used O
for O
IntentBERT-ReImp B-MethodName
, O
IntentRoBERTa B-MethodName
, I-MethodName
and O
our O
method O
. O
Evaluation. O
The O
baselines O
and O
our O
method O
are O
evaluated O
on O
C-way B-MetricName
K-shot B-MetricName
tasks. O
For O
each O
task O
, O
we O
randomly O
sample O
C B-MetricName
classes O
and O
K B-MetricName
examples O
per O
class. O
The O
C B-MetricName
×K B-MetricName
labeled O
examples O
are O
used O

to O
train O
the O
logistic O
regression O
classifier. O
Note O
that O
we O
do O
not O
further O
fine-tune O
the O
PLM B-MethodName
using O
the O
labeled O
data O
of O
the O
task. O
We O
then O
sample O
another O
5 O
examples O
per O
class O
as O
queries. O
Fig. O
1 O
gives O
an O
example O
with O
C B-MetricName
= O
2 B-MetricValue
and O
K B-MetricName
= O
1. B-MetricValue
We O
report O
the O
averaged O
accuracy B-MetricName

of O
500 O
tasks O
randomly O
sampled O
from O
D O
target O
. O
Main O
Results O
The O
main O
results O
are O
provided O
in O
Table O
5 O
( O
BERTbased B-MethodName
) O
and O
Table O
6 O
( O
RoBERTa-based B-MethodName
) O
. O
The O
following O
observations O
can O
be O
made. O
First O
, O
our O
proposed O
regularized O
supervised O
pre-training O
, O
with O
either O
CL-Reg B-HyperparameterName
or O
Cor-Reg B-HyperparameterName
, O
consistently O

outperforms O
all O
the O
baselines O
by O
a O
notable O
margin O
in O
most O
cases O
, O
indicating O
the O
effectiveness O
of O
our O
method. O
Our O
method O
also O
outperforms O
whitening O
transformation O
, O
demonstrating O
the O
superiority O
of O
the O
proposed O
joint O
finetuning O
and O
isotropization O
framework. O
Second O
, O
Cor-Reg B-HyperparameterName
slightly O
outperforms O
CL-Reg B-HyperparameterName
in O
most O
cases O
, O
showing O
the O
advantage O
of O

enforcing O
isotropy O
explicitly O
with O
the O
correlation O
matrix. O
Finally O
, O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
show O
a O
complementary O
effect O
in O
many O
cases O
, O
especially O
on O
BANKING77. B-DatasetName
The O
above O
observations O
are O
consistent O
for O
both O
BERT B-MethodName
and O
RoBERTa. B-MethodName
It O
can O
be O
also O
seen O
that O
higher O
performance O
is O
often O
attained O
with O
RoBERTa. B-MethodName
The O
observed O
improvement O
in O

performance O
comes O
with O
an O
improvement O
in O
isotropy. B-MetricName
We O
report O
the O
change O
in O
isotropy O
by O
the O
proposed O
regularizers O
in O
Table O
7. O
It O
can O
be O
seen O
that O
both O
regularizers O
and O
their O
combination O
make O
the O
feature O
space O
more O
isotropic O
compared O
to O
IntentBERT-ReImp B-MethodName
that O
only O
uses O
supervised O
pre-training. O
In O
addition O
, O
in O
general O
, O

Cor-Reg B-HyperparameterName
can O
achieve O
better O
isotropy B-MetricName
than O
CL-Reg B-HyperparameterName
. O
Ablation O
Study O
and O
Analysis O
Moderate O
isotropy B-MetricName
is O
helpful. O
To O
investigate O
the O
relation O
between O
the O
isotropy B-MetricName
of O
the O
feature O
space O
and O
the O
performance O
of O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
tune O
the O
weight B-HyperparameterName
parameter I-HyperparameterName
λ B-HyperparameterName
of O
Cor-Reg B-HyperparameterName
to O
increase O
the O
isotropy B-MetricName
and O
examine O
the O

performance. O
As O
shown O
in O
Fig. O
5 O
, O
a O
common O
pattern O
is O
observed O
: O
the O
best O
performance O
is O
achieved O
when O
the O
isotropy B-MetricName
is O
moderate. O
This O
observation O
indicates O
that O
it O
is O
important O
to O
find O
an O
appropriate O
trade-off O
between O
learning O
intent O
detection O
skills O
and O
learning O
an O
insotropic O
feature O
space. O
In O
our O
method O
, O

we O
select O
the O
appropriate O
λ B-HyperparameterName
by O
validation. O
Correlation O
matrix O
is O
better O
than O
covariance O
matrix O
as O
regularizer. O
In O
the O
design O
of O
Cor-Reg B-HyperparameterName
( O
Section O
4.2 O
) O
, O
we O
use O
the O
correlation O
matrix O
, O
rather O
than O
the O
covariance O
matrix O
, O
to O
characterize O
isotropy B-MetricName
, O
although O
the O
latter O
contains O
more O
informationvariance. O
The O
reason O

is O
that O
it O
is O
difficult O
to O
determine O
the O
proper O
scale O
of O
the O
variances. O
Here O
, O
we O
conduct O
experiments O
using O
the O
covariance O
matrix O
, O
by O
pushing O
the O
non-diagonal B-HyperparameterName
elements I-HyperparameterName
( O
covariances B-HyperparameterName
) O
towards O
0 B-HyperparameterValue
and O
the O
diagonal B-HyperparameterName
elements I-HyperparameterName
( O
variances B-HyperparameterName
) O
towards O
1 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
or O
the O
mean O
value O
, O

which O
are O
denoted O
by O
Cov-Reg-1 B-HyperparameterName
, O
Cov-Reg-0.5 B-HyperparameterName
, I-HyperparameterName
and O
Cov-Reg-mean B-HyperparameterName
respectively O
in O
Table O
8. O
It O
can O
be O
seen O
that O
all O
the O
variants O
perform O
worse O
than O
Cor-Reg O
. O
Our O
method O
is O
complementary O
with O
batch O
normalization. O
Batch O
normalization O
( O
Ioffe O
and O
Szegedy O
, O
2015 O
) O
anisotropy O
problem O
via O
normalizing O
each O
dimension O
with O

unit O
variance. O
We O
find O
that O
combining O
our O
method O
with O
batch O
normalization O
yields O
better O
performance O
, O
as O
shown O
in O
Table O
9. O
The O
performance O
gain O
is O
not O
from O
the O
reduction O
in O
model O
variance. O
Regularization O
techniques O
such O
as O
L1 O
regularization O
( O
Tibshirani O
, O
1996 O
) O
andL2 O
regularization O
( O
Hoerl O
andKennard O
, O
1970 O
) O

are O
often O
used O
to O
improve O
model O
performance O
by O
reducing O
model O
variance. O
Here O
, O
we O
show O
that O
the O
performance O
gain O
of O
our O
method O
is O
ascribed O
to O
the O
improved O
isotropy B-MetricName
( O
Table O
7 O
) O
rather O
than O
the O
reduction O
in O
model O
variance. O
To O
this O
end O
, O
we O
compare O
our O
method O
against O
L2 O
regularization O

with O
a O
wide O
range O
of O
weights O
, O
and O
it O
is O
observed O
that O
reducing O
model O
variance O
can O
not O
achieve O
comparable O
performance O
to O
our O
method O
, O
as O
shown O
in O
Fig. O
6. O
The O
computational O
overhead O
is O
small. O
To O
analyze O
the O
computational O
overheads O
incurred O
by O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
, O
we O
decompose O
the O
duration O
of O

one O
epoch O
of O
our O
method O
using O
the O
two O
regularizers O
jointly. O
As O
shown O
in O
Fig. O
7 O
, O
the O
overheads O
of O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
are O
small O
, O
only O
taking O
up O
a O
small O
portion O
of O
the O
time O
. O
Acknowledgments O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
comments. O
This O
research O

was O
supported O
by O
the O
grants O
of O
HK O
ITF O
UIM O
/ O
377 O
and O
PolyU O
DaSAIL O
project O
P0030935 O
funded O
by O
RGC O
. O


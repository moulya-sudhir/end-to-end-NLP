-DOCSTART- O
Semantic B-MethodName
Diversity I-MethodName
in I-MethodName
Dialogue I-MethodName
with I-MethodName
Natural I-MethodName
Language I-MethodName
Inference I-MethodName
Generating O
diverse O
, O
interesting O
responses O
to O
chitchat O
conversations O
is O
a O
problem O
for O
neural O
conversational O
agents. O
This O
paper O
makes O
two O
substantial O
contributions O
to O
improving O
diversity O
in O
dialogue B-TaskName
generation. I-TaskName
First O
, O
we O
propose O
a O
novel O
metric O
which O
uses O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( I-TaskName
NLI I-TaskName

) I-TaskName
to O
measure O
the O
semantic B-MetricName
diversity I-MetricName
of O
a O
set O
of O
model O
responses O
for O
a O
conversation. O
We O
evaluate O
this O
metric O
using O
an O
established O
framework O
( O
Tevet O
and O
Berant O
, O
2021 O
) O
and O
find O
strong O
evidence O
indicating O
NLI B-TaskName
Diversity B-MetricName
is O
correlated O
with O
semantic B-MetricName
diversity. I-MetricName
Specifically O
, O
we O
show O
that O
the O
contradiction O
relation O

is O
more O
useful O
than O
the O
neutral O
relation O
for O
measuring O
this O
diversity O
and O
that O
incorporating O
the O
NLI B-TaskName
model O
's O
confidence B-MetricName
achieves O
state-of-the-art O
results. O
Second O
, O
we O
demonstrate O
how O
to O
iteratively O
improve O
the O
semantic B-MetricName
diversity I-MetricName
of O
a O
sampled O
set O
of O
responses O
via O
a O
new O
generation O
procedure O
called O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
which O

results O
in O
an O
average O
137 B-MetricValue
% I-MetricValue
increase O
in O
NLI B-MetricName
Diversity O
compared O
to O
standard O
generation O
procedures O
. O
Introduction O
Dialogue O
models O
often O
struggle O
to O
produce O
engaging O
utterances O
in O
conversations O
, O
tending O
to O
generate O
responses O
which O
are O
common O
in O
the O
training O
data O
, O
such O
as O
" O
OK O
, O
" O
" O
Yeah O
, O
" O

or O
" O
I O
do O
n't O
know O
" O
( O
Li O
et O
al. O
, O
2016 O
) O
. O
While O
these O
responses O
are O
appropriate O
for O
a O
wide O
variety O
of O
contexts O
, O
their O
over-production O
can O
result O
in O
a O
dull O
conversation O
( O
See O
et O
al. O
, O
2019 O
) O
. O
An O
evaluation O
task O
has O
emerged O
that O
consists O

of O
measuring O
the O
diversity B-MetricName
of O
chitchat O
model O
responses O
over O
a O
test O
set. O
While O
some O
past O
work O
uses O
human B-MetricName
evaluation I-MetricName
to O
measure O
model O
response O
diversity B-MetricName
according O
to O
engagingness B-MetricName
, O
specificity B-MetricName
, O
or O
interestingness B-MetricName
( O
Li O
et O
al. O
, O
2016 O
; O
See O
et O
al. O
, O
2019 O
; O
Ghandeharioun O
et O
al. O
, O
2019 O

) O
, O
several O
automated O
metrics O
have O
also O
been O
proposed O
to O
measure O
diversity O
of O
model O
responses. O
Some O
metrics O
measure O
lexical B-MetricName
diversity I-MetricName
, O
typically O
via O
n-gram B-MetricName
overlap I-MetricName
( O
Li O
Figure O
1 O
: O
Illustration O
of O
NLI O
Diversity O
using O
human O
responses O
from O
DailyDialog++. O
Contradictions O
are O
weighted O
by O
1 O
, O
entailments O
by O
-1 O
, O
and O

neutrals O
by O
0 O
, O
so O
the O
score O
is O
( O
2 O
× O
1 O
) O
+ O
( O
3 O
× O
0 O
) O
+ O
( O
1 O
× O
−1 O
) O
= O
1. O
et O
al. O
, O
2016 O
) O
or O
computing O
the O
BLEU B-MetricName
score O
( O
Zhu O
et O
al. O
, O
2018 O
) O
among O
model O
responses O
generated O
from O
the O

test O
set. O
Other O
past O
work O
attempts O
to O
measure O
semantic B-MetricName
diversity I-MetricName
via O
repurposing O
sentence B-MetricName
similarity I-MetricName
metrics O
( O
Tevet O
and O
Berant O
, O
2021 O
; O
Zhang O
et O
al. O
, O
2020a O
; O
Cer O
et O
al. O
, O
2017 O
) O
. O
We O
propose O
a O
new O
metric O
aimed O
at O
measuring O
semantic B-MetricName
diversity I-MetricName
by O
leveraging O
a O
Natural B-TaskName
Language I-TaskName

Inference I-TaskName
( I-TaskName
NLI I-TaskName
) I-TaskName
model O
to O
score O
a O
set O
of O
multiple O
dialogue O
model O
responses O
for O
a O
single O
conversation O
, O
as O
illustrated O
in O
Figure O
1. O
NLI B-TaskName
is O
a O
three-way O
classification O
task O
to O
determine O
whether O
one O
sentence O
entails O
, O
contradicts O
, O
or O
is O
neutral O
toward O
a O
second O
sentence. O
We O
hypothesize O
that O
a O

diverse O
set O
of O
responses O
for O
a O
conversation O
captures O
contradictory O
ways O
one O
could O
respond O
, O
which O
can O
be O
measured O
by O
the O
NLI B-TaskName
model. O
We O
aggregate O
the O
contradiction O
, O
neutral O
, O
and O
entailment O
predictions O
among O
pairs O
of O
responses O
from O
the O
set O
and O
combine O
the O
predictions O
into O
a O
new O
diversity O
metric O
, O
called O

NLI B-MetricName
Diversity I-MetricName
. O
We O
additionally O
explore O
two O
modifications O
of O
NLI B-MetricName
Diversity. I-MetricName
First O
, O
because O
the O
neutral O
prediction O
may O
be O
indicative O
of O
diversity O
, O
we O
propose O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
where O
neutral O
predictions O
are O
weighted O
the O
same O
as O
contradiction O
predictions. O
Second O
, O
since O
our O
Baseline O
NLI B-MetricName
Diversity I-MetricName
method O
does O
not O
take O

into O
account O
the O
confidence B-MetricName
of O
the O
model O
's O
prediction O
, O
we O
propose O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
which O
aggregates O
the O
probability O
mass O
of O
the O
model O
's O
predicted O
class O
instead O
of O
aggregating O
the O
number O
of O
predictions O
for O
each O
class O
. O
We O
assess O
NLI B-MetricName
Diversity I-MetricName
using O
Tevet O
and O
Berant O
( O
2021 O
) O
's O

diversity O
metric O
evaluation O
framework O
, O
finding O
that O
NLI B-MetricName
Diversity I-MetricName
is O
highly O
correlated O
both O
with O
human O
judgments O
of O
diversity B-MetricName
and O
with O
the O
diversity B-MetricName
parameter O
, O
a O
gold O
standard O
diversity B-MetricName
value O
used O
to O
generate O
the O
set O
of O
responses. O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
state-of-the-art O
performance O
in O
terms O
of O
correlation O
with O
semantic B-MetricName
diversity. I-MetricName
Also O

, O
through O
an O
ablation O
study O
, O
we O
find O
positive O
, O
neutral O
, O
and O
negative O
correlations O
between O
human O
judgments O
and O
the O
number O
of O
contradiction O
, O
neutral O
, O
and O
entailment O
predictions O
, O
respectively O
. O
We O
next O
explore O
the O
use O
of O
a O
dialogue O
model O
to O
generate O
a O
set O
of O
candidate O
responses O
with O
a O

minimum O
target O
level O
of O
semantic B-MetricName
diversity I-MetricName
, O
such O
as O
10 B-MetricValue
Contradictions. O
Our O
new O
generation O
procedure O
, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
iteratively O
improves O
a O
set O
of O
model O
responses O
until O
this O
intended O
threshold O
is O
reached. O
If O
a O
set O
of O
sampled O
responses O
does O
not O
meet O
the O
intended O
threshold O
, O
the O
lowest-scoring O
response O
is O

thrown O
out O
and O
a O
new O
response O
is O
sampled O
until O
the O
diversity O
threshold O
is O
reached. O
We O
show O
this O
procedure O
results O
in O
a O
more O
diverse O
set O
of O
responses O
than O
the O
original O
sampled O
set O
, O
often O
with O
only O
a O
few O
resampled O
responses. O
Results O
of O
automated O
analysis O
shows O
relevancy O
is O
maintained O
from O
initial O
to O

final O
sets O
of O
responses O
. O
In O
summary O
, O
our O
contributions O
are O
: O
• O
A O
novel O
diversity O
metric O
, O
NLI B-MetricName
Diversity I-MetricName
, O
evaluated O
using O
Tevet O
and O
Berant O
( O
2021 O
) O
Measuring O
Model O
Response O
Diversity O
Traditionally O
, O
a O
model O
's O
diversity B-MetricName
has O
been O
measured O
in O
terms O
of O
its O
predictions O
over O
the O
test O

set O
( O
Li O
et O
al. O
, O
2016 O
) O
, O
which O
we O
call O
Test B-MetricName
Set I-MetricName
Diversity. I-MetricName
In O
this O
setup O
, O
the O
model O
predicts O
one O
response O
for O
each O
conversation O
in O
the O
test O
set O
( O
containing O
n O
conversations O
) O
, O
resulting O
in O
n O
predictions. O
The O
diversity B-MetricName
measure O
is O
computed O
over O
these O
n O
predictions O

, O
resulting O
in O
a O
score O
over O
the O
entire O
test O
set O
. O
The O
notion O
of O
diversity B-MetricName
we O
investigate O
, O
however O
, O
measures O
the O
model O
's O
ability O
to O
generate O
a O
set O
of O
responses O
for O
a O
single O
conversation O
Tevet O
and O
Berant O
, O
2021 O
) O
, O
which O
we O
call O
Multi-Response B-MetricName
Diversity. I-MetricName
Instead O
of O
generating O

one O
response O
for O
each O
of O
the O
conversations O
in O
the O
test O
set O
, O
we O
evaluate O
a O
model O
's O
ability O
to O
generate O
m O
responses O
for O
each O
of O
the O
n O
conversations O
. O
As O
shown O
by O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
metrics O
which O
have O
been O
proposed O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting O

can O
still O
be O
applied O
in O
the O
Multi-Response O
Diversity O
setting O
, O
however O
, O
by O
treating O
each O
set O
of O
m O
responses O
as O
its O
own O
" O
test O
set O
" O
and O
averaging O
over O
the O
n O
total O
sets O
. O
Diversity O
Metrics O
Lexical B-MetricName
diversity I-MetricName
metrics O
measure O
differences O
in O
word O
choice O
, O
as O
opposed O
to O
diversity O
of O

content. O
Li O
et O
al. O
( O
2016 O
) O
propose O
distinct-n O
, O
which O
measures O
the O
number O
of O
unique O
n-grams O
generated O
divided O
by O
the O
total O
number O
of O
n-grams O
generated O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting. O
Some O
past O
work O
has O
applied O
this O
metric O
to O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
( O
Tevet O
and O
Berant O
, O
2021 O

) O
. O
Cao O
and O
Clark O
( O
2017 O
) O
propose O
examining O
the O
percent O
of O
unique O
responses O
over O
the O
test O
set. O
Other O
past O
work O
has O
proposed O
using O
BLEU B-MetricName
score O
over O
a O
set O
of O
model O
responses O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting O
( O
Zhu O
et O
al. O
, O
2018 O
) O
. O
Semantic B-MetricName
diversity I-MetricName
metrics O

, O
on O
the O
other O
hand O
, O
compare O
diversity B-MetricName
of O
the O
content O
present O
in O
each O
response. O
Many O
of O
these O
measures O
are O
adapted O
from O
semantic B-MetricName
similarity I-MetricName
scores I-MetricName
, O
since O
lower O
similarity B-MetricName
can O
indicate O
higher O
diversity B-MetricName
( O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
BERTScore B-MetricName
measures O
the O
similarity O
of O
BERT O
embeddings O
for O
each O

token O
in O
two O
sentences O
( O
Zhang O
et O
al. O
, O
2020a O
) O
. O
Bert-STS B-MetricName
assigns O
a O
score O
based O
on O
the O
semantic B-MetricName
similarity I-MetricName
of O
two O
sentences O
( O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
The O
Sent-BERT B-MetricName
metric O
computes O
cosine O
similarity O
between O
BERT O
sentence O
embeddings O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O

Larson O
et O
al. O
( O
2019 O
) O
propose O
identifying O
diverse O
paraphrases O
by O
identifying O
embedding O
outliers O
. O
Other O
past O
work O
has O
used O
human B-MetricName
evaluation I-MetricName
to O
measure O
a O
model O
's O
diversity. B-MetricName
Li O
et O
al. O
( O
2016 O
) O
ask O
humans O
to O
choose O
the O
better O
of O
two O
responses O
based O
on O
specificity O
to O
the O
past O
conversation. O

See O
et O
al. O
( O
2019 O
) O
ask O
humans O
to O
rank O
dialogue O
responses O
on O
a O
variety O
of O
factors O
, O
including O
interestingness B-MetricName
and O
inquisitiveness. B-MetricName
Tevet O
and O
Berant O
( O
2021 O
) O
compare O
participants O
' O
ability O
to O
judge O
diversity B-MetricName
of O
a O
set O
of O
responses O
in O
two O
ways O
: O
( O
i O
) O
by O
ranking O
one O

response O
as O
more O
diverse O
than O
a O
second O
response O
and O
( O
ii O
) O
by O
judging O
the O
diversity O
of O
a O
single O
response O
on O
a O
Likert B-MetricName
scale I-MetricName
, O
finding O
that O
participants O
were O
equally O
able O
to O
judge O
diversity B-MetricName
in O
both O
conditions. O
They O
also O
find O
that O
human O
judges O
are O
better O
at O
distinguishing O
semantic B-MetricName
diversity I-MetricName
than O

lexical B-MetricName
diversity I-MetricName
. O
Other O
past O
work O
has O
incorporated O
diversity B-MetricName
metrics O
into O
the O
dialogue B-MethodName
dataset I-MethodName
creation I-MethodName
pipeline. I-MethodName
Stasaski O
et O
al. O
( O
2020 O
) O
propose O
a O
method O
which O
measures O
the O
diversity B-MetricName
of O
a O
crowdworker O
's O
contributions O
compared O
to O
a O
corpus O
, O
using O
that O
information O
to O
determine O
when O
to O
stop O
collecting O
data O
from O

the O
worker. O
This O
results O
in O
a O
more O
diverse O
dataset O
. O
Evaluation O
of O
Diversity B-MetricName
Metrics I-MetricName
Tevet O
and O
Berant O
( O
2021 O
) O
propose O
a O
framework O
to O
examine O
the O
reliability O
of O
diversity O
metrics. O
They O
propose O
the O
notion O
of O
a O
diversity O
parameter O
, O
which O
is O
used O
to O
generate O
a O
set O
of O
model O
responses O
, O

e.g. O
, O
the O
p-value O
in O
nucleus O
sampling O
, O
which O
specifies O
the O
vocabulary O
probability O
distribution O
cutoff O
used O
to O
restrict O
sampling O
to O
the O
most-likely O
words O
whose O
combined O
likelihood O
≥ O
p. O
If O
p O
is O
higher O
, O
the O
set O
of O
responses O
should O
have O
higher O
diversity B-MetricName
, O
and O
viceversa. O
This O
diversity O
parameter O
is O
treated O
as O

a O
gold O
standard O
for O
a O
set O
of O
responses O
' O
diversity. B-MetricName
Diversity B-MetricName
metrics O
assign O
scores O
in O
the O
Multi-Response B-MetricName
Diversity I-MetricName
condition O
and O
are O
evaluated O
in O
terms O
of O
correlation O
to O
the O
diversity O
parameter. O
They O
further O
propose O
two O
datasets O
to O
evaluate O
diversity O
metrics O
: O
one O
which O
includes O
model O
responses O
and O
contains O
varying O
levels O
of O

lexical B-MetricName
diversity I-MetricName
and O
one O
which O
is O
human-created O
and O
maintains O
high O
lexical B-MetricName
diversity I-MetricName
to O
allow O
focused O
evaluation O
of O
semantic B-MetricName
diversity I-MetricName
. O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
is O
a O
task O
aimed O
at O
predicting O
whether O
one O
sentence O
contradicts O
, O
entails O
, O
or O
is O
neutral O
towards O
a O
second O
sentence. O
Models O
for O
NLI B-TaskName
are O

typically O
trained O
using O
one O
of O
two O
datasets O
: O
Stanford B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
SNLI B-DatasetName
) O
( O
Bowman O
et O
al. O
, O
2015 O
) O
or O
Multi-Genre B-DatasetName
NLI I-DatasetName
( O
MNLI B-DatasetName
) O
( O
Williams O
et O
al. O
, O
2018 O
) O
. O
More O
recent O
datasets O
include O
FEVER B-DatasetName
( O
Thorne O
et O
al. O
, O
2018 O
; O
Nie O
et O

al. O
, O
2019 O
) O
, O
adapted O
from O
a O
fact-checking O
dataset O
, O
and O
ANLI B-DatasetName
( O
Nie O
et O
al. O
, O
2020 O
) O
, O
collected O
in O
an O
adversarial O
human-in-the-loop O
procedure. O
With O
the O
rise O
of O
transformer O
architectures O
, O
models O
have O
achieved O
high O
performance O
on O
NLI B-TaskName
tasks O
( O
Liu O
et O
al. O
, O
2019 O
) O
. O

In O
a O
dialogue O
setting O
, O
NLI B-TaskName
has O
been O
used O
to O
improve O
consistency O
between O
a O
persona O
and O
model O
responses O
over O
the O
course O
of O
a O
conversation O
by O
integrating O
an O
NLI-based B-TaskName
reward O
into O
a O
reinforcement B-MethodName
learning I-MethodName
training I-MethodName
procedure O
( O
Song O
et O
al. O
, O
2020 O
) O
. O
To O
our O
knowledge O
, O
however O
, O
NLI B-TaskName

has O
not O
been O
used O
to O
measure O
the O
diversity B-MetricName
of O
model O
responses O
in O
either O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
or O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
. O
Generating O
Diverse O
Sets O
of O
Hypotheses O
While O
work O
has O
only O
recently O
begun O
to O
explore O
the O
task O
of O
generating O
multiple O
dialogue O
responses O
to O
a O
conversation O
Tevet O
and O
Berant O
, O

2021 O
) O
, O
past O
work O
has O
explored O
generating O
diverse O
sets O
of O
hypotheses O
in O
some O
other O
application O
areas. O
Carbonell O
and O
Goldstein O
( O
1998 O
) O
explored O
using O
Maximal B-MethodName
Mutual I-MethodName
Relevance I-MethodName
to O
reduce O
redundancy O
without O
sacrificing O
relevancy O
in O
document B-TaskName
selection I-TaskName
for O
summarization. B-TaskName
Batra O
et O
al. O
( O
2012 O
) O
proposed O
a O
greedy O
iterative O
algorithm O

to O
generate O
diverse O
, O
probable O
hypotheses O
for O
multiple O
vision O
tasks. O
Most O
related O
to O
our O
work O
is O
Gimpel O
et O
al. O
( O
2013 O
) O
, O
which O
applied O
Batra O
et O
al. O
( O
2012 O
) O
's O
approach O
to O
machine B-TaskName
translation I-TaskName
, O
generating O
a O
set O
of O
translations O
instead O
of O
a O
single O
translation. O
In O
contrast O
to O

Gimpel O
et O
al. O
( O
2013 O
) O
, O
by O
holding O
the O
sampling O
procedure O
constant O
throughout O
the O
iterative O
process O
, O
our O
method O
can O
explore O
the O
extent O
to O
which O
diversity B-MetricName
can O
be O
increased O
without O
altering O
standard O
decoding O
practices O
. O
NLI B-MetricName
Diversity I-MetricName
Metric I-MetricName
We O
propose O
three O
diversity O
metrics O
in O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
which O

leverage O
the O
predictions O
of O
an O
NLI O
model. O
Two O
metrics O
( O
Baseline O
and O
Neutral O
) O
aggregate O
the O
NLI O
model O
's O
class O
predictions O
and O
one O
metric O
( O
Confidence O
) O
aggregates O
the O
weight O
of O
these O
predictions O
. O
Baseline O
NLI B-MetricName
Diversity I-MetricName
We O
propose O
a O
new O
metric O
, O
called O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
which O
uses O

an O
NLI B-TaskName
model O
's O
predictions O
to O
measure O
diversity. B-MetricName
More O
formally O
, O
for O
a O
given O
conversation O
, O
c O
, O
and O
a O
dialogue B-TaskName
generation I-TaskName
model O
M O
, O
a O
set O
of O
utterances O
u O
1 O
, O
... O
, O
u O
n O
is O
produced O
by O
the O
model. O
Each O
pair O
of O
utterances O
is O
compared O
in O
both O
directions O

using O
an O
NLI B-MetricName
model O
, O
N O
LI O
( O
u O
1 O
, O
u O
2 O
) O
, O
N O
LI O
( O
u O
2 O
, O
u O
1 O
) O
, O
... O
, O
N O
LI O
( O
u O
n O
, O
u O
n−1 O
) O
. O
The O
NLI B-TaskName
model O
predicts O
a O
distribution O
over O
the O
three O
potential O
classes O
: O
contradiction O

, O
neutral O
, O
and O
entailment. O
We O
take O
the O
argmax O
over O
these O
classes O
, O
resulting O
in O
a O
list O
of O
NLI B-TaskName
predictions O
, O
N O
LI O
preds O
( O
N O
LI O
( O
u O
1 O
, O
u O
2 O
) O
, O
... O
, O
N O
LI O
( O
u O
n−1 O
, O
u O
n O
) O
) O
of O
size O
n O

( O
n O
− O
1 O
) O
. O
To O
produce O
an O
overall O
diversity B-MetricName
score I-MetricName
for O
N O
LI O
preds O
( O
u O
1 O
, O
... O
, O
u O
n O
) O
, O
we O
assign O
each O
of O
these O
classes O
a O
value O
representing O
their O
diversity O
, O
denoted O
N B-MetricName
LI I-MetricName
score I-MetricName
( O
N O
LI O
preds O
( O
u O
1 O
, O

... O
, O
u O
n O
) O
) O
. O
We O
hypothesize O
that O
larger O
numbers O
of O
entailment O
predictions O
found O
in O
a O
set O
of O
model-generated O
utterances O
is O
indicative O
of O
a O
lack O
of O
diversity O
; O
similarly O
, O
larger O
number O
of O
contradiction O
predictions O
is O
indicative O
of O
a O
larger O
amount O
of O
diversity. O
Because O
we O
want O
a O
higher O

value O
of O
N B-MetricName
LI I-MetricName
score I-MetricName
to O
indicate O
higher O
diversity B-MetricName
, O
we O
assign O
values O
as O
: O
N B-MetricName
LI I-MetricName
score I-MetricName
= O
 O
 O
 O
 O
 O
1 O
if O
contradiction O
0 O
if O
neutral O
-1 O
if O
entailment O
The O
sum O
of O
the O
N B-MetricName
LI I-MetricName
score I-MetricName
values O
for O
the O
set O
of O
utterances O
results O
in O
the O
final O

NLI B-MetricName
Diversity I-MetricName
score I-MetricName
, O
formally O
defined O
as O
: O
Baseline B-MetricName
N I-MetricName
LIDiversity I-MetricName
= O
u O
i O
, O
u O
j O
∈u O
1 O
, O
... O
, O
un O
N B-MetricName
LI I-MetricName
score I-MetricName
( O
N O
LI O
pred O
( O
N O
LI O
( O
u O
i O
, O
u O
j O
) O
) O
While O
the O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
metric O
aggregates O
all O
classes O

, O
we O
also O
investigate O
the O
separate O
number O
of O
entailment O
, O
contradiction O
, O
and O
neutral O
predictions O
in O
N O
LI O
preds O
, O
denoted O
# O
Entailment O
, O
# O
Contradiction O
, O
and O
# O
Neutral O
, O
respectively O
. O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
Our O
primary O
hypothesis O
is O
that O
contradictions O
indicate O
diversity O
and O
entailments O
indicate O
lack O
of O
diversity. B-MetricName

Because O
it O
is O
unclear O
what O
the O
role O
of O
neutrals O
might O
be O
, O
we O
explore O
a O
version O
of O
NLI B-MetricName
Diversity I-MetricName
which O
weights O
neutral O
and O
contradiction O
predictions O
as O
equally O
diverse. O
This O
metric O
is O
the O
same O
as O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
except O
the O
N B-MetricName
LI I-MetricName
score I-MetricName
used O
to O
assign O
values O
is O
: O
N B-MetricName
LI I-MetricName

score_neutral I-MetricName
= O
 O
 O
 O
 O
 O
1 O
if O
contradiction O
1 O
if O
neutral O
-1 O
if O
entailment O
Confidence B-MetricName
NLI B-MetricName
Diversity I-MetricName
Because O
the O
prior O
two O
NLI B-MetricName
Diversity I-MetricName
metrics O
do O
not O
incorporate O
the O
confidence O
of O
the O
NLI O
model O
's O
class O
predictions O
, O
we O
explore O
an O
additional O
metric O
which O
incorporates O
this O
value. O
Letting O
conf O

class O
( O
u O
1 O
, O
u O
2 O
) O
represent O
the O
model O
's O
probability O
mass O
assigned O
to O
the O
predicted O
NLI O
class O
after O
sof O
tmax O
, O
the O
function O
is O
defined O
as O
: O
N B-MetricName
LI I-MetricName
score_conf I-MetricName
idence I-MetricName
= O
 O
 O
 O
 O
 O
1 O
× O
conf O
con O
( O
u O
1 O
, O
u O
2 O

) O
if O
contradiction O
0 O
if O
neutral O
-1 O
× O
conf O
ent O
( O
u O
1 O
, O
u O
2 O
) O
if O
entailment O
Intuitively O
, O
instead O
of O
assigning O
a O
1 O
value O
for O
a O
contradiction O
prediction O
, O
this O
metric O
assigns O
the O
probability O
of O
the O
contradiction O
class. O
Likewise O
, O
instead O
of O
a O
-1 O
for O
an O
entailment O

prediction O
, O
this O
metric O
assigns O
the O
negative O
probability O
mass O
of O
the O
entailment O
class O
. O
Evaluation O
of O
NLI B-MetricName
Diversity I-MetricName
We O
evaluate O
NLI B-MetricName
Diversity I-MetricName
by O
computing O
the O
correlation O
between O
the O
metric O
and O
both O
human O
labels O
and O
diversity B-MetricName
parameter O
labels. O
Below O
we O
first O
describe O
the O
models O
and O
data O
and O
then O
present O
the O
results O

of O
the O
evaluation O
. O
Models O
We O
explore O
two O
NLI O
models O
: O
a O
Roberta-large B-MethodName
model O
( O
Liu O
et O
al. O
, O
2019 O
) O
Tevet O
and O
Berant O
( O
2021 O
) O
. O
Corresponding O
temperature B-HyperparameterValue
parameter O
( O
higher O
is O
more O
diverse O
) O
or O
semantic B-MetricName
and O
lexical B-MetricName
diversity I-MetricName
levels O
accompany O
each O
example. O
containing O
300M O
parameters. O
We O

refer O
to O
these O
models O
as O
NLI B-MethodName
Diversity I-MethodName
-MNLI I-MethodName
and O
NLI B-MethodName
Diversity I-MethodName
-Combined I-MethodName
, O
respectively. O
We O
do O
not O
employ O
additional O
fine-tuning O
of O
these O
models O
. O
Data O
There O
are O
two O
different O
English O
datasets O
released O
to O
evaluate O
diversity O
metrics O
in O
Tevet O
and O
Berant O
( O
2021 O
) O
: O
conTest B-DatasetName
and O
decTest B-DatasetName
, O
described O
in O

Table O
1. O
The O
conTest B-DatasetName
dataset O
is O
human-created O
and O
captures O
content O
, O
or O
semantic O
, O
diversity B-MetricName
independent O
of O
lexical B-MetricName
diversity. I-MetricName
Low-diversity O
examples O
in O
this O
dataset O
have O
high O
lexical B-MetricName
diversity I-MetricName
but O
low O
semantic B-MetricName
diversity. I-MetricName
This O
dataset O
was O
created O
by O
asking O
crowdworkers O
to O
generate O
sets O
of O
utterances O
with O
either O
low O
or O
high O
semantic B-MetricName

diversity I-MetricName
using O
varied O
language O
, O
in O
order O
to O
keep O
a O
high O
level O
of O
lexical B-MetricName
diversity I-MetricName
constant O
across O
both O
conditions O
. O
The O
decTest B-DatasetName
dataset O
includes O
model-generated O
responses O
, O
with O
diversity O
controlled O
by O
a O
decoding B-HyperparameterName
parameter O
, O
such O
as O
a O
temperature B-HyperparameterName
parameter. O
The O
dataset O
can O
include O
duplicate O
responses O
, O
and O
does O
not O

attempt O
to O
mediate O
lexical B-MetricName
diversity I-MetricName
; O
therefore O
, O
low-diversity O
examples O
in O
this O
dataset O
may O
reflect O
low O
lexical B-MetricName
as O
well O
as O
low O
semantic B-MetricName
diversity I-MetricName
. O
While O
the O
original O
dataset O
includes O
multiple O
generation O
tasks O
, O
we O
evaluate O
on O
the O
dialogue O
task O
, O
respGen B-TaskName
, O
which O
is O
drawn O
from O
Reddit O
conversations O
( O
Hashimoto O

et O
al. O
, O
2019 O
) O
3 O
. O
There O
are O
200 O
conversations O
for O
each O
of O
conTest B-DatasetName
and O
decTest B-DatasetName
for O
the O
respGen B-TaskName
task O
, O
with O
multiple O
responses O
for O
each O
conversation O
( O
5 O
for O
conTest O
, O
10 O
for O
decTest O
) O
. O
Diversity B-HyperparameterName
Parameter I-HyperparameterName
Correlation O
The O
diversity B-HyperparameterName
parameter I-HyperparameterName
from O
Tevet O
and O
Berant O
( O
2021 O

) O
represents O
either O
a O
parameter O
directly O
used O
to O
generate O
responses O
via O
a O
dialogue O
model O
, O
such O
as O
p O
in O
nucleus O
sampling O
, O
or O
a O
binary O
value O
indicating O
whether O
crowdworkers O
were O
instructed O
to O
generate O
a O
high-or O
low-diversity O
set O
of O
responses. O
A O
measure O
which O
is O
able O
to O
capture O
diversity O
will O
be O
positively O

correlated O
with O
this O
diversity O
parameter O
. O
Table O
2 O
shows O
Spearman O
's O
correlations O
between O
NLI B-MetricName
Diversity I-MetricName
and O
the O
diversity B-HyperparameterName
parameter. I-HyperparameterName
On O
the O
conTest B-DatasetName
semantic B-MetricName
diversity I-MetricName
dataset O
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
the O
highest O
correlation O
of O
all O
metrics O
( O
0.62 O
) O
and O
approaches O
human O
performance. O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
performs O
comparably O
to O
the O

top-performing O
automatic O
metric O
from O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
at O
0.59 B-MetricValue
correlation. O
We O
note O
the O
95 B-MetricValue
% I-MetricValue
confidence B-MetricName
intervals O
overlaps O
between O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Sent-BERT B-MetricName
, O
and O
human B-MetricName
judgements I-MetricName
, O
indicating O
a O
lack O
of O
significant O
differences O
( O
see O
Appendix O
A O
) O
. O
Although O

Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
does O
relatively O
poorly O
on O
conTest B-DatasetName
( O
0.24 O
) O
, O
it O
is O
the O
highest-performing O
NLI B-TaskName
metric O
on O
decTest O
( O
0.72 O
) O
, O
suggesting O
that O
incorporating O
neutral O
predictions O
may O
capture O
lexical O
instead O
of O
semantic B-MetricName
diversity I-MetricName
. O
A O
histogram O
of O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
values O
for O
low O
and O
high O
semantic B-MetricName
diversity I-MetricName

sets O
of O
responses O
is O
shown O
in O
Figure O
2. O
We O
note O
the O
lack O
of O
large O
overlap O
between O
the O
distributions O
of O
low O
and O
high O
semantic B-MetricName
diversity I-MetricName
data. O
In O
addition O
to O
Human B-MetricName
Correlation I-MetricName
In O
this O
subsection O
, O
we O
examine O
the O
NLI B-MetricName
Diversity I-MetricName
metric O
's O
correlation B-MetricName
to O
the O
human O
annotations O
collected O
by O
Tevet O
and O

Berant O
( O
2021 O
) O
. O
Each O
set O
of O
responses O
in O
conTest B-DatasetName
and O
decTest B-DatasetName
is O
scored O
by O
10 O
annotators O
from O
1 O
( O
not O
diverse O
at O
all O
) O
to O
5 O
( O
very O
diverse O
) O
with O
half-point O
increments. O
We O
compute O
correlation O
with O
respect O
to O
the O
averaged O
rating O
. O
In O
addition O
to O
NLI B-MetricName
Diversity I-MetricName

, O
we O
explore O
the O
prediction O
counts O
for O
each O
category. O
We O
expect O
that O
a O
higher O
# O
Entailment O
value O
will O
be O
negatively O
correlated O
with O
diversity B-MetricName
because O
the O
more O
pairs O
of O
responses O
that O
entail O
each O
other O
, O
the O
more O
similar O
the O
set O
of O
responses O
is. O
Similarly O
, O
we O
expect O
that O
a O
higher O
# O

Contradiction O
value O
will O
be O
positively O
correlated O
with O
diversity. O
Since O
the O
NLI B-MetricName
Diversity I-MetricName
metric O
incorporates O
both O
# O
Entailment O
and O
# O
Contradiction O
, O
we O
would O
expect O
this O
metric O
to O
be O
highly O
correlated O
with O
human O
judgments O
as O
well. O
Spearmean B-MetricName
's I-MetricName
ρ I-MetricName
rank I-MetricName
correlation O
results O
between O
our O
metrics O
and O
the O
human B-MetricName
diversity I-MetricName
scores I-MetricName
are O

shown O
in O
Table O
3. O
The O
highest-performing O
correlation O
for O
lexical B-MetricName
diversity I-MetricName
is O
the O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
( O
0.69 B-MetricValue
) O
. O
The O
highest-performing O
semantic B-MetricName
diversity I-MetricName
correlation O
is O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
( O
0.64 B-MetricValue
) O
. O
Additionally O
, O
Baseline O
and O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
correlations O
are O
stronger O
when O
evaluating O
with O
the O
conTest B-DatasetName
dataset O
than O
the O

decTest B-DatasetName
dataset O
( O
an O
increase O
of O
0.48 B-MetricValue
to O
0.63 B-MetricValue
for O
Baseline B-MetricName
MNLI I-MetricName
and O
0.41 B-MetricValue
to O
0.64 B-MetricValue
for O
Confidence B-MetricName
NLI I-MetricName
) O
, O
indicating O
these O
metrics O
are O
more O
correlated O
with O
human O
ratings O
of O
semantic B-MetricName
diversity I-MetricName
than O
lexical B-MetricName
diversity I-MetricName
. O
Across O
both O
datasets O
, O
# O
Entailment O
is O
negatively O
correlated O
with O
diversity B-MetricName
, O
# O

Neutral O
does O
not O
have O
a O
strong O
correlation O
, O
and O
# O
Contradiction O
is O
positively O
correlated O
, O
as O
hypothesized. O
This O
supports O
our O
motivation O
to O
use O
NLI B-TaskName
as O
a O
diversity O
metric O
. O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
We O
have O
verified O
that O
NLI B-MetricName
Diversity I-MetricName
is O
both O
able O
to O
capture O
semantic B-MetricValue
diversity I-MetricValue
and O
aligns O
with O
human B-MetricValue
judgements. I-MetricValue

We O
can O
additionally O
use O
NLI B-MetricName
Diversity I-MetricName
to O
define O
a O
straightforward O
desired O
diversity O
threshold O
, O
div O
thresh O
for O
a O
set O
of O
model-generated O
responses O
, O
u O
1 O
, O
... O
, O
u O
n O
. O
For O
example O
, O
we O
might O
intend O
there O
to O
be O
10 O
Contradictions O
within O
the O
set. O
We O
propose O
a O
generation O
procedure O

, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
designed O
to O
iteratively O
increase O
the O
diversity O
of O
a O
set O
of O
responses O
for O
a O
conversation O
. O
For O
a O
conversation O
, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
begins O
by O
sampling O
n O
responses. O
We O
score O
the O
diversity B-MetricName
of O
these O
responses O
using O
a O
diversity B-MetricName
metric O
, O
div_metric O
( O
u O
1 O
, O
... O

, O
u O
n O
) O
. O
If O
the O
diversity O
score O
falls O
above O
div O
thresh O
, O
the O
process O
is O
finished O
. O
If O
, O
however O
, O
the O
score O
falls O
below O
div B-HyperparameterName
thresh I-HyperparameterName
, O
we O
identify O
the O
model O
response O
which O
contributes O
least O
to O
the O
diversity O
score O
by O
calculating O
div_metric O
( O
u O
1 O
, O
... O

, O
u O
n−1 O
) O
for O
each O
sub-group O
of O
model O
responses O
of O
size O
n O
− O
1. O
We O
discard O
the O
model O
response O
not O
present O
in O
the O
highestscoring O
subgroup O
and O
resample O
a O
new O
response. O
We O
re-calculate O
div_metric O
( O
u O
1 O
, O
... O
, O
u O
n O
) O
and O
if O
div_metric O
( O
u O
1 O
, O

... O
, O
u O
n O
) O
> O
div B-HyperparameterName
thresh I-HyperparameterName
, O
the O
process O
finishes. O
We O
continue O
resampling O
until O
the O
maximum O
cutoff O
of O
S O
is O
reached O
. O
Evaluation O
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
Method O
Models O
and O
Datasets O
We O
experiment O
with O
two O
neural O
dialogue O
models O
, O
DialoGPT B-MethodName
( O
700M O
parameters O
) O
( O
Zhang O
et O
al. O

, O
2020b O
) O
4 O
and O
BlenderBot B-MethodName
1.0 O
( O
300M O
parameters O
) O
( O
Roller O
et O
al. O
, O
2021 O
) O
5 O
. O
We O
use O
the O
default O
Transformers O
implementation O
for O
each O
model O
( O
Wolf O
et O
al. O
, O
2020 O
) O
and O
do O
not O
fine-tune O
them. O
Runtime O
was O
between O
3 O
and O
36 O
hours O
on O
one O

Titan-X O
GPU O
. O
All O
experiments O
involve O
the O
dialogue O
model O
M O
generating O
5 O
responses O
for O
each O
conversation. O
The O
maximum O
number O
of O
samples O
, O
S O
, O
is O
set O
to O
20. O
All O
experiments O
are O
averaged O
over O
10 O
trials O
for O
stability O
. O
We O
evaluate O
each O
model O
on O
the O
development O
set O
of O
two O
public O
English O

conversational O
datasets O
: O
Dai-lyDialog++ B-DatasetName
( O
1,028 O
conversations O
) O
( O
Sai O
et O
al. O
, O
2020 O
; O
Li O
et O
al. O
, O
2017 O
) O
and O
EmpatheticDialogues B-DatasetName
( O
2,763 O
conversations O
) O
( O
Rashkin O
et O
al. O
, O
2019 O
) O
. O
DailyDia-log++ B-DatasetName
includes O
5 O
human-written O
responses O
per O
conversation O
, O
allowing O
for O
multi-reference O
comparison. O
We O
split O
each O

EmpatheticDialogues B-DatasetName
conversation O
at O
a O
random O
turn O
( O
consistent O
for O
all O
experiments O
) O
for O
generation. O
Since O
BlenderBot O
supports O
up O
to O
128 O
positional O
embeddings O
, O
we O
pass O
in O
the O
last O
128 O
tokens O
of O
the O
conversation O
for O
this O
condition O
. O
Metrics O
We O
evaluate O
three O
diversity O
metrics O
: O
two O
semantic O
diversity O
metrics O
, O
Baseline B-MetricName

NLI I-MetricName
Diversity I-MetricName
( O
Section O
3 O
) O
and O
Sent-BERT B-MetricName
( O
Reimers O
and O
Gurevych O
, O
2019 O
; O
Tevet O
and O
Berant O
, O
2021 O
) O
, O
and O
one O
lexical O
diversity O
metric O
, O
distinct-n B-MetricName
( O
Li O
et O
al. O
, O
2016 O
; O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
For O
Sent-BERT B-MetricName
, O
we O
compute O
the O
average B-MetricName

negative I-MetricName
cosine I-MetricName
similarity I-MetricName
between O
BERT O
sentence O
embeddings O
for O
each O
pair O
of O
responses. O
Like O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
for O
distinct-n B-MetricName
, O
we O
compute O
the O
average O
distinct O
n-grams O
from O
n O
∈ O
1 O
, O
2 O
, O
3 O
, O
4 O
, O
5 O
. O
Because O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
is O
more O
humaninterpretable O
than O

Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
we O
use O
this O
version O
for O
experimentation. O
For O
all O
NLI B-TaskName
Diversity O
experiments O
, O
div B-HyperparameterName
thresh I-HyperparameterName
is O
achieved O
when O
# O
Contradictions O
is O
greater O
than O
10 O
out O
of O
a O
total O
of O
20 O
pair-wise O
comparisons. O
For O
both O
Sent-BERT B-MetricName
and O
distinct-n B-MetricName
, O
however O
, O
we O
do O
not O
have O
a O
humanspecifiable O
threshold. O

We O
use O
empirical O
thresholds O
measured O
from O
the O
sets O
of O
5 O
human O
responses O
for O
each O
conversation O
in O
DailyDialog++. B-DatasetName
We O
choose O
the O
90th B-HyperparameterValue
percentile I-HyperparameterValue
for O
div B-HyperparameterName
thresh I-HyperparameterName
( O
0.98 B-HyperparameterValue
and O
-0.179 B-HyperparameterValue
for O
distinct-n B-MetricName
and O
Sent-BERT B-MetricName
, O
respectively O
) O
. O
We O
decode O
using O
nucleus O
sampling O
( O
p O
= O
0.9 O
) O
, O
as O
it O

has O
been O
shown O
to O
increase O
response O
diversity O
( O
Holtzman O
et O
al. O
, O
2020 O
) O
. O
However O
our O
method O
could O
be O
applied O
with O
other O
decoding O
procedures O
. O
In O
order O
to O
robustly O
evaluate O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
we O
measure O
both O
( O
i O
) O
whether O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
is O
able O
to O
generate O
more O

diverse O
sets O
of O
responses O
than O
was O
originally O
sampled O
and O
( O
ii O
) O
whether O
the O
increased O
diversity O
comes O
at O
the O
expense O
of O
decreased O
relevancy O
of O
the O
responses O
. O
Diversity O
Results O
We O
aim O
to O
measure O
whether O
the O
diversity O
of O
the O
5 O
responses O
from O
M O
increases O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
compared O
to O

the O
initial O
5 O
sampled O
responses. O
Diversity O
of O
the O
starting O
and O
ending O
sets O
of O
utterances O
is O
measured O
by O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
distinct-n B-MetricName
, O
or O
Sent-BERT. B-MetricName
We O
also O
report O
the O
number O
of O
sampled O
utterances O
required O
to O
reach O
div O
thresh O
. O
Results O
for O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
are O
shown O
in O
Table O
4. O
For O

every O
condition O
, O
we O
see O
an O
increase O
from O
starting O
to O
ending O
diversity O
; O
for O
NLI B-MetricName
Diversity I-MetricName
, O
this O
results O
in O
an O
average O
137 O
% O
increase. O
For O
most O
conditions O
, O
distinct-n B-MetricName
requires O
more O
samples O
than O
Sent-BERT B-MetricName
and O
Baseline O
NLI B-MetricName
Diversity I-MetricName
. O
We O
can O
use O
the O
results O
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
to O

probe O
differences O
in O
the O
models. O
In O
our O
experimental O
setup O
, O
DialoGPT B-MethodName
generates O
more O
diverse O
utterances O
across O
all O
conditions O
than O
BlenderBot. B-MethodName
The O
models O
change O
by O
similar O
proportions O
from O
starting O
to O
ending O
diversity O
using O
the O
NLI B-TaskName
metric. O
However O
, O
the O
starting O
diversity O
for O
BlenderBot B-MethodName
is O
far O
lower O
than O
DialoGPT B-MethodName
; O
the O
negative O

value O
for O
BlenderBot B-MethodName
indicates O
that O
a O
large O
number O
of O
entailment O
predictions O
were O
present O
in O
the O
starting O
response O
set O
. O
We O
can O
also O
examine O
differences O
between O
the O
datasets. O
For O
instance O
, O
we O
observe O
lower O
starting O
diversities O
for O
the O
Empathetic B-DatasetName
Dialogues I-DatasetName
dataset O
than O
for O
DailyDialog++ B-DatasetName
for O
both O
models. O
Additionally O
, O
the O
number O

of O
samples O
required O
for O
Em-patheticDialogues B-DatasetName
is O
consistently O
higher O
than O
for O
DailyDialog++. B-DatasetName
This O
is O
likely O
because O
div O
thresh O
for O
both O
datasets O
was O
calculated O
using O
human O
responses O
from O
DailyDialog++ B-DatasetName
, O
since O
EmpatheticDialogues B-DatasetName
does O
not O
include O
multiple O
human O
responses O
. O
Sampled O
responses O
can O
be O
seen O
in O
Appendix O
B O
and O
results O
reporting O
the O
average O

overlap O
from O
starting O
to O
ending O
sets O
of O
responses O
is O
in O
Appendix O
C. O
Appendix O
D O
includes O
results O
using O
beam O
search O
instead O
of O
nucleus O
sampling O
, O
and O
Appendix O
E O
reports O
the O
stability B-MetricName
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
. O
Relevance O
Results O
Since O
past O
work O
has O
documented O
a O
tradeoff O
between O
diversity B-MetricName
and O
relevancy B-MetricName
, O
we O

also O
report O
results O
for O
the O
relevancy O
of O
the O
starting O
and O
ending O
sets O
of O
responses O
for O
Diversity B-MethodName
Threshold I-MethodName
Generation. I-MethodName
We O
use O
two O
established O
relevancy O
metrics O
: O
BLEU B-MetricName
Score O
( O
Papineni O
et O
al. O
, O
2002 O
) O
6 O
and O
BERTScore B-MetricName
( O
Zhang O
et O
al. O
, O
2020a O
) O
7 O
. O
We O
show O
results O
on O

DailyDialog++ B-DatasetName
, O
which O
has O
multiple O
human-generated O
responses O
for O
comparison O
, O
which O
is O
more O
correlated O
to O
human O
judgements O
than O
single-reference O
evaluation O
( O
Gupta O
et O
al. O
, O
2019 O
) O
. O
Results O
are O
shown O
in O
Table O
5. O
The O
key O
takeaway O
is O
that O
the O
relevancy O
values O
remain O
virtually O
unchanged O
when O
using O
the O
Diversity B-MethodName
Threshold I-MethodName

Generation I-MethodName
procedure O
, O
according O
to O
both O
BLEU B-MetricName
score O
and O
BERTScore. B-MetricName
The O
average O
percent O
difference O
is O
0.08 O
% O
for O
BertScore B-MetricName
and O
1.1 O
% O
for O
BLEU B-MetricName
. O
Conclusion O
We O
propose O
a O
novel O
semantic O
diversity O
metric O
, O
NLI B-MetricName
Diversity I-MetricName
, O
which O
is O
highly O
correlated O
to O
human O
judgments. O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
state-ofthe-art O
results O

on O
measuring O
semantic O
diversity. O
We O
propose O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
to O
incentivize O
production O
of O
diverse O
sets O
of O
responses O
for O
a O
conversation. O
This O
results O
in O
more O
diverse O
sets O
of O
responses O
than O
originally O
sampled O
for O
multiple O
models O
, O
datasets O
, O
and O
metrics O
while O
maintaining O
relevancy O
, O
and O
can O
also O
be O
used O
to O
investigate O

a O
model O
's O
ability O
to O
produce O
diverse O
responses O
. O
A O
Confidence O
Interval O
Analysis O
We O
perform O
experimentation O
using O
bootstrapping O
to O
determine O
confidence B-MetricName
intervals O
for O
conTest B-DatasetName
correlations O
to O
the O
diversity O
parameter. O
We O
sample O
a O
dataset O
of O
110 O
elements O
( O
50 O
% O
of O
the O
original O
con-Test B-DatasetName
dataset O
's O
size O
) O
from O
conTest B-DatasetName
with O

replacement O
and O
compute O
corresponding O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
values O
using O
the O
sampled O
dataset O
for O
Sent-BERT B-MetricName
, O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, I-MetricName
and O
human B-MetricName
judgements. I-MetricName
We O
repeat O
this O
process O
1,000 O
times O
for O
stability B-MetricName
and O
calculate O
95 B-MetricValue
% I-MetricValue
Confidence B-MetricName
Intervals. O
The O
full O
conTest B-DatasetName
correlation O
value O
plotted O
with O
these O
intervals O
can O

be O
seen O
in O
Figure O
3. O
While O
the O
Confidence O
Interval O
values O
overlap O
between O
all O
4 O
conditions O
, O
the O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
distribution O
closely O
matches O
the O
human O
distribution O
. O
B O
Sampled O
Responses O
Table O
7 O
shows O
randomly-sampled O
examples O
from O
the O
DailyDialog++ B-DatasetName
dataset O
, O
created O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
with O
the O
DialoGPT B-MethodName
model O
and O

NLI B-MetricName
Diversity O
as O
the O
intended O
div_metric O
. O
C O
Average B-MetricName
Utterance I-MetricName
Overlap I-MetricName
We O
measure O
the O
number O
of O
utterances O
which O
occur O
in O
both O
the O
starting O
and O
ending O
sets O
of O
responses O
, O
called O
utterance B-MetricName
overlap. I-MetricName
A O
high O
utterance O
overlap O
represents O
a O
set O
of O
responses O
which O
did O
not O
need O
to O
be O
significantly O
changed O
to O

reach O
div B-HyperparameterName
thresh I-HyperparameterName
. O
For O
example O
, O
an O
utterance B-MetricName
overlap I-MetricName
of O
4 B-MetricValue
indicates O
that O
only O
1 O
response O
needed O
to O
be O
resampled O
( O
potentially O
multiple O
times O
) O
from O
the O
starting O
set O
to O
reach O
div B-HyperparameterName
thresh I-HyperparameterName
. O
Results O
are O
seen O
in O
Table O
6. O
Keeping O
in O
mind O
that O
higher O
Average O
Overlap O
indicates O
less O

resampling O
was O
needed O
, O
we O
note O
higher O
overlap O
for O
DialoGPT B-MethodName
than O
BlenderBot B-MethodName
1.0 I-MethodName
( O
with O
the O
exception O
of O
distinct-n B-MetricName
and O
EmpatheticDialogues B-DatasetName
) O
. O
D O
Beam O
Search O
We O
evaluate O
beam O
search O
's O
ability O
to O
generate O
diverse O
utterances O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
for O
DailyDialog++ B-DatasetName
and O
NLI B-TaskName
Diversity. B-MetricName
To O
compare O
nucleus O
sampling O
to O

beam O
search O
, O
we O
generate O
25 O
beams O
and O
consider O
these O
responses O
from O
most O
to O
least O
probable O
, O
i.e. O
if O
the O
5 O
most O
likely O
beams O
do O
not O
satisfy O
the O
diversity O
threshold O
, O
we O
remove O
the O
lowest-scoring O
beam O
and O
replace O
it O
with O
the O
6th O
most O
likely O
beam. O
We O
find O
the O
starting O
NLI B-MetricName

Diversity I-MetricName
for O
beam O
search O
is O
-5.05 B-MetricValue
, O
the O
ending O
diversity O
is O
5.35 B-MetricValue
, O
and O
an O
average O
of O
10.97 O
sampled O
utterances O
is O
required. O
While O
the O
NLI B-TaskName
Diversity B-MetricName
does O
improve O
from O
the O
starting O
to O
ending O
set O
of O
responses O
, O
beam O
search O
has O
a O
much O
lower O
ending O
diversity O
than O
nucleus O
sampling. O
While O
past O

work O
has O
confirmed O
that O
nucleus O
sampling O
is O
more O
lexically O
diverse O
than O
beam O
search O
using O
Self-BLEU B-MetricName
( O
Holtzman O
et O
al. O
, O
2020 O
) O
, O
our O
results O
confirm O
that O
nucleus O
sampling O
is O
also O
able O
to O
generate O
more O
semantically O
diverse O
utterances O
. O

-DOCSTART- O
Learning O
Natural O
Language O
Generation O
with O
Truncated B-MethodName
Reinforcement I-MethodName
Learning I-MethodName
Learning B-MethodName
for I-MethodName
Language I-MethodName
( I-MethodName
TrufLL I-MethodName
) I-MethodName
, O
an O
original O
approach O
to O
train O
conditional O
language O
models O
without O
a O
supervised O
learning O
phase O
, O
by O
only O
using O
reinforcement O
learning O
( O
RL O
) O
. O
As O
RL O
methods O
unsuccessfully O
scale O
to O
large O
action O
spaces O
, O
we O

dynamically O
truncate O
the O
vocabulary O
space O
using O
a O
generic O
language O
model. O
TrufLL B-MethodName
thus O
enables O
to O
train O
a O
language O
agent O
by O
solely O
interacting O
with O
its O
environment O
without O
any O
task-specific O
prior O
knowledge O
; O
it O
is O
only O
guided O
with O
a O
task-agnostic O
language O
model. O
Interestingly O
, O
this O
approach O
avoids O
the O
dependency O
to O
labelled O
datasets O
and O

inherently O
reduces O
pretrained O
policy O
flaws O
such O
as O
language O
or O
exposure O
biases. O
We O
evaluate O
TrufLL B-MethodName
on O
two O
visual O
question O
generation O
tasks O
, O
for O
which O
we O
report O
positive O
results O
over O
performance O
and O
language O
metrics O
, O
which O
we O
then O
corroborate O
with O
a O
human O
evaluation. O
To O
our O
knowledge O
, O
it O
is O
the O
first O
approach O

that O
successfully O
learns O
a O
language O
generation O
policy O
without O
pre-training O
, O
using O
only O
reinforcement O
learning. O
1 O
Introduction O
Since O
the O
development O
of O
generic O
language B-MethodName
models I-MethodName
trained O
on O
massive O
unlabelled O
text O
corpora O
Brown O
et O
al. O
, O
2020 O
) O
, O
state-of-the O
art O
language O
processing O
systems O
rely O
on O
sequential B-MethodName
transfer I-MethodName
learning I-MethodName
( O
Ruder O
, O
2019 O

) O
. O
The O
pretrained O
Language B-MethodName
Model I-MethodName
( O
LM B-MethodName
) O
is O
fine-tuned O
on O
the O
downstream O
task O
using O
a O
standard B-MethodName
supervised I-MethodName
learning I-MethodName
( I-MethodName
SL I-MethodName
) I-MethodName
1 O
Code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
AMDonati O
/ O
RL-NLP O
VQG O
, O
TrufLL B-MethodName
truncates O
the O
vocabulary O
space O
by O
using O
a O
language O
model. O
Here O

, O
'run O
, O
' O
and O
'the O
' O
are O
syntactically O
incorrect O
and O
thus O
truncated. O
Yet O
, O
'car O
' O
is O
not O
trimmed O
as O
the O
LM B-MethodName
is O
not O
visually O
grounded. O
( O
right O
) O
In O
a O
VQG B-MethodName
training O
loop O
, O
the O
agent O
generates O
a O
question O
given O
an O
image-answer O
pair O
, O
which O
is O
then O
fed O

to O
a O
VQA B-MethodName
model O
predicting O
an O
expected O
answer. O
If O
both O
answers O
match O
, O
the O
agent O
is O
rewarded O
. O
objective O
Peters O
et O
al. O
, O
2019 O
) O
. O
Yet O
, O
such O
an O
approach O
suffers O
from O
several O
issues O
: O
( O
i O
) O
catastrophic O
forgetting O
when O
a O
model O
forgets O
previously O
learned O
knowledge O
and O
overfits O

to O
target O
domains O
, O
( O
ii O
) O
computational O
inefficiency O
from O
fine-tuning O
billion-parameters O
networks O
, O
and O
( O
iii O
) O
the O
need O
of O
supervised O
datasets. O
Moreover O
, O
task-specific O
language B-MethodName
models I-MethodName
learned O
with O
SL B-MethodName
suffer O
from O
well-studied O
text B-TaskName
degeneration I-TaskName
issues O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
such O
as O
the O
exposure O
bias O

, O
language O
biases O
( O
Saleh O
et O
al. O
, O
2020 O
; O
, O
or O
a O
lack O
of O
diversity O
( O
Li O
et O
al. O
, O
2015 O
) O
. O
On O
the O
other O
hand O
, O
text B-TaskName
generation I-TaskName
can O
be O
naturally O
framed O
as O
a O
sequential B-TaskName
decision I-TaskName
making I-TaskName
problem O
, O
with O
the O
sequence O
of O
words O
seen O
as O
successive O

actions O
over O
a O
vocabulary. O
Thus O
, O
some O
researchers O
have O
recently O
focused O
on O
learning O
language O
models O
using O
instead O
Reinforcement B-MethodName
Learning I-MethodName
( I-MethodName
RL I-MethodName
) I-MethodName
Das O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
. O
RL B-MethodName
methods O
allow O
acquiring O
language O
through O
interactions O
within O
rich O
and O
diverse O
environments O
( O
Luketina O
et O

al. O
, O
2019 O
) O
, O
help O
understanding O
language O
acquisition O
and O
language O
pragmatics O
( O
Lazaridou O
et O
al. O
, O
2016 O
; O
Bisk O
et O
al. O
, O
2020 O
) O
. O
" O
Reward O
is O
enough O
" O
( O
Silver O
et O
al. O
, O
2021 O
) O
highlights O
the O
necessity O
of O
using O
RL B-MethodName
for O
AI O
systems O
to O
acquire O
language O

in O
its O
full O
richness. O
Indeed O
, O
( O
i O
) O
language O
may O
be O
intertwined O
with O
other O
modalities O
of O
action O
and O
observation O
, O
( O
ii O
) O
the O
utility O
of O
language O
varies O
according O
to O
situations O
and O
behaviours O
, O
( O
iii O
) O
it O
is O
consequential O
and O
purposeful O
, O
and O
( O
iv O
) O
some O
linguistic O

problems O
are O
better O
solved O
dynamically O
, O
through O
experience O
( O
such O
as O
using O
a O
diplomatic O
tone O
in O
a O
speech. O
) O
In O
addition O
, O
RL B-MethodName
allows O
optimizing O
a O
non-differentiable O
learning O
signal O
, O
hence O
handles O
more O
diverse O
objective O
functions O
, O
and O
also O
avoids O
some O
of O
the O
text B-TaskName
degeneration I-TaskName
issues O
previously O
mentioned. O
So O
far O

, O
RL-based B-MethodName
text-generation B-TaskName
tasks O
have O
relied O
on O
a O
pre-training O
phase O
to O
ease O
learning O
: O
the O
policy O
language O
model O
is O
trained O
with O
SL B-MethodName
on O
the O
task O
dataset O
, O
before O
being O
fine-tuned O
with O
policy O
gradient O
methods O
( O
Sutton O
et O
al. O
, O
1999 O
) O
on O
the O
task O
at O
hand. O
Those O
approaches O
often O
require O

human-labelled O
datasets. O
Besides O
, O
combining O
pre-training O
and O
fine-tuning O
phases O
either O
barely O
change O
the O
policy O
distribution O
, O
or O
induces O
language O
drift O
( O
Lazaridou O
et O
al. O
, O
2020 O
; O
Lu O
et O
al. O
, O
2020b O
) O
, O
i.e O
the O
generated O
language O
drifts O
semantically O
or O
syntactically O
from O
natural O
language O
. O
In O
this O
paper O
, O

we O
aim O
at O
learning O
a O
conditional O
language O
model O
using O
RL B-MethodName
without O
a O
pre-training O
phase O
, O
so O
that O
( O
i O
) O
we O
get O
free O
from O
datasets O
with O
human O
annotations O
, O
and O
( O
ii O
) O
we O
avoid O
the O
text B-TaskName
generation I-TaskName
flaws O
induced O
by O
the O
common O
methods. O
While O
appealing O
, O
such O
an O
approach O

requires O
overcoming O
the O
hurdle O
of O
the O
combinatorial O
language O
action O
space O
, O
a O
vocabulary O
usually O
containing O
more O
than O
10,000 O
words. O
Yet O
, O
while O
large O
and O
discrete O
, O
a O
language O
action O
space O
contains O
a O
specific O
structure O
, O
made O
of O
all O
the O
syntactical O
and O
semantics O
rules O
of O
a O
given O
language. O
TrufLL B-MethodName
leverages O
such O

structure O
to O
drive O
the O
exploration O
of O
the O
RL-based B-MethodName
language O
agent O
during O
training. O
At O
each O
time O
step O
of O
the O
text B-TaskName
generation I-TaskName
process O
, O
TrufLL B-MethodName
truncates O
its O
effective O
action O
space O
to O
a O
small O
subset O
of O
words O
provided O
by O
a O
pretrained O
task-agnostic O
language O
model. O
Such O
an O
approach O
injects O
a O
generic O
prior O
linguistic O
knowledge O

into O
the O
RL B-MethodName
algorithm O
, O
is O
usable O
on O
tasks O
lacking O
in-domain O
labeled O
data O
, O
and O
can O
be O
easily O
transferred O
to O
new O
RL-based B-MethodName
text O
generation O
tasks. O
Thus O
, O
TrufLL B-MethodName
can O
be O
applied O
to O
any O
language O
generation O
task O
given O
a O
generic O
LM O
and O
a O
reward. O
We O
here O
evaluate O
it O
on O
two O
Visual O

Question O
Generation O
( O
VQG O
) O
tasks O
, O
the O
synthetic O
CLEVR B-DatasetName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
, O
and O
the O
natural O
language O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
. O
Unlike O
alternative O
RL B-MethodName
without O
pre-training O
approaches O
, O
TrufLL B-MethodName
manages O
to O
ask O
meaningful O
and O
valid O
questions O
on O
large O

vocabularies O
, O
exhibiting O
success B-MetricName
rate I-MetricName
and O
language B-MetricName
metrics I-MetricName
close O
to O
pretrain O
models O
with O
labeled O
data O
, O
while O
producing O
more O
original O
language O
. O
TrufLL B-MethodName
We O
here O
aim O
at O
making O
RL B-MethodName
methods O
feasible O
in O
the O
language O
setting O
by O
dynamically O
reducing O
the O
action O
space O
, O
i.e. O
, O
by O
restricting O
the O
language O
agent O
to O

select O
a O
word O
within O
a O
subset O
of O
the O
vocabulary O
at O
each O
time O
step. O
We O
detail O
below O
the O
action O
space O
's O
truncation O
model O
and O
the O
associated O
RL B-MethodName
algorithm O
to O
learn O
the O
language O
agent O
. O
Dynamic B-MethodName
Vocabulary I-MethodName
Truncation I-MethodName
TrufLL I-MethodName
combines O
two O
distinct O
language O
models O
, O
which O
share O
the O
same O
vocabulary O
V O
: O

a O
RL O
language O
agent O
π O
θ O
and O
a O
pretrained O
language O
model O
f O
LM O
. O
At O
each O
timestep O
t O
, O
TrufLL B-MethodName
restricts O
the O
vocabulary O
space O
of O
the O
RL B-MethodName
language O
agent O
with O
: O
V O
− O
t O
= O
{ O
w|w O
∈V O
, O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
} O
, O
where O

g O
trunc O
is O
a O
truncation O
function O
based O
on O
f O
LM O
which O
either O
associates O
0 O
or O
1 O
with O
each O
word O
in O
the O
vocabulary O
given O
the O
past O
words O
w O
< O
t O
. O
From O
a O
language O
modelling O
perspective O
, O
the O
vocabulary O
space O
of O
the O
language O
agent O
is O
reduced O
from O
V O
to O
V O
− O

where O
|V O
− O
|≪|V| O
, O
with O
|•| O
the O
cardinal O
of O
a O
finite O
set. O
From O
a O
RL B-MethodName
perspective O
, O
the O
RL B-MethodName
agent O
follows O
a O
truncated O
policy O
π O
− O
θ O
which O
only O
samples O
actions O
over O
the O
subset O
V O
− O
. O
In O
practice O
, O
such O
a O
policy O
is O
computed O
using O
a O
masked O
softmax O

function O
over O
the O
truncated O
vocabulary O
V O
− O
t O
: O
π O
− O
θ O
( O
.|w O
< O
t O
, O
c O
) O
= O
softmax O
( O
m O
* O
logits O
π O
θ O
( O
w O
< O
t O
, O
c O
) O
) O
where O
m=1 O
when O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
otherwise O
m=−∞ O
. O
Truncation O

Functions O
We O
here O
list O
the O
different O
truncation O
functions O
g O
trunc O
explored O
through O
the O
paper O
. O
Top-k O
words O
: O
This O
function O
selects O
the O
k O
words O
with O
the O
highest O
probability O
given O
by O
f O
LM O
( O
.|w O
< O
t O
) O
: O
g O
top O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O

k O
) O
=1 O
wt∈top O
( O
k O
) O
( O
f O
LM O
( O
.|w O
< O
t O
) O
) O
. O
Probability O
threshold O
( O
α O
) O
: O
This O
function O
only O
keeps O
words O
having O
a O
probability O
f O
LM O
( O
.|w O
< O
t O
) O
greater O
than O
α O
: O
g O
p O
th O
( O
α O
) O
( O
w O

t O
|w O
< O
t O
; O
α O
) O
=1 O
f O
LM O
( O
wt|w O
< O
t O
) O
> O
α O
. O
Top-p B-HyperparameterName
: O
This O
function O
is O
based O
on O
nucleus O
sampling O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
and O
it O
keeps O
the O
most O
likely O
words O
contained O
in O
a O
probability O
mass O
p O
of O
f O

LM O
( O
.|w O
< O
t O
) O
. O
Formally O
, O
we O
define O
V O
p O
t O
as O
: O
V O
p O
t O
= O
g O
sample O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈ O
{ O
w O
i O
∼f O
LM O
( O
.|w O
< O
t O
) O
i∈ O
1 O
, O
... O

, O
k O
} O
. O
Only O
top B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
provides O
a O
fixed O
number O
of O
words O
at O
each O
time O
step. O
p B-HyperparameterName
th I-HyperparameterName
( I-HyperparameterName
α I-HyperparameterName
) I-HyperparameterName
, O
top B-HyperparameterName
( I-HyperparameterName
p I-HyperparameterName
) I-HyperparameterName
, O
and O
sample B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
have O
a O
dynamic O
truncation O
, O
whose O
size O
at O
t O
depends O
on O
the O
language O
model O
entropy O

. O
Task-Specific O
vs. O
Generic O
LM O
We O
benchmark O
two O
types O
of O
language O
models O
for O
truncation. O
On O
the O
one O
hand O
, O
we O
use O
an O
external O
language O
model O
pretrained O
on O
a O
large O
task-agnostic O
language O
corpora. O
Such O
a O
model O
provides O
a O
generic O
linguistic O
prior O
to O
the O
RL B-MethodName
agent O
exploration O
process O
, O
solely O
encoding O
syntactic O

and O
semantic O
information. O
On O
the O
other O
hand O
, O
we O
use O
a O
task-related O
language O
model O
pretrained O
on O
the O
supervised O
dataset O
associated O
with O
the O
task. O
Such O
a O
model O
provides O
a O
task-specific O
linguistic O
prior O
to O
the O
RL B-MethodName
language O
agent O
, O
and O
captures O
language O
pragmatics. O
We O
emphasize O
that O
this O
paper O
aims O
at O
leveraging O
taskagnostic O

language O
models O
as O
they O
discard O
the O
need O
for O
task-specific O
data. O
For O
the O
sake O
of O
completeness O
, O
we O
also O
study O
the O
truncation O
with O
the O
task-related O
LM O
as O
an O
additional O
benchmark O
to O
assess O
our O
approach O
. O
Experimental O
Setting O
We O
here O
list O
the O
experimental O
setting O
and O
detail O
the O
network O
and O
hyperparameters O
in O
Appendix O

A.4 O
. O
Visual O
Question O
Generation O
We O
showcase O
TrufLL B-MethodName
on O
the O
task O
of O
Visual B-MethodName
Question I-MethodName
Generation I-MethodName
( O
VQG B-MethodName
) O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
, O
which O
is O
a O
form O
of O
Visual O
Jeopardy O
! O
™ O
( O
Ferrucci O
, O
2012 O
) O
. O
There O
, O
the O
language O
agent O
observes O
an O
image-answer O
pair O

and O
has O
to O
generate O
a O
question O
that O
results O
in O
a O
similar O
answer O
, O
as O
illustrated O
in O
Figure O
1. O
Such O
a O
task O
presents O
multiple O
advantages. O
First O
, O
by O
combining O
vision O
, O
scene O
understanding O
and O
language O
generation O
, O
it O
requires O
high-level O
reasoning O
and O
exhibits O
a O
large O
spectrum O
of O
language O
difficulties. O
Secondly O
, O

the O
success O
criterion O
is O
naturally O
non-differentiable O
, O
hence O
a O
natural O
fit O
for O
RL B-MethodName
methods. O
Such O
a O
criterion O
, O
unlike O
metrics O
based O
on O
ground-truth O
sentences O
, O
allows O
generating O
diverse O
grounded O
questions O
given O
an O
image-answer O
pair O
. O
Formally O
, O
the O
initial O
context O
c O
is O
composed O
of O
the O
image-answer O
pair O
( O
I O
, O

A O
) O
. O
The O
RL B-MethodName
agent O
then O
generates O
a O
sequence O
of O
words O
w O
< O
t O
of O
maximum O
length O
T O
. O
We O
then O
provide O
the O
generated O
question O
to O
a O
pretrained O
VQA B-MethodName
model. O
This O
model O
takes O
as O
inputs O
the O
image O
I O
, O
the O
generated O
question O
w O
< O
t O
and O
outputs O
a O
predicted O

answerÂ. O
Finally O
, O
the O
agent O
receives O
a O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
based O
on O
A O
andÂ O
. O
Datasets O
We O
evaluate O
TrufLL B-MethodName
on O
the O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
datasets O
to O
simulate O
large-scale O
VQG B-DatasetName
datasets. O
The O
two O
datasets O
have O
been O
originally O
created O
for O
the O
task O
of O

Visual B-MethodName
Question I-MethodName
Answering I-MethodName
( I-MethodName
VQA I-MethodName
) I-MethodName
, O
i.e. O
for O
multi-modal O
classification O
algorithms O
predicting O
an O
answer O
given O
an O
image-question O
pair O
. O
CLEVR B-DatasetName
The O
CLEVR B-DatasetName
VQA B-MethodName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
is O
made O
of O
template O
questions O
on O
synthetic O
images O
, O
which O
contain O
simple O
objects O
with O
four O
distinct O
properties O
( O

shape O
, O
material O
, O
color O
, O
size O
) O
. O
The O
vocabulary O
contains O
86 O
words O
and O
28 O
potential O
answers O
, O
making O
it O
a O
valuable O
proof O
of O
concept O
for O
assessing O
TrufLL. B-MethodName
Both O
language O
models O
are O
single-layer O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
512 O
units O
, O
and O
512 O
word O
embedding O

dimension. O
The O
task-specific O
LM O
is O
trained O
over O
the O
full O
train O
dataset O
of O
CLEVR B-DatasetName
questions. O
The O
external O
language O
model O
is O
trained O
on O
the O
mixture O
of O
CLOSURE B-DatasetName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
and O
CLEVR-Dialog B-DatasetName
( O
Kottur O
et O
al. O
, O
2019 O
) O
datasets. O
Although O
those O
two O
datasets O
share O
the O
CLEVR B-DatasetName
vocabulary O

, O
their O
language O
distribution O
differs O
from O
vanilla B-DatasetName
CLEVR. I-DatasetName
Finally O
, O
we O
use O
a O
pretrained O
GT-Vector-NMN B-MethodName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
to O
compute O
the O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
A=Â O
, O
t=T O
−1 O
, O
where O
1 O
is O
the O
indicator O
function O

. O
VQAv2 B-DatasetName
The O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
is O
made O
of O
natural O
language O
and O
open-formed O
questions O
on O
images O
from O
the O
MS-Coco B-DatasetName
Dataset O
( O
Lin O
et O
al. O
, O
2014 O
) O
. O
It O
has O
a O
vocabulary O
of O
14,810 O
words O
and O
3,149 O
answers. O
The O
task-specific O
language O
model O
is O
a O

one-layer B-MethodName
LSTM I-MethodName
with O
512 O
units O
and O
a O
512 O
word O
embedding O
dimension O
, O
pretrained O
over O
the O
full O
training O
dataset O
of O
VQAv2 B-DatasetName
questions. O
The O
External O
Language O
Model O
is O
Open-AI B-MethodName
's I-MethodName
GPT-2 I-MethodName
. O
The O
original O
language O
model O
outputs O
a O
probability O
distribution O
over O
50,257 O
tokens O
, O
but O
we O
use O
a O
masked O
softmax O
function O
to O

restrict O
the O
probability O
distribution O
to O
the O
14,810 O
tokens O
of O
the O
VQAv2 B-DatasetName
dataset. O
Unlike O
most O
NLP B-MethodName
tasks O
relying O
on O
pretrained O
generic O
language O
models O
, O
we O
do O
not O
fine-tune O
it O
on O
the O
task O
dataset. O
Instead O
, O
we O
leverage O
the O
few-shot O
generalization O
capabilities O
of O
GPT-2 B-MethodName
, O
by O
feeding O
the O
language O
model O
with O
the O

prompt O
" O
Here O
are O
a O
few O
examples O
: O
" O
followed O
by O
100 O
random O
questions O
q O
< O
100 O
from O
the O
dataset. O
The O
truncation O
is O
then O
based O
on O
the O
probability O
distribution O
f O
gpt2 O
LM O
( O
.|q O
< O
100 O
, O
w O
< O
t O
) O
. O
Finally O
, O
we O
used O
a O
pretrained O
Vil-BERT B-MethodName
to O

compute O
the O
reward O
( O
Lu O
et O
al. O
, O
2020a O
) O
. O
Given O
the O
large O
number O
of O
answers O
, O
we O
use O
as O
reward O
a O
decreasing O
function O
of O
the O
rank O
of O
the O
reference O
answer O
rk O
( O
A O
) O
: O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O

rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
as O
further O
explained O
in O
Appendix O
A.5 O
. O
In O
these O
two O
settings O
, O
we O
acknowledge O
that O
the O
task O
dataset O
is O
still O
used O
to O
train O
the O
VQA B-MethodName
models. O
Please O
note O
that O
the O
VQA B-MethodName
modules O
are O
only O

used O
to O
model O
the O
environment O
, O
i.e. O
to O
provide O
a O
positive O
/ O
negative O
feedback O
to O
the O
agent. O
In O
other O
settings O
, O
TrufLL B-MethodName
would O
still O
work O
if O
we O
replace O
the O
VQA B-MethodName
model O
by O
any O
language O
interface O
: O
text-game O
( O
e.g. O
Zork O
) O
, O
expert-systems O
, O
or O
humans. O
Here O
, O
we O
only O

use O
the O
VQG O
framework O
as O
a O
proof O
of O
concept O
that O
natural O
language O
can O
be O
learned O
through O
pure O
interaction O
given O
any O
task O
reward. O
Other O
language O
generation O
applications O
are O
discussed O
in O
Section O
5.3 O
. O
Baselines O
In O
this O
paper O
, O
we O
aim O
to O
show O
that O
a O
RL B-MethodName
language O
agent O
can O
be O
trained O
from O

scratch O
, O
i.e. O
without O
the O
usual O
pre-training O
phase O
by O
solely O
interacting O
with O
another O
language O
system O
, O
the O
VQA B-MethodName
model O
, O
when O
supported O
by O
truncation O
methods. O
The O
truncation O
with O
the O
task-related O
LM O
is O
referred O
to O
as O
TrufLL B-MethodName
( O
Task-LM B-MethodName
) O
, O
while O
the O
one O
with O
the O
External O
LM O
is O
referred O
as O

TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
. O
We O
first O
emphasize O
the O
difficulty O
of O
training O
an O
RL B-MethodName
language O
agent O
without O
a O
supervised O
pre-training O
phase O
through O
two O
baselines. O
We O
trained O
a O
simple O
on-policy O
PPO O
algorithm O
without O
any O
action O
space O
pruning O
, O
and O
refer O
to O
it O
as O
scratch. O
Then O
, O
we O
added O
a O
Kullback-Leibler B-HyperparameterName
( I-HyperparameterName

KL I-HyperparameterName
) I-HyperparameterName
regularization I-HyperparameterName
term O
to O
the O
loss O
, O
λ O
KL B-HyperparameterName
KL B-HyperparameterName
( O
π O
θ O
||f O
LM O
) O
, O
with O
λ O
KL B-HyperparameterName
> O
0 O
, O
to O
incorporate O
language O
prior O
to O
the O
agent O
as O
in O
( O
Jaques O
et O
al. O
, O
2017 O
( O
Jaques O
et O
al. O
, O
, O
2019. O
We O
refer O
to O
it O

as O
scratch O
+ O
KL-task B-HyperparameterName
when O
distilling O
the O
task-specific O
language O
model O
, O
and O
scratch O
+ O
KL-ext B-HyperparameterName
with O
the O
external O
language O
model. O
Finally O
, O
we O
include O
two O
baselines O
with O
a O
pre-training O
phase. O
We O
trained O
a O
language O
agent O
on O
the O
task-dataset O
with O
a O
log-likelihood O
objective O
, O
and O
refer O
to O
it O
as O
pretrain. O
Then O

, O
we O
fine-tune O
the O
pretrained O
language O
agent O
with O
PPO O
without O
truncation O
, O
and O
refer O
to O
it O
as O
pretrain O
+ O
RL B-MethodName
fine-tune. O
These O
two O
baselines O
should O
be O
viewed O
as O
gold O
standards O
as O
they O
rely O
on O
task-related O
data O
; O
additionally O
, O
pretrain O
+ O
RL B-MethodName
fine-tune O
is O
today O
the O
state-of-the-art O
method O
for O
learning O

RL-based B-MethodName
LM O
. O
Metrics O
and O
Evaluation O
Methods O
Evaluating O
text O
generation O
is O
an O
open-research O
problem O
in O
language O
literature. O
We O
decompose O
automatic O
language O
evaluation O
into O
three O
categories O
to O
assess O
different O
facets O
of O
language O
, O
and O
perform O
as O
well O
a O
human O
evaluation O
study O
. O
Performance O
metrics. O
We O
measure O
the O
taskcompletion B-MetricName
score I-MetricName
or O
recall B-MetricName

@ O
1 B-MetricValue
which O
states O
whether O
the O
target O
answer O
A O
is O
the O
top O
answer O
of O
the O
VQA B-MethodName
models O
, O
and O
the O
recall B-MetricName
@ O
5 B-MetricValue
( O
R O
@ O
5 O
) O
, O
which O
assesses O
whether O
A O
is O
in O
the O
5 O
top O
answers. O
These O
scores O
measure O
the O
task-solving O
abilities O
of O
the O
agent O
, O
but O

they O
are O
also O
conditioned O
by O
the O
VQA B-MethodName
model O
abilities O
. O
Language O
Metrics. O
First O
, O
we O
used O
n-grams B-MetricName
metrics O
, O
BLEU B-MetricName
( O
Papineni O
et O
al. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
and O
CIDEr B-MetricName
( O
Vedantam O
et O
al. O
, O
2015 O
) O
, O
to O
measure O
the O
similarity O

between O
the O
generated O
question O
and O
the O
reference O
questions O
in O
the O
evaluation O
set. O
While O
those O
scores O
can O
capture O
syntactic O
and O
semantic O
properties O
of O
language O
, O
they O
also O
fall O
short O
when O
dealing O
with O
open-form O
language O
, O
e.g. O
an O
identical O
answer O
may O
arise O
from O
two O
non-overlapping O
but O
syntactically O
correct O
questions. O
Thus O
, O
we O

also O
compute O
two O
metrics O
assessing O
the O
quality O
of O
the O
language O
independently O
of O
reference O
questions O
, O
the O
perplexity B-MetricName
of I-MetricName
the I-MetricName
question I-MetricName
given I-MetricName
an I-MetricName
external I-MetricName
LM I-MetricName
( I-MetricName
ppl-e I-MetricName
) I-MetricName
, O
and O
its O
perplexity B-MetricName
given I-MetricName
the I-MetricName
task-related I-MetricName
LM I-MetricName
( I-MetricName
ppl-t I-MetricName
) I-MetricName
. O
Diversity O
Metrics. O
We O
here O
estimate O
a O
self-BLEU B-MetricName
( I-MetricName
sBLEU I-MetricName
) I-MetricName
score O

( O
Zhang O
et O
al. O
, O
2017 O
) O
over O
10 O
questions O
generated O
on O
the O
same O
image-answer O
pair. O
Although O
such O
score O
detects O
potential O
mode O
collapse O
, O
i.e. O
, O
when O
the O
language O
utters O
identical O
sequences O
of O
words O
, O
it O
also O
values O
babbling O
, O
i.e. O
, O
outputting O
random O
words. O
We O
thus O
also O
measure O
the O

probability O
mass O
of O
the O
ten O
most O
frequent O
words O
( O
Choshen O
et O
al. O
, O
2020 O
) O
, O
and O
refer O
to O
it O
as O
peakiness B-MetricName
( I-MetricName
peak I-MetricName
) I-MetricName
. O
Human O
Evaluation. O
On O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
performed O
human O
evaluation O
by O
surveying O
53 O
participants O
on O
the O
first O
50 O
questions O
produced O
by O
some O

of O
the O
models O
at O
test O
time. O
The O
study O
( O
further O
detailed O
in O
Appendix O
C O
) O
is O
based O
on O
pairwise O
comparison O
of O
question O
samples O
produced O
by O
the O
concurrent O
algorithms O
according O
to O
four O
criteria. O
First O
, O
we O
evaluated O
the O
language O
quality O
of O
the O
question O
samples O
, O
by O
asking O
the O
participants O
to O
select O

the O
most O
syntactically O
and O
semantically O
correct O
question O
among O
the O
two O
samples O
of O
the O
questions O
pair. O
Secondly O
, O
we O
evaluated O
language O
grounding O
, O
i.e O
adequacy O
of O
the O
sample O
to O
the O
image-answer O
pair O
, O
by O
asking O
the O
participants O
to O
select O
the O
question O
most O
suitable O
given O
the O
two O
elements. O
Thirdly O
, O
we O
evaluated O

the O
language O
originality O
and O
diversity O
, O
by O
asking O
participants O
to O
select O
the O
question O
the O
most O
different O
from O
the O
dataset O
reference O
question. O
Finally O
, O
we O
evaluated O
the O
number O
of O
syntax O
errors O
by O
asking O
participants O
to O
tick O
the O
question O
if O
it O
is O
grammatically O
incorrect. O
Examples O
of O
questions O
asked O
during O
the O
study O
are O

included O
in O
the O
Appendix O
C O
. O
Sampling O
methods O
for O
text B-TaskName
generation I-TaskName
When O
generating O
text O
from O
a O
trained O
language O
model O
, O
the O
quality O
and O
diversity O
of O
samples O
depend O
on O
the O
decoding O
algorithm O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O
We O
consider O
three O
text B-TaskName
generation I-TaskName
methods. O
greedy O
uses O
the O
argmax O
of O

the O
policy O
, O
while O
sampling O
uses O
the O
multinomial O
distribution. O
Finally O
, O
we O
sampled O
ten O
text O
sequences O
from O
the O
policy O
, O
and O
selected O
the O
one O
with O
the O
lowest O
perplexity B-MetricName
according O
to O
the O
external O
language O
model O
, O
and O
refer O
to O
it O
as O
lm-ranking. O
This O
process O
has O
been O
used O
recently O
in O
Text-to-Image B-TaskName
Generation O

tasks O
( O
Ramesh O
et O
al. O
, O
2021 O
5 O
Results O
CLEVR B-DatasetName
results O
Quantitative O
performance O
: O
In O
Table O
1 O
, O
vanilla O
RL B-MethodName
from O
scratch O
fails O
to O
have O
a O
decent O
performance O
even O
with O
synthetic O
language. O
Besides O
, O
adding O
a O
KL B-HyperparameterName
regularisation I-HyperparameterName
term O
does O
kick-start O
the O
learning O
process. O
Yet O
, O
as O
soon O
as O
we O

apply O
the O
dynamic O
truncation O
, O
TrufLL B-MethodName
matches O
the O
pretrained O
baselines O
performance O
when O
using O
the O
external O
LM O
, O
and O
even O
outperforms O
them O
with O
the O
task-specific O
LM. O
In O
this O
synthetic O
VQG B-MethodName
setting O
, O
TrufLL B-MethodName
seems O
to O
be O
a O
viable O
and O
promising O
procedure O
to O
learn O
a O
RL B-MethodName
language O
agent O
without O
a O
supervised O
training O

phase. O
Pretrained O
baselines O
have O
high O
language O
scores O
when O
assessed O
with O
datasetbased O
metrics O
, O
e.g O
BLEU B-MetricName
or O
task-perplexity. B-MetricName
Yet O
, O
they O
also O
remain O
close O
to O
the O
original O
dataset O
distribution O
with O
a O
medium O
external O
perplexity. O
Noticeably O
, O
TrufLL B-MethodName
with O
the O
task-specific O
LM O
follows O
the O
same O
pattern. O
On O
the O
other O
hand O
, O
TrufLL B-MethodName

with O
the O
external O
LM O
reports O
poor O
dataset-based O
language O
scores O
, O
while O
maintaining O
a O
low O
external O
perplexity. O
Therefore O
, O
TrufLL B-MethodName
seems O
to O
correctly O
capture O
the O
language O
distribution O
of O
the O
initial O
LM. O
As O
the O
performance O
score O
is O
high O
when O
using O
an O
external O
LM O
, O
it O
suggests O
that O
our O
approach O
can O
learn O
a O

policy O
on O
a O
language O
task O
with-out O
the O
need O
of O
a O
task-related O
dataset. O
Less O
positively O
, O
TrufLL B-MethodName
diversity O
metrics O
suggest O
potential O
mode O
collapse O
, O
with O
a O
high O
peakiness B-MetricName
and O
self-BLEU B-MetricName
score O
. O
Qualitative O
performance O
: O
We O
display O
qualitative O
samples O
in O
Figure O
2 O
and O
Appendix O
D. O
On O
the O
one O
hand O
, O
the O

pretrained O
baselines O
generate O
either O
a O
question O
inconsistent O
with O
the O
visual O
context O
, O
or O
which O
fails O
to O
answer O
the O
expected O
answer. O
action O
space O
, O
while O
having O
a O
lower O
performance O
, O
yields O
to O
the O
most O
correct O
and O
diverse O
language O
, O
with O
higher O
language B-MetricName
scores I-MetricName
and O
a O
lower O
self-BLEU. B-MetricName
A O
stochastic O
action O
space O

might O
be O
harder O
to O
explore O
efficiently O
for O
reaching O
good O
task-solving O
abilities O
, O
but O
might O
strengthen O
the O
agent O
language O
generation O
properties O
. O
VQAv2 B-DatasetName
task O
In O
CLEVR B-DatasetName
, O
we O
observe O
that O
TrufLL B-MethodName
seems O
a O
promising O
approach O
to O
learn O
a O
language O
policy O
without O
a O
supervised O
training O
phase O
, O
by O
solely O
interacting O
with O
another O

language O
system. O
We O
scale O
our O
approach O
to O
natural O
language O
with O
large O
vocabulary O
( O
15k O
tokens O
) O
through O
the O
VQAv2 B-DatasetName
dataset O
. O
Quantitative O
performance O
: O
Table O
3 O
reports O
the O
VQAv2 B-DatasetName
results O
, O
for O
which O
TrufLL B-MethodName
and O
the O
baselines O
present O
a O
similar O
trend O
than O
on O
CLEVR. B-DatasetName
First O
, O
the O
scratch O
baselines O
keep O

failing O
to O
learn O
a O
valuable O
policy O
, O
with O
performance B-MetricName
scores I-MetricName
and O
n-grams B-MetricName
metrics O
close O
to O
zero. O
Although O
TrufLL B-MethodName
does O
not O
outperform O
the O
performance O
of O
the O
pretrained O
baselines O
anymore O
, O
it O
still O
leads O
to O
similar O
performances O
, O
and O
satisfactory O
language B-MetricName
scores. I-MetricName
The O
similarity O
between O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName

Ext-LM I-MethodName
) I-MethodName
results O
suggests O
that O
the O
truncation O
approach O
is O
viable O
when O
using O
a O
generic O
LM O
whose O
original O
vocabulary O
distribution O
differs O
from O
the O
task. O
Interestingly O
, O
TrufLL B-MethodName
displays O
a O
self-BLEU B-MetricName
score O
similar O
to O
the O
pretrained O
baselines. O
This O
suggests O
that O
the O
poor O
diversity O
behavior O
observed O
on O
CLEVR B-DatasetName
is O
likely O
attributable O
to O
the O

small O
vocabulary O
and O
synthetic O
language O
distribution O
. O
Qualitative O
performance O
: O
In O
Figure O
2 O
and O
Appendix O
D O
, O
we O
display O
question O
samples O
for O
all O
models O
. O
TrufLL B-MethodName
and O
the O
pretrained O
baselines O
successfully O
generate O
a O
question O
giving O
the O
expected O
answer O
( O
" O
Black O
" O
) O
, O
while O
the O
RL B-MethodName
from O
scratch O
baselines O

fail O
, O
and O
even O
showcase O
degenerated O
language. O
Pretrained O
baselines O
tend O
to O
output O
a O
question O
closer O
to O
the O
reference O
question O
whereas O
TrufLL B-MethodName
outputs O
original O
questions O
which O
differs O
from O
the O
VQA B-MethodName
distribution O
, O
yet O
consistent O
with O
the O
context O
. O
Human O
Evaluation O
: O
Figure O
3 O
details O
the O
Human O
Evaluation O
results. O
Among O
the O
RL B-MethodName

from O
scratch O
baselines O
, O
we O
selected O
scratch+KL-task O
as O
the O
only O
model O
producing O
sometimes O
meaningful O
questions. O
Yet O
, O
it O
fails O
to O
generate O
correct O
and O
grounded O
language O
; O
it O
is O
thus O
not O
a O
viable O
approach O
despite O
its O
diverse O
output. O
In O
line O
with O
the O
automatic O
metrics O
, O
the O
supervised O
baselines O
produce O
the O
best O

language O
, O
while O
being O
accurately O
grounded. O
Yet O
, O
they O
exhibit O
significantly O
less O
diversity O
with O
the O
reference O
language O
; O
this O
suggests O
in O
particular O
that O
pretrain+RL O
fails O
to O
go O
beyond O
the O
initial O
task-data O
distribution. O
Finally O
, O
unlike O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
which O
suffers O
from O
syntactic O
errors O
, O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
produces O

language O
that O
qualitatively O
competes O
with O
pretrain O
models O
( O
53 B-MetricValue
% I-MetricValue
) O
, O
with O
a O
similar O
ratio O
of O
syntactic O
uncorrect O
samples. O
Although O
its O
questions O
are O
less O
grounded O
, O
they O
are O
diverse O
, O
which O
suggests O
that O
they O
follow O
a O
different O
distribution O
from O
the O
initial O
VQA B-MethodName
dataset. O
It O
confirms O
that O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName

) I-MethodName
could O
be O
an O
alternative O
approach O
as O
it O
has O
an O
excellent O
trade-off O
between O
language O
quality O
, O
diversity O
, O
and O
grounding O
. O
Decoding O
procedure O
: O
In O
Table O
4 O
, O
we O
evaluate O
the O
text O
sampling O
procedures O
described O
in O
Section O
4.5. O
While O
greedy O
decoding O
produces O
the O
best O
outcome O
for O
pretrained O
models O
, O
lm-ranking O

provides O
an O
excellent O
trade-off O
between O
task O
performance O
and O
language O
quality O
with O
RL-based B-MethodName
methods. O
As O
PG O
solely O
optimizes O
the O
task B-MetricName
success I-MetricName
ratio I-MetricName
, O
this O
may O
reduce O
overall O
language O
quality O
, O
the O
re-ranking O
thus O
retrieves O
the O
best O
syntactically O
sentences O
a O
posteriori O
. O
Discussion O
Removing O
the O
truncation O
at O
evaluation O
with O
offpolicy O
RL. B-MethodName
So O

far O
, O
TrufLL B-MethodName
directly O
learns O
the O
truncated O
policy O
over O
the O
truncated O
vocabulary O
V O
− O
each O
cell O
displays O
the O
proportion O
of O
questions O
chosen O
for O
the O
models O
in O
the O
row O
( O
bold O
) O
when O
compared O
to O
the O
concurrent O
model O
in O
the O
column. O
The O
table O
at O
the O
bottom O
displays O
the O
proportion O
of O
incorrect O

questions O
coming O
from O
each O
model O
among O
all O
incorrect O
samples. O
In O
all O
figures O
, O
bracket O
numbers O
indicates O
the O
model O
rank O
per O
criteria O
, O
from O
1= O
" O
best O
" O
to O
5= O
" O
worst O
" O
. O
2012 O
) O
. O
Formally O
, O
the O
off-policy O
PPO B-MetricName
loss I-MetricName
is O
defined O
by O
: O
L B-MetricName
off I-MetricName
ppo I-MetricName
( I-MetricName

θ I-MetricName
) I-MetricName
=E O
π O
− O
θ O
min O
( O
ρ O
θ O
t O
A O
t O
, O
clip O
( O
1−ϵ O
, O
ρ O
θ O
t O
,1+ϵ O
) O
A O
t O
) O
, O
whereρ O
θ O
t O
= O
π O
θ O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
− O

θ O
old O
( O
at|st O
) O
is O
the O
new O
ratio. O
4 O
Table O
5 O
displays O
the O
on-policy O
and O
off-policy O
results O
on O
both O
VQG B-MethodName
tasks O
for O
TrufLL B-MethodName
( O
task-LM O
) O
, O
and O
is O
further O
detailed O
in O
Appendix O
B.3. O
We O
also O
monitor O
the O
probability B-MetricName
mass I-MetricName
of O
the O
policy O
attributed O
to O
the O
truncated O
action O
space O

( O
sumVA B-MetricName
) O
. O
The O
policy O
only O
samples O
words O
within O
the O
truncated O
action O
space O
when O
sumVA B-MetricName
= O
1 O
, O
without O
needing O
the O
truncation. O
On O
CLEVR B-DatasetName
, O
the O
TrufLL B-MethodName
off O
has O
lower O
-yet O
close O
-performance O
on O
language O
and O
task O
scores O
than O
TrufLL. B-MethodName
As O
its O
sumVA B-MetricName
ratios O
are O
very O
close O
to O
1 O

, O
the O
agent O
has O
learned O
to O
generalize O
over O
the O
full O
vocabulary. O
However O
, O
the O
approach O
does O
not O
manage O
to O
sufficiently O
scale O
to O
VQAv2. B-DatasetName
It O
could O
be O
improved O
with O
regularisation O
techniques O
and O
the O
use O
of O
TruFLL B-MethodName
within O
state-of-the-art O
off-policy O
RL B-MethodName
algorithms. O
We O
leave O
such O
possibilities O
to O
future O
works. O
Additional O
experiments. O
We O

sweep O
over O
truncation O
hyper-parameters O
in O
Table O
6 O
of O
Appendix O
B. O
In O
Table O
8 O
, O
we O
observe O
that O
rewarding O
an O
agent O
with O
a O
BLEU B-MetricName
score O
is O
sub-optimal O
in O
both O
language O
and O
task O
scores O
on O
CLEVR. B-DatasetName
In O
VQA B-MethodName
, O
we O
apply O
temperature O
scheduling O
on O
the O
LM O
to O
perform O
fine-grained O
truncations O
in O
Table O

9 O
of O
B.2. O
Finally O
, O
we O
explore O
TrufLL B-MethodName
with O
a O
pre-training O
phase O
in O
Table O
10 O
. O
Generalization O
of O
the O
approach. O
TrufLL B-MethodName
learns O
conditional O
language O
models O
able O
to O
solve O
specific O
Natural B-MethodName
Language I-MethodName
Generation I-MethodName
tasks O
given O
a O
context O
c. O
For O
solving O
such O
tasks O
, O
it O
only O
requires O
the O
context O
, O
a O
reward O

function O
that O
scores O
the O
language O
generated O
by O
the O
RL B-MethodName
agent O
with O
respect O
to O
the O
task O
, O
and O
eventually O
a O
few O
natural O
language O
demonstrations O
fed O
as O
input O
prompt O
to O
the O
generic O
language O
model O
used O
in O
the O
truncation O
algorithm. O
Hence O
, O
the O
method O
is O
transferable O
to O
a O
wide O
variety O
of O
NLG B-MethodName
tasks O

, O
without O
requiring O
upfront O
large-scale O
labelled O
datasets. O
Additionally O
, O
the O
RL B-MethodName
framework O
allows O
to O
optimize O
non-differentiable O
objectives O
, O
making O
TrufLL B-MethodName
a O
natural O
choice O
to O
learn O
end-to-end O
task-oriented O
dialogs O
, O
such O
as O
Das O
et O
al. O
, O
2017 O
) O
. O
Other O
interesting O
tasks O
for O
TrufLL B-MethodName
include O
the O
ones O
typically O
found O
in O
Vision O

and O
Language O
Representation O
Learning O
( O
Lu O
et O
al. O
, O
2020a O
) O
, O
such O
as O
Image B-TaskName
Captioning I-TaskName
, O
Grounding B-TaskName
Referring I-TaskName
Expressions I-TaskName
( O
generation O
of O
a O
referring O
expression O
over O
a O
specific O
bounding O
box O
of O
an O
image O
) O
, O
Captionbased B-TaskName
Image I-TaskName
Retrieval I-TaskName
( O
generation O
of O
a O
caption O
that O
discriminates O
an O
image O
between O
a O

set O
of O
images O
) O
. O
Reward O
functions O
for O
such O
tasks O
can O
be O
based O
on O
similarity O
scores O
between O
the O
generated O
language O
and O
the O
associated O
image O
or O
image O
region O
, O
which O
can O
be O
computed O
using O
pretrained O
language O
representations O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
or O
multi-modal O
pretrained O
systems O

such O
as O
ViLBERT B-MethodName
( O
Lu O
et O
al. O
, O
2019 O
) O
. O
The O
context O
can O
be O
any O
kind O
of O
data O
structure O
( O
natural O
language O
, O
database O
, O
video O
, O
etc O
) O
: O
if O
it O
is O
a O
linguistic O
input O
, O
TrufLL B-MethodName
can O
be O
applied O
for O
instance O
to O
text O
summarization O
, O
paraphrase O
generation O

( O
with O
reward O
functions O
based O
on O
similarity O
scores O
between O
the O
context O
and O
the O
generated O
language O
) O
or O
text-based O
games O
( O
Ammanabrolu O
and O
Riedl O
, O
2018 O
) O
. O
6 O
Related O
work O
RL B-MethodName
and O
NLP B-MethodName
Tasks. O
Following O
( O
Singh O
et O
al. O
, O
2002 O
; O
Lemon O
and O
Pietquin O
, O
2007 O
) O
, O
recent O

RL-based O
taskoriented O
dialogues O
Das O
et O
al. O
, O
2017 O
; O
Lewis O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
have O
been O
developed O
, O
where O
the O
policy O
language O
model O
is O
generally O
pretrained O
with O
SL B-MethodName
followed O
RL B-MethodName
( O
Yao O
et O
al. O
, O
2020 O
) O
combines O
a O
pretrained O
language O
model O
to O

prune O
the O
action O
space O
with O
a O
Deep-Q B-MethodName
network O
, O
aka O
DRNN O
) O
. O
Yet O
, O
its O
truncation O
language O
model O
remains O
fine-tuned O
on O
the O
RL B-MethodName
dataset. O
Besides O
, O
CALM B-MethodName
is O
only O
evaluated O
on O
a O
vocabulary O
of O
697 O
tokens O
, O
and O
on O
4-words O
action O
sequences O
. O
Learning O
Language O
Models O
from O
scratch. O
( O

Ziegler O
et O
al. O
, O
2019 O
; O
Garg O
et O
al. O
, O
2021 O
) O
finetune O
pretrained O
GPT-2 B-MethodName
models O
with O
RL B-MethodName
for O
language O
generation O
tasks O
without O
task-related O
data O
, O
only O
using O
reward O
signals. O
Yet O
, O
they O
still O
face O
optimization O
and O
computational O
challenges O
( O
Parisotto O
et O
al. O
, O
2020 O
) O
. O
Conclusion O
We O
proposed O

TrufLL B-MethodName
, O
an O
original O
approach O
to O
learn O
a O
natural B-MethodName
language I-MethodName
generation I-MethodName
( O
NLG B-MethodName
) O
task O
using O
RL B-MethodName
, O
without O
the O
usual O
pre-training O
phase O
requiring O
supervised O
datasets. O
To O
our O
knowledge O
, O
this O
is O
the O
first O
RL-based B-MethodName
algorithm O
dedicated O
to O
learning O
a O
word-based O
text-generation O
task O
, O
which O
does O
not O
rely O
on O
a O

pre-training O
phase O
while O
scaling O
to O
large O
vocabularies. O
Although O
it O
comes O
with O
its O
limitations O
, O
the O
truncated O
RL B-MethodName
algorithm O
provided O
by O
TrufLL B-MethodName
gets O
free O
from O
labelled O
data O
in O
task-oriented O
language O
models O
, O
presents O
interesting O
language O
generation O
properties O
, O
and O
provides O
a O
generic O
and O
transferable O
method O
to O
learn O
any O
NLG B-MethodName
problem O
. O

A O
Dataset O
and O
training O
details O
A.1 O
Evaluation O
Metrics O
For O
the O
BLEU B-MetricName
and O
METEOR B-MetricName
scores O
, O
we O
used O
the O
NLTK B-MethodName
5 I-MethodName
implementations O
with O
the O
smoothing O
function O
number O
2 O
for O
the O
BLEU B-MetricName
score. O
For O
the O
CIDEr B-MetricName
score O
, O
we O
used O
the O
nlg-eval B-MethodName
implementation O
6 O
. O
A.2 O
Answer O
filtering O
For O
each O
dataset O
, O

we O
remove O
yes O
and O
no O
question-answer O
pairs O
which O
frequency O
largely O
exceeds O
other O
answers O
, O
to O
avoid O
any O
bias O
in O
the O
question O
generation O
process O
, O
as O
usually O
done O
in O
the O
VQG B-MethodName
litterature O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
. O
A.3 O
Dataset O
split O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
the O

RL B-MethodName
language O
agent O
is O
trained O
for O
50k O
( O
resp. O
100k O
) O
episodes O
over O
the O
first O
20k O
images O
( O
resp. O
all O
the O
images O
) O
of O
the O
training O
dataset O
, O
and O
is O
then O
evaluated O
on O
the O
first O
5k O
( O
resp. O
20k O
) O
images O
of O
the O
validation O
set. O
Besides O
, O
we O
uniformly O
sample O

the O
answer O
in O
the O
set O
of O
reference O
answers O
for O
each O
image O
to O
reduce O
the O
bias O
in O
the O
distribution O
of O
answers. O
Finally O
, O
questions O
are O
limited O
to O
20 O
( O
resp. O
10 O
) O
words O
. O
A.4 O
Language O
Agent O
Networks O
and O
Training O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
we O
used O
a O
single-layer O

LSTM B-MethodName
with O
64 B-HyperparameterValue
( O
resp. O
256 O
) O
units O
for O
the O
policy B-HyperparameterName
network. I-HyperparameterName
At O
every O
time O
step O
, O
the O
LSTM B-MethodName
input O
is O
then O
the O
concatenation O
of O
the O
word B-HyperparameterName
embedding I-HyperparameterName
of I-HyperparameterName
dimension I-HyperparameterName
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
the O
answer B-HyperparameterName
embedding I-HyperparameterName
of O
dimension O
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
and O
the O
image O

representation. O
For O
CLEVR B-DatasetName
, O
the O
image O
representation O
is O
extracted O
from O
a O
pretrained O
ResNet50 B-MethodName
and O
projected O
into O
a O
tensor B-HyperparameterName
of O
size O
( O
32,7,7 B-HyperparameterValue
) O
before O
being O
flattened. O
For O
VQAv2 B-DatasetName
, O
the O
image O
representation O
is O
the O
average O
of O
200 O
bounding O
box O
features O
of O
dimension O
1048 B-HyperparameterValue
, O
extracted O
from O
a O
faster O
R- B-MethodName
CNN I-MethodName

( O
Ren O
et O
al. O
, O
2015 O
) O
. O
We O
optimize O
the O
full O
loss B-MetricName
L=L O
P O
P O
O O
+αL O
V O
F O
+βL O
E O
with O
α=0.5 B-HyperparameterName
, O
β B-HyperparameterName
=0.01 B-HyperparameterValue
and O
a O
PPO B-HyperparameterName
clipping I-HyperparameterName
ratio I-HyperparameterName
ϵ=0.02 I-HyperparameterName
( O
resp. O
0.01 O
) O
for O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
. O
We O
use O
Adam O
optimizer O
( O
Kingma O

and O
Ba O
, O
2014 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
) O
of O
10 B-MetricName
−3 I-MetricName
for O
TrufLL B-MethodName
and O
the O
scratch O
baseline O
, O
10 B-HyperparameterValue
−5 I-HyperparameterValue
( O
resp. O
10 O
−6 O
) O
for O
RL B-MethodName
algorithms O
with O
a O
pre-training O
phase O
on O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
and O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
models O

including O
a O
KL B-MethodName
regularization I-MethodName
term. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
( O
bs O
) O
of O
128 B-HyperparameterValue
for O
all O
models O
except O
the O
ones O
with O
KL O
regularization O
, O
for O
which O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64. B-HyperparameterValue
Finally O
, O
for O
the O
RL B-MethodName
from O
scratch O
baselines O
, O
we O
perform O
gradient B-HyperparameterName
clipping I-HyperparameterName
( O
gladclip B-HyperparameterName
) O

of O
1 B-HyperparameterValue
( O
resp. O
5 O
) O
for O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
. O
Such O
hyper-parameters O
were O
selected O
, O
after O
conducting O
an O
extensive O
hyper-parameter O
search. O
The O
following O
values O
were O
tested O
: O
β B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue
} O
, O
ϵ B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue

, I-HyperparameterValue
0.5 I-HyperparameterValue
, I-HyperparameterValue
0.9 I-HyperparameterValue
} O
, O
lr B-HyperparameterName
∈ O
{ O
10 B-HyperparameterValue
−6 I-HyperparameterValue
,10 I-HyperparameterValue
−5 I-HyperparameterValue
,10 I-HyperparameterValue
−4 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
,10 I-HyperparameterValue
−3 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−3 I-HyperparameterValue
,10 I-HyperparameterValue
−2 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−2 I-HyperparameterValue
} O
, O
gradclip B-HyperparameterName
∈ O
{ O
None,1,5,10,100 B-HyperparameterValue
} O
, O
bs B-HyperparameterName
∈ O
{ O
32,64,128 B-HyperparameterValue
} O
. O
Additionally O
, O
we O
also O
tested O

for O
VQAv2 B-DatasetName
policy B-HyperparameterName
networks I-HyperparameterName
with O
64 B-HyperparameterValue
, O
256 B-HyperparameterValue
and O
1024 B-HyperparameterValue
units O
, O
with O
respectively O
32 B-HyperparameterValue
, O
128 B-HyperparameterValue
and O
512 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimensions. I-HyperparameterName
We O
kept O
the O
network B-HyperparameterName
size I-HyperparameterName
giving O
the O
best O
performances O
, O
i.e. O
policy B-HyperparameterName
network I-HyperparameterName
of O
256 B-HyperparameterValue
units O
and O
128 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimension I-HyperparameterName
. O
A.5 O
Reward O
formula O
for O
VQAv2 B-DatasetName
In O

this O
section O
, O
we O
detail O
the O
reward O
function O
used O
for O
the O
VQAv2 B-DatasetName
task. O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
=1 O
rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
with O
rk O
( O
A O
) O
the O
rank O
of O
the O

ground-truth O
answer O
given O
by O
the O
VQA B-MethodName
model O
, O
when O
predicting O
the O
actual O
answer O
from O
the O
terminal O
state O
( O
c O
, O
w O
< O
T O
) O
. O
Formally O
, O
it O
is O
defined O
as O
: O
rk O
( O
A O
) O
=rank O
( O
VQA O
( O
c O
, O
w O
< O
T O
) O
[ O
A O
] O
) O

, O
with O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
the O
probability O
distribution O
given O
by O
the O
VQA B-MethodName
model O
over O
the O
set O
of O
answers O
, O
and O
rank O
the O
function O
which O
ranks O
the O
probability O
of O
answer O
A O
within O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
probability O
distribution O
. O
B O
Additional O
experiments O

B.1 O
CLEVR B-DatasetName
Table O
6 O
displays O
the O
complete O
ablation O
on O
the O
truncation O
functions O
with O
parameters O
sweep. O
The O
'sizeVA O
' O
variable O
indicates O
the O
average O
size O
of O
the O
truncated O
action O
space O
for O
each O
truncation O
function. O
Table O
7 O
displays O
the O
ablation O
over O
the O
three O
decoding O
procedures O
defined O
in O
Section O
4.5. O
Such O
an O
ablation O
presents O

a O
similar O
pattern O
than O
VQAv2 B-DatasetName
results O
described O
in O
section O
5.2 O
. O
Finally O
, O
Table O
8 O
reports O
CLEVR B-DatasetName
metrics O
when O
using O
the O
BLEU B-MetricName
score O
as O
the O
reward. O
While O
on O
such O
a O
task O
TrufLL B-MethodName
still O
exhibits O
promising O
language B-MetricName
scores I-MetricName
, O
the O
n-grams B-MetricName
metrics O
remain O
lower O
than O
the O
pretrained O
baselines. O
This O
illustrates O
that O

using O
a O
language O
similarity B-MetricName
score I-MetricName
as O
a O
reward O
signal O
is O
much O
less O
interesting O
than O
a O
reward O
based O
on O
a O
task O
completion O
score O
. O
B.2 O
VQAv2 B-DatasetName
Temperature O
scheduling O
: O
On O
the O
CLEVR B-DatasetName
task O
, O
we O
observed O
that O
dynamic O
truncations O
outperform O
static O
ones O
such O
as O
top B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
: O
indeed O
, O

they O
better O
take O
into O
account O
the O
inherent O
variability O
of O
the O
language O
structure O
at O
the O
sentence-level. O
When O
scaling O
up O
to O
the O
15k O
words O
of O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
dynamically O
decrease O
the O
truncation O
size O
through O
training O
, O
by O
applying O
a O
decreasing O
temperature O
schedule O
on O
the O
language O
model. O
While O
temperature O
scaling O

( O
Bahdanau O
et O
al. O
, O
2015 O
) O
is O
usually O
used O
at O
test O
time O
to O
control O
the O
smoothness O
of O
the O
language O
model O
distribution O
, O
temperature O
schedules O
during O
training O
of O
language O
models O
have O
been O
used O
in O
several O
settings O
( O
Jang O
et O
al. O
, O
2016 O
; O
Zhang O
et O
al. O
, O
2018 O
; O
Wang O

et O
al. O
, O
2020 O
) O
. O
Formally O
, O
f O
LM O
( O
w O
i O
|w O
< O
t O
) O
distribution O
is O
computed O
as O
softmax B-MetricName
( I-MetricName
x I-MetricName
i I-MetricName
) I-MetricName
=e O
−x O
i O
/ O
τ O
/ O
j O
e O
−x O
j O
/ O
τ O
, O
with O
x O
j O
the O
LM O
logits O
and O
τ O
the O
temperature O
, O

which O
decreases O
from O
τ O
max O
to O
τ O
min O
by O
a O
factor O
T O
F O
every O
T O
u O
training O
step. O
In O
Table O
9 O
, O
both O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
benefit O
slightly O
from O
truncation O
with O
a O
temperature O
schedule O
compared O
to O
a O
vanilla O
truncation. O
The O
former O
displays O
the O
best O

performance B-MetricName
/ I-MetricName
language I-MetricName
scores I-MetricName
trade-off O
for O
the O
schedule O
" O
τ O
: O
3 O
> O
1. O
& O
T O
u O
=5,000 O
" O
, O
while O
the O
latter O
has O
the O
best O
metrics O
trade-off O
for O
" O
τ O
: O
1.5 O
> O
1. O
& O
T O
u O
=5,000 O
" O
. O
Finally O
, O
Figure O
4 O
displays O
the O
evolution O
of O
the O

training O
return O
for O
TrufLL B-MethodName
and O
the O
baselines. O
As O
expected O
, O
the O
pretrain+RL O
fine-tune O
baseline O
return O
does O
not O
evolve O
much O
, O
confirming O
that O
the O
policy O
distribution O
almost O
does O
not O
shift O
through O
the O
fine-tuning O
phase. O
The O
training O
curves O
of O
TrufLL B-MethodName
present O
a O
steady O
increase O
in O
the O
return O
until O
reaching O
convergence O
, O
confirming O

that O
our O
approach O
, O
by O
guiding O
the O
exploration O
of O
the O
action O
space O
, O
provides O
a O
sufficient O
learning O
signal. O
On O
the O
other O
hand O
, O
the O
scratch+KL O
baselines O
stay O
stuck O
to O
a O
low O
training O
return. O
This O
suggests O
that O
the O
KL B-MethodName
regularization I-MethodName
term O
, O
while O
encouraging O
the O
policy O
distribution O
to O
resemble O
the O
language O

model O
distribution O
, O
fails O
to O
capture O
the O
task O
pragmatics O
, O
which O
requires O
generating O
a O
language O
that O
is O
visually O
grounded O
. O

-DOCSTART- O
Language B-MethodName
Model I-MethodName
Augmented I-MethodName
Monotonic I-MethodName
Attention I-MethodName
for I-MethodName
Simultaneous I-MethodName
Translation I-MethodName
The B-MethodName
state-of-the-art I-MethodName
adaptive I-MethodName
policies I-MethodName
for I-MethodName
simultaneous I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
use O
monotonic O
attention O
to O
perform O
read O
/ O
write O
decisions O
based O
on O
the O
partial O
source O
and O
target O
sequences. O
The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic O
attention O
to O
take O

poor O
read O
/ O
write O
decisions O
, O
which O
in O
turn O
negatively O
affects O
the O
performance O
of O
the O
SNMT B-MethodName
model. O
On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read O
/ O
write O
decisions O
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge. O
In O
this O
work O
, O
we O
propose O

a O
framework O
to O
aid O
monotonic B-MethodName
attention I-MethodName
with O
an O
external O
language O
model O
to O
improve O
its O
decisions. O
Experiments O
on O
MuST-C B-DatasetName
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state-of-the-art O
monotonic O
multi-head O
attention O
model O
further O
. O
Introduction O
A O
typical O
application O
of O
simultaneous O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( I-TaskName

SNMT I-TaskName
) I-TaskName
is O
conversational O
speech O
or O
live O
video O
caption O
translation. O
In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT B-TaskName
model O
alternates O
between O
performing O
read O
from O
source O
sequence O
and O
write O
to O
target O
sequence. O
For O
a O
model O
to O
decide O
whether O
to O
read O
or O
write O
at O
certain O
moment O
, O
either O
a O
fixed O
or O

an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O
Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al. O
( O
2019a O
) O
and O
Dalvi O
et O
al. O
( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
and O
write O
after O
the O
waiting O
period O
of O
k O
tokens. O
To O
alleviate O
possible O

long O
delay O
of O
fixed O
polices O
, O
recent O
works O
such O
as O
monotonic B-MethodName
infinite I-MethodName
lookback I-MethodName
attention I-MethodName
( I-MethodName
MILk I-MethodName
) I-MethodName
( O
Arivazhagan O
et O
al. O
, O
2019 O
) O
, O
and O
monotonic B-MethodName
multihead I-MethodName
attention I-MethodName
( I-MethodName
MMA I-MethodName
) I-MethodName
( O
Ma O
et O
al. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic B-MethodName
attention I-MethodName
( O
Raffel O
et O
al. O
, O

2017 O
) O
. O
* O
⋆ O
Work O
done O
while O
at O
Samsung O
Research O
† O
Equal O
contribution O
Figure O
1 O
: O
The O
finetuned O
XLM-RoBERTa B-MethodName
language O
model O
predicts O
German O
words O
using O
the O
prefix O
as O
input. O
( O
Green O
: O
Correct O
, O
Red O
: O
Incorrect O
, O
Black O
: O
Neutral O
) O
. O
While O
these O
monotonic B-MethodName
attention I-MethodName
anticipates O
target O

words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguistic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra-linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O
Inspired O
by O
human B-MethodName
translation I-MethodName
experts O
, O
we O
aim O
to O
augment O
monotonic B-MethodName

attention I-MethodName
with O
future O
information O
using O
language O
models O
( O
LM O
) O
( O
Devlin O
et O
al. O
, O
2019 O
; O
Conneau O
et O
al. O
, O
2019 O
) O
. O
Integrating O
the O
external O
information O
effectively O
into O
text-to-text B-TaskName
machine I-TaskName
translation I-TaskName
( B-MethodName
MT I-MethodName
) I-MethodName
systems I-MethodName
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al. O
, O
2020 O
; O

Gulcehre O
et O
al. O
, O
2015Gulcehre O
et O
al. O
, O
, O
2017Stahlberg O
et O
al. O
, O
2018 O
) O
. O
Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT B-MethodName
system O
during O
training O
is O
explored O
in O
Wu O
et O
al. O
( O
2020 O
) O
by O
simultaneously O
training O
different O
wait-k O
SNMT B-MethodName
systems. O
However O
, O
no O
previous O
works O
make O
use O

of O
explicit O
future O
information O
both O
during O
training O
and O
inference. O
To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
integrate O
future O
information O
from O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O

to O
explore O
a O
tighter O
integration O
of O
the O
LM B-MethodName
information O
into O
SNMT B-MethodName
model O
. O
In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future O
information O
from O
LM O
during O
training O
by O
transforming O
the O
monotonic O
attention O
mechanism. O
As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM O
takes O
the O
prefix O
target O
( O

and O
source O
, O
for O
cross-lingual O
LM O
) O
sequence O
and O
predicts O
the O
probable O
future O
information. O
We O
hypothesize O
that O
aiding O
the O
monotonic B-TaskName
attention I-TaskName
with O
this O
future O
information O
can O
improve O
MMA B-MethodName
model O
's O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay. O
Several O
experiments O
on O
MuST-C O
( O
Di O
Gangi O

et O
al. O
, O
2019 O
) O
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency-quality B-MetricName
trade-offs O
over O
the O
state-of-the-art O
MMA B-MethodName
models O
. O
2 O
Monotonic B-MethodName
Attention I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
Model I-MethodName
Monotonic B-MethodName
Attention I-MethodName
In O
simultaneous O
machine B-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
models I-MethodName
, O
the O
probability O
of O
predicting O
the O

target O
token O
y O
i O
∈ O
y O
depends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x O
≤j O
∈ O
x O
, O
y O
< O
i O
∈ O
y O
) O
. O
In O
sequence-tosequence B-MethodName
based I-MethodName
SNMT I-MethodName
model I-MethodName
, O
each O
target O
token O
y O
i O
is O
generated O
as O
follows O
: O
h O
j O
= O
E O
( O
x O
≤j O

) O
( O
1 O
) O
s O
i O
= O
D O
( O
y O
< O
i O
, O
c O
i O
= O
A O
( O
s O
i−1 O
, O
h O
≤j O
) O
) O
( O
2 O
) O
y O
i O
= O
Output O
( O
s O
i O
) O
( O
3 O
) O
where O
E O
( O
. O
) O
and O
D O
( O
. O
) O
are O

the O
encoder O
and O
decoder O
layers O
, O
and O
c O
i O
is O
a O
context O
vector. O
In O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
SNMT I-MethodName
, O
the O
context O
vector O
is O
computed O
as O
follows O
: O
e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i−1 O
, O
h O
j O
) O
( O
4 O
) O
p O
i O
, O
j O
= O
Sigmoid O

( O
e O
i O
, O
j O
) O
( O
5 O
) O
z O
i O
, O
j O
∼ O
Bernoulli O
( O
p O
i O
, O
j O
) O
( O
6 O
) O
When O
generating O
a O
target O
token O
y O
i O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
p O
i O
, O
j O
. O

When O
z O
i O
, O
j O
= O
1 O
( O
write O
) O
, O
model O
sets O
t O
i O
= O
j O
, O
c O
i O
= O
h O
j O
and O
generates O
the O
target O
token O
y O
i O
. O
For O
z O
i O
, O
j O
= O
0 O
( O
read O
) O
, O
it O
sets O
t O
i O
= O
j O
+ O
1 O

and O
repeats O
Eq. O
4 O
to O
6. O
Here O
t O
i O
refers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
i O
th O
target O
token. O
Instead O
of O
hard O
alignment O
of O
c O
i O
= O
h O
j O
, O
Raffel O
et O
al. O
( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O

manner O
and O
propose O
a O
closed-form O
parallel O
solution. O
Arivazhagan O
et O
al. O
( O
2019 O
) O
adopt O
monotonic B-MethodName
attention I-MethodName
into O
SNMT B-MethodName
and O
later O
, O
Ma O
et O
al. O
( O
2019c O
) O
extend O
it O
to O
MMA B-MethodName
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O
Monotonic B-MethodName
Attention I-MethodName
with B-MethodName
Future I-MethodName

Information I-MethodName
The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information. O
To O
augment O
this O
anticipation O
process O
using O
future O
information O
extracted O
using O
LMs B-MethodName
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic B-MethodName
attention I-MethodName
. O
Future O
Representation O
Layer O
: O
At O
every O
decoding O
step O

i O
, O
the O
previous O
target O
token O
y O
i−1 O
is O
equipped O
with O
a O
plausible O
future O
tokenŷ O
i O
as O
shown O
in O
the O
Figure O
2. O
Since O
the O
tokenŷ O
i O
comes O
from O
an O
LM B-MethodName
possibly O
with O
a O
different O
tokenizer O
and O
vocabulary O
set O
, O
applying O
the O
model O
's O
tokenizer O
and O
vocabulary O
might O
split O
the O
tokenŷ O

i O
further O
into O
multiple O
sub-tokens O
{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
. O
To O
get O
a O
single O
future O
token O
representations O
i O
∈ O
R O
d O
from O
all O
the O
sub-tokens O
, O
we O
apply O
a O
sub-token O
summary O
layer O
: O
s O
i O
= O
Γ O

( O
{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
) O
( O
7 O
) O
The O
Γ B-HyperparameterName
represents O
a O
general O
sequence O
representation O
layer O
such O
as O
a O
Transformer B-HyperparameterName
encoder I-HyperparameterName
layer I-HyperparameterName
or O
a O
simple O
normalized O
sum O
of O
sub-token O
representations O
. O
We O
enrichs O
i O
at O
every O

layer B-HyperparameterName
l B-HyperparameterName
of O
the O
decoder O
block O
by O
applying O
a O
residual O
feed-forward O
network O
. O
s O
l O
i O
= O
F O
F O
N O
( O
ỹ O
l−1 O
i O
) O
( O
8 O
) O
Monotonic B-MethodName
Energy I-MethodName
Layer I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plausible O
future O
information O
to O
the O
output O
layer O

( O
Appendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
representation O
y O
i−1 O
, O
the O
MMA B-MethodName
read O
/ O
write O
decisions O
happen O
in O
Eq. O
4. O
Therefore O
, O
we O
integrates O
i O
into O
the O
Eq. O
4 O
instead O
. O
The O
integration O
is O
carried O
out O
by O
modifying O
Eq. O
4 O
-Eq. O
5. O
We O
compute O
the O

monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representations O
i O
available O
at O
each O
layer O
: O
e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i O
, O
h O
j O
) O
( O
9 O
) O
We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq. O
5 O
as O
follows O
: O
p O
i O
, O

j O
= O
Sigmoid O
( O
e O
i O
, O
j O
+ẽ O
i O
, O
j O
) O
( O
10 O
) O
After O
computingp O
i O
, O
j O
, O
we O
compute O
c O
i O
similar O
to O
MMA O
model. O
This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM O
output O
usage O
on O
the O
input O
sequence. O

The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM O
output O
by O
varying O
theẽ O
i O
, O
j O
. O
In O
case O
of O
insufficient O
source O
information O
in O
the O
low O
latency B-MetricName
regime O
, O
we O
expect O
the O
model O
's O
decision O
policy O
to O
rely O
more O
onẽ O
i O
, O
j O
. O
Inference O
: O
During O
inference O

, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information. O
After O
predicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
y O
i O
, O
we O
invoke O
the O
LM O
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq. O
10 O
. O
Experiments O
and O
Results O

Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experiments O
on O
the O
MuST-C B-DatasetName
English O
( O
En O
) O
-German O
( O
De O
) O
and O
English O
( O
En O
) O
-French O
( O
Fr O
) O
speech-to-text B-TaskName
( I-TaskName
ST I-TaskName
) I-TaskName
translation I-TaskName
task. O
The O
speech O
sequence O
is O
represented O
using O
80-dimensional O
log-mel O
filter O
bank O
features. O
The O
target O
sequence O

is O
represented O
as O
subwords O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary O
of O
size O
10,000. O
We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency B-MetricName
and O
quality B-MetricName
aspects. O
We O
use O
Average B-MetricName
Lagging I-MetricName
( I-MetricName
AL I-MetricName
) I-MetricName
as O
our O
latency B-MetricName
metric O
and O
case-sensitive O
detokenized O

SacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality B-MetricName
, O
similar O
to O
( O
Ma O
et O
al. O
, O
2020 O
) O
. O
The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST-C B-DatasetName
test O
( O
tst-COMMON O
) O
sets O
. O
Language O
Models O
We O
use O

two O
language O
models O
to O
train O
our O
proposed O
modified O
MMA B-MethodName
model. O
Firstly O
, O
we O
use O
the O
pretrained O
XLM-RoBERTa B-MethodName
( O
Conneau O
et O
al. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers O
1 O
model O
repository. O
Since O
the O
LM O
output O
can O
be O
very O
open-ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O

dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST-C B-DatasetName
target O
text O
data O
for O
each O
task O
. O
We O
also O
train O
a O
smaller B-MethodName
language I-MethodName
model I-MethodName
( O
SLM B-MethodName
) O
, O
which O
contains O
6 B-HyperparameterValue
Transformer B-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
512 B-HyperparameterValue
hidden-states B-HyperparameterName
and O
24M O
parameters. O
We O
use O
the O
MuST-C B-DatasetName
data O
along O
with O
additional O

data O
augmentation O
to O
reduce O
overfitting. O
The O
SLM B-MethodName
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O
Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al. O
( O
2020 O
) O
. O
We O
use O
a O
predecision B-MetricName
ratio I-MetricName
of O
7 B-MetricValue
, O
which O
means O
that O

the O
simultaneous O
read O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states. O
We O
use O
λ B-HyperparameterName
or O
λ B-HyperparameterName
latency B-HyperparameterName
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted O
average O
( O
λ B-HyperparameterName
avg I-HyperparameterName
) O
in O
MMA. B-MethodName
The O
values O
of O
this O
hyperparameter O
λ B-HyperparameterName
are O
chosen O
from O
the O
set O
{ O
0.01 B-HyperparameterValue
, O
0.05 B-HyperparameterValue

, O
0.1 B-HyperparameterValue
} O
. O
The O
Γ O
layer O
in O
Eq. O
7 O
computes O
the O
normalized O
sum O
of O
the O
sub-token O
representations. O
For O
SLM B-MethodName
, O
it O
simply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set. O
All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
update_f B-MetricName
req I-MetricName
set O
to O
8 B-MetricValue
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait-K O
( O
Ma O
et O
al. O
, O
2019b O
) O
, O
we O
choose O
monotonic B-MethodName
attention I-MethodName
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al. O
, O
2019 O
; O
Ma O
et O
al. O

, O
2019c O
) O
. O
We O
train O
a O
baseline O
based O
on O
Ma O
et O
al. O
( O
2020 O
) O
work O
, O
called O
as O
MMA B-MethodName
model. O
The O
MMA B-MethodName
model O
encoder O
and O
decoder O
embedding B-HyperparameterName
dimensions I-HyperparameterName
are O
set O
to O
392 B-HyperparameterValue
, O
whereas O
our O
proposed O
model O
's O
encoder B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
embeddings I-HyperparameterName
are O
set O
to O
256 B-HyperparameterValue
to O
have O

similar O
parameters O
( O
≈ O
39M O
) O
for O
a O
fair O
comparison. O
We O
train O
two O
models O
using O
the O
Results O
We O
first O
analyze O
how O
the O
LM O
predictions O
are O
being O
utilized O
by O
the O
our O
model. O
In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
's O
internal O
states O
versus O
the O
predictions O
from O
the O
LM O

, O
we O
compare O
the O
norm B-MetricName
of I-MetricName
the I-MetricName
monotonic I-MetricName
energies I-MetricName
corresponding O
to O
the O
LM O
predictions O
e O
pred O
( O
Eq. O
9 O
) O
and O
the O
previous O
output O
tokens O
e O
output O
( O
Eq. O
4 O
) O
. O
Let O
us O
define O
LM O
prediction O
weight O
as O
follows O
: O
LM B-MethodName
pw B-HyperparameterName
= O
∥e O
pred O
∥ O
∥e O
output O
∥ O

( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LM O
pw B-HyperparameterName
( O
averaged O
) O
vs. O
λ. B-HyperparameterName
We O
use O
two O
additional O
values O
of O
λ B-HyperparameterName
∈ O
{ O
0.005 B-HyperparameterValue
, O
0.001 B-HyperparameterValue
} O
to O
obtain O
this O
plot. O
We O
can O
observe O
that O
as O
the O
latency B-MetricName
requirements O
become O
more O
and O
more O
strict O
, O

the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM. O
This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
information O
coming O
from O
LM O
predictions O
based O
on O
latency B-MetricName
requirements. O
Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach. O
By O
varying O
the O
λ B-HyperparameterName
, O
we O

train O
separate O
models O
for O
different O
latency B-MetricName
regimes. O
Moreover O
, O
the O
quality B-MetricName
and O
latency B-MetricName
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
controlling O
the O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
the O
inference. O
Speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
or O
step B-HyperparameterName
size I-HyperparameterName
refers O
to O
the O
duration O
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O

read O
decision. O
We O
vary O
these O
hyperparameters O
for O
all O
the O
three O
models O
, O
namely O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
. O
The O
BLEU-AL B-MetricName
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU-AL B-MetricName
numbers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference. O
We O
vary O
the O
step B-HyperparameterName
sizes I-HyperparameterName

in O
intervals O
of O
80ms B-HyperparameterValue
from O
120 B-HyperparameterValue
ms I-HyperparameterValue
to O
520 B-HyperparameterValue
ms I-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency B-MetricName
regimes. O
We O
can O
observe O
that O
the O
LM-based O
models O
using O
both O
XLM B-MethodName
and O
SLM B-MethodName
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA O
model. O
We O
observe O
improvements O
in O
the O
range O
of O
1-2 B-MetricValue

BLEU B-MetricName
scores O
consistently O
across O
all O
the O
latency O
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.01 B-HyperparameterValue
) O
. O
The O
MMA B-MethodName
using O
SLM B-MethodName
language O
model O
performs O
slightly O
better O
than O
MMA B-MethodName
using O
XLM B-MethodName
language O
model. O
This O
is O
due O
to O
SLM B-MethodName
's I-MethodName
higher O
accuracy B-MetricName
on O
the O
next O
token O
prediction O
task O
as O
compared O
to O

XLM B-MethodName
, O
30.15 B-MetricValue
% O
vs. O
A O
LM B-MethodName
at O
MMA B-MethodName
Output O
Layer O
We O
explored O
a O
naive O
approach O
of O
integrating O
LM O
information O
into O
the O
MMA. B-MethodName
In O
this O
approach O
, O
we O
integrate O
the O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
We O
refer O
to O
this O
experiment O

as O
'LM B-MethodName
Rescoring I-MethodName
( I-MethodName
LMR I-MethodName
) I-MethodName
' O
, O
and O
the O
corresponding O
model O
is O
called O
MMA-LMR. B-MethodName
As O
observed O
in O
Figure O
5 O
, O
MMA-LMR B-MethodName
has O
inferior O
performance O
compared O
to O
the O
MMA B-MethodName
model. O
Since O
the O
LM O
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA O
model O
can O

not O
easily O
discard O
the O
incorrect O
information O
from O
LM. O
This O
motivates O
us O
to O
tightly O
integrate O
the O
LM O
information O
into O
the O
simultaneous O
model O
. O
B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
language O
models O
( O
LMs O
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
Simultaneous I-MethodName

models I-MethodName
. O
B.1 O
XLM-Roberta B-MethodName
( I-MethodName
XLM-R I-MethodName
) I-MethodName
XLM-R B-MethodName
Large I-MethodName
model I-MethodName
2 O
was O
trained O
on O
the O
100 O
languages O
CommonCrawl B-DatasetName
corpora O
total O
size O
of O
2.5TB O
with O
550M O
parameters O
from O
24 B-HyperparameterValue
layers B-HyperparameterName
, O
1024 B-HyperparameterValue
hidden B-HyperparameterName
states I-HyperparameterName
, O
4096 B-HyperparameterValue
feed-forward B-HyperparameterName
hidden-states I-HyperparameterName
, O
and O
16 B-HyperparameterValue
heads. B-HyperparameterName
Total O
number O
of O
parameters O
is O
558M. O
We O
finetune O
the O

head O
of O
the O
XLM-R O
LM O
model O
using O
the O
Masked O
Language O
Modeling O
objective O
which O
accounts O
for O
0.23 O
% O
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3M O
parameters O
. O
B.2 O
Smaller O
Language O
Model O
Since O
the O
LM B-MethodName
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
2 O
https O

: O
/ O
/ O
huggingface.co O
/ O
xlm-roberta-large O
LM O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency B-MetricName
requirements. O
To O
reduce O
the O
LM O
computation B-MetricName
time I-MetricName
, O
we O
train O
a O
smaller B-MethodName
Language I-MethodName
Model I-MethodName
( I-MethodName
SLM I-MethodName
) I-MethodName
from O
scratch O
using O
the O
Causal B-MethodName
Language I-MethodName
Modeling I-MethodName
objective. O
SLM B-MethodName
is O
composed O
of O
6 O
Transformer O
decoder O
blocks O
, O
512 O

hidden-states O
, O
2048 O
feed-forward O
hidden-states O
& O
8 O
attention O
heads. O
It O
alleviates O
the O
need O
for O
the O
sub-token O
summary O
layer O
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models. O
The O
train O
examples O
are O
at O
the O
sentence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multiple O
sentences O
( O
which O
is O

the O
usual O
case O
for O
Language O
Models O
) O
. O
Since O
the O
target O
texts O
contain O
lesser O
than O
250k O
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data. O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST-C B-DatasetName
target O
text. O
Details O
have O
been O
provided O
in O
B.2.1 O
. O
B.2.1 O

Data O
Augmentation O
Up-Sampling O
: O
To O
boost O
the O
LM O
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST-C B-DatasetName
target O
text O
data O
by O
substituting O
and O
inserting O
words O
based O
on O
LM O
predictions. O
We O
use O
the O
NLPAUG O
3 O
package O
to O
get O
similar O
words O
based O

on O
contextual O
embeddings. O
From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
We O
observe O
that O
both O
upsampling O
and O
data O
augmentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST-C B-DatasetName
dev O
set O
. O
B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O

the O
LM O
prediction O
is O
obtained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM O
model. O
These O
predictions O
are O
pre-computed O
for O
training O
and O
validation O
sets. O
This O
ensures O
parallelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM O
simultaneously O
during O
the O
training O
process. O
During O
inference O
, O
the O
LM O
model O
is O
called O
every O

time O
a O
new O
output O
token O
is O
written O
. O
C O
Dataset O
The O
MuST-C B-DatasetName
dataset O
comprises O
of O
English O
TED O
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level. O
Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O
D O
Effect O
of O
LM B-HyperparameterName
Size I-HyperparameterName
on O
Latency-Quality B-MetricName
We O

train O
several O
SLM O
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 B-MetricName
accuracy. I-MetricName
As O
we O
increase O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
model O
from O
2 B-HyperparameterValue
to O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
layers O
, O
the O
SLM B-MethodName
and O
the O
proposed O
MMA B-MethodName
with O
future O
information O
models O
have O

shown O
performance O
improvements. O
However O
, O
increasing O
the O
number O
of O
layers O
greater O
than O
6 B-HyperparameterValue
does O
not O
yield O
any O
performance O
improvements. O
We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM B-MethodName
model O
while O
varying O
the O
number B-HyperparameterName
of I-HyperparameterName
hidden I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
head O
. O
E O
Training O
Details O
We O
follow O
the O
training O
process O

similar O
to O
Ma O
et O
al. O
( O
2020 O
) O
training O
process. O
We O
train O
an O
English O
ASR B-MethodName
model O
using O
the O
source O
speech O
data. O
Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency B-HyperparameterName
loss I-HyperparameterName
( O
setting O
λ B-HyperparameterName
latency B-HyperparameterName
= O
0 B-HyperparameterValue
) O
after O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model. O
After O
this O

step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs. B-HyperparameterName
This O
training O
process O
is O
repeated O
for O
all O
the O
reported O
models O
and O
for O
each O
task. O
The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O
F O
BLEU-AL O
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O

main O
paper O
, O
we O
vary O
the O
latency O
weight O
hyperparameter O
( O
λ O
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes. O
We O
also O
vary O
the O
step-size B-HyperparameterName
/ O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
inference. O
In O
total O
, O
we O
obtain O
18 O
different O
data O
points O
corresponding O
to O
each O
model. O
In O
Table O
3 O
, O
we O

compare O
the O
results O
obtained O
using O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
under O
similar O
hyperparameter O
settings. O
It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach O
. O

-DOCSTART- O
TSTR B-MethodName
: O
Too B-MethodName
Short I-MethodName
to I-MethodName
Represent I-MethodName
, O
Summarize O
with O
Details O
! O
Intro-Guided O
Extended O
Summary O
Generation O
Many O
scientific O
papers O
such O
as O
those O
in O
arXiv O
and O
PubMed O
data O
collections O
have O
abstracts O
with O
varying O
lengths O
of O
50-1000 O
words O
and O
average O
length O
of O
approximately O
200 O
words O
, O
where O
longer O
abstracts O
typically O
convey O
more O

information O
about O
the O
source O
paper. O
Up O
to O
recently O
, O
scientific O
summarization O
research O
has O
typically O
focused O
on O
generating O
short O
, O
abstractlike O
summaries O
following O
the O
existing O
datasets O
used O
for O
scientific O
summarization. O
In O
domains O
where O
the O
source O
text O
is O
relatively O
long-form O
, O
such O
as O
in O
scientific O
documents O
, O
such O
summary O
is O
not O
able O

to O
go O
beyond O
the O
general O
and O
coarse O
overview O
and O
provide O
salient O
information O
from O
the O
source O
document. O
The O
recent O
interest O
to O
tackle O
this O
problem O
motivated O
curation O
of O
scientific O
datasets O
, O
arXiv-Long O
and O
PubMed-Long O
, O
containing O
human-written O
summaries O
of O
400-600 O
words O
, O
hence O
, O
providing O
a O
venue O
for O
research O
in O
generating O
long O

/ O
extended O
summaries. O
Extended O
summaries O
facilitate O
a O
faster O
read O
while O
providing O
details O
beyond O
coarse O
information. O
In O
this O
paper O
, O
we O
propose O
TSTR B-MethodName
, O
an O
extractive O
summarizer O
that O
utilizes O
the O
introductory O
information O
of O
documents O
as O
pointers O
to O
their O
salient O
information. O
The O
evaluations O
on O
two O
existing O
large-scale O
extended O
summarization O
datasets O
indicate O
statistically O

significant O
improvement O
in O
terms O
of O
ROUGE B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( I-MetricName
F1 I-MetricName
) I-MetricName
scores O
( O
except O
in O
one O
case O
) O
as O
compared O
to O
strong O
baselines O
and O
state-of-theart. O
Comprehensive O
human O
evaluations O
favor O
our O
generated O
extended O
summaries O
in O
terms O
of O
cohesion O
and O
completeness O
. O
Introduction O
Over O
the O
past O
few O
years O
, O
summarization O
task O

has O
witnessed O
a O
huge O
deal O
of O
progress O
in O
extractive B-TaskName
( O
Nallapati O
et O
al. O
, O
2017 O
; O
Liu O
and O
Lapata O
, O
2019 O
; O
Yuan O
et O
al. O
, O
2020 O
; O
Cui O
et O
al. O
, O
2020 O
; O
Jia O
et O
al. O
, O
2020 O
; O
Feng O
et O
al. O
, O
2018 O
) O
and O
abstractive B-TaskName
( O
See O

et O
al. O
, O
2017 O
; O
Gehrmann O
et O
al. O
, O
2018 O
; O
Zhang O
et O
al. O
, O
2019 O
; O
Tian O
et O
al. O
, O
2019 O
; O
Zou O
et O
al. O
, O
2020 O
) O
[ O
Introductory O
] O
Neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
@ O
xcite O
) O
, O
directly O
applying O
a O
single O
neural O
network O
to O
transform O
the O
source O

sentence O
into O
the O
target O
sentence O
, O
has O
now O
reached O
impressive O
performance O
( O
@ O
xcite O
[ O
… O
] O
Motivated O
by O
recent O
success O
in O
unsupervised O
cross-lingual O
embeddings O
( O
@ O
xcite O
) O
, O
the O
models O
proposed O
for O
unsupervised O
NMT B-TaskName
often O
assume O
that O
a O
pair O
of O
sentences O
from O
two O
different O
languages O
can O
be O
mapped O

to O
a O
same O
latent O
representation O
in O
a O
shared-latent O
space O
( O
@ O
xcite O
) O
[ O
… O
] O
Although O
the O
shared O
encoder O
is O
vital O
for O
mapping O
sentences O
from O
different O
languages O
into O
the O
shared-latent O
space O
, O
it O
is O
weak O
in O
keeping O
the O
uniqueness O
and O
internal O
characteristics O
of O
each O
language O
, O
such O
as O
the O

style O
, O
terminology O
and O
sentence O
structure. O
[ O
… O
] O
For O
each O
language O
, O
the O
encoder O
and O
its O
corresponding O
decoder O
perform O
an O
AE O
, O
where O
the O
encoder O
generates O
the O
latent O
representations O
from O
the O
perturbed O
input O
sentences O
and O
the O
decoder O
reconstructs O
the O
sentences O
from O
the O
latent O
representations. O
Experimental O
results O
show O
that O
the O

proposed O
approach O
consistently O
achieves O
great O
success O
. O
Related O
Work O
Summarizing O
scientific O
documents O
has O
gained O
a O
huge O
deal O
of O
attention O
from O
researchers O
, O
although O
it O
has O
been O
studied O
for O
decades. O
Neural O
efforts O
in O
scientific O
text O
have O
used O
specific O
characteristics O
of O
papers O
such O
as O
discourse O
structure O
Xiao O
and O
Carenini O
, O
2019 O
) O

and O
citation O
information O
( O
Qazvinian O
and O
Radev O
, O
2008 O
; O
Goharian O
, O
2015 O
, O
2018 O
) O
to O
aid O
summarization O
model. O
While O
prior O
work O
has O
mostly O
covered O
the O
generation O
of O
shorter-form O
summaries O
( O
approx. O
200 O
terms O
) O
, O
generating O
extended O
summaries O
of O
roughly O
600 O
terms O
for O
long-form O
source O
documents O
such O
as O

scientific O
papers O
has O
been O
motivated O
very O
recently O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
. O
The O
proposed O
models O
for O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task I-TaskName
include O
jointly O
learning O
to O
predict O
sentence O
importance O
and O
sentence O
section O
to O
extract O
top O
sentences O
( O
Sotudeh O
et O
al. O
, O
2020 O
) O
; O
utilizing O
section-contribution O
computations O
to O

pick O
sentences O
from O
important O
section O
for O
forming O
the O
final O
summary O
( O
Ghosh O
Roy O
et O
al. O
, O
2020 O
) O
; O
identifying O
salient O
sections O
for O
generating B-TaskName
abstractive I-TaskName
summaries I-TaskName
( O
Gidiotis O
et O
al. O
, O
2020 O
) O
; O
ensembling O
of O
extraction O
and O
abstraction O
models O
to O
form O
final O
summary O
( O
Ying O
et O
al. O
, O
2021 O

) O
; O
an O
extractive O
model O
with O
TextRank O
algorithm O
equipped O
with O
BM25 O
as O
similarity O
function O
( O
Kaushik O
et O
al. O
, O
2021 O
) O
; O
and O
incorporating O
sentences O
embeddings O
into O
graph-based O
extractive O
summarizer O
in O
an O
unsupervised O
manner O
( O
Ramirez-Orta O
and O
Milios O
, O
2021 O
) O
. O
Unlike O
these O
works O
, O
we O
do O
not O
exploit O

any O
sectional O
nor O
citation O
information O
in O
this O
work. O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
at O
proposing O
the O
novel O
method O
of O
utilizing O
introductory O
information O
of O
the O
scientific O
paper O
to O
guide O
the O
model O
to O
learn O
to O
generate O
summary O
from O
the O
salient O
and O
related O
information O
. O
3 O
Background O

: O
Contextualized O
language O
models O
for O
summarization O
Contextualized O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
and O
ROBERTA B-MethodName
have O
achieved O
state-of-the-art O
performance O
on O
a O
variety O
of O
downstream O
NLP O
tasks O
including O
text O
summarization. O
Liu O
and O
Lapata O
( O
2019 O
) O
were O
the O
first O
to O
fine-tune O
a O
contextualized O
language O

model O
( O
i.e. O
, O
BERT B-MethodName
) O
for O
the O
summarization O
task. O
They O
proposed O
BERTSUM B-MethodName
-a O
fine-tuning O
scheme O
for O
text O
summarization-that O
outputs O
the O
sentence O
representations O
of O
the O
source O
document O
( O
we O
use O
the O
term O
source O
and O
source O
document O
interchangeably O
, O
referring O
to O
the O
entire O
document O
) O
. O
The O
BERT-SUMEXT B-MethodName
model O
, O
which O

is O
built O
based O
on O
BERT-SUM B-MethodName
, O
was O
proposed O
for O
the O
extractive B-TaskName
summarization I-TaskName
task. O
It O
utilizes O
the O
representations O
produced O
by O
BERTSUM B-MethodName
, O
passes O
them O
through O
Transformers O
encoder O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
, O
and O
finally O
uses O
a O
linear O
layer O
with O
Sigmoid O
function O
to O
compute O
copying O
probabilities O
for O
each O
input O

sentence. O
Formally O
, O
let O
l O
1 O
, O
l O
2 O
, O
... O
, O
l O
n O
be O
the O
binary O
tags O
over O
the O
source O
sentences O
x O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
n O
} O
of O
a O
long O
document O
, O
in O
which O
n O
is O
the O
number O
of O
sentences O
in O

the O
paper. O
The O
BERTSUMEXT B-MethodName
network O
runs O
over O
the O
source O
documents O
as O
follows O
( O
Eq. O
1 O
) O
, O
h O
b O
= O
BertSum B-MethodName
( O
x O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
o O
h O
+ O
b O
o O
) O
( O
1 O
) O
where O
h O
b O
and O

h O
are O
the O
representations O
of O
source O
sentences O
encoded O
by O
BERTSUM B-MethodName
and O
Trasformers O
encoder O
, O
respectively. O
W O
o O
and O
b O
o O
are O
trainable O
parameters O
, O
and O
p O
is O
the O
probability O
distribution O
over O
the O
source O
sentences O
, O
signifying O
extraction O
copy O
likelihood. O
The O
goal O
of O
this O
network O
is O
to O
train O
a O
network O
that O

can O
identify O
the O
positive O
sets O
of O
sentences O
as O
the O
summary. O
To O
prevent O
the O
network O
from O
selecting O
redundant O
sentences O
, O
BERTSUM B-MethodName
uses O
Trigram O
Blocking O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
for O
sentence O
selection O
in O
inference O
time. O
We O
refer O
the O
reader O
to O
the O
main O
paper O
for O
more O
details O
. O
TSTR B-MethodName
: O

Intro-guided O
Summarization O
In O
this O
section O
, O
we O
describe O
our O
methodology O
to O
tackle O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task. O
Our O
approach O
exploits O
the O
introductory O
information O
3 O
. O
3 O
Introductory O
information O
is O
defined O
in O
Section O
1 O
of O
the O
paper O
as O
pointers O
to O
salient O
sentences O
within O
it O
, O
as O
shown O
in O
Figure O
2. O
It O

is O
ultimately O
expected O
that O
the O
extractive O
summarizer O
is O
guided O
to O
pick O
salient O
sentences O
across O
the O
entire O
paper O
. O
The O
detailed O
illustration O
of O
our O
model O
is O
shown O
in O
Figure O
3. O
To O
aid O
the O
extractive B-TaskName
summarization I-TaskName
model O
( O
i.e. O
, O
right-hand O
box O
in O
Figure O
3 O
) O
which O
takes O
in O
source O
sentences O
of O

a O
scientific O
paper O
, O
we O
utilize O
an O
additional O
BERTSUM B-MethodName
encoder O
called O
Introductory O
encoder O
( O
left-hand O
box O
in O
Fig. O
3 O
) O
that O
receives O
x O
intro O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
m O
} O
, O
with O
m O
being O
the O
number O
of O
sentences O
in O
introductory O
section. O
The O
aim O

of O
adding O
second O
encoder O
in O
this O
framework O
is O
to O
identify O
the O
clues O
in O
the O
introductory O
section O
which O
point O
to O
the O
salient O
supplementary O
sentences O
4 O
. O
The O
BERTSUM B-MethodName
network O
computes O
the O
extraction O
probabilities O
for O
introductory O
sentences O
as O
follow O
( O
same O
way O
as O
in O
Eq. O
1 O
) O
, O
h O
b O
= O
BertSum B-MethodName

( O
x O
intro O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
jh O
+ O
b O
j O
) O
( O
2 O
) O
in O
whichh O
b O
, O
andh O
are O
the O
introductory O
sentence O
representations O
by O
BERTSUM B-MethodName
, O
Transformers O
encoder O
, O
respectively.p O
is O
the O
introductory O
sentence O
extraction O
probabilities. O
W O
j O

and O
b O
j O
are O
trainable O
matrices O
. O
After O
identifying O
salient O
introductory O
sentences O
, O
the O
representations O
associated O
with O
them O
are O
retrieved O
using O
a O
pooling O
function O
and O
further O
used O
to O
guide O
the O
first O
task O
( O
i.e. O
, O
right-hand O
side O
in O
Figure O
3 O
) O
as O
follows O
, O
where O
Select O
( O
• O
) O
is O

a O
function O
that O
takes O
in O
all O
introductory O
sentence O
representations O
( O
i.e. O
, O
h O
) O
, O
and O
introductory O
sentence O
probabilitiesp. O
It O
then O
outputs O
the O
representations O
associated O
with O
top O
k O
introductory O
sentences O
, O
sorted O
byp. O
To O
extract O
top O
introductory O
sentences O
, O
we O
first O
sorth O
vectors O
based O
on O
their O
computed O
probabilitiesp O
and O
then O

we O
pick O
up O
top B-HyperparameterName
k I-HyperparameterName
hidden I-HyperparameterName
vectors I-HyperparameterName
( O
i.e. O
, O
h B-HyperparameterName
top I-HyperparameterName
) O
that O
has O
the O
highest O
probability. O
MLP O
1 O
is O
a O
multi-layer O
perceptron O
that O
takes O
in O
concatenated O
vector O
of O
top O
introductory O
sentences O
and O
projects O
it O
into O
a O
new O
vector O
calledĥ O
. O
h B-HyperparameterName
top I-HyperparameterName
= O
Select O
( O
h O
, O
p O

, O
k O
) O
h O
= O
MLP O
1 O
( O
h O
top O
) O
( O
3 O
) O
At O
the O
final O
stage O
, O
we O
concatenate O
the O
transformed O
introductory O
top O
sentence O
representations O
( O
i.e. O
, O
ĥ O
) O
with O
each O
source O
sentence O
representations O
from O
Eq. O
1 O
( O
i.e. O
, O
h O
i O
where O
i O
shows O
the O
ith O

paper O
sentence O
) O
and O
process O
them O
to O
produce O
a O
resulting O
vector O
r O
which O
is O
intro-aware O
source O
sentence O
hidden O
representations. O
After O
processing O
the O
resulting O
vector O
through O
a O
linear O
output O
layer O
( O
with O
W O
z O
and O
b O
z O
as O
trainable O
parameters O
) O
, O
we O
obtain O
final O
introaware O
sentence O
extraction O
probabilities O
( O
i.e. O

, O
p O
) O
as O
follows O
, O
r O
= O
MLP O
2 O
( O
h O
i O
; O
ĥ O
) O
p O
= O
σ O
( O
W O
z O
r O
+ O
b O
z O
) O
( O
4 O
) O
in O
which O
MLP O
2 O
is O
a O
multi-layer O
perceptron O
, O
influencing O
the O
knowledge O
from O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O

t O
2 O
) O
into O
the O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O
t O
1 O
) O
. O
We O
train O
both O
tasks O
through O
our O
end-to-end O
system O
jointly O
as O
follows O
, O
ℓ O
total O
= O
( O
α O
) O
ℓ O
t O
1 O
+ O
( O
1 O
− O
α O
) O
ℓ O
t O
2 O
( O
5 O
) O
where O

ℓ B-MetricName
t I-MetricName
1 I-MetricName
, O
and O
ℓ B-MetricName
t I-MetricName
2 I-MetricName
are O
the O
losses B-MetricName
computed O
for O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
and O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
tasks O
, O
α O
is O
the O
regularizing O
parameter O
that O
balances O
the O
learning O
process O
between O
two O
tasks O
, O
and O
ℓ B-MetricName
total I-MetricName
is O
the O
total O
computed O
loss B-MetricName
that O
is O
optimized O
during O
the O
training O

. O
Experimental O
Setup O
In O
this O
section O
, O
we O
explain O
the O
datasets O
, O
baselines O
, O
and O
preprocessing O
and O
training O
parameters O
. O
Dataset O
We O
use O
two O
publicly O
available O
scientific O
extended O
summarization O
datasets O
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
. O
-arXiv-Long O
: O
A O
set O
of O
arXiv O
scientific O
papers O
containing O
papers O
from O
various O

scientific O
domains O
such O
as O
physics O
, O
mathematics O
, O
computer O
science O
, O
quantitative O
biology. O
arXiv-Long B-DatasetName
is O
intended O
for O
extended B-TaskName
summarization I-TaskName
task O
and O
was O
filtered O
from O
a O
larger O
dataset O
i.e. O
, O
arXiv O
for O
the O
summaries O
of O
more O
than O
350 O
tokens. O
The O
ground-truth O
summaries O
( O
i.e. O
, O
abstract O
) O
are O
long O
, O
with O

the O
average O
length O
of O
574 O
tokens. O
It O
contains O
7816 O
( O
train O
) O
, O
1381 O
( O
validation O
) O
, O
and O
1952 O
( O
test O
) O
papers O
. O
-PubMed-Long B-DatasetName
: O
A O
set O
of O
biomedical O
scientific O
papers O
from O
PubMed O
with O
average O
summary O
length O
of O
403 O
tokens. O
This O
dataset O
contains O
79893 O
( O
train O
) O
, O

4406 O
( O
validation O
) O
, O
and O
4402 O
( O
test O
) O
scientific O
papers O
. O
Baselines O
We O
compare O
our O
model O
with O
two O
strong O
non-neural O
systems O
, O
and O
four O
state-of-the-art O
neural O
summarizers. O
We O
use O
all O
of O
these O
baselines O
for O
the O
purpose O
of O
extended O
summary O
generation O
whose O
documents O
hold O
different O
characteristics O
in O
length O
, O

writing O
style O
, O
and O
discourse O
structure O
as O
compared O
to O
documents O
in O
the O
other O
domains O
of O
summarization O
. O
-LSA B-MethodName
( O
Steinberger O
and O
Jezek O
, O
2004 O
) O
: O
an O
extractive O
vector-based O
model O
that O
utilizes O
Singular B-MethodName
Value I-MethodName
Decomposition I-MethodName
( O
SVD B-MethodName
) O
to O
find O
the O
semantically O
important O
sentences O
. O
-LEXRANK B-MethodName
( O
Erkan O
and O
Radev O

, O
2004 O
) O
: O
a O
widely O
adopted O
extractive O
summarization O
baseline O
that O
utilizes O
a O
graph-based O
approach O
based O
on O
eigenvector O
centrality O
to O
identify O
the O
most O
salient O
sentences O
. O
-BERTSUMEXT B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
: O
a O
contextualized O
summarizer O
fine-tuned O
for O
summarization O
task O
, O
which O
encodes O
input O
sentence O
representations O
, O
and O

then O
processes O
them O
through O
a O
multi-layer O
Transformers O
encoder O
to O
obtain O
document-level O
sentence O
representation. O
Finally O
, O
a O
linear O
output O
layer O
with O
Sigmoid O
activation O
function O
outputs O
a O
probability O
distribution O
over O
each O
input O
sentence O
, O
denoting O
the O
extent O
to O
which O
they O
are O
probable O
to O
be O
extracted O
. O
-BERTSUMEXT-INTRO B-MethodName
( O
Liu O
and O
Lapata O
, O

2019 O
) O
: O
a O
BERTSUMEXT B-MethodName
model O
that O
only O
runs O
on O
the O
introductory O
sentences O
as O
the O
input O
, O
and O
extracts O
the O
salient O
introductory O
sentences O
as O
the O
summary O
. O
-BERTSUMEXTMULTI B-MethodName
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
: O
an O
extension O
of O
the O
BERTSUMEXT B-MethodName
model O
that O
incorporates O
an O
additional O
linear O
layer O
with O
Sigmoid O

classifier O
to O
output O
a O
probability O
distribution O
over O
a O
fixed O
number O
of O
pre-defined O
sections O
that O
an O
input O
sentence O
might O
belong O
to. O
The O
additional O
network O
is O
expected O
to O
predict O
a O
single O
section O
for O
an O
input O
sentence O
and O
is O
trained O
jointly O
with O
BERTSUMEXT B-MethodName
module O
( O
i.e. O
, O
sentence O
extractor O
) O
. O
-BART B-MethodName
( O

Lewis O
et O
al. O
, O
2020 O
) O
: O
a O
state-of-the-art O
abstractive O
summarization O
model O
that O
makes O
use O
of O
pretrained O
encoder O
and O
decoder. O
BART B-MethodName
can O
be O
thought O
of O
as O
an O
extension O
of O
BERTSUM B-MethodName
in O
which O
merely O
encoder O
is O
pre-trained O
, O
but O
decoder O
is O
trained O
from O
scratch. O
While O
our O
model O
is O
an O
extractive O
one O

, O
at O
the O
same O
time O
, O
we O
find O
it O
of O
value O
to O
measure O
the O
abstractive O
model O
performance O
in O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task O
. O
Preprocessing O
, O
parameters O
, O
labeling O
, O
and O
implementation O
details O
We O
used O
the O
open O
implementation O
of O
BERT-SUMEXT B-MethodName
with O
default O
parameters O
5 B-HyperparameterValue
. O
To O
implement O
the O
non-neural O
baseline O

models O
, O
we O
utilized O
Sumy O
python O
package O
6 O
. O
Longformer B-MethodName
model O
( O
Beltagy O
et O
al. O
, O
2020 O
) O
is O
utilized O
as O
our O
contextualized O
language O
model O
for O
running O
all O
the O
models O
due O
to O
its O
efficacy O
at O
processing O
long O
documents. O
For O
our O
model O
, O
the O
cross-entropy O
loss O
function O
is O
set O
for O
two O

tasks O
( O
i.e. O
, O
t O
1 O
: O
source O
sentence O
extraction O
and O
t O
2 O
: O
introductory O
sentences O
extraction O
in O
Figure O
3 O
) O
and O
the O
model O
is O
optimized O
through O
multi-tasking O
approach O
as O
discussed O
in O
Section O
3. O
The O
model O
with O
the O
highest O
ROUGE-2 B-MetricName
on O
validation O
set O
is O
selected O
for O
inference. O
The O
validation O
is O

performed O
every O
2k B-HyperparameterValue
training B-HyperparameterName
steps. I-HyperparameterName
α B-HyperparameterName
( O
in O
Eq. O
5 O
) O
is O
set O
to O
be O
0.5 B-HyperparameterValue
( O
empirically O
determined O
) O
. O
Our O
model O
includes O
474M B-HyperparameterValue
trainable B-HyperparameterName
parameters I-HyperparameterName
, O
trained O
on O
dual O
GeForce O
GTX O
1080Ti O
GPUs O
for O
approximately O
a O
week. O
We O
use O
k B-HyperparameterName
= O
5 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
k B-HyperparameterName
= O
8 B-HyperparameterValue

for O
PubMed-Long B-DatasetName
datasets O
( O
Eq. O
3 O
) O
. O
We O
make O
our O
model O
implementation O
as O
well O
as O
sample O
summaries O
publicly O
available O
to O
expedite O
ongoing O
research O
in O
this O
direction O
7 O
. O
A O
two-stage O
labeling O
approach O
was O
employed O
to O
identify O
ground-truth O
introductory O
and O
nonintroductory O
sentences. O
In O
the O
first O
stage O
, O
we O
used O
a O

greedy O
labeling O
approach O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
to O
label O
sentences O
within O
the O
first O
section O
of O
a O
given O
paper O
( O
i.e. O
, O
labeling O
introductory O
sentences O
) O
with O
respect O
to O
their O
ROUGE B-MetricName
overlap O
8 O
with O
the O
groundtruth O
summary O
( O
i.e. O
, O
abstract O
) O
. O
In O
the O
second O
stage O
, O

the O
same O
greedy O
approach O
was O
exploited O
over O
the O
rest O
of O
sentences O
( O
i.e. O
, O
non-introductory O
) O
9 O
with O
regard O
to O
their O
ROUGE B-MetricName
overlap O
with O
the O
identified O
introductory O
sentences O
in O
the O
first O
stage. O
Our O
choice O
of O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
is O
based O
on O
the O
fact O
that O
these O
express O
higher O
similarity O
with O
human O

judgments O
( O
Cohan O
and O
Goharian O
, O
2016 O
) O
. O
We O
continued O
the O
second O
stage O
until O
a O
fixed O
length O
of O
the O
summary O
was O
reached. O
Specifically O
, O
the O
fixed B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
labels I-HyperparameterName
is O
set O
to O
be O
15 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
and O
20 B-HyperparameterValue
for O
PubMed-Long B-DatasetName
datasets O
as O
these O
achieved O
the O
highest O
oracle O

ROUGE B-MetricName
scores O
in O
our O
experiments O
. O
Results O
Experimental O
evaluation O
The O
recent O
effort O
in O
extended O
summarization O
and O
its O
shared O
task O
of O
LongSumm O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
used O
average O
ROUGE B-MetricName
( O
F1 B-MetricName
) O
to O
rank O
the O
participating O
systems O
, O
in O
addition O
to O
commonly-used O
ROUGE-N B-MetricName
scores. O
Table O
2 O
shows O
the O

performance O
of O
the O
participated O
systems O
on O
the O
blind O
test O
set. O
As O
shown O
, O
BERTSUMEXTMULTI B-MethodName
model O
outperforms O
other O
models O
by O
a O
large O
margin O
( O
i.e. O
, O
with O
relative O
improvements O
of O
6 B-MetricValue
% I-MetricValue
and O
3 B-MetricValue
% I-MetricValue
on O
ROUGE-1 B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( O
F1 B-MetricName
) O
, O
respectively O
) O
; O
hence O
, O
we O
use O

the O
best-performing O
in O
terms O
of O
F1 B-MetricName
( O
i.e. O
, O
BERTSUMEXTMULTI B-MethodName
model O
) O
in O
our O
experiments. O
Tables. O
1 O
presents O
our O
results O
on O
the O
test O
sets O
of O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
datasets O
, O
respectively. O
As O
observed O
, O
our O
model O
statistically O
significantly O
outperforms O
the O
state-of-the-art O
systems O
on O
both O
datasets O
across O
most O
of O
the O
ROUGE B-MetricName

vari-ants O
, O
except O
ROUGE-L B-MetricName
on O
PubMed-Long. B-DatasetName
The O
improvements O
gained O
by O
our O
model O
validates O
our O
hypothesis O
that O
incorporating O
the O
salient O
introductory O
sentence O
representations O
into O
the O
extractive O
summarizer O
yields O
a O
promising O
improvement. O
Two O
nonneural O
models O
( O
i.e. O
, O
LSA B-MethodName
and O
LEXRANK B-MethodName
) O
underperform O
the O
neural O
models O
, O
as O
expected. O
Comparing O
the O
abstractive O

model O
( O
i.e. O
, O
BART B-MethodName
) O
with O
extractive O
neural O
ones O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
, O
we O
see O
that O
while O
there O
is O
relatively O
a O
smaller O
gap O
in O
terms O
of O
ROUGE-1 B-MetricName
, O
the O
gap O
is O
larger O
for O
ROUGE-2 B-MetricName
, O
and O
ROUGE-L. B-MetricName
Interestingly O
, O
in O
the O
case O
of O
BART B-MethodName
, O

we O
found O
that O
generating O
extended O
summaries O
is O
rather O
challenging O
for O
abstractive O
summarizers. O
Current O
abstractive O
summarizers O
including O
BART B-MethodName
have O
difficulty O
in O
abstracting O
very O
detailed O
information O
, O
such O
as O
numbers O
, O
and O
quantities O
, O
which O
hurts O
the O
faithfulness O
of O
the O
generated O
summaries O
to O
the O
source. O
This O
behavior O
has O
a O
detrimental O
effect O
, O

specifically O
, O
on O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
as O
their O
high O
correlation O
with O
human O
judgments O
in O
terms O
of O
faithfulness O
has O
been O
shown O
( O
Pagnoni O
et O
al. O
, O
2021 O
) O
. O
Comparing O
the O
extractive O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
models O
, O
while O
BERTSUMMULTIEXT B-MethodName
is O
expected O
to O
outperfom O
BERTSUMEXT B-MethodName
, O
it O
is O
observed O
that O
they O
perform O

almost O
similarly O
, O
with O
small O
( O
i.e. O
, O
insignificant O
) O
improved O
metrics. O
This O
might O
be O
due O
to O
the O
fact O
that O
BERTSUMEXTMULTI B-MethodName
works O
out-of-the-box O
when O
a O
handful O
amount O
of O
sentences O
are O
sampled O
from O
diverse O
sections O
to O
form O
the O
oracle O
summary O
as O
also O
reported O
by O
its O
authors. O
However O
, O
when O
labeling O
oracle O

sentences O
in O
our O
framework O
( O
i.e. O
, O
Intro-guided O
labeling O
) O
, O
there O
is O
no O
guarantee O
that O
the O
final O
set O
of O
oracle O
sentences O
are O
labeled O
from O
diverse O
sections. O
Overall O
, O
our O
model O
achieves O
about O
1.4 B-MetricValue
% I-MetricValue
, O
2.4 B-MetricValue
% I-MetricValue
, O
3.5 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
and O
1.0 B-MetricValue
% I-MetricValue
, O
2.5 B-MetricValue

% I-MetricValue
, O
1.3 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
across O
ROUGE B-MetricName
score O
variants O
; O
and O
2.2 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
1.4 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
over O
F1 B-MetricName
, O
compared O
to O
the O
neural O
baselines O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
. O
While O
comparing O
our O
model O
with O
BERTSUMEXT-INTRO B-MethodName
, O
we O

see O
the O
vital O
effect O
of O
adding O
second O
encoder O
at O
finding O
supplementary O
sentences O
across O
non-introductory O
sections O
, O
where O
our O
model O
gains O
relative O
improvements O
of O
9.62 B-MetricValue
% I-MetricValue
-26.26 B-MetricValue
% I-MetricValue
-16.09 B-MetricValue
% I-MetricValue
and O
9.40 B-MetricValue
% I-MetricValue
-5.27 B-MetricValue
% I-MetricValue
-9.99 B-MetricValue
% I-MetricValue
for O
ROUGE-1 B-MetricName
, O
ROUGE-2 B-MetricName
, O
ROUGE-L B-MetricName
on O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
, O
respectively. O
In O
fact O

, O
the O
sentences O
that O
are O
picked O
as O
summary O
from O
the O
in- O
troduction O
section O
are O
not O
comprehensive O
as O
such O
they O
are O
clues O
to O
the O
main O
points O
of O
the O
paper. O
The O
other O
important O
sentences O
are O
picked O
from O
the O
supplementary O
parts O
( O
i.e. O
, O
non-introductory O
) O
of O
the O
paper O
. O
Human O
evaluation O
While O

our O
model O
statistically O
significantly O
improves O
upon O
the O
state-of-the-art O
baselines O
in O
terms O
of O
ROUGE B-MetricName
scores O
, O
a O
few O
works O
have O
reported O
the O
low O
correlation O
of O
ROUGE B-MetricName
with O
human O
judgments O
( O
Liu O
and O
Liu O
, O
2008 O
; O
Cohan O
and O
Goharian O
, O
2016 O
; O
Fabbri O
et O
al. O
, O
2021 O
) O
. O
In O
order O

to O
provide O
insights O
into O
why O
and O
how O
our O
model O
outperforms O
the O
bestperforming O
baselines O
, O
we O
perform O
a O
manual O
analysis O
of O
our O
system O
's O
generated O
summaries O
, O
BERT-SUMEXT B-MethodName
's I-MethodName
, O
and O
BERTSUMEXTMULTI's. B-MethodName
For O
the O
sake O
of O
evaluation O
, O
two O
annotators O
were O
asked O
to O
manually O
evaluate O
two O
sets O
of O
40 O
papers O
' O

groundtruth O
abstracts O
( O
40 O
for O
arXiv-Long B-DatasetName
, O
and O
40 O
for O
PubMed-Long B-DatasetName
) O
with O
their O
generated B-TaskName
extended I-TaskName
summaries I-TaskName
( O
baselines O
' O
and O
ours O
) O
to O
gain O
insights O
into O
qualities O
of O
each O
model. O
Annotators O
were O
Electrical O
Engineering O
and O
Computer O
Science O
PhD O
students O
and O
familiar O
with O
principles O
of O
reading O
scientific O
papers. O
Samples O
were O

randomly O
selected O
from O
the O
test O
set O
, O
one O
from O
each O
40 O
evenly-spaced O
bins O
sorted O
by O
the O
difference O
of O
ROUGE-L B-MetricName
between O
two O
experimented O
systems O
. O
The O
evaluations O
were O
performed O
according O
to O
two O
metrics O
: O
( O
1 O
) O
Cohesion B-MetricName
: O
whether O
the O
ordering O
of O
sentences O
in O
summary O
is O
cohesive O
, O
namely O
sentences O

entail O
each O
other. O
( O
2 O
) O
Completeness B-MetricName
: O
whether O
the O
summary O
covers O
all O
salient O
information O
provided O
in O
the O
ground-truth O
summary. O
To O
prevent O
bias O
in O
selecting O
summaries O
, O
the O
ordering O
of O
systemgenerated O
summaries O
were O
shuffled O
such O
that O
it O
could O
not O
be O
guessed O
by O
the O
annotators. O
Annotators O
were O
asked O
to O
specify O
if O

the O
first O
system-generated O
summary O
wins O
/ O
loses O
or O
ties O
with O
the O
second O
systemgenerated O
summary O
in O
terms O
of O
qualitative O
metrics. O
It O
has O
to O
be O
mentioned O
that O
since O
our O
model O
is O
purely O
extractive O
, O
it O
does O
not O
introduce O
any O
fact O
that O
is O
unfaithful O
to O
the O
source O
. O
Our O
human O
evaluation O
results O
along O

with O
Cohen O
's O
kappa O
( O
Cohen O
, O
1960 O
) O
inter-rater O
agreements O
are O
shown O
in O
Table O
3 O
( O
agr. O
column O
) O
. O
As O
shown O
, O
our O
system O
's O
generated O
summaries O
improve O
completeness B-MetricName
and O
cohesion B-MetricName
in O
over O
40 B-MetricValue
% I-MetricValue
for O
most O
of O
the O
cases O
( O
6 O
out O
of O
8 O
for O
win O
cases O

10 O
) O
. O
Specifically O
, O
when O
comparing O
with O
BERTSUMEXT B-MethodName
, O
we O
see O
that O
68 B-MetricValue
% I-MetricValue
, O
80 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
60 B-MetricValue
% I-MetricValue
, O
66 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
sampled O
summaries O
are O
at O
least O
as O
good O
as O
or O
better O
than O
the O
corresponding O
baseline O
's O
generated O
summaries O
in O

terms O
of O
cohesion O
and O
completeness O
, O
respectively. O
Overall O
, O
across O
two O
metrics O
for O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
, O
we O
gain O
relative O
improvements O
over O
the O
baselines O
: O
25.6 B-MetricValue
% I-MetricValue
, O
19.0 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
56.5 B-MetricValue
% I-MetricValue
, O
[ O
Introductory O
] O
The O
objective O
of O
the O
work O
presented O
here O
is O
to O

study O
the O
mechanism O
of O
radiative O
line O
driving O
and O
the O
corresponding O
properties O
of O
the O
winds O
of O
possible O
generations O
of O
very O
massive O
stars O
at O
extremely O
low O
metallicities O
and O
to O
investigate O
the O
principal O
influence O
of O
these O
winds O
on O
ionizing O
fluxes O
and O
observable O
ultraviolet O
spectra O
. O
[ O
" O
# O
] O
The O
basic O
new O
element O

of O
this O
approach O
, O
needed O
in O
the O
domain O
of O
The O
purpose O
of O
this O
first O
study O
is O
to O
provide O
an O
estimate O
about O
the O
strengths O
of O
stellar O
winds O
at O
very O
low O
metallicity O
for O
very O
massive O
hot O
stars O
in O
a O
mass O
range O
roughly O
between O
100 O
to O
300 O
m O
@ O
xmath3 O
. O
[ O
" O

) O
] O
With O
our O
new O
approach O
to O
describe O
line O
driven O
stellar O
winds O
at O
extremely O
low O
metallicity O
we O
were O
able O
to O
make O
first O
predictions O
of O
stellar O
wind O
properties O
, O
ionizing O
fluxes O
and O
synthetic O
spectra O
of O
a O
possible O
population O
of O
very O
massive O
stars O
in O
this O
range O
of O
metallicity. O
46.7 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName

) O
on O
arXiv-Long B-DatasetName
; O
and O
23.1 B-MetricValue
% I-MetricValue
, O
13.5 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
27.7 B-MetricValue
% I-MetricValue
, O
21.9 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName
) O
on O
PubMed-Long. B-DatasetName
11 O
These O
improvements O
, O
qualitatively O
evaluated O
by O
the O
human O
annotators O
, O
show O
the O
promising O
capability O
of O
our O
purposed O
model O
in O
generating O
improved O
extended O
summaries O
which O

are O
more O
preferable O
than O
the O
baselines'. O
We O
observe O
a O
similar O
improvement O
trend O
when O
comparing O
our O
summaries O
with O
BERTSUMEXTMULTI B-MethodName
, O
where O
66 B-MetricValue
% I-MetricValue
, O
77 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
58 B-MetricValue
% I-MetricValue
, O
58 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
our O
summaries O
are O
as O
good O
as O
or O
better O
than O
the O
baseline O

's O
in O
terms O
of O
cohesion O
and O
completeness. O
Looking O
at O
the O
Cohen O
's O
inter-rater O
agreement O
, O
the O
correlation B-MetricName
scores O
fall O
into O
" B-MetricValue
moder- I-MetricValue
11 I-MetricValue
Relative O
improvement O
of O
win O
rate O
over O
lose O
rate O
. O
ate O
" O
agreement O
range O
according O
to O
the O
interpretation O
of O
Cohen O
's O
kappa O
range O
( O
McHugh O
, O
2012 O
) O

. O

-DOCSTART- O
CERES B-MethodName
: O
Pretraining O
of O
Graph-Conditioned O
Transformer O
for O
Semi-Structured O
Session O
Data O
User O
sessions O
empower O
many O
search O
and O
recommendation O
tasks O
on O
a O
daily O
basis. O
Such O
session O
data O
are O
semi-structured O
, O
which O
encode O
heterogeneous O
relations O
between O
queries O
and O
products O
, O
and O
each O
item O
is O
described O
by O
the O
unstructured O
text. O
Despite O
recent O
advances O

in O
self-supervised O
learning O
for O
text O
or O
graphs O
, O
there O
lack O
of O
self-supervised O
learning O
models O
that O
can O
effectively O
capture O
both O
intra-item O
semantics O
and O
inter-item O
interactions O
for O
semi-structured O
sessions. O
To O
fill O
this O
gap O
, O
we O
propose O
CERES B-MethodName
, O
a O
graph-based O
transformer O
model O
for O
semi-structured O
session O
data. O
CERES B-MethodName
learns O
representations O
that O
capture O
both O

inter-and O
intra-item O
semantics O
with O
( O
1 O
) O
a O
graph-conditioned O
masked O
language O
pretraining O
task O
that O
jointly O
learns O
from O
item O
text O
and O
item-item O
relations O
; O
and O
( O
2 O
) O
a O
graph-conditioned O
transformer O
architecture O
that O
propagates O
inter-item O
contexts O
to O
itemlevel O
representations. O
We O
pretrained O
CERES B-MethodName
using O
∼468 O
million O
Amazon O
sessions O
and O
find O
that O
CERES B-MethodName

outperforms O
strong O
pretraining O
baselines O
by O
up O
to O
9 O
% O
in O
three O
session O
search O
and O
entity O
linking O
tasks O
. O
Introduction O
User O
sessions O
are O
ubiquitous O
in O
online O
e-commerce O
stores. O
An O
e-commerce O
session O
contains O
customer O
interactions O
with O
the O
platform O
in O
a O
continuous O
period. O
Within O
one O
session O
, O
the O
customer O
can O
issue O
multiple O
queries O

and O
take O
various O
actions O
on O
the O
retrieved O
products O
for O
these O
queries O
, O
such O
as O
clicking O
, O
adding O
to O
cart O
, O
and O
purchasing. O
Sessions O
are O
important O
in O
many O
e-commerce O
applications O
, O
e.g. O
, O
product O
recommendation O
( O
Wu O
et O
al. O
, O
2019a O
) O
, O
query O
recommendation O
( O
Cucerzan O
and O
White O
, O
2007 O

) O
, O
and O
query O
understanding O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O
This O
paper O
considers O
sessions O
as O
semi-structured O
data O
, O
as O
illustrated O
in O
Figure O
1. O
At O
the O
higher O
level O
, O
sessions O
are O
heterogeneous O
graphs O
that O
contain O
interactions O
between O
items. O
At O
the O
lower O
level O
, O
each O
graph O
node O
has O
unstructured O

text O
descriptions O
: O
we O
can O
describe O
queries O
by O
search O
keywords O
and O
products O
by O
titles O
, O
attributes O
, O
customer O
reviews O
, O
and O
other O
descriptors. O
Our O
goal O
is O
to O
simultaneously O
encode O
both O
the O
graph O
and O
text O
aspects O
of O
the O
session O
data O
to O
understand O
customer O
preferences O
and O
intents O
in O
a O
session O
context O
. O

Pretraining O
on O
semi-structured O
session O
data O
remains O
an O
open O
problem. O
First O
, O
existing O
works O
on O
learning O
from O
session O
data O
usually O
treat O
a O
session O
as O
a O
sequence O
or O
a O
graph O
( O
Xu O
et O
al. O
, O
2019 O
; O
You O
et O
al. O
, O
2019 O
; O
Qiu O
et O
al. O
, O
2020b O
) O
. O
While O
they O

can O
model O
inter-item O
relations O
, O
they O
do O
not O
capture O
the O
rich O
intra-item O
semantics O
when O
text O
descriptions O
are O
available. O
Furthermore O
, O
these O
models O
are O
usually O
large O
neural O
networks O
that O
require O
massive O
labeled O
data O
to O
train O
from O
scratch. O
Another O
line O
of O
research O
utilizes O
large-scale O
pretrained O
language O
models O
( O
Lan O
et O
al. O
, O

2019 O
; O
Clark O
et O
al. O
, O
2020 O
) O
as O
text O
encoders O
for O
session O
items. O
However O
, O
they O
fail O
to O
model O
the O
relational O
graph O
structure. O
Several O
works O
attempt O
to O
improve O
language O
models O
with O
a O
graph-structured O
knowledge O
base O
, O
such O
as O
in O
( O
Liu O
et O
al. O
, O
2020 O
; O
Yao O
et O
al. O

, O
2019 O
; O
. O
While O
adjusting O
the O
semantics O
of O
entities O
according O
to O
the O
knowledge O
graph O
, O
they O
fail O
to O
encode O
general O
graph O
structures O
in O
sessions O
. O
We O
propose O
CERES B-MethodName
( O
Graph B-MethodName
Conditioned I-MethodName
Encoder I-MethodName
Representations I-MethodName
for I-MethodName
Session I-MethodName
Data I-MethodName
) O
, O
a O
pretraining O
model O
for O
semi-structured O
e-commerce O
session O
data O
, O
which O
can O

serve O
as O
a O
generic O
session O
encoder O
that O
simultaneously O
captures O
both O
intra-item O
semantics O
and O
inter-item O
relations. O
Beyond O
training O
a O
potent O
language O
model O
for O
intra-item O
semantics O
, O
our O
model O
also O
conditions O
the O
language O
modeling O
task O
on O
graph-level O
session O
information O
, O
thus O
encouraging O
the O
pretrained O
model O
to O
learn O
how O
to O
utilize O
inter-item O
signals. O

Our O
model O
architecture O
tightly O
integrates O
two O
key O
components O
: O
( O
1 O
) O
an O
item O
Transformer O
encoder O
, O
which O
captures O
text O
semantics O
of O
session O
items O
; O
and O
( O
2 O
) O
a O
graph O
conditioned O
Transformer O
, O
which O
aggregates O
and O
propagates O
inter-item O
relations O
for O
cross-item O
prediction. O
As O
a O
result O
, O
CERES B-MethodName
models O
the O

higher-level O
interactions O
between O
items O
. O
We O
have O
pretrained O
CERES B-MethodName
using O
468,199,822 O
sessions O
and O
performed O
experiments O
on O
three O
session-based O
tasks O
: O
product O
search O
, O
query O
search O
, O
and O
entity O
linking. O
By O
comparing O
with O
publicly O
available O
state-of-the-art O
language O
models O
and O
domain-specific O
language O
models O
trained O
on O
alternative O
representations O
of O
session O
data O
, O
we O

show O
that O
CERES B-MethodName
outperforms O
strong O
baselines O
on O
various O
session-based O
tasks O
by O
large O
margins. O
Experiments O
show O
that O
CERES B-MethodName
can O
effectively O
utilize O
sessionlevel O
information O
for O
downstream O
tasks O
, O
better O
capture O
text O
semantics O
for O
session O
items O
, O
and O
perform O
well O
even O
with O
very O
scarce O
training O
examples O
. O
We O
summarize O
our O
contributions O
as O
follows O

: O
1 O
) O
We O
propose O
CERES B-MethodName
, O
a O
pretrained O
model O
for O
semistructured O
e-commerce O
session O
data. O
CERES B-MethodName
can O
effectively O
encode O
both O
e-commerce O
items O
and O
sessions O
and O
generically O
support O
various O
sessionbased O
downstream O
tasks. O
2 O
) O
We O
propose O
a O
new O
graph-conditioned O
transformer O
model O
for O
pretraining O
on O
general O
relational O
structures O
on O
text O
data. O
3 O

) O
We O
conducted O
extensive O
experiments O
on O
a O
largescale O
e-commerce O
benchmark O
for O
three O
sessionrelated O
tasks. O
The O
results O
show O
the O
superiority O
of O
CERES B-MethodName
over O
strong O
baselines O
, O
including O
mainstream O
pretrained O
language O
models O
and O
state-ofthe-art O
deep O
session O
recommendation O
models O
. O
Customer O
Sessions O
A O
customer O
session O
is O
the O
search O
log O
before O
a O
final O
purchase O

action. O
It O
consists O
of O
customer-queryproduct O
interactions O
: O
a O
customer O
submits O
search O
queries O
obtains O
a O
list O
of O
products. O
The O
customer O
may O
take O
specific O
actions O
, O
including O
view O
and O
purchase O
on O
the O
retrieved O
products. O
Hence O
, O
a O
session O
contains O
two O
types O
of O
items O
: O
queries O
and O
products O
, O
and O
various O
relations O
between O

them O
established O
by O
customer O
actions O
. O
We O
define O
each O
session O
as O
a O
relational O
graph O
G O
= O
( O
V O
, O
E O
) O
that O
contains O
all O
queries O
and O
products O
in O
a O
session O
and O
their O
relations. O
The O
vertex O
set O
V O
= O
( O
Q O
, O
P O
) O
is O
partitioned O
into O
ordered B-HyperparameterName
query I-HyperparameterName
set I-HyperparameterName
Q I-HyperparameterName

and O
unordered B-HyperparameterName
product I-HyperparameterName
set I-HyperparameterName
P. I-HyperparameterName
The O
queries O
Q B-HyperparameterName
= O
( O
q O
1 O
, O
. O
. O
. O
, O
q O
n O
) O
are O
indexed O
by O
order O
of O
the O
customer O
's O
searches. O
The O
edge B-HyperparameterName
set I-HyperparameterName
E I-HyperparameterName
contains O
two O
types O
of O
edges O
: O
{ O
( O
q O
i O
, O
q O
j O
) O
, O
i O
< O

j O
} O
are O
one-directional O
edges O
that O
connect O
each O
query O
to O
its O
previous O
queries O
; O
and O
{ O
q O
i O
, O
p O
j O
, O
a O
ij O
} O
are O
bidirectional O
edges O
that O
connects O
the O
ith O
query O
and O
jth O
product O
, O
if O
the O
customer O
took O
action O
a O
ij O
on O
product O
p O
j O
retrieved O
by O

query O
q O
j O
. O
The O
queries O
and O
products O
are O
represented O
by O
textual O
descriptions. O
Specifically O
, O
each O
query O
is O
represented O
by O
customer-generated O
search O
keywords. O
Each O
product O
is O
represented O
with O
a O
table O
of O
textual O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
description. O
In O
this O
paper O
, O
we O
call O

" O
product O
sequence O
" O
as O
the O
concatenation O
of O
title O
and O
description. O
A O
product O
may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
their O
specific O
categories O
. O
Our O
Method O
In O
this O
section O
we O
present O
the O
details O
of O
CERES. B-MethodName
We O
first O
describe O

our O
designed O
session O
pretraining O
task O
in O
Section O
3.1 O
, O
and O
then O
describe O
the O
model O
architecture O
of O
CERES B-MethodName
in O
Section O
3.2 O
. O
Graph-Conditioned O
Masked O
Language O
Modeling O
Task O
Suppose O
G O
= O
( O
V O
, O
E O
) O
is O
a O
graph O
on O
T O
text O
items O
as O
vertices O
, O
v O
1 O
, O
. O
. O
. O

, O
v O
T O
, O
each O
of O
which O
is O
a O
sequence O
of O
text O
tokens O
: O
v O
i O
= O
[ O
v O
i1 O
, O
. O
. O
. O
, O
v O
iT O
i O
] O
, O
i O
= O
1 O
, O
. O
. O
. O
, O
T O
. O
We O
propose O
graph-conditioned B-MethodName
masked I-MethodName
language I-MethodName
modeling I-MethodName
( O
GMLM B-MethodName
) O
, O

where O
masked O
tokens O
are O
predicted O
with O
both O
intra-item O
context O
and O
inter-item O
context O
: O
p O
GMLM B-MethodName
( O
v O
masked O
) O
= O
jth O
masked O
P O
( O
vij|G O
, O
{ O
v O
ik O
} O
kth O
unmasked O
) O
, O
( O
1 O
) O
which O
encourages O
the O
model O
to O
leverage O
information O
graph-level O
inter-item O
semantics O
efficiently O
in O
order O

to O
predict O
masked O
tokens. O
To O
optimize O
( O
1 O
) O
, O
we O
need O
to O
learn O
token-level O
embeddings O
that O
are O
infused O
with O
session-level O
information O
, O
which O
we O
introduce O
in O
Section O
3.2.2. O
Suppose O
certain O
tokens O
in O
the O
input O
sequence O
of O
items O
as O
masked O
( O
detailed O
below O
) O
, O
we O
optimize O
the O
predictions O
of O

the O
masked O
tokens O
with O
cross O
entropy O
loss. O
The O
pretraining O
framework O
is O
illustrated O
in O
Figure O
3. O
Token O
Masking O
Strategy O
. O
To O
mask O
tokens O
in O
long O
sequences O
, O
including O
product O
titles O
and O
descriptions O
, O
we O
follow O
( O
Devlin O
et O
al. O
, O
2018 O
) O
and O
choose O
15 O
% O
of O
the O
tokens O
for O
masking. O

For O
short O
sequences O
, O
including O
queries O
and O
product O
attributes O
, O
there O
is O
a O
50 O
% O
probability O
that O
a O
short O
sequence O
will O
be O
masked O
, O
and O
for O
those O
sequences O
50 O
% O
of O
their O
tokens O
are O
randomly O
selected O
for O
masking O
. O
Model O
Architecture O
To O
model O
the O
probability O
in O
( O
1 O
) O
, O

we O
design O
two O
key O
components O
in O
the O
CERES B-MethodName
model O
: O
1 O
) O
a O
Transformer-based O
item O
encoder O
, O
which O
produces O
token-level O
intra-item O
embeddings O
that O
contain O
context O
information O
within O
a O
single O
item O
; O
and O
2 O
) O
a O
graph-conditioned O
Transformer O
for O
session O
encoding O
, O
which O
produces O
session-level O
embeddings O
that O
encodes O
inter-item O
relations O
, O

and O
propagates O
the O
session O
information O
back O
to O
the O
token-level. O
We O
illustrate O
our O
model O
architecture O
in O
Figure O
2 O
. O
Item O
Transformer O
Encoder O
The O
session O
item O
encoder O
aims O
to O
encode O
intra-item O
textual O
information O
for O
each O
item O
in O
a O
session. O
We O
design O
the O
item O
encoder O
based O
on O
Transformers O
, O
which O
allows O
CERES B-MethodName
to O

leverage O
the O
expressive O
power O
of O
the O
self-attention O
mechanism O
for O
modeling O
domain-specific O
language O
in O
e-commerce O
sessions. O
Given O
an O
item O
i O
, O
the O
transformer-based O
item O
encoder O
compute O
its O
token O
embeddings O
as O
follows O
: O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
= O
Transformeritem O
( O
[ O
vi1 O
, O
. O
. O
. O

, O
viT O
i O
] O
) O
vi O
= O
Pool O
( O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
) O
, O
( O
2 O
) O
where O
v O
ij O
is O
the O
embedding O
of O
the O
jth O
token O
in O
the O
ith O
item O
, O
and O
v O
i O
is O
the O
pooled O
embedding O
of O
the O
ith O
item. O

At O
this O
stage O
, O
{ O
v O
ij O
} O
, O
{ O
v O
i O
} O
are O
embeddings O
that O
only O
encode O
the O
intra-item O
information. O
Details O
of O
Item O
Encoding. O
We O
detail O
the O
encoding O
method O
for O
the O
two O
types O
of O
items O
, O
queries O
and O
products O
, O
in O
the O
following O
paragraphs. O
Each O
query O
q O
i O
= O

[ O
q O
i1 O
, O
. O
. O
. O
, O
q O
iT O
i O
] O
is O
a O
sequence O
of O
tokens O
generated O
by O
customers O
as O
search O
keywords. O
We O
add O
a O
special O
token O
at O
the O
beginning O
of O
the O
queries O
, O
[ O
SEARCH O
] O
, O
to O
indicate O
that O
the O
sequence O
represents O
a O
customer O
's O
search O
keywords. O

Then O
, O
to O
obtain O
the O
token-level O
embedding O
of O
the O
queries O
and O
the O
pooled O
query O
embedding O
by O
taking O
the O
embedding O
of O
the O
special O
token O
[ O
SEARCH O
] O
. O
Each O
product O
p O
i O
is O
a O
table O
of O
K O
attributes O
: O
p O
1 O
, O
. O
. O
. O
, O
p O
K O
, O
where O
p O

1 O
is O
always O
the O
product O
sequence O
, O
which O
is O
the O
concatenation O
of O
product O
title O
and O
bullet O
description. O
Each O
attribute O
p O
k O
i O
= O
[ O
p O
k O
i1 O
, O
p O
k O
i2 O
, O
. O
. O
. O
] O
starts O
with O
a O
special O
token O
[ O
ATTRTYPE O
] O
, O
where O
ATTRTYPE O
is O
replaced O
with O

the O
language O
descriptor O
of O
the O
attribtue. O
Then O
, O
the O
Transformer O
is O
used O
to O
compute O
token O
and O
sentence O
embeddings O
for O
all O
attributes. O
The O
product O
embedding O
is O
obtained O
by O
average O
pooling O
of O
all O
attribute O
's O
sentence O
embeddings O
. O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
The O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
aims O
to O
infuse O
intra-item O
and O
inter-item O
information O

to O
produce O
item O
and O
token O
embeddings. O
For O
this O
purpose O
, O
we O
first O
design O
a O
position-aware B-MethodName
graph I-MethodName
neural I-MethodName
network I-MethodName
( O
PGNN B-MethodName
) O
to O
capture O
the O
interitem O
dependencies O
in O
a O
session O
graph O
to O
produce O
Item B-MethodName
Token I-MethodName
Embeddings I-MethodName
Latent B-MethodName
Conditioning I-MethodName
Tokens I-MethodName
Figure O
4 O
: O
Illustration O
of O
cross-attention O
over O
latent O
conditioning O
tokens. O
The O
item O

token O
embeddings O
perform O
self-attention O
as O
well O
as O
cross-attention O
over O
latent O
conditioning O
tokens O
, O
thus O
incorporating O
session-level O
information. O
Latent O
conditioning O
tokens O
perform O
selfattention O
to O
update O
their O
embeddings O
, O
but O
do O
not O
attend O
to O
item O
tokens O
to O
preserve O
session-level O
information O
. O
item O
embeddings. O
The O
effect O
of O
PGNN B-MethodName
is O
analyzed O
in O
Section O
4.4. O

Then O
conditioned O
on O
the O
PGNN-learned B-MethodName
item O
embedding O
, O
we O
propose O
a O
cross-attention O
Transformer O
, O
which O
produces O
infused O
item O
and O
token O
embeddings O
for O
the O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
task I-MethodName
. O
Position-Aware B-MethodName
Graph I-MethodName
Neural I-MethodName
Network. I-MethodName
We O
use O
a O
GNN B-MethodName
to O
capture O
inter-item O
relations. O
This O
will O
allow O
CERES B-MethodName
to O
obtain O
item O
embeddings O
that O

encode O
the O
information O
from O
other O
locally O
correlated O
items O
in O
the O
session. O
Let O
[ O
v O
1 O
, O
. O
. O
. O
, O
v O
N O
] O
denote O
the O
item O
embeddings O
produced O
by O
the O
intra-item O
transformer O
encoder. O
We O
treat O
them O
as O
hidden O
states O
of O
nodes O
in O
the O
session O
graph O
G O
and O
feed O
them O
to O

the O
GNN B-MethodName
model O
, O
obtaining O
session-level O
item O
embeddings O
[ O
v O
h O
1 O
, O
. O
. O
. O
, O
v O
h O
N O
] O
. O
The O
items O
in O
a O
session O
graph O
are O
sequential O
according O
to O
the O
order O
the O
customers O
generated O
them O
. O
To O
let O
the O
GNN B-MethodName
model O
learn O
of O
the O
positional O
information O
of O

items O
, O
we O
train O
an O
item O
positional O
embedding O
in O
the O
same O
way O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
trains O
positional O
embeddings O
of O
tokens. O
Before O
feeding O
the O
item O
embeddings O
to O
GNN B-MethodName
, O
the O
pooled O
item O
embeddings O
are O
added O
item O
positional O
embeddings O
according O
to O
their O
positions O
in O
the O
session O
's O

item O
sequence. O
In O
this O
way O
, O
the O
item O
embeddings O
{ O
v O
i O
} O
i∈V O
are O
encoded O
their O
positional O
information O
as O
well O
. O
Cross-Attention B-MethodName
Transformer. I-MethodName
Conditioned O
on O
PGNN B-MethodName
, O
we O
design O
a O
cross-attention O
transformer O
which O
propagates O
session-level O
information O
in O
PGNN-produced B-MethodName
item O
embeddings O
to O
all O
tokens O
to O
produce O
token O
embeddings O
that O
are O

infused O
with O
both O
intra-item O
and O
inter-item O
information O
. O
In O
order O
to O
propagate O
item O
embeddings O
to O
tokens O
, O
we O
treat O
item O
embeddings O
as O
latent O
tokens O
that O
can O
be O
treated O
as O
a O
" O
part O
" O
of O
item O
texts. O
for O
each O
item O
i O
, O
we O
first O
expand O
v O
h O
i O
to O
K O
latent O

conditioning O
tokens O
by O
using O
a O
multilayer O
perceptron O
module O
to O
map O
v O
h O
i O
to O
K O
embedding O
vectors O
[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
] O
of O
the O
same O
size. O
For O
each O
item O
i O
, O
we O
compute O
its O
latent O
conditioning O
tokens O
by O
averaging O
all O
latent O
tokens O

in O
its O
neighborhood. O
Suppose O
N O
( O
i O
) O
is O
the O
set O
of O
all O
neighboring O
items O
in O
the O
session O
graph O
, O
itself O
included. O
In O
each O
position O
, O
we O
take O
the O
average O
of O
the O
latent O
token O
embeddings O
in O
N O
( O
i O
) O
as O
the O
kth O
latent O
conditioning O
token O
, O
v O
h O
ik O

, O
for O
the O
ith O
item. O
Then O
, O
we O
concatenate O
the O
latent O
conditioning O
token O
embeddings O
and O
the O
item O
token O
embeddings O
obtained O
by O
the O
session O
item O
encoder O
: O
[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
, O
v O
i1 O
, O
. O
. O
. O
, O
v O
iN O
i O
] O

. O
( O
3 O
) O
Finally O
, O
we O
compute O
the O
token-level O
embeddings O
with O
session O
information O
by O
feeding O
the O
concatenated O
sequence O
to O
a O
shallow O
cross-attention O
Transformer. O
The O
cross-attention O
Transformer O
is O
of O
the O
same O
structure O
as O
normal O
Transformers. O
The O
difference O
is O
that O
we O
prohibit O
the O
latent O
conditioning O
tokens O
from O
attending O
over O
original O
item O

tokens O
to O
prevent O
the O
influx O
of O
intra-item O
information O
potentially O
diluating O
session-level O
information O
stored O
in O
latent O
conditioning O
tokens. O
Illustration O
of O
crossattention O
Transformer O
is O
provided O
in O
Figrue O
4 O
. O
We O
use O
the O
embeddings O
produced O
by O
this O
crossattention O
Transformer O
as O
the O
final O
embeddings O
for O
modeling O
the O
token O
probabilities O
in O
Equation O
( O
1 O
) O

and O
learning O
the O
masked O
language O
modeling O
tasks O
. O
During O
training O
, O
the O
model O
is O
encouraged O
to O
learn O
good O
token O
embeddings O
with O
the O
Item O
Transformer O
Encoder O
, O
as O
better O
embeddings O
{ O
v O
ij O
} O
N O
i O
j=1 O
is O
necessary O
to O
improve O
the O
quality O
of O
{ O
v O
c O
ij O
} O
N O
i O

j=1 O
. O
The O
Graph-Conditioned B-MethodName
Transformer I-MethodName
will O
be O
encouraged O
to O
produce O
high-quality O
session-level O
embeddings O
for O
the O
GMLM B-MethodName
task. O
Hence O
, O
CERES B-MethodName
is O
encouraged O
to O
produce O
high-quality O
embeddings O
that O
unify O
both O
intra-item O
and O
inter-item O
information O
. O
Finetuning O
When O
finetuning O
CERES B-MethodName
for O
downstream O
tasks O
, O
we O
first O
obtain O
session-level O
item O
embeddings. O
The O
session O

embedding O
is O
computed O
as O
the O
average O
of O
all O
item O
embeddings. O
To O
obtain O
embedding O
for O
a O
single O
item O
without O
session O
context O
, O
such O
as O
for O
retrieved O
items O
in O
recommendation O
tasks O
, O
only O
the O
Item O
Transformer O
Encoder O
is O
used O
. O
To O
measure O
the O
relevance O
of O
an O
item O
to O
a O
given O
session O
, O

we O
first O
transform O
the O
obtained O
embeddings O
by O
separate O
linear O
maps. O
Denote O
the O
transformed O
session O
embeddings O
as O
s O
and O
item O
embeddings O
as O
y. O
The O
similarity O
between O
them O
is O
computed O
by O
cosine O
similarity O
d O
cos O
( O
s O
, O
y O
) O
. O
To O
finetune O
the O
model O
, O
we O
optimize O
a O
hinge O
loss O
on O

the O
cosine O
similarity O
between O
sessions O
and O
items O
. O
Experiments O
Experiment O
Setup O
Dataset. O
We O
collected O
customer B-DatasetName
sessions I-DatasetName
from O
Amazon O
for O
pretraining O
and O
finetuning O
on O
downstream O
tasks. O
468,199,822 O
customer B-DatasetName
sessions I-DatasetName
are O
collected O
from O
August O
1 O
2020 O
to O
August O
31 O
2020 O
for O
pretraining. O
30,000 O
sessions O
are O
collected O
from O
September O
2020 O
to O
September O
7 O

2020 O
for O
downstream O
tasks. O
The O
pretraining O
and O
downstreaming O
datasets O
are O
from O
disjoint O
time O
spans O
to O
prevent O
data O
leakage. O
All O
data O
are O
cleaned O
and O
anonymized O
so O
that O
no O
personal O
information O
about O
customers O
was O
used. O
Each O
session O
is O
collected O
as O
follows O
: O
when O
a O
customer O
perform O
a O
purchase O
action O
, O
we O
backtrace O

all O
actions O
by O
the O
customer O
in O
600 O
seconds O
before O
the O
purchase O
until O
a O
previous O
purchase O
is O
encountered. O
The O
actions O
of O
customers O
include O
: O
1 O
) O
search O
, O
2 O
) O
view O
, O
3 O
) O
, O
add-to-cart O
, O
and O
4 O
) O
purchase. O
Search O
action O
is O
associated O
with O
customer O
generated O
query O
keywords. O
View O

, O
add-to-cart O
, O
and O
purchase O
are O
associated O
with O
the O
target O
products. O
All O
the O
products O
in O
the O
these O
sessions O
are O
gathered O
with O
their O
product O
title O
, O
bullet O
description O
, O
and O
various O
other O
attributes O
, O
including O
color O
, O
manufacturer O
, O
product O
type O
, O
size O
, O
etc. O
In O
total O
, O
we O
have O
37,580,637 O

products. O
The O
sessions O
have O
an O
average O
of O
3.24 O
queries O
and O
4.36 O
products. O
Queries O
have O
on O
average O
5.63 O
tokens O
, O
while O
product O
titles O
and O
bullet O
descriptions O
have O
averagely O
17.42 O
and O
96.01 O
tokens O
. O
Evaluation O
Tasks O
and O
Metrics. O
We O
evaluate O
all O
the O
compared O
models O
on O
the O
following O
tasks O
: O
1 O
) O
Product B-TaskName

Search. I-TaskName
In O
this O
task O
, O
given O
observed O
customer O
behaviors O
in O
a O
session O
, O
the O
model O
is O
asked O
to O
predict O
which O
product O
will O
be O
purchased O
from O
a O
pool O
of O
candidate O
products. O
The O
purchased O
products O
are O
removed O
from O
sessions O
to O
avoid O
trivial O
inference. O
The O
candidate O
product O
pool O
is O
the O
union O
of O
all O

purchased O
products O
in O
the O
test O
set O
and O
the O
first O
10 O
products O
returned O
by O
the O
search O
engine O
of O
all O
sessions O
in O
the O
test O
set O
. O
2 O
) O
Query B-TaskName
Search. I-TaskName
Query O
Search O
is O
a O
recommendation O
task O
where O
the O
model O
retrieves O
next O
queries O
for O
customers O
which O
will O
lead O
to O
a O
purchase. O
Given O
a O

session O
, O
we O
hide O
the O
last O
query O
along O
with O
products O
associated O
with O
it O
, O
i.e. O
viewed O
or O
purchased O
with O
the O
removed O
query. O
Then O
, O
we O
ask O
the O
model O
to O
predict O
the O
last O
query O
from O
a O
pool O
of O
candidate O
queries. O
The O
candidate O
query O
pool O
consists O
of O
all O
last O
queries O
in O
the O

test O
set O
. O
3 O
) O
Entity B-TaskName
Linking. I-TaskName
In O
this O
task O
we O
try O
to O
understand O
the O
deeper O
semantics O
of O
customer O
sessions. O
Specifically O
, O
if O
customer O
purchases O
a O
product O
in O
a O
session O
, O
the O
task O
is O
to O
predict O
the O
attributes O
of O
the O
purchased O
product O
from O
the O
rest O
contexts O
in O
the O
session. O
In O

total O
, O
we O
have O
60K O
possible O
product O
attributes O
. O
Baselines. O
The O
compared O
baselines O
can O
be O
categorized O
into O
three O
groups O
: O
1 O
) O
General-domain O
pretrained O
language O
models O
which O
include O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
, O
RoBERTa B-MethodName
, O
and O
ELECTRA B-MethodName
( O
Clark O
et O
al. O
, O
2020 O
) O
. O
These O

models O
are O
state-of-the-art O
pretrained O
language O
models O
, O
which O
can O
serve O
as O
general-purpose O
language O
encoders O
for O
items O
and O
enable O
downstream O
session-related O
tasks. O
Specifically O
, O
the O
language O
encoders O
produce O
item O
embeddings O
first O
, O
and O
compose O
session O
embeddings O
by O
pooling O
the O
items O
in O
sessions. O
To O
retrieve O
items O
for O
sessions O
, O
one O
can O
compare O

the O
cosine O
similarity O
between O
sessions O
and O
retrieved O
items O
. O
2 O
) O
Pretrained O
session O
models O
which O
are O
pretrained O
models O
on O
e-commerce O
session O
data. O
Specifically O
, O
we O
pretrain O
the O
following O
language O
models O
using O
our O
session O
data O
: O
a O
) O
Product-BERT B-MethodName
, O
which O
is O
a O
domain-specific O
BERT B-MethodName
model O
pretrained O
with O
product O
information O
; O

b O
) O
SQSP-BERT B-MethodName
, O
where O
SQSP B-MethodName
is O
short O
for O
Single-query O
Single-Product. O
SQSP-BERT B-MethodName
is O
pretrained O
on O
query-product O
interaction O
pairs O
with O
language O
modeling O
and O
contrastive O
learning O
objectives. O
They O
are O
used O
in O
the O
same O
manner O
in O
downstream O
tasks O
as O
general-domain O
pretrained O
language O
models. O
The O
detailed O
configurations O
are O
provided O
in O
the O
Appendix O
. O
3 O

) O
Session-based O
recommendation O
methods O
including O
SR-GNN B-MethodName
( O
Wu O
et O
al. O
, O
2019b O
) O
and O
NISER+ B-MethodName
( O
Gupta O
et O
al. O
, O
2019 O
) O
, O
which O
are O
state-ofthe-art O
models O
for O
session-based O
product O
recommendation O
on O
traditional O
benchmarks O
, O
including O
YOOCHOOSE B-DatasetName
and O
DIGINETICA B-DatasetName
; O
and O
Nvidia B-MethodName
's I-MethodName
MERLIN I-MethodName
( O
Mobasher O
et O
al. O
, O
2001 O

) O
, O
which O
is O
the O
bestperforming O
model O
in O
the O
recent O
SIGIR O
Next O
Items O
Prediction O
challenge O
( O
Kallumadi O
et O
al. O
, O
2021 O
) O
To O
evaluate O
the O
performance O
on O
these O
tasks O
, O
we O
employ O
standard O
metrics O
for O
recommendation O
systems O
, O
including O
MAP B-MetricName
@ I-MetricName
K I-MetricName
, I-MetricName
and O
Recall B-MetricName
@ I-MetricName
K I-MetricName
. O
Implementation O
Details O

The O
implementation O
details O
for O
pretraining O
and O
finetuning O
stages O
are O
described O
as O
follows O
. O
Pretraining O
details. O
We O
developed O
our O
model O
based O
on O
Megatron-LM B-MethodName
( O
Shoeybi O
et O
al. O
, O
2019 O
) O
. O
We O
used O
768 B-HyperparameterValue
as O
the O
hidden B-HyperparameterName
size I-HyperparameterName
, O
a O
12-layer O
transformer O
blocks O
as O
the O
backbone O
language O
model O
, O
a O
twolayer O

Graph B-HyperparameterName
Attention I-HyperparameterName
Network I-HyperparameterName
and O
three-layer O
Transformer O
as O
the O
conditioned O
language O
model O
layers. O
In O
total O
, O
our O
model O
has O
141M O
parameters. O
The O
model O
is O
trained O
for O
300,000 O
steps O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
sessions. O
The O
parameters O
are O
updated O
with O
Adam O
, O
with O
peak O
learning B-HyperparameterName
rate I-HyperparameterName
as O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
, O

1 B-HyperparameterValue
% I-HyperparameterValue
steps B-HyperparameterName
for O
linear O
warm-up O
, O
and O
linear O
learning B-HyperparameterName
rate I-HyperparameterName
decay O
after O
warm-up O
until O
the O
learning B-HyperparameterName
rate I-HyperparameterName
reaches O
the O
minimum O
1e B-HyperparameterValue
− I-HyperparameterValue
5. I-HyperparameterValue
We O
trained O
our O
model O
on O
16 O
A400 O
GPUs O
on O
Amazon O
AWS O
for O
one O
week. O
Finetuning O
details. O
For O
each O
downstream O
task O
, O
we O
collected O
30,000 O
sessions O
for O

training O
, O
3000 O
for O
validation O
and O
5000 O
for O
testing. O
For O
each O
of O
the O
pretrained O
model O
, O
we O
finetune O
them O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
maximal O
learning B-HyperparameterName
rate I-HyperparameterName
chosen O
from O
[ O
1e-4 O
, O
1e-5 O
, O
5e-5 O
, O
5e-6 O
] O
to O
maximize O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
on O
the O
validation O
set. O
The O
rest O
of O

the O
configuration O
of O
optimizers O
is O
the O
same O
as O
in O
pretraining O
. O
Main O
Results O
Product B-TaskName
Search I-TaskName
Table O
1 O
shows O
the O
performance O
of O
different O
methods O
for O
the O
product B-TaskName
search I-TaskName
task. O
We O
observe O
that O
CERES B-MethodName
outperforms O
domain-specific O
methods O
by O
more O
than O
1 B-MetricValue
% I-MetricValue
and O
general-domain O
methods O
by O
over O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName

1. O
The O
second O
best O
performing O
model O
is O
Product-BERT B-MethodName
, O
which O
is O
pretrained O
on O
product O
information O
alone O
. O
We O
also O
compared O
with O
session-based O
recommendation O
systems. O
SR-GNN B-MethodName
and O
NISER+ B-MethodName
model O
only O
session O
graph O
structure O
but O
not O
text O
semantics O
; O
hence O
they O
have O
limited O
performance O
because O
of O
the O
suboptimal O
representation O
of O
session O
items. O

While O
MERLIN B-MethodName
can O
capture O
better O
text O
semantics O
, O
its O
text O
encoder O
is O
not O
trained O
on O
domain-specific O
e-commerce O
data. O
While O
it O
can O
outperform O
generaldomain O
methods O
, O
its O
performance O
is O
lower O
than O
Product-BERT B-MethodName
and O
CERES. B-MethodName
The O
benefits O
of O
joint O
modeling O
of O
text O
and O
graph O
data O
and O
the O
Graph-Conditioned B-MethodName
MLM I-MethodName
allow O
CERES B-MethodName
to O

outperform O
existing O
session O
recommendation O
models O
. O
Query B-TaskName
Search I-TaskName
Table O
2 O
shows O
the O
performance O
of O
different O
methods O
on O
Query B-TaskName
Search. I-TaskName
Query B-TaskName
Search I-TaskName
is O
a O
more O
difficult O
task O
than O
Product B-TaskName
Search I-TaskName
because O
customergenerated O
next O
queries O
are O
of O
higher O
variance. B-MetricName
In O
this O
challenging O
task O
, O
CERES B-MethodName
outperforms O
the O
best O
domain-specific O
model O
by O
over O

7 B-MetricValue
% I-MetricValue
and O
generaldomain O
model O
by O
12 B-MetricValue
% I-MetricValue
in O
all O
metrics O
. O
Entity B-TaskName
Linking I-TaskName
Table O
3 O
shows O
the O
results O
on O
Entity B-TaskName
Linking. I-TaskName
Similar O
to O
Query B-DatasetName
Search I-DatasetName
, O
this O
task O
also O
requires O
the O
models O
to O
tie O
text O
semantics O
( O
queries O
/ O
product O
attributes O
) O
to O
a O
customer B-DatasetName
session I-DatasetName
, O
which O
requires O

a O
deeper O
understanding O
of O
customer O
preferences. O
It O
is O
easier O
than O
Query B-TaskName
Search I-TaskName
as O
product O
attributes O
are O
of O
lower O
variance. O
However O
, O
the O
product O
attributes O
that O
the O
customer O
prefer O
rely O
more O
on O
session O
information O
, O
as O
they O
may O
have O
been O
reflected O
in O
the O
past O
search O
queries O
and O
viewed O
products. O
In O
this O

task O
, O
CERES B-MethodName
outperforms O
domain-specific O
models O
and O
general-domain O
models O
by O
averagely O
9 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
and O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
32 I-MetricName
and O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
. O
Further O
Analysis O
and O
Ablation O
Studies O
In O
this O
section O
we O
present O
further O
studies O
to O
understand O
: O
1 O
) O
the O
effect O
of O
training O

data O
sizes O
in O
the O
downstream O
task O
; O
2 O
) O
the O
effects O
of O
different O
components O
in O
CERES B-MethodName
for O
both O
the O
pretraining O
and O
finetuning O
stages. O
following O
observations O
: O
CERES B-MethodName
is O
highly O
effective O
when O
training O
data O
are O
scarce O
. O
We O
compare O
CERES B-MethodName
with O
two O
strongest O
baselines O
( O
BERT B-MethodName
, O
and O
Product-BERT B-MethodName
) O
when O

the O
training O
sample O
size O
varies. O
Figure O
5 O
shows O
the O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
these O
methods O
on O
Product B-TaskName
Search I-TaskName
and O
Query B-TaskName
Search I-TaskName
when O
training O
size O
varies. O
Clearly O
, O
the O
advantage O
of O
CERES B-MethodName
is O
greater O
when O
training O
data O
is O
extremely O
small. O
With O
a O
training O
size O
of O
300 O
, O
CERES B-MethodName
can O
achieve O

a O
decent O
performance O
of O
about O
37.55 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
and O
36.37 B-MetricValue
% O
in O
Query B-TaskName
Search I-TaskName
, O
while O
the O
baseline O
models O
can O
not O
be O
trained O
sufficiently O
with O
such O
small-sized O
data. O
This O
shows O
that O
the O
efficient O
utilization O
of O
session-level O
information O
in O
pretraining O
and O
fine-tuning O
stages O
make O
the O
model O
more O
data O
efficient O

than O
other O
pretrained O
models O
. O
Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
Vital O
to O
Pretraining. O
Without O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
pretraining O
, O
our O
model O
is O
essentially O
the O
same O
as O
domain-specific O
baselines O
, O
such O
as O
Product-BERT B-MethodName
, O
which O
are O
trained O
on O
session O
data O
but O
only O
with O
intra-item O
text O
signals. O
While O
SQSP-BERT B-MethodName
has O
access O
to O
session-level O

information O
when O
maximizing O
the O
masked O
language O
modeling O
objective O
, O
the O
lack O
of O
a O
dedicated O
module O
for O
GMLM B-MethodName
results O
in O
worse O
performance O
, O
as O
shown O
in O
the O
main O
experiment O
results. O
We O
could O
train O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
from O
scratch O
in O
the O
finetuning O
stage. O
We O
present O
a O
model O
called O
CERES B-MethodName
w O
/ O
o O

Pretrain O
, O
which O
attaches O
the O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
to O
Product-BERT B-MethodName
as O
the O
Item O
Transformer O
Encoder. O
As O
shown O
in O
Figure O
6 O
, O
this O
ablation O
method O
achieves O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
89.341 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
, O
64.890 B-MetricValue
% I-MetricValue
in O
Query B-TaskName
Search I-TaskName
, O
and O
74.031 B-MetricValue
% I-MetricValue
in O
Entity B-TaskName
Linking I-TaskName
, O
which O

are O
below O
Product-BERT. B-MethodName
This O
shows O
that O
the O
pretraining O
stage O
of O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
necessary O
to O
facilitate O
its O
ability O
to O
aggregate O
and O
propagate O
session-level O
information O
for O
downstream O
tasks O
. O
Graph-Conditioned B-MethodName
Transformer I-MethodName
Improves O
Item-level O
Embeddings. O
We O
also O
present O
CERES B-MethodName
w O
/ O
o O
Cond O
, O
which O
has O
the O
same O
pretrained O
model O
as O

CERES B-MethodName
, O
but O
only O
uses O
the O
Item O
Transformer O
Encoder O
in O
the O
finetuning O
stage. O
The O
Item O
Transformer O
Encoder O
is O
used O
to O
compute O
session O
item O
embeddings O
that O
contain O
only O
item-level O
information O
, O
and O
then O
takes O
the O
average O
of O
these O
embeddings O
as O
session O
embedding. O
As O
shown O
in O
Figure O
6 O
, O
CERES B-MethodName
w O
/ O

o O
Cond O
acheives O
94.741 B-MetricValue
% I-MetricValue
, O
72.175 B-MetricValue
% I-MetricValue
, O
and O
81.03 B-MetricValue
% I-MetricValue
respectively O
in O
Product B-TaskName
Search I-TaskName
, O
Query B-TaskName
Search I-TaskName
, O
and O
Entity B-TaskName
Linking I-TaskName
, O
observing O
a O
drop B-MetricName
of O
0.1 B-MetricValue
% I-MetricValue
to O
0.2 B-MetricValue
% I-MetricValue
in O
performance O
compared O
with O
CERES. B-MethodName
The O
performance O
drop O
is O
minor O
and O
CERES B-MethodName
w O
/ O
o O
Cond O
still O

outperforms O
baseline O
pretrained O
language O
models. O
Hence O
, O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
the O
pretraining O
stage O
helps O
the O
Item O
Transformer O
Encoder O
to O
learn O
better O
item-level O
embeddings O
that O
can O
be O
used O
for O
more O
effective O
leveraging O
of O
session O
information O
in O
the O
downstream O
tasks O
. O
Graph B-MethodName
Neural I-MethodName
Networks I-MethodName
Improve O
Representation O
of O
Sessions. O
In O
CERES B-MethodName
w O

/ O
o O
GNN B-MethodName
, O
we O
pretrain O
a O
CERES B-MethodName
model O
without O
a O
Graph B-MethodName
Neural I-MethodName
Network. I-MethodName
Specifically O
, O
CERES B-MethodName
w O
/ O
o O
GNN B-MethodName
skips O
the O
neighborhood O
information O
aggregation O
for O
items O
, O
and O
uses O
item-level O
embeddings O
obtained O
by O
the O
Item O
Transformer O
Encoder O
directly O
as O
latent O
conditioning O
tokens. O
We O
train O
and O
finetune O
this O
model O

with O
the O
same O
setup O
as O
CERES. B-MethodName
Without O
GNN B-MethodName
, O
the O
model O
's O
performance O
is O
consistently O
lower O
than O
CERES B-MethodName
, O
achieving O
93.453 B-MetricValue
% I-MetricValue
, O
71.231 B-MetricValue
% I-MetricValue
, O
80.26 B-MetricValue
% I-MetricValue
MAP B-MetricName
@ I-MetricName
64 I-MetricName
in O
three O
downstream O
tasks O
, O
observing O
a O
1.13 B-MetricValue
% I-MetricValue
performance O
drop. B-MetricName
This O
shows O
that O
GNN B-MethodName
's I-MethodName
aggregation O
of O
information O

can O
help O
item-level O
embeddings O
encode O
more O
session-level O
information O
, O
improving O
performance O
in O
downstream O
tasks O
. O
Model O
Efficiency. O
CERES B-MethodName
has O
additional O
few O
GNN B-MethodName
and O
Transformer O
layers O
attached O
to O
the O
end O
of O
the O
model. O
The O
additional O
layers O
bring O
∼20 O
% O
additional O
inference O
time O
compared O
to O
standard O
BERT B-MethodName
with O
12 B-HyperparameterValue
layers B-HyperparameterName
and O
768 B-HyperparameterValue

hidden B-HyperparameterName
size I-HyperparameterName
. O
Conclusion O
We O
proposed O
a O
pretraining O
framework O
, O
CERES B-MethodName
, O
for O
learning O
representations O
for O
semi-structured O
ecommerce O
sessions. O
We O
are O
the O
first O
to O
jointly O
model O
intra-item O
text O
and O
inter-item O
relations O
in O
session O
graphs O
with O
an O
end-to-end O
pretraining O
framework. O
By O
modeling O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
, O
our O
model O
is O
encouraged O

to O
learn O
high-quality O
representations O
for O
both O
intraitem O
and O
inter-item O
information O
during O
its O
pretraining O
on O
massive O
unlabeled O
session O
graphs. O
Furthermore O
, O
as O
a O
generic O
session O
encoder O
, O
our O
model O
enabled O
effective O
leverage O
of O
session O
information O
in O
downstream O
tasks. O
We O
conducted O
extensive O
experiments O
and O
ablation O
studies O
on O
CERES B-MethodName
in O
comparison O
to O
state-of-the-art O

pretrained O
models O
and O
recommendation O
systems. O
Experiments O
show O
that O
CERES B-MethodName
can O
produce O
higher O
quality O
text O
representations O
as O
well O
as O
better O
leverage O
of O
session O
graph O
structure O
, O
which O
are O
important O
to O
many O
ecommerce O
related O
tasks O
, O
including O
product B-TaskName
search I-TaskName
, O
query B-TaskName
search I-TaskName
, O
and O
query B-TaskName
understanding I-TaskName
. O
A O
Details O
on O
Session O
Data O

A.1 O
Product O
Attributes O
. O
A O
product O
is O
represented O
with O
a O
table O
of O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
bullet O
description. O
In O
this O
paper O
, O
we O
regard O
the O
product O
title O
as O
the O
representative O
sequence O
of O
the O
product O
, O
called O
" O
product O
sequence O
" O
. O
A O
product O

may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
specific O
products O
. O
A.2 O
Alternative O
Pretraining O
Corpora O
In O
this O
section O
we O
introduce O
alternative O
pretraining O
corpora O
that O
encode O
information O
in O
a O
session O
, O
including O
products O
and O
queries O
, O
but O
not O
treating O
sessions O

as O
a O
whole O
. O
A.2.1 O
Product B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
gathered O
all O
product O
information O
that O
appeared O
in O
the O
sessions O
from O
August O
2020 O
to O
September O
2020. O
Each O
product O
will O
have O
descriptions O
such O
as O
product O
title O
and O
bullet O
description O
, O
and O
other O
attributes O
like O
entity O
type O
, O
product O
type O
, O

manufacturer O
, O
etc. O
Particularly O
, O
bullet O
description O
is O
composed O
of O
several O
lines O
of O
descriptive O
facts O
about O
the O
product. O
All O
products O
without O
titles O
are O
removed. O
Each O
of O
the O
remaining O
product O
forms O
a O
paragraph O
, O
where O
the O
product O
title O
comes O
as O
the O
first O
sentence O
, O
followed O
by O
the O
entries O
of O
bullet O
descriptions O

each O
as O
a O
sentence O
, O
and O
product O
attributes. O
An O
example O
document O
in O
this O
corpora O
is O
as O
follows O
: O
[ O
Title O
] O
product O
title O
[ O
Description O
] O
A.2.3 O
Session B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
treat O
each O
session O
as O
a O
document O
and O
sequentially O
put O
text O
representations O
of O
items O
in O
a O
session O

to O
the O
document O
with O
special O
tokens O
indicating O
the O
fields O
of O
items. O
An O
example O
document O
looks O
like O
the O
follows O
: O
[ O
SEARCH O
] O
keywords O
1 O
[ O
SEARCH O
] O
keywords O
2 O
[ O
CLICK O
] O
[ O
TITLE O
] O
product O
1 O
[ O
SEARCH O
] O
keywords O
3 O
[ O
PURCHASE O
] O
[ O
TITLE O
] O
product O
2 O

In O
this O
example O
, O
the O
customer O
first O
attempted O
to O
search O
with O
keywords O
1 O
and O
then O
modified O
the O
keywords O
to O
keywords O
2. O
The O
customer O
then O
clicked O
on O
product O
1. O
At O
last O
, O
the O
customer O
modified O
his O
search O
to O
keywords O
3 O
and O
purchased O
product O
2. O
In O
this O
corpus O
, O
session O
information O
is O

present O
in O
a O
document O
, O
but O
the O
specific O
relations O
between O
elements O
are O
not O
specified. O
The O
comparison O
of O
different O
datasets O
are O
in O
Table O
5 O
. O
A.3 O
Alternative O
Pretraining O
Methods O
We O
introduce O
the O
alternative O
pretraining O
models O
. O
• O
Product-Bert. B-MethodName
It O
is O
pretrained O
on O
the O
Product B-DatasetName
Corpus. I-DatasetName
Specifically O
, O
we O
treat O
each O
product O

in O
the O
Product B-DatasetName
Corpus I-DatasetName
as O
an O
article. O
Product O
titles O
is O
always O
the O
first O
sentence O
, O
followed O
by O
paragraphs O
of O
bullet O
descriptions O
, O
which O
can O
contain O
multiple O
sentences. O
Then O
, O
each O
additional O
product O
attribute O
is O
a O
sentence O
added O
after O
the O
bullet O
descriptions. O
5 O
: O
Comparision O
of O
different O
pretraining O
dataset. O
Product B-DatasetName
Corpus I-DatasetName

has O
access O
only O
to O
product O
information. O
SQSP B-MethodName
models O
on O
the O
queries O
and O
query-product O
relations O
, O
without O
access O
to O
session O
context. O
Session B-DatasetName
Corpus I-DatasetName
has O
access O
to O
contextual O
information O
in O
a O
session O
, O
but O
does O
not O
model O
on O
relations O
between O
objects. O
Session-Graph O
has O
access O
to O
all O
information O
and O
models O
on O
the O
relational O

nature O
of O
nodes O
in O
the O
session O
graph O
. O
Product B-MethodName
Bert I-MethodName
is O
trained O
for O
300,000 B-HyperparameterValue
steps B-HyperparameterName
, O
with O
a O
12-layer B-HyperparameterValue
transformer O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
6144 B-HyperparameterValue
and O
peak B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-HyperparameterValue
, O
1 B-HyperparameterValue
% I-HyperparameterValue
linear B-HyperparameterName
warm-up I-HyperparameterName
steps I-HyperparameterName
, O
and O
1e−2 B-HyperparameterValue
linear B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
to O
a O
minimum B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O

1e-5 B-HyperparameterValue
. O
• O
SQSP-Bert. B-MethodName
It O
is O
pretrained O
on O
SQSP B-MethodName
Corpus. O
The O
SQSP B-MethodName
Bert I-MethodName
uses O
the O
same O
Transformer O
backbone O
as O
Product B-MethodName
Bert. I-MethodName
Given O
each O
query-product O
pair O
, O
SQSP B-MethodName
feeds O
the O
text O
pair O
sequence O
to O
the O
Transformer O
for O
token O
embeddings O
for O
masked O
language O
modeling O
loss. O
In O
addition O
to O
language O
modeling O
, O
for O

each O
queryproduct O
pair O
, O
we O
sample O
a O
random O
product O
for O
the O
query O
as O
a O
negative O
query-product O
pair O
. O
The O
text O
pair O
sequence O
of O
the O
negative O
sample O
is O
also O
fed O
to O
the O
Transformer. O
Then O
, O
a O
discriminator O
is O
trained O
in O
the O
pretraining O
stage O
to O
distinguish O
the O
ground-truth O
query-product O
pairs O
and O
randomly O

sampled O
pairs. O
The O
discriminator O
's O
classification O
loss O
should O
serve O
as O
a O
contrastive O
loss O
. O
SQSP B-MethodName
Bert I-MethodName
is O
trained O
with O
the O
same O
configuration O
of O
Product B-MethodName
Bert I-MethodName
. O
B O
Details O
on O
Evaluation O
Metrics O
Mean B-MetricName
Average I-MetricName
Precision. I-MetricName
Suppose O
that O
for O
a O
session O
, O
m O
items O
are O
relevant O
and O
N O
items O
are O
retrieved O
by O

the O
model O
, O
the O
Average B-MetricName
Precision I-MetricName
( O
AP B-MetricName
) O
of O
a O
session O
is O
defined O
as O
AP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
min O
( O
m O
, O
N O
) O
N O
k=1 O
P O
( O
k O
) O
rel O
( O
k O
) O
, O
( O
4 O
) O
where O
P B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
is O
the O
precision O
of O
the O

top O
k O
retrieved O
items O
, O
and O
rel O
( O
k O
) O
is O
an O
indicator O
function O
of O
whether O
the O
kth O
item O
is O
relevant. O
As O
we O
have O
at O
most O
one O
relevant O
item O
for O
each O
session O
, O
the O
above O
metric O
reduces O
to O
1 O
r O
, O
where O
r O
is O
the O
rank O
of O
the O
relevant O
item O

in O
the O
retrieved O
list O
, O
and O
k O
= O
∞ O
when O
the O
relevant O
item O
is O
not O
retrieved. O
MAP B-MetricName
@ I-MetricName
N I-MetricName
averages O
AP B-MetricName
@ I-MetricName
N I-MetricName
over O
all O
sessions O
, O
MAP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|S| O
s∈S O
1 O
r O
s O
( O
5 O
) O
where O
r O
s O
is O
the O
rank O
of O
the O
relevant O
item O

for O
a O
specific O
session O
s. O
MAP B-MetricName
in O
this O
case O
is O
equivalent O
to O
MRR B-MetricName
. O
Mean B-MetricName
Average I-MetricName
Precision I-MetricName
by I-MetricName
Queries I-MetricName
( O
MAPQ B-MetricName
) O
. O
Different O
from O
MAP B-MetricName
, O
MAPQ B-MetricName
averages O
AP B-MetricName
over O
last O
queries O
instead O
of O
sessions. O
Suppose O
Q O
is O
the O
set O
of O
unique O
last O
queries O
, O
and O
S O
( O
q O

) O
, O
q O
∈ O
Q O
is O
the O
set O
of O
sessions O
whose O
last O
queries O
are O
q O
, O
then O
the O
average B-MetricName
precision I-MetricName
for O
one O
query O
q O
is O
1 O
k O
i=1 O
rel O
( O
k O
) O
N O
k=1 O
min O
( O
1 O
, O
rs≤k O
rel O
( O
k O
) O
k O
) O
( O
6 O
) O
then O
we O

sum O
over O
all O
queries O
to O
obtain O
MAPQ B-MetricName
@ I-MetricName
N I-MetricName
. O
Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
by I-MetricName
Queries I-MetricName
( O
MRRQ B-MetricName
) O
. O
MRRQ B-MetricName
averages O
MRR B-MetricName
over O
session O
last O
queries O
instead O
of O
sessions O
. O
M B-MetricName
RRQ I-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|Q| O
q∈Q O
max O
s∈S O
( O
q O
) O
( O
r O
s O
) O
( O
7 O
) O

Recall. O
Recall B-MetricName
@ I-MetricName
N I-MetricName
calculates O
the O
percentage O
of O
sessions O
whose O
relevant O
items O
were O
retrieved O
among O
the O
top O
N O
predictions O
. O

-DOCSTART- O
Putting O
the O
Con O
in O
Context O
: O
Identifying O
Deceptive O
Actors O
in O
the O
Game O
of O
Mafia O
While O
neural O
networks O
demonstrate O
a O
remarkable O
ability O
to O
model O
linguistic O
content O
, O
capturing O
contextual O
information O
related O
to O
a O
speaker O
's O
conversational O
role O
is O
an O
open O
area O
of O
research. O
In O
this O
work O
, O
we O
analyze O
the O

effect O
of O
speaker O
role O
on O
language O
use O
through O
the O
game O
of O
Mafia O
, O
in O
which O
participants O
are O
assigned O
either O
an O
honest O
or O
a O
deceptive O
role. O
In O
addition O
to O
building O
a O
framework O
to O
collect O
a O
dataset O
of O
Mafia O
game O
records O
, O
we O
demonstrate O
that O
there O
are O
differences O
in O
the O
language O
produced O

by O
players O
with O
different O
roles. O
We O
confirm O
that O
classification B-TaskName
models O
are O
able O
to O
rank O
deceptive O
players O
as O
more O
suspicious O
than O
honest O
ones O
based O
only O
on O
their O
use O
of O
language. O
Furthermore O
, O
we O
show O
that O
training O
models O
on O
two O
auxiliary O
tasks O
outperforms O
a O
standard O
BERT-based B-MethodName
text B-MethodName
classification I-MethodName
approach. O
We O
also O
present O

methods O
for O
using O
our O
trained O
models O
to O
identify O
features O
that O
distinguish O
between O
player O
roles O
, O
which O
could O
be O
used O
to O
assist O
players O
during O
the O
Mafia O
game O
. O
Introduction O
Correct O
interpretation O
of O
language O
must O
take O
into O
account O
not O
only O
the O
meaning O
of O
utterances O
, O
but O
also O
characteristics O
of O
the O
speaker O
and O

the O
context O
in O
which O
their O
utterances O
are O
produced. O
Modeling O
the O
impact O
of O
this O
context O
on O
language O
is O
still O
challenging O
for O
NLP B-TaskName
systems. O
For O
example O
, O
differences O
in O
language B-TaskName
identification I-TaskName
accuracy B-MetricName
, O
speech B-TaskName
recognition I-TaskName
word B-MetricName
error I-MetricName
rates I-MetricName
, O
and O
translation B-MetricName
quality I-MetricName
have O
been O
observed O
on O
the O
basis O
of O
attributes O
such O
as O

a O
speaker O
's O
gender O
, O
race O
, O
dialect O
, O
or O
role O
( O
Blodgett O
and O
O'Connor O
, O
2017 O
; O
Tatman O
and O
Kasten O
, O
2017 O
; O
Tatman O
, O
2017 O
; O
Stanovsky O
et O
al. O
, O
2019 O
) O
. O
Moreover O
, O
these O
systems O
systematically O
underperform O
on O
data O
generated O
by O
those O
in O
the O
minority O
, O

having O
implications O
for O
the O
ethics O
and O
fairness O
of O
using O
these O
technologies. O
* O
Equal O
contribution. O
This O
work O
explores O
language O
used O
for O
deception O
: O
a O
type O
of O
speaker O
context O
that O
is O
particularly O
challenging O
to O
model O
because O
it O
is O
intentionally O
hidden O
by O
the O
speaker. O
To O
do O
so O
, O
we O
collect O
and O
release O
a O

set O
of O
records O
for O
the O
game O
of O
Mafia O
, O
in O
which O
each O
player O
is O
assigned O
either O
an O
honest O
or O
a O
deceptive O
role. O
Then O
, O
we O
develop O
models O
that O
distinguish O
players O
' O
roles O
based O
only O
on O
the O
text O
of O
the O
players O
' O
dialog. O
We O
describe O
two O
auxiliary O
tasks O
that O
improve O
classification B-TaskName

accuracy B-MetricName
over O
a O
BERT-based B-MethodName
text B-MethodName
classifier I-MethodName
. O
The O
novel O
contributions O
of O
this O
paper O
include O
: O
1. O
A O
methodology O
for O
collecting O
records O
of O
online O
Mafia O
games O
and O
a O
dataset O
collected O
from O
460 O
human O
subjects O
, O
2. O
Three O
classification B-TaskName
models O
that O
can O
distinguish O
between O
honest O
and O
deceptive O
players O
, O
3. O
An O
approach O

for O
identifying O
features O
of O
the O
game O
dialog O
text O
that O
can O
be O
used O
to O
help O
identify O
deceptive O
players O
during O
the O
game O
. O
The O
task O
of O
identifying O
deception O
in O
dialog O
is O
far O
from O
solved. O
Our O
classification B-TaskName
methods O
, O
while O
not O
accurate O
enough O
to O
reliably O
identify O
deceptive O
players O
in O
a O
game O
, O
do O

show O
that O
the O
text O
of O
a O
dialog O
in O
the O
setting O
we O
study O
does O
contain O
information O
about O
the O
roles O
of O
the O
participants O
, O
even O
when O
those O
participants O
are O
motivated O
to O
hide O
those O
characteristics O
by O
deceiving O
the O
listener. O
Although O
the O
models O
and O
results O
described O
in O
this O
work O
only O
apply O
to O
a O
particular O

game O
setting O
rather O
than O
dialog O
in O
general O
, O
the O
approaches O
we O
describe O
are O
general O
in O
character O
and O
therefore O
may O
inform O
future O
work O
on O
determining O
speaker O
roles O
from O
the O
contents O
of O
dialog O
. O
Dataset O
A O
total O
of O
460 O
English-speaking O
participants O
based O
in O
the O
United O
States O
were O
recruited O
from O
Amazon B-MetricName
Mechanical I-MetricName
Turk I-MetricName

using O
the O
experiment O
platform O
Dallinger B-MetricName
1 O
. O
Between O
4 O
and O
10 O
participants O
were O
recruited O
for O
each O
Mafia O
game O
: O
1 O
to O
2 O
participants O
were O
designated O
mafia O
, O
and O
the O
rest O
were O
bystanders. O
Forty-four O
of O
these O
Mafia O
games O
are O
included O
in O
the O
final O
analysis. O
Participants O
were O
paid O
$ O
2.50 O
for O
completing O

the O
task O
, O
plus O
bonuses O
for O
time O
spent O
waiting O
for O
other O
participants O
to O
arrive O
in O
a O
chatroom O
to O
begin O
the O
experiment. O
Waiting O
was O
paid O
at O
$ O
5 O
/ O
hour O
. O
Upon O
recruitment O
, O
participants O
were O
shown O
a O
consent O
form O
, O
per O
IRB O
approval O
, O
followed O
by O
an O
instructional O
video O
and O

accompanying O
transcript O
describing O
how O
to O
play O
the O
text-based O
Mafia O
game O
using O
an O
interface O
we O
developed O
( O
see O
Appendix O
) O
. O
After O
they O
completed O
a O
quiz O
demonstrating O
they O
understood O
the O
information O
, O
they O
entered O
a O
waiting O
room O
until O
the O
desired O
number O
of O
participants O
was O
reached. O
Participants O
were O
then O
assigned O
a O
role O

( O
mafioso O
or O
bystander O
) O
and O
fake O
name O
, O
after O
which O
they O
began O
playing O
the O
game O
. O
The O
game O
dynamics O
were O
as O
follows. O
Each O
mafia O
member O
was O
aware O
of O
the O
roles O
of O
their O
fellow O
mafia O
members O
and O
thus O
, O
by O
process O
of O
elimination O
, O
knew O
the O
roles O
of O
the O
bystanders. O

However O
, O
the O
bystanders O
did O
not O
know O
the O
true O
role O
of O
anyone O
else O
in O
the O
game. O
The O
goal O
of O
the O
mafia O
was O
to O
eliminate O
bystanders O
until O
the O
number O
of O
mafia O
was O
greater O
than O
or O
equal O
to O
that O
of O
the O
bystanders. O
The O
goal O
of O
the O
bystanders O
was O
to O
identify O
and O
eliminate O

all O
of O
the O
mafia O
members. O
Since O
the O
incentive O
structure O
was O
set O
up O
such O
that O
bystanders O
benefited O
from O
true O
beliefs O
about O
who O
the O
mafia O
members O
were O
, O
whereas O
mafia O
members O
benefited O
from O
false O
beliefs O
, O
bystanders O
were O
thus O
motivated O
to O
be O
honest O
actors O
, O
whereas O
mafia O
members O
were O
motivated O
to O
M O

and O
B O
denote O
the O
mafioso O
and O
bystander O
classes O
, O
respectively O
, O
while O
T O
denotes O
the O
total O
number O
for O
both O
groups. O
The O
last O
row O
shows O
the O
distribution O
of O
roles O
among O
the O
players O
with O
no O
utterances O
throughout O
the O
game. O
Note O
that O
nearly O
all O
of O
the O
no-utterance O
players O
are O
bystanders O
. O
be O
deceptive O

actors O
in O
the O
Mafia O
game. O
The O
game O
proceeded O
in O
phases O
, O
alternating O
between O
nighttime O
and O
daytime O
( O
Figure O
1 O
) O
. O
During O
the O
nighttime O
, O
mafia O
members O
could O
secretly O
communicate O
to O
decide O
on O
who O
to O
eliminate O
, O
after O
which O
they O
discretely O
voted O
, O
and O
the O
person O
with O
the O
majority O
vote O

was O
eliminated O
from O
the O
game. O
If O
there O
was O
a O
tie O
, O
one O
of O
the O
people O
involved O
in O
the O
tie O
was O
randomly O
chosen O
to O
be O
eliminated. O
During O
the O
daytime O
, O
everyone O
was O
made O
aware O
of O
who O
was O
eliminated O
during O
the O
nighttime O
, O
and O
then O
all O
players O
could O
openly O
communicate O
to O
decide O

who O
to O
eliminate. O
All O
the O
players O
then O
voted O
publicly O
, O
and O
the O
person O
with O
the O
majority O
vote O
was O
eliminated O
and O
announced O
to O
be O
a O
bystander O
or O
mafioso. O
Thus O
, O
during O
the O
nighttime O
mafia O
could O
secretly O
communicate O
and O
eliminate O
anyone O
, O
whereas O
during O
the O
daytime O
mafia O
could O
participate O
in O
the O
voting O

and O
communication O
protocols O
in O
the O
same O
way O
as O
bystanders. O
The O
game O
proceeded O
until O
there O
was O
a O
winning O
faction O
according O
to O
the O
goals O
described O
above O
. O
From O
these O
experiments O
, O
we O
collected O
a O
dataset O
consisting O
of O
both O
mafia O
and O
bystander O
utterances O
over O
the O
course O
of O
each O
game O
, O
as O
well O
as O

the O
participants O
' O
voting O
behavior. O
Dataset O
statistics O
appear O
in O
Table O
1. O
Figure O
2 O
displays O
a O
snippet O
of O
the O
daytime O
dialog O
from O
one O
Mafia O
game. O
As O
shown O
, O
many O
utterances O
are O
either O
social O
interactions O
( O
eg. O
" O
hi O
erybody O
" O
) O
or O
discussions O
about O
what O
to O
do O
in O
the O
game O
, O

such O
as O
accusations O
or O
comments O
about O
voting O
( O
eg. O
" O
I O
bet O
it O
's O
Mandy O
... O
" O
) O
. O
Upon O
further O
inspection O
of O
the O
data O
, O
we O
can O
observe O
several O
strategies O
used O
by O
mafia O
members O
to O
deceive O
bystanders O
: O
Figure O
1 O
: O
Mafia O
experiment O
screenshot O
during O
( O
left O
) O
first O

nighttime O
phase O
, O
with O
participant O
as O
a O
mafioso O
, O
and O
( O
right O
) O
first O
daytime O
phase O
, O
with O
participant O
as O
a O
bystander O
( O
note O
that O
mafia O
messages O
are O
not O
visible O
to O
the O
bystander O
) O
. O
1. O
Mafia O
members O
may O
suggest O
that O
there O
is O
not O
enough O
information O
to O
decide O
on O
who O

to O
eliminate O
, O
despite O
their O
knowledge O
of O
everyone O
's O
roles O
( O
eg. O
" O
Should O
we O
wait O
to O
eliminate O
someone O
? O
" O
/ O
" O
It O
's O
a O
little O
early O
to O
tell. O
" O
/ O
" O
It O
's O
a O
shot O
in O
the O
dark. O
" O
) O
, O
2. O
Mafia O
members O
may O
raise O
suspicion O
about O

another O
player O
, O
despite O
knowing O
that O
said O
player O
is O
a O
bystander O
( O
eg. O
hmm O
ok O
analyzing O
this O
conversation O
... O
.I O
think O
bianca O
was O
a O
little O
to O
flippant O
in O
how O
she O
was O
like O
" O
sucks O
to O
be O
andrew O
" O
haha O
/ O
I O
'm O
going O
to O
vote O
bianca. O
she O
's O
so O
casual O

with O
life O
and O
death O
) O
, O
3. O
Mafia O
members O
may O
invent O
a O
false O
motive O
and O
assign O
that O
motive O
to O
another O
player O
, O
despite O
knowing O
that O
the O
player O
is O
a O
bystander O
( O
eg. O
It O
might O
be O
Jonathan O
Kim O
... O
killing O
off O
Erin O
who O
accused O
him O
" O
yesterday O
" O
) O
. O
Approach O

Given O
our O
mafia O
dataset O
, O
there O
are O
several O
tasks O
that O
one O
might O
address O
, O
for O
example O
, O
predicting O
participants O
' O
daytime O
voting O
behavior O
or O
generating O
mafia O
members O
' O
nighttime O
dialog. O
As O
our O
aim O
is O
to O
identify O
deceptive O
actors O
, O
however O
, O
we O
focus O
on O
predicting O
participants O
' O
roles O
, O
i.e. O

bystander O
or O
mafioso. O
Due O
to O
the O
asymmetry O
in O
the O
knowledge O
available O
to O
each O
group O
and O
the O
goals O
which O
incentivize O
bystanders O
to O
increase O
true O
belief O
and O
mafia O
members O
to O
reduce O
it O
, O
the O
bystanders O
are O
said O
to O
take O
on O
an O
honest O
role O
in O
the O
game O
, O
whereas O
the O
mafia O
members O
take O

on O
a O
deceptive O
role. O
To O
focus O
on O
the O
relationship O
between O
language O
and O
deception O
, O
we O
ignore O
voting O
behavior O
and O
consider O
just O
the O
daytime O
dialog O
in O
the O
game O
, O
as O
only O
the O
mafia O
members O
were O
able O
to O
converse O
during O
the O
nighttime. O
As O
shown O
in O
Table O
1 O
, O
since O
most O
of O
the O

players O
with O
no O
utterances O
are O
bystanders O
, O
we O
only O
consider O
players O
who O
make O
at O
least O
one O
utterance O
throughout O
the O
game O
. O
To O
investigate O
whether O
linguistic O
information O
can O
be O
used O
to O
identify O
players O
' O
roles O
, O
we O
train O
and O
evaluate O
classifiers O
that O
predict O
the O
role O
of O
a O
particular O
player. O
Since O
we O

have O
a O
small O
dataset O
, O
we O
chose O
to O
fine-tune B-MethodName
pre-trained I-MethodName
Transformer I-MethodName
models O
rather O
than O
train O
them O
from O
scratch O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O
To O
predict O
the O
role O
for O
a O
player O
p O
, O
we O
construct O
an O
input O
representation O
r O
( O
C O
, O
p O
) O
of O
the O
full O
game O

dialog O
C O
that O
encodes O
the O
player O
of O
interest O
p. O
We O
develop O
three O
approaches O
which O
differ O
in O
both O
the O
dialog B-TaskName
representation I-TaskName
function O
r O
and O
the O
modeling O
approach O
. O
Standard O
Classification B-TaskName
Our O
baseline O
approach O
uses O
a O
standard O
BERT-based B-MethodName
text B-MethodName
classifier I-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
. O
To O
classify O
player O
p O

via O
the O
full O
record O
of O
the O
game O
C O
, O
let O
boolean O
variable O
M O
p O
be O
true O
if O
p O
is O
a O
mafioso. O
Let O
T O
p O
be O
the O
concatenation O
2 O
of O
utterances O
made O
by O
p. O
We O
train O
BERT B-MethodName
parameters O
θ O
M O
to O
predict O
P O
( O
M O
p O
|T O
p O
; O
θ O
M O

) O
. O
This O
approach O
, O
which O
provides O
as O
input O
to O
the O
classifier O
only O
the O
utterances O
of O
the O
player O
to O
be O
classified O
, O
outperformed O
an O
alternative O
representation O
r O
( O
C O
, O
p O
) O
that O
included O
the O
entire O
record O
of O
all O
utterances O
by O
all O
players O
. O
Auxiliary O
Tasks O
Limiting O
the O
input O
representation O

r O
to O
contain O
only O
the O
speech O
of O
the O
player O
p O
being O
classified O
is O
not O
ideal O
; O
correctly O
interpreting O
a O
dialog O
requires O
considering O
all O
other O
players O
' O
statements O
as O
well. O
We O
introduce O
two O
auxiliary O
tasks O
that O
involve O
the O
entire O
game O
dialog O
C O
: O
1. O
Given O
all O
of O
the O
prior O
utterances O
, O

is O
a O
bystander O
or O
a O
mafia O
member O
more O
likely O
to O
have O
produced O
the O
current O
utterance O
? O
( O
Utterance O
Classification O
) O
2. O
Given O
all O
of O
the O
prior O
utterances O
, O
what O
current O
utterance O
would O
a O
player O
produce O
, O
given O
that O
they O
are O
a O
bystander O
or O
a O
mafia O
member O
? O
( O
Utterance B-TaskName
Generation I-TaskName

) O
We O
develop O
a O
BERT-based B-MethodName
classification B-MethodName
model O
for O
task O
1 O
and O
fine-tune O
the O
GPT-2 B-MethodName
language O
model O
for O
task O
2 O
( O
Radford O
et O
al. O
, O
2019 O
) O
. O
Then O
, O
we O
use O
each O
of O
these O
auxiliary O
models O
to O
classify O
the O
role O
of O
a O
particular O
player O
p O
in O
the O
game O
. O
Utterance B-TaskName

Classification I-TaskName
To O
classify O
player O
p O
using O
the O
auxiliary O
task O
of O
utterance B-TaskName
classification I-TaskName
, O
let O
boolean O
variable O
S O
i O
be O
true O
if O
utterance O
C O
i O
was O
made O
by O
a O
mafioso O
( O
rather O
than O
a O
bystander O
) O
. O
Let O
C O
be O
the O
full O
record O
of O
utterances O
in O
the O
game O
and O
C O
≤i O

be O
the O
concatenation O
of O
all O
utterances O
C O
1 O
. O
. O
. O
C O
i O
. O
We O
train O
BERT B-MethodName
parameters O
θ O
S O
to O
predict O
P O
( O
S O
i O
|C O
≤i O
; O
θ O
S O
) O
. O
Finally O
, O
let O
I O
p O
be O
the O
set O
of O
indices O
of O
utterances O
by O
player O
p. O
M O
relates O

to O
S O
in O
that O
if O
M O
p O
is O
true O
, O
then O
S O
i O
is O
true O
for O
all O
i O
∈ O
I O
p O
. O
We O
thus O
calculate O
P O
( O
M O
p O
|C O
; O
θ O
S O
) O
∝ O
i∈Ip O
P O
( O
S O
i O
|C O
≤i O
; O
θ O
S O
) O
N O
, O
where O
N O

= O
|I O
p O
|. O
The O
original O
data O
is O
shown O
on O
the O
left-hand O
side O
, O
while O
the O
right-hand O
side O
shows O
the O
processed O
data O
containing O
two O
versions O
of O
each O
utterance O
, O
one O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
and O
one O
assuming O
that O
they O
are O
a O
bystander O
, O
with O
the O
prior O

conversation O
context O
preceding O
each O
and O
labels O
corresponding O
to O
whether O
the O
assumed O
role O
matches O
the O
actual O
role O
of O
the O
player O
. O
Utterance B-TaskName
Generation I-TaskName
To O
classify O
player O
p O
using O
the O
auxiliary O
task O
of O
utterance B-TaskName
generation I-TaskName
, O
we O
fine-tune B-TaskName
GPT-2 B-MethodName
to O
generate O
utterance O
C O
i O
conditioned O
on O
prior O
utterances O
C O
< O
i O
and O

the O
role O
S O
i O
of O
the O
speaker O
that O
produced O
C O
i O
. O
From O
Bayes O
' O
rule O
, O
we O
have O
P O
( O
M O
p O
|C O
) O
∝ O
P O
( O
M O
p O
) O
P O
( O
C|M O
p O
) O
. O
To O
estimate O
P O
( O
C|M O
p O
) O
, O
let O
C O
p O
include O
all O

C O
i O
for O
i O
∈ O
I O
p O
. O
We O
make O
the O
simplifying O
assumption O
that O
P O
( O
C|M O
p O
) O
∝ O
P O
( O
C O
p O
|M O
p O
) O
, O
which O
assumes O
that O
the O
utterances O
made O
by O
players O
other O
than O
p O
are O
independent O
of O
the O
role O
of O
player O
p. O
Then O
, O
if O

M O
p O
is O
true O
, O
S O
i O
is O
true O
for O
all O
i O
∈ O
I O
p O
, O
and O
so O
, O
P O
( O
C O
p O
|M O
p O
; O
θ O
C O
) O
= O
i∈Ip O
P O
( O
C O
i O
|C O
< O
i O
, O
S O
i O
; O
θ O
C O
) O
. O
Using O
the O
full O
dialog O

C O
, O
the O
final O
probability O
of O
player O
p O
being O
mafioso O
is O
calculated O
as O
follows O
: O
P O
( O
M O
p O
|C O
) O
= O
P O
( O
M O
p O
) O
P O
( O
C O
p O
|M O
p O
; O
θ O
C O
) O
R∈ O
{ O
M O
, O
¬M O
} O
P O
( O
R O
p O
) O
P O
( O

C O
p O
|R O
p O
; O
θ O
C O
) O
( O
1 O
) O
Figure O
4 O
: O
Data B-TaskName
processing I-TaskName
for O
fine-tuning B-TaskName
GPT-2. B-MethodName
The O
original O
data O
is O
shown O
on O
the O
left-hand O
side O
, O
while O
the O
right-hand O
side O
shows O
the O
processed O
data O
containing O
a O
version O
of O
the O
corresponding O
utterance O
with O
the O
prior O
conversation O
context O
preceding O

. O
Figure O
5 O
: O
Prediction O
pipeline O
for O
our O
fine-tuned O
GPT-2 B-MethodName
model. O
Similar O
to O
the O
pipeline O
used O
to O
produce O
the O
training O
utterances O
, O
for O
prediction O
, O
there O
are O
now O
two O
versions O
of O
each O
, O
one O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
and O
one O
assuming O
that O
they O
are O
a O
bystander O

. O
The O
losses O
for O
each O
utterance O
of O
the O
target O
player O
are O
summed O
together O
in O
order O
to O
calculate O
the O
mafia O
and O
bystander O
probabilities O
as O
described O
in O
Equation O
1 O
. O
Data B-TaskName
Processing I-TaskName
To O
train O
models O
for O
utterance B-TaskName
classification I-TaskName
( O
using O
BERT B-MethodName
) O
and O
utterance B-TaskName
generation I-TaskName
( O
using O
GPT-2 B-MethodName
) O
, O
we O
perform O

data B-TaskName
processing I-TaskName
procedures O
on O
the O
games O
' O
original O
dataset O
to O
create O
input O
representations O
r O
( O
C O
, O
p O
) O
for O
each O
player O
p O
and O
obtain O
our O
training O
datasets O
as O
shown O
in O
Figures O
3 O
and O
4. O
The O
left O
side O
of O
each O
figure O
shows O
a O
snippet O
of O
a O
game O
's O
data O
, O

where O
" O
Mafioso O
" O
and O
" O
Bystander O
" O
denote O
the O
true O
roles O
of O
the O
players. O
The O
utterances O
to O
the O
right O
of O
each O
figure O
are O
training O
examples O
used O
for O
finetuning O
the O
BERT B-MethodName
and O
GPT-2 B-MethodName
models. O
Structuring O
the O
data O
in O
this O
way O
provides O
both O
the O
prior O
context O
of O
utterances O
and O
the O
current O

utterance O
that O
happened O
within O
this O
context. O
This O
not O
only O
gives O
us O
the O
information O
needed O
for O
the O
auxiliary O
tasks O
, O
but O
also O
provides O
us O
with O
more O
training O
examples O
, O
as O
we O
only O
have O
44 O
games O
and O
only O
421 O
players O
in O
total O
, O
with O
only O
2162 O
total O
utterances. O
Moreover O
, O
this O
mimics O

the O
real O
game O
scenario O
from O
the O
bystander O
view O
in O
that O
they O
can O
only O
confirm O
their O
own O
role O
, O
but O
no O
one O
else O
's O
, O
which O
is O
the O
appropriate O
setting O
for O
us O
in O
which O
to O
detect O
deception. O
Figure O
5 O
shows O
the O
pipeline O
for O
using O
the O
GPT-2 B-MethodName
model O
to O
predict O
players O
' O

roles. O
Let O
us O
assume O
that O
the O
target O
player O
for O
whom O
we O
want O
to O
predict O
their O
role O
is O
Mafioso O
1. O
From O
the O
original O
game O
log O
on O
the O
left O
, O
we O
first O
perform O
the O
data B-TaskName
processing I-TaskName
scheme O
from O
Figure O
4 O
twice O
, O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
( O
top O

of O
Figure O
5 O
) O
and O
a O
bystander O
( O
bottom O
of O
Figure O
5 O
) O
. O
Using O
our O
trained O
GPT-2 B-MethodName
model O
, O
we O
then O
obtain O
a O
loss O
for O
each O
utterance O
denoted O
by O
L1 O
through O
L4. O
Summing O
all O
the O
losses O
for O
each O
role O
, O
as O
they O
denote O
log O
probabilities O
, O
we O
calculate O
P B-MetricName

( O
M O
p O
|C O
) O
and O
P B-MetricName
( O
¬M O
p O
|C O
) O
via O
Equation O
1. O
The O
target O
player O
's O
role O
as O
predicted O
by O
the O
model O
is O
finally O
given O
by O
comparing O
the O
two O
probabilities. O
A O
similar O
process O
is O
used O
to O
calculate O
P B-MetricName
( I-MetricName
M I-MetricName
p I-MetricName
|C I-MetricName
) I-MetricName
and O
P B-MetricName
( I-MetricName
¬M I-MetricName

p I-MetricName
|C I-MetricName
) I-MetricName
for O
the O
utterance B-TaskName
classification I-TaskName
BERT B-MethodName
model I-MethodName
. O
Random O
Baseline O
This O
random O
classifier O
classifies O
each O
player O
as O
a O
mafioso O
or O
a O
bystander O
with O
probabilities O
equal O
to O
the O
prior O
distribution O
of O
each O
class O
, O
estimated O
as O
the O
ratio O
of O
roles O
across O
all O
training O
games. O
This O
serves O
as O
a O
baseline O

to O
be O
compared O
to O
for O
all O
other O
methods. O
In O
the O
game O
setting O
, O
this O
mimics O
a O
bystander O
player O
with O
only O
public O
information O
of O
how O
many O
mafia O
and O
bystanders O
are O
in O
the O
game O
. O
Standard O
Classification B-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained B-TaskName
BERT B-MethodName
Base O
model O
( O
12 B-HyperparameterValue
layers B-HyperparameterName
, O

768 B-HyperparameterValue
hidden B-HyperparameterName
dimension I-HyperparameterName
size I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
. O
We O
train O
with O
a O
maximum O
sequence B-HyperparameterName
length I-HyperparameterName
of O
256 B-HyperparameterValue
, O
which O
is O
sufficient O
for O
our O
post-processed O
dataset O
, O
setting O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
1e-5 B-HyperparameterValue
, O
and O
the O
maximum O
number O
of O
epochs B-HyperparameterName
to O
25 B-HyperparameterValue
. O

Utterance B-TaskName
Classification I-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained B-MethodName
BERT I-MethodName
Base I-MethodName
model I-MethodName
( O
12 B-HyperparameterValue
layers B-HyperparameterName
, O
768 B-HyperparameterValue
hidden B-HyperparameterName
dimension I-HyperparameterName
size I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
. O
We O
train O
with O
a O
maximum O
sequence B-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
, O
which O
is O
sufficient O
for O
our O
post-processed O
dataset O
, O
setting O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O

5 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e-5 B-HyperparameterValue
, O
and O
the O
maximum O
number O
of O
epochs B-HyperparameterName
to O
25. B-HyperparameterValue
Table O
2 O
: O
Experiment O
results O
on O
the O
validation O
set O
for O
random O
baseline O
( O
Random O
) O
, O
standard B-TaskName
classification I-TaskName
( O
Std O
Class O
) O
, O
utterance B-TaskName
classification I-TaskName
( O
Utt O
Class O
) O
, O
and O
utterance B-TaskName
generation I-TaskName
( O

Utt O
Gen O
) O
approaches. O
Methods O
that O
use O
auxiliary O
tasks O
( O
Utt O
Class O
and O
Utt O
Gen O
) O
outperform O
other O
methods O
in O
terms O
of O
average O
ranking O
overall O
and O
per O
game O
while O
also O
maintaining O
higher O
accuracy B-MetricName
and O
F1-score B-MetricName
for O
each O
class O
. O
Utterance B-TaskName
Generation I-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained O
12-layer B-HyperparameterValue

GPT-2 B-MethodName
model O
with O
an O
embedding B-HyperparameterName
size I-HyperparameterName
of O
768. B-HyperparameterValue
For O
the O
dataset O
, O
we O
set O
the O
maximum O
length B-HyperparameterName
of O
each O
sentence O
to O
be O
512 B-HyperparameterName
, O
which O
is O
sufficient O
for O
our O
dataset O
after O
post-processing. O
During O
training O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
be O
5 B-HyperparameterValue
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-5. B-HyperparameterValue

We O
train O
the O
model O
for O
a O
maximum O
of O
100 B-HyperparameterValue
epochs B-HyperparameterName
. O
Metrics O
These O
approaches O
each O
estimate O
a O
probability B-MetricName
P O
( O
M O
p O
|C O
) O
that O
a O
player O
p O
is O
a O
mafioso O
given O
the O
full O
record O
of O
game O
texts O
C. O
In O
Mafia O
, O
bystanders O
do O
not O
declare O
who O
is O
and O
is O

not O
a O
mafioso O
, O
but O
instead O
vote O
each O
day O
to O
eliminate O
one O
of O
the O
players. O
Because O
the O
act O
of O
voting O
involves O
choosing O
one O
player O
among O
them O
all O
, O
a O
natural O
metric O
for O
evaluating O
the O
usefulness O
of O
a O
model O
is O
to O
order O
all O
players O
p O
from O
greatest O
to O
least O
P O
( O

M O
p O
|C O
) O
, O
their O
probability B-MetricName
of O
being O
a O
mafioso O
under O
the O
model O
, O
and O
then O
to O
compute O
the O
average B-MetricName
rank I-MetricName
of O
the O
true O
mafia O
members. O
Therefore O
, O
the O
first O
metric O
in O
Table O
2 O
is O
the O
average O
ranking O
of O
all O
mafia O
members O
when O
each O
player O
is O
ranked O
by O
P O

( O
M O
p O
|C O
) O
across O
the O
entire O
validation O
set O
composed O
of O
5 O
games. O
It O
is O
also O
natural O
to O
consider O
player O
ranking O
within O
a O
single O
game O
, O
so O
we O
calculate O
the O
average B-MetricName
ranking I-MetricName
of O
mafia O
members O
within O
each O
game O
as O
a O
second O
metric. O
Smaller O
average O
ranking O
for O
mafia O
members O
means O

that O
the O
model O
is O
able O
to O
assign O
mafia O
players O
a O
high O
P O
( O
M O
p O
|C O
) O
relative O
to O
bystanders O
, O
which O
is O
desired O
. O
In O
addition O
, O
we O
evaluate O
the O
accuracy B-MetricName
of O
the O
classifiers O
and O
the O
F1-score B-MetricName
for O
each O
class. O
To O
calculate O
these O
metrics O
, O
we O
first O
assign O
the O

mafioso O
label O
to O
the O
top O
k O
players O
with O
the O
highest O
P O
( O
M O
p O
|C O
) O
and O
the O
rest O
of O
the O
players O
with O
the O
bystander O
label O
, O
where O
k O
is O
the O
known O
number O
of O
mafia O
among O
all O
validation O
games O
( O
k O
= O
10 O
in O
our O
case O
) O
. O
Aside O
from O

the O
ranking O
metrics O
, O
these O
give O
further O
information O
of O
the O
models O
' O
quality O
after O
utilizing O
available O
game O
information O
. O
Results O
and O
Analysis O
We O
trained O
all O
models O
on O
39 O
training O
games O
and O
evaluated O
on O
the O
remaining O
5 O
validation O
games. O
The O
evaluation O
results O
are O
shown O
in O
Table O
2. O
We O
have O
a O
total O

of O
49 O
players O
in O
the O
validation O
games O
, O
but O
only O
considered O
the O
39 O
players O
who O
had O
spoken O
at O
least O
one O
utterance O
throughout O
the O
game O
when O
calculating O
the O
metrics. O
Players O
with O
no O
utterances O
are O
almost O
exclusively O
bystanders O
and O
are O
therefore O
easy O
to O
classify O
without O
considering O
language O
. O
First O
, O
we O
see O

that O
it O
is O
possible O
to O
achieve O
an O
average O
rank O
that O
is O
smaller O
than O
the O
random O
baseline O
, O
which O
demonstrates O
that O
there O
is O
information O
in O
the O
dialog O
about O
the O
roles O
of O
players O
, O
despite O
the O
fact O
that O
mafia O
members O
seek O
to O
hide O
their O
role O
while O
conversing. O
However O
, O
standard O
classification B-TaskName
is O

comparable O
to O
random. O
Next O
, O
we O
observe O
that O
both O
models O
using O
auxiliary O
tasks O
outperform O
the O
standard O
classifier O
in O
rank-based O
metrics O
, O
which O
demonstrates O
that O
the O
auxiliary O
tasks O
provide O
useful O
inductive O
bias O
for O
the O
mafia O
classification B-TaskName
task. O
Additionally O
, O
the O
accuracy B-TaskName
is O
similar O
for O
all O
approaches O
, O
including O
random O
classification B-TaskName
, O

which O
indicates O
that O
there O
is O
not O
enough O
information O
in O
the O
text O
of O
a O
Mafia O
game O
for O
these O
models O
to O
determine O
players O
' O
roles O
reliably. O
If O
the O
goal O
of O
the O
game O
were O
to O
guess O
the O
role O
of O
each O
player O
individually O
, O
then O
always O
guessing O
bystander O
( O
i.e. O
the O
majority O
class O
) O

would O
be O
the O
best O
strategy. O
However O
, O
since O
the O
goal O
for O
the O
bystanders O
is O
to O
vote O
to O
eliminate O
a O
mafia O
member O
each O
round O
, O
the O
utterance B-TaskName
classification I-TaskName
and O
utterance B-TaskName
generation I-TaskName
approaches O
, O
which O
achieve O
the O
lowest O
average O
mafia O
ranking B-MetricName
per O
game O
and O
overall O
, O
respectively O
, O
are O
the O
most O
favorable O

. O
Note O
that O
the O
precision B-MetricName
for O
the O
mafia O
is O
much O
lower O
than O
that O
of O
the O
bystanders O
for O
all O
models. O
This O
is O
due O
to O
the O
usual O
lack O
of O
information O
avail- O
able O
to O
predict O
that O
any O
player O
is O
a O
mafioso O
, O
which O
makes O
finding O
the O
mafia O
a O
much O
harder O
task O
than O
finding O

bystanders O
. O

-DOCSTART- O
Enhancing B-MethodName
Self-Attention I-MethodName
with I-MethodName
Knowledge-Assisted I-MethodName
Attention I-MethodName
Maps I-MethodName
Large-scale B-MethodName
pre-trained I-MethodName
language I-MethodName
models I-MethodName
have O
attracted O
extensive O
attentions O
in O
the O
research O
community O
and O
shown O
promising O
results O
on O
various O
tasks O
of O
natural B-TaskName
language I-TaskName
processing. I-TaskName
However O
, O
the O
attention O
maps O
, O
which O
record O
the O
attention B-MetricName
scores I-MetricName
between O
tokens O
in O
self-attention B-MethodName
mechanism I-MethodName
, O
are O
sometimes O
ineffective O

as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantic O
knowledge. O
Thus O
, O
we O
aim O
to O
infuse O
explicit O
external O
knowledge O
into O
pretrained B-MethodName
language I-MethodName
models I-MethodName
to O
further O
boost O
their O
performance. O
Existing O
works O
of O
knowledge O
infusion O
largely O
depend O
on O
multi-task B-MethodName
learning I-MethodName
frameworks O
, O
which O
are O
inefficient O
and O
require O
large-scale O
re-training O
when O

new O
knowledge O
is O
considered. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
solution O
, O
KAM-BERT B-MethodName
, O
which O
directly O
incorporates O
knowledge-generated B-MethodName
attention I-MethodName
maps I-MethodName
into O
the O
self-attention B-MethodName
mechanism. I-MethodName
It O
requires O
only O
a O
few O
extra O
parameters O
and O
supports O
efficient O
fine-tuning B-TaskName
once O
new O
knowledge O
is O
added. O
KAM-BERT B-MethodName
achieves O
consistent O
improvements O
on O
various O

academic O
datasets O
for O
natural B-TaskName
language I-TaskName
understanding. I-TaskName
It O
also O
outperforms O
other O
state-of-the-art O
methods O
which O
conduct O
knowledge B-TaskName
infusion I-TaskName
into O
transformerbased O
architectures. O
Moreover O
, O
we O
apply O
our O
model O
to O
an O
industry-scale O
ad O
relevance O
application O
and O
show O
its O
advantages O
in O
the O
real-world O
scenario O
. O
Introduction O
Language O
models O
pre-trained B-TaskName
by O
a O
large O
text O
corpus O
have O

shown O
superior O
performances O
on O
a O
wide O
range O
of O
natural B-TaskName
language I-TaskName
processing I-TaskName
tasks. O
Many O
advanced O
models O
based O
on O
the O
transformer O
architectures O
achieve O
state-of-the-art O
results O
on O
various O
NLP B-TaskName
benchmarks. O
Existing O
literature O
( O
Jawahar O
et O
al. O
, O
2019 O
; O
Hewitt O
and O
Manning O
, O
2019 O
) O
shows O
that O
pre-training B-TaskName
enables O
a O
model O
to O
capture O

syntactic O
and O
semantic O
information O
in O
the O
self-attention B-MethodName
mechanism. I-MethodName
However O
, O
the O
attention O
maps O
, O
which O
* O
The O
work O
was O
done O
when O
the O
author O
visited O
Microsoft. O
record O
the O
attention O
scores O
between O
tokens O
in O
a O
selfattention O
mechanism O
, O
are O
sometimes O
ineffective O
as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantics O

( O
Jain O
and O
Wallace O
, O
2019 O
) O
. O
If O
the O
knowledge O
can O
be O
leveraged O
in O
a O
reasonable O
way O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
we O
have O
a O
good O
chance O
to O
improve O
the O
quality O
of O
attention B-MetricName
scores I-MetricName
as O
well O
as O
the O
performance O
of O
downstream O
applications O
. O
Recently O
, O
there O
have O
been O

multiple O
attempts O
for O
incorporating O
knowledge O
into O
transformer O
architectures. O
ERNIE B-MethodName
( O
Zhang O
et O
al. O
, O
2019 O
) O
and O
KE-PLER B-MethodName
utilize O
both O
large-scale O
textual O
corpora O
and O
knowledge O
graphs O
to O
train O
a O
representation O
model O
in O
a O
multi-task B-MethodName
learning I-MethodName
framework. I-MethodName
They O
need O
to O
be O
retrained O
from O
scratch O
when O
injecting O
new O
knowledge O
, O
which O
is O

inefficient O
and O
can O
not O
benefit O
from O
existing O
pre-trained O
checkpoints. O
K-Adapter B-MethodName
( O
Wang O
et O
al. O
, O
2020 O
) O
integrates O
additional O
neural O
models O
to O
capture O
different O
kinds O
of O
knowledge. O
It O
enables O
adaptation O
based O
on O
pretrained B-TaskName
language B-MethodName
models. I-MethodName
However O
, O
it O
does O
not O
instruct O
the O
self-attention B-MethodName
mechanism I-MethodName
directly O
and O
introduces O
a O
relatively O
large O

number O
of O
parameters O
to O
the O
original O
model O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
self-attention B-MethodName
mechanism I-MethodName
enhanced O
by O
explicit O
knowledge O
to O
address O
problems O
mentioned O
above. O
First O
, O
we O
show O
a O
failure O
case O
of O
query-ad B-TaskName
matching I-TaskName
, O
which O
motivates O
us O
to O
inject O
explicit O
knowledge O
into O
self-attention B-MethodName
mechanism. I-MethodName

In O
Figure O
1 O
, O
the O
attention O
map O
of O
a O
query-ad O
pair O
is O
visualized O
, O
and O
the O
goal O
is O
to O
judge O
if O
the O
query O
and O
ad O
text O
are O
semantically O
relevant. O
As O
shown O
in O
the O
figure O
, O
BERT B-MethodName
misclassifies O
this O
pair O
as O
irrelevant O
, O
probably O
because O
it O
does O
not O
understand O
the O
query O

word O
" O
glipizide O
" O
, O
which O
rarely O
appears O
in O
the O
pre-training O
corpus. O
In O
fact O
, O
" O
glipizide O
" O
is O
a O
kind O
of O
medicine O
and O
highly O
related O
to O
the O
word O
" O
Pharmacy O
" O
in O
ad O
text O
, O
so O
this O
case O
should O
be O
classified O
as O
relevant. O
In O
this O
case O
, O
if O
we O

know O
" O
glipizide O
" O
is O
semantically O
correlated O
to O
" O
Pharmacy O
" O
as O
prior O
knowledge O
, O
we O
can O
enrich O
the O
attention O
maps O
accordingly. O
In O
addition O
, O
terms O
" O
Fred O
" O
and O
" O
Meyer O
" O
are O
from O
the O
same O
entity O
, O
so O
the O
attention B-MetricName
scores I-MetricName
between O
these O
two O
terms O
should O
be O
relatively O

high. O
Based O
on O
this O
fact O
, O
we O
believe O
that O
simply O
using O
language O
models O
pre-trained B-TaskName
by O
a O
general O
corpus O
is O
not O
enough O
to O
meet O
the O
satisfaction O
of O
a O
specific O
application. O
Thus O
, O
our O
motivation O
is O
to O
inject O
explicit O
knowledge O
into O
the O
transformer O
architecture O
, O
which O
guides O
the O
pre-trained B-TaskName
language O
model O
to O

perform O
better O
adaptation O
in O
an O
efficient O
fine-tuning B-TaskName
procedure O
. O
To O
address O
the O
above O
motivation O
, O
we O
propose O
a O
novel O
architecture O
, O
namely O
KAM-BERT B-MethodName
( O
Knowledge-assisted B-MethodName
Attention I-MethodName
Maps I-MethodName
for I-MethodName
BERT I-MethodName
) O
. O
First O
, O
it O
constructs O
semantic O
attention O
maps O
based O
on O
corresponding O
relevance O
functions O
defined O
by O
various O
kinds O
of O
semantic O
knowledge. O

Specifically O
, O
we O
consider O
three O
kinds O
of O
semantic O
knowledge O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
i.e. O
, O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O
and O
term B-TaskName
correlation. I-TaskName
Entity O
and O
phrase B-TaskName
segmentation I-TaskName
indicate O
the O
cohesion O
of O
continuous O
terms O
, O
while O
term B-TaskName
correlation I-TaskName
can O
help O
to O
enrich O
the O
semantic O
representation O
of O
a O
sentence. O
Then O

, O
the O
knowledge B-TaskName
infusion I-TaskName
procedure O
is O
completed O
by O
concatenating O
these O
attention O
maps O
with O
vanilla O
self-attention O
and O
then O
performing O
2Dconvolution O
for O
integration. O
Finally O
, O
the O
result O
attention O
maps O
are O
served O
as O
inputs O
for O
value O
projection O
, O
and O
the O
rest O
part O
is O
the O
same O
as O
a O
standard O
transformer. O
The O
KAM-BERT B-MethodName
model O
can O

be O
fine-tuned O
on O
existing O
pre-trained B-TaskName
checkpoints O
in O
a O
plug O
and O
play O
mode O
, O
which O
is O
highly O
efficient O
in O
practice O
. O
As O
illustrated O
in O
Section O
4 O
, O
we O
compare O
KAM-BERT B-MethodName
with O
BERT B-MethodName
and O
other O
knowledge-enhanced O
SOTAs O
on O
various O
natural B-TaskName
language I-TaskName
understanding I-TaskName
tasks O
, O
where O
KAM-BERT B-MethodName
shows O
consistent O
superiority. O
Especially O
, O
we O

lift O
the O
average O
score O
of O
BERT-Base B-MetricName
from O
77.5 B-MetricValue
to O
78.7 B-MetricValue
on O
the O
GLUE B-MetricName
benchmark. O
We O
also O
demonstrate O
the O
advantage O
of O
KAM-BERT B-MethodName
for O
knowledge B-TaskName
injection I-TaskName
on O
LAMA B-DatasetName
, O
a O
probing O
benchmark O
to O
analyze O
the O
factual O
and O
commonsense O
knowledge O
contained O
in O
a O
model. O
Furthermore O
, O
KAM-BERT B-MethodName
is O
successfully O
applied O
to O
the O
query-ad B-TaskName

relevance I-TaskName
scenario O
in O
a O
commercial O
search O
engine O
and O
shows O
significant O
lift O
in O
AUC B-MetricName
score O
. O
The O
major O
contributions O
of O
this O
paper O
are O
summarized O
as O
follows O
: O
• O
First O
, O
we O
propose O
a O
novel O
self-attention O
mechanism O
enhanced O
by O
semantic O
attention O
maps O
, O
which O
incorporates O
knowledge O
from O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O

and O
term B-TaskName
correlation. I-TaskName
Ablation O
study O
will O
demonstrate O
the O
effectiveness O
of O
these O
kinds O
of O
semantic O
attention O
maps O
. O
• O
Second O
, O
KAM-BERT B-MethodName
requires O
little O
extra O
memory O
and O
computation O
cost O
compared O
to O
vanilla O
BERT B-MethodName
and O
other O
SOTAs. O
It O
can O
be O
finetuned B-TaskName
efficiently O
on O
existing O
language O
models O
for O
a O
given O
application. O
We O
have O

successfully O
applied O
it O
to O
improve O
the O
performance O
of O
query-ad B-TaskName
relevance I-TaskName
in O
a O
commercial O
search O
engine. O
• O
Last O
but O
not O
least O
, O
the O
proposed O
framework O
is O
generic O
and O
flexible O
for O
infusing O
various O
kinds O
of O
knowledge O
into O
the O
transformer O
architecture. O
Except O
for O
the O
three O
kinds O
of O
knowledge O
considered O
in O
this O
paper O
, O

we O
will O
also O
showcase O
how O
to O
incorporate O
other O
kinds O
of O
information O
, O
such O
as O
a O
knowledge O
graph. O
It O
opens O
up O
new O
opportunities O
for O
further O
exploration O
. O
KAM-BERT B-MethodName
As O
illustrated O
in O
Figure O
2 O
, O
KAM-BERT B-MethodName
injects O
multiple O
kinds O
of O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models I-MethodName
in O
the O
form O
of O
multi-channel O
semantic O
attention O

maps. O
Different O
kinds O
of O
knowledge O
can O
be O
extracted O
independently O
and O
infused O
together O
into O
one O
self-attention O
map O
in O
the O
transformer O
architecture. O
KAM-BERT B-MethodName
can O
be O
fine-tuned B-TaskName
directly O
from O
an O
existing O
checkpoint O
of O
BERT B-MethodName
, O
while O
additional O
parameters O
are O
initialized O
randomly O
and O
learned O
in O
the O
fine-tuning B-TaskName
stage. O
Thus O
, O
it O
is O
quite O
efficient O

and O
flexible O
to O
incorporate O
new O
kinds O
of O
knowledge O
into O
the O
model O
. O
Below O
we O
first O
describe O
the O
standard O
selfattention B-MethodName
mechanism. I-MethodName
Then O
, O
we O
will O
introduce O
the O
generic O
definition O
of O
semantic O
attention O
maps O
, O
as O
well O
as O
the O
methodology O
of O
multi-channel B-MethodName
knowledge I-MethodName
infusion I-MethodName
which O
integrates O
semantic O
attention O
maps O
into O
transformer O
models. O

At O
last O
, O
the O
generation O
of O
different O
kinds O
of O
semantic O
attention O
maps O
will O
be O
presented. O
Note O
that O
the O
time O
complexity O
of O
KAM-BERT B-MethodName
is O
on-par O
with O
a O
vanilla O
BERT. B-MethodName
A O
detailed O
analysis O
can O
be O
found O
in O
the O
supplementary O
material O
. O
Self-Attention B-MethodName
The O
representation O
of O
a O
text O
sequence O
can O
be O
written O
as O

X O
∈ O
R O
N O
×C O
, O
where O
N O
denotes O
the O
sequence O
length O
and O
C O
is O
the O
word O
embedding O
dimension O
size. O
A O
standard O
Transformer O
block O
is O
composed O
of O
a O
self-attention O
layer O
and O
a O
position-wise O
feedforward O
layer O
, O
while O
each O
attention O
map O
is O
generated O
by O
a O
self-attention O
layer O
without O
any O
other O
prior O

knowledge O
introduced O
. O
The O
self-attention B-MethodName
mechanism I-MethodName
plays O
an O
important O
role O
in O
the O
transformer-based O
model. O
In O
a O
vanilla O
Transformer O
, O
the O
self-attention O
map O
A O
i O
sa O
of O
layer O
i O
is O
calculated O
by O
the O
dimension-normalized O
dotproduct O
operation O
. O
A O
i O
sa O
=Self-Attention B-MethodName
( O
X O
) O
= O
QK O
⊤ O
√ O
d O
( O
1 O

) O
where O
d O
is O
the O
dimension O
of O
representation O
vectors O
. O
In O
a O
vanilla O
transformer O
, O
A O
i O
sa O
is O
then O
normalized O
by O
softmax O
and O
fed O
into O
position-wise O
feed-forward O
layers. O
In O
KAM-BERT B-MethodName
, O
the O
self-attention O
map O
A O
i O
sa O
is O
infused O
with O
semantic O
attention O
maps O
to O
calculate O
the O
final O
attention O
matrix O

, O
which O
will O
be O
described O
in O
the O
following O
sub-sections O
. O
Semantic O
Attention O
Maps O
Given O
a O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
, O
the O
semantic O
attention O
map O
can O
be O
defined O
in O
a O
generic O
form O
M O
∈ O
R O
n×n O
, O
where O
M O
i O

, O
j O
∈ O
[ O
0 O
, O
1 O
] O
denotes O
the O
attention B-MetricName
score I-MetricName
from O
token O
i O
to O
token O
j O
, O
and O
n O
is O
the O
number O
of O
tokens O
in O
the O
current O
sentence. O
Then O
, O
for O
a O
specific O
kind O
of O
knowledge O
, O
we O
need O
a O
corresponding O
relevance O
function O
to O
calculate O
the O
attention B-MetricName
score I-MetricName

, O
i.e. O
, O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
x O
i O
, O
x O
j O
) O
. O
Note O
that O
if O
x O
i O
denotes O
a O
sub-word O
as O
in O
the O
BERT B-MethodName
model O
, O
we O
define O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
W O
( O
x O
i O
) O
, O
W O
( O
x O
j O

) O
) O
, O
where O
W O
( O
x O
i O
) O
denotes O
the O
entire O
word O
which O
contains O
the O
sub-word O
x O
i O
. O
For O
example O
, O
BERT B-MethodName
will O
convert O
a O
sequence O
" O
I O
like O
tacos O
! O
" O
into O
a O
sequence O
of O
sub-words O
, O
i.e. O
, O
{ O
I O
, O
like O
, O
ta O
, O
# O

# O
cos O
, O
! O
} O
, O
where O
" O
ta O
" O
and O
" O
# O
# O
cos O
" O
are O
sub-words O
from O
" O
tacos O
" O
, O
so O
both O
W O
( O
ta O
) O
and O
W O
( O
# O
# O
cos O
) O
denote O
the O
word O
" O
tacos O
" O
. O
In O
Section O
3.4 O
, O
we O
will O
introduce O

three O
kinds O
of O
semantic O
attention O
maps O
considered O
in O
this O
paper O
and O
the O
generation O
method O
for O
other O
knowledgeassisted O
attention O
maps O
. O
Multi-Channel B-MethodName
Knowledge I-MethodName
Infusion I-MethodName
In O
order O
to O
incorporate O
external O
knowledge O
into O
self-attention B-MethodName
, O
we O
concatenate O
semantic O
attention O
maps O
with O
vanilla O
self-attention B-MethodName
, O
and O
then O
infuse O
them O
into O
a O
single O
multi-head O
attention O

map O
using O
multi-channel O
2D-convolutions. O
Applying O
2Dconvolution O
to O
a O
self-attention O
map O
is O
first O
pro- O
posed O
by O
( O
Wang O
et O
al. O
, O
2021 O
) O
and O
shows O
advantages O
in O
both O
NLP B-TaskName
and O
CV B-TaskName
tasks. O
Here O
we O
use O
2D-convolution O
to O
infuse O
different O
kinds O
of O
knowledge O
. O
Q O
K O
V O
multi-channel O
convolution O
block O
0 O
X O

X O
X O
X O
X O
input O
tokens O
feed-forward O
concatenate O
Knowledge-assisted O
attention O
maps O
Q O
K O
V O
multi-channel O
convolution O
block O
1 O
X O
X O
X O
X O
X O
feed-forward O
linear O
fusion O
First O
, O
we O
perform O
Channel B-MethodName
Wise I-MethodName
Concatenation I-MethodName
( O
CWC B-MethodName
) O
: O
the O
vanilla O
self-attention O
map O
A O
i O
sa O
will O
be O
concatenated O
with O
each O
semantic O

attention O
map O
M O
i O
separately O
along O
the O
channel O
dimension. O
Then O
, O
a O
multi-channel O
2D-convolution O
is O
applied O
to O
generate O
an O
knowledge-infused O
attention O
map O
, O
denoted O
by O
A O
i O
sem O
. O
The O
entire O
process O
can O
be O
formulated O
as O
below O
. O
A O
i O
sem O
= O
Conv O
( O
CW O
C O
( O
A O
i O
sa O

|M O
1..k O
) O
) O
( O
2 O
) O
where O
M O
1..k O
is O
a O
set O
of O
semantic O
attention O
maps O
obtained O
by O
k O
different O
kinds O
of O
knowledge O
, O
including O
but O
not O
limited O
to O
the O
three O
ones O
considered O
in O
this O
paper O
; O
To O
infuse O
different O
types O
of O
knowledge O
, O
we O
apply O
a O
standard O
2D O

convolution O
operation O
, O
the O
output O
dimension O
of O
which O
is O
the O
same O
as O
that O
of O
A O
sa O
. O
If O
A O
sa O
has O
m O
attention O
heads O
, O
then O
A O
sem O
will O
also O
has O
m O
attention O
heads. O
We O
adopt O
3 O
× O
3 O
kernel O
for O
the O
convolution O
empirically O
as O
it O
performs O
better O
than O
1 O

× O
1 O
and O
5 O
× O
5 O
kernels O
according O
to O
( O
Wang O
et O
al. O
, O
2021 O
) O
. O
At O
last O
, O
we O
adopt O
a O
hyper-parameter O
α B-HyperparameterName
to O
balance O
the O
importance O
of O
A O
i O
sa O
and O
A O
i O
sem O
. O
A O
i O
= O
Sof O
tmax O
( O
α B-HyperparameterName
• O
A O
i O
sem O
+ O

( O
1 O
− O
α B-HyperparameterName
) O
• O
A O
i O
sa O
) O
( O
3 O
) O
After O
softmax O
activation O
, O
we O
get O
the O
final O
selfattention O
map O
A O
i O
with O
m O
heads O
for O
the O
i-th O
layer O
. O
Given O
the O
self-attention B-MethodName
map O
, O
the O
rest O
components O
are O
identical O
to O
a O
vanilla O
Transformer O
, O
which O
can O

be O
calculated O
as O
h O
i O
j O
= O
A O
i O
j O
V O
i O
, O
H O
i O
= O
( O
m O
j=1 O
h O
j O
) O
W O
O O
, O
j O
∈ O
m. O
( O
4 O
) O
In O
detail O
, O
we O
use O
the O
obtained O
attention O
map O
A O
i O
to O
multiply O
the O
value O
matrix O
V O
in O
the O

attention B-MethodName
mechanism I-MethodName
to O
get O
the O
representation O
h O
j O
of O
the O
j-th O
attention O
head. O
Next O
, O
the O
outputs O
of O
all O
attention O
heads O
from O
each O
layer O
will O
be O
concatenated O
along O
the O
embedding O
dimension. O
Finally O
, O
we O
multiply O
this O
result O
with O
a O
linear O
transformation O
matrix O
W O
O O
to O
get O
the O
output O
representation O
of O

the O
i-th O
KAM-BERT B-MethodName
layer. O
Besides O
, O
we O
add O
a O
skip O
connection O
from O
the O
result O
attention O
map O
in O
the O
i-th O
layer O
to O
the O
self-attention O
map O
of O
the O
i O
+ O
1 O
layer O
to O
enhance O
the O
flow O
of O
information O
between O
layers O
. O
Generation O
of O
Semantic O
Attention O
Maps O
The O
knowledge O
we O
inject O
into O
KAM-BERT B-MethodName

includes O
entity B-TaskName
information I-TaskName
, O
phrase B-TaskName
segmentation I-TaskName
information I-TaskName
, O
and O
term B-TaskName
correlation I-TaskName
information. I-TaskName
We O
consider O
these O
three O
types O
of O
knowledge O
because O
they O
reflect O
language O
semantics O
from O
different O
perspectives. O
Entity O
and O
phrase O
represent O
coherence O
between O
adjacent O
words O
while O
term O
correlations O
build O
a O
semantic O
bridge O
for O
related O
words O
which O
may O
be O
far O
away O

or O
even O
unseen O
in O
the O
current O
sentence. O
The O
first O
two O
kinds O
of O
knowledge O
are O
presented O
as O
labeled O
sequences O
, O
and O
the O
last O
one O
is O
presented O
as O
relationship O
between O
tokens. O
As O
defined O
in O
Section O
3.2 O
, O
a O
specific O
kind O
of O
knowledge O
can O
be O
transferred O
to O
semantic O
attention O
maps O
once O
the O
corresponding O

Relevance O
function O
is O
defined O
. O
In O
the O
following O
paragraphs O
, O
we O
will O
demonstrate O
how O
to O
define O
Relevance O
functions O
for O
the O
three O
types O
of O
knowledge O
used O
in O
this O
paper. O
Also O
, O
we O
need O
to O
emphasize O
that O
the O
proposed O
framework O
is O
generic O
and O
is O
feasible O
to O
incorporate O
other O
semantic O
information O
like O
a O

knowledge O
graph. O
Thus O
, O
we O
will O
discuss O
how O
to O
generate O
other O
knowledgeassisted O
attention O
maps O
as O
our O
future O
work. O
Entity B-TaskName
Attention I-TaskName
Map I-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
( O
Nadeau O
and O
Sekine O
, O
2007 O
) O
is O
a O
standard O
task O
for O
natural B-TaskName
language I-TaskName
processing I-TaskName
which O
has O
been O
studied O
for O
years. O
Mathematically O
, O

a O
entity O
extractor O
transforms O
the O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
into O
a O
sequence O
of O
labels O
{ O
label O
0 O
, O
label O
1 O
, O
... O
, O
label O
n−1 O
} O
, O
where O
label O
i O
falls O
into O
one O
of O
three O
classes O
, O
denoting O
non-entity O

words O
, O
starting O
words O
in O
entities O
and O
other O
words O
in O
entities. O
Based O
on O
the O
labeled O
sequence O
, O
the O
Relevance O
function O
for O
entity O
attention O
map O
can O
be O
defined O
as O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
E O
W O
j O
0 O
otherwise O
( O
5 O
) O
where O

A O
≡ O
E O
B O
denotes O
A O
and O
B O
belong O
to O
the O
same O
entity O
. O
Phrase B-TaskName
Segmentation I-TaskName
Attention O
Map O
Similar O
to O
the O
entity O
attention O
map O
, O
one O
can O
highlight O
the O
term O
correlations O
within O
the O
same O
phrase O
segmentation O
to O
emphasize O
the O
locality O
inductive O
bias. O
Syntax O
tree O
is O
a O
generic O
source O
to O
extract O

phrases O
in O
different O
semantic O
levels O
, O
which O
can O
be O
generated O
by O
a O
trained O
syntax O
parser. O
In O
a O
syntax O
tree O
, O
each O
internal O
node O
represents O
a O
phrase O
segment O
for O
a O
specific O
level. O
For O
example O
, O
we O
can O
select O
the O
parents O
of O
leaf O
nodes O
in O
the O
syntax O
tree O
as O
the O
root O
of O

each O
sub-tree O
which O
represents O
a O
phrase. O
We O
define O
the O
distance O
of O
an O
internal O
node O
i O
to O
the O
leaf O
node O
as O
level O
( O
i O
) O
. O
Thus O
, O
the O
relevance O
function O
can O
be O
computed O
by O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
T O
W O
j O

0 O
otherwise O
( O
6 O
) O
where O
A O
≡ O
T O
B O
denotes O
that O
A O
and O
B O
belong O
to O
the O
same O
sub-tree O
at O
level O
( O
i O
) O
. O
Term B-TaskName
Correlation I-TaskName
Attention O
Map O
In O
computational O
linguistics O
, O
Pointwise B-MetricName
Mutual I-MetricName
Information I-MetricName
( O
PMI B-MetricName
) O
has O
been O
widely O
used O
for O
finding O
associations O
between O
words O
( O

Arora O
et O
al. O
, O
2016 O
) O
. O
In O
our O
work O
, O
we O
adopt O
PMI B-MetricName
to O
measure O
the O
semantic O
correlations O
between O
terms. O
The O
PMI B-MetricName
of O
a O
pair O
( O
x O
, O
y O
) O
from O
discrete O
random O
variables O
( O
X O
, O
Y O
) O
quantifies O
the O
discrepancy O
between O
the O
probability O
of O
their O
coincidence O
given O

joint O
distributions O
and O
individual O
distributions. O
We O
define O
the O
PMI-based B-MetricName
relevance I-MetricName
function O
as O
P O
M O
I O
( O
x O
; O
y O
) O
= O
log O
p O
( O
x O
, O
y O
) O
p O
( O
x O
) O
p O
( O
y O
) O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
=P O
M O
I O
( O
W O
i O

; O
W O
j O
) O
/ O
Z O
( O
7 O
) O
where O
Z O
denotes O
the O
normalized O
factor O
of O
PMI B-MetricName
matrix. O
In O
our O
experiments O
, O
PMI B-MetricName
is O
calculated O
using O
a O
large O
web O
corpus. O
We O
calculate O
the O
probability O
p O
( O
x O
, O
y O
) O
of O
a O
word O
pair O
appearing O
jointly O
, O
and O
the O
probability O

of O
single O
word O
appearance O
is O
denoted O
as O
p O
( O
x O
) O
and O
p O
( O
y O
) O
. O
Finally O
, O
we O
use O
log O
p O
( O
x O
, O
y O
) O
− O
log O
p O
( O
x O
) O
− O
log O
p O
( O
y O
) O
to O
compute O
the O
PMI B-MetricName
score O
. O
To O
better O
incorporate O
semantic O

knowledge O
into O
attention O
maps O
, O
we O
further O
enrich O
each O
attention O
map O
by O
adding O
top O
k O
terms O
which O
do O
not O
appear O
in O
the O
current O
sentence O
but O
hold O
the O
highest O
average O
PMI B-MetricName
scores O
with O
terms O
in O
the O
original O
sentence. O
Note O
that O
we O
should O
expand O
the O
selected O
k O
words O
to O
K O
subwords O
for O

BERT. B-MethodName
Then O
the O
subwords O
will O
be O
appended O
to O
the O
original O
sentence. O
After O
augmentation O
, O
the O
input O
sentence O
has O
N O
+ O
K O
words O
and O
the O
shape O
of O
an O
attention O
map O
becomes O
( O
N O
+ O
K O
) O
× O
( O
N O
+ O
K O
) O
, O
where O
N O
and O
K O
denote O
the O
number O
of O

original O
terms O
and O
auxiliary O
terms O
respectively O
( O
see O
an O
example O
in O
Fig. O
2 O
( O
d O
) O
) O
. O
In O
order O
to O
align O
the O
shapes O
of O
different O
attention O
maps O
( O
including O
the O
vanilla O
self-attention O
map O
) O
, O
we O
add O
zero-padding O
for O
smaller O
ones. O
After O
passing O
one O
transformer O
layer O
, O
the O
output O

sequence O
length O
is O
still O
N O
+ O
K. O
Note O
that O
the O
auxiliary O
words O
are O
only O
utilized O
to O
enrich O
the O
semantics O
of O
original O
word O
representations O
, O
which O
is O
done O
within O
each O
transformer O
layer. O
Thus O
, O
we O
trim O
the O
output O
sequence O
length O
to O
N O
before O
taking O
it O
as O
input O
to O
the O
next O
transformer O

layer O
( O
while O
a O
new O
round O
of O
augmentation O
will O
be O
done O
in O
the O
next O
layer O
) O
. O
Other O
Knowledge-assisted O
Attention O
Maps. O
The O
KAM-BERT B-MethodName
framework O
is O
generic O
and O
can O
be O
extended O
to O
other O
kinds O
of O
knowledge O
in O
future O
works. O
For O
each O
semantic O
type O
, O
we O
can O
define O
a O
specific O
Relevance B-MetricName
function O

to O
transfer O
the O
corresponding O
information O
into O
semantic O
attention O
maps. O
For O
example O
, O
we O
can O
define O
the O
relevance O
function O
for O
a O
Knowledge O
Graph O
( O
KG O
) O
as O
: O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
E O
( O
W O
i O
) O
≡ O
KG O
E O
( O
W O
j O
) O
0 O

otherwise O
( O
8 O
) O
where O
E O
( O
W O
i O
) O
is O
the O
corresponding O
entity O
of O
word O
or O
sub-word O
W O
i O
in O
a O
KG O
, O
and O
A O
≡ O
KG O
B O
represents O
that O
both O
A O
and O
B O
exist O
and O
are O
adjacent O
in O
a O
KG O
. O
Experiments O
We O
briefly O
introduced O
the O
extraction O
of O

semantic O
information O
in O
Section O
4.1. O
Then O
we O
report O
experimental O
results O
on O
natural B-TaskName
language I-TaskName
understand I-TaskName
and O
question B-TaskName
answering I-TaskName
tasks O
in O
Section O
4.2 O
and O
4.3 O
respectively. O
In O
Section O
4.4 O
, O
we O
show O
evaluation O
on O
LAMA B-DatasetName
, O
a O
benchmark O
especially O
designed O
to O
study O
how O
much O
semantic O
knowledge O
is O
contained O
in O
a O
language O
model. O

Experiments O
on O
query-ad B-TaskName
relevance I-TaskName
is O
described O
in O
Section O
4.5. O
At O
last O
, O
we O
present O
ablation O
study O
in O
Section O
4.6 O
. O
Semantic O
Information O
Extraction O
We O
use O
Stanza O
library O
( O
Qi O
et O
al. O
, O
2020 O
) O
to O
extract O
NER B-TaskName
information O
and O
syntax O
information. O
Stanza O
NER B-TaskName
takes O
one O
sentence O
as O
input O
and O
returns O

the O
start O
and O
end O
indices O
of O
the O
corresponding O
named O
entity O
in O
the O
sentence. O
While O
Stanza O
Parser O
can O
extract O
the O
corresponding O
syntax O
tree O
for O
each O
sentence. O
We O
use O
query-ad O
logs O
from O
a O
commercial O
search O
engine O
to O
calculate O
PMI B-MetricName
matrix. O
These O
steps O
gain O
the O
knowledge O
required O
to O
generate O
the O
semantic O
attention O
maps O

mentioned O
in O
Section O
3.4 O
. O
GLUE B-MetricName
Benchmark O
The O
GLUE B-MetricName
benchmark O
offers O
a O
collection O
of O
tools O
for O
evaluating O
the O
performance O
of O
models O
across O
a O
diverse O
set O
of O
NLP B-TaskName
applications. O
It O
contains O
singlesentence B-TaskName
classification I-TaskName
tasks O
( O
CoLA B-TaskName
and O
SST-2 B-TaskName
) O
, O
similarity O
and O
paraphrase O
tasks O
( O
MRPC B-TaskName
, O
QQP B-TaskName
and O
STS-B B-TaskName
) O

and O
pairwise O
inference O
tasks O
( O
MNLI B-TaskName
, O
RTE B-TaskName
and O
QNLI B-TaskName
) O
. O
We O
use O
the O
default O
train O
/ O
dev O
/ O
test O
split O
for O
each O
dataset. O
The O
hyper-parameters O
are O
chosen O
based O
on O
the O
validation O
set O
( O
refer O
to O
appendix O
for O
details O
) O
. O
After O
the O
model O
is O
trained O
, O
we O
make O

predictions O
on O
the O
test O
data O
and O
send O
the O
results O
to O
GLUE B-MetricName
online O
evaluation O
service O
1 O
to O
get O
testing O
scores. O
Note O
that O
the O
original O
WNLI B-DatasetName
dataset O
in O
the O
GLUE B-MetricValue
benchmark O
is O
problematic O
, O
which O
causes O
the O
evaluation O
results O
to O
be O
65.1. B-MetricValue
In O
order O
to O
make O
a O
fair O
comparison O
, O
most O
papers O

( O
Devlin O
et O
al. O
, O
2019 O
; O
choose O
to O
ignore O
the O
results O
of O
WNLI B-DatasetName
when O
calculating O
GLUE B-MetricName
average B-MetricName
score I-MetricName
. O
The O
scores O
on O
all O
datasets O
in O
GLUE B-MetricName
benchmark O
are O
listed O
in O
Table O
1. O
We O
report O
test O
scores O
on O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
, O
RoBERTa B-MethodName
related O
models O
and O
their O
corresponding O
enhanced O
models. O

The O
performances O
of O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
and O
1 O
https O
: O
/ O
/ O
gluebenchmark.com O
RoBERTa-Large B-MethodName
are O
reproduced O
using O
the O
official O
checkpoints O
provided O
by O
corresponding O
authors O
. O
As O
shown O
in O
the O
table O
, O
our O
models O
outperform O
all O
corresponding O
baselines. O
KAM-BERT-Base B-MethodName
achieves O
an O
average O
GLUE B-MetricName
score O
of O
78.7 B-MetricValue
, O
lifting O
1.2 B-MetricValue
scores O
from O

standard O
BERT-Base B-MethodName
model O
with O
only O
a O
few O
extra O
parameters O
introduced O
to O
the O
baseline O
model. O
Particularly O
, O
the O
improvements O
on O
CoLA B-DatasetName
datasets O
are O
fairly O
large O
, O
showing O
that O
our O
knowledge O
integration O
method O
has O
good O
generalization O
performance O
for O
natural B-TaskName
language I-TaskName
inference I-TaskName
and O
understanding. O
ERNIE B-MethodName
have O
also O
added O
external O
information O
such O
as O
entity O

and O
knowledge O
graph O
, O
but O
it O
needs O
much O
more O
time O
for O
a O
joint O
re-training. O
As O
for O
BERT-Large B-MethodName
and O
its O
counterpart O
KAM-BERT-Large B-MethodName
, O
the O
average O
improvement O
on O
GLUE B-MetricName
benchmark O
is O
0.9. B-MetricValue
We O
can O
see O
that O
the O
improvement O
becomes O
smaller O
when O
the O
model O
grows O
larger O
, O
because O
larger O
models O
often O
capture O
more O

semantic O
knowledge O
in O
the O
pre-training B-TaskName
phrase. O
But O
incorporating O
explicit O
knowledge O
is O
still O
indispensable O
for O
achieving O
a O
superior O
performance O
. O
Question B-TaskName
Answering I-TaskName
We O
conduct O
experiments O
on O
two O
kinds O
of O
question O
answering O
tasks O
, O
i.e. O
, O
commonsense B-TaskName
QA I-TaskName
and O
open-domain B-TaskName
QA. I-TaskName
Commonsense B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
with O
commonsense. O
We O
adopt O
CosmosQA B-DatasetName

for O
evaluation O
, O
which O
requires O
commonsense-based O
reading O
comprehension O
formulated O
as O
multiple O
answer O
selection. O
Opendomain B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
using O
external O
resources O
such O
as O
collections O
of O
documents O
and O
webpages. O
We O
consider O
two O
public O
datasets O
for O
this O
task O
, O
i.e. O
, O
Quasar-T B-DatasetName
and O
SearchQA B-DatasetName
. O
The O
results O
of O
CosmosQA B-DatasetName
are O
shown O

in O
Table O
2. O
Compared O
with O
BERT-Large B-MethodName
, O
KAM-BERT-Large B-MethodName
achieves O
10.6 B-MetricValue
% O
improvement O
in O
accuracy. O
KAM-RoBERTa-Large B-MethodName
further O
improves O
the O
accuracy O
of O
RoBERTa-Large B-MethodName
by O
5.4 B-MetricValue
% O
, O
which O
indicates O
that O
our O
models O
has O
better O
knowledge O
inference O
ability. O
For O
open-domain B-DatasetName
QA I-DatasetName
, O
our O
model O
also O
achieves O
better O
results O
compared O
to O
corresponding O
baselines. O
This O

because O
that O
KAM-based B-MethodName
models I-MethodName
can O
make O
full O
use O
of O
the O
infused O
knowledge. O
At O
the O
same O
time O
, O
one O
can O
notice O
that O
KAM-based B-MethodName
models I-MethodName
have O
fewer O
parameters O
than O
K-Adaptor B-MethodName
, O
demonstrating O
its O
effectiveness O
for O
knowledge O
infusion. O
WKLM B-MethodName
( O
Xiong O
et O
al. O
, O
2019 O
) O
forces O
the O
pre-trained O
language O
model O
to O
incorporate O

knowledge O
from O
a O
knowledge O
graph. O
This O
makes O
WKLM B-MethodName
to O
achieve O
a O
better O
score O
on O
QA B-TaskName
tasks O
, O
but O
KAM-BERT B-MethodName
performs O
even O
better O
than O
WKLM B-MethodName
. O
LAMA B-DatasetName
Benchmark O
To O
further O
verify O
whether O
KAM-BERT B-MethodName
better O
integrate O
internal O
knowledge O
into O
pre-trained B-MetricName
language O
models O
, O
we O
conduct O
experiments O
on O
LAMA B-DatasetName
, O
a O
widely O
used O

benchmark O
for O
knowledge B-TaskName
probing I-TaskName
. O
LAMA B-DatasetName
examines O
models O
' O
abilities O
on O
recalling O
relational O
facts O
by O
cloze-style O
questions. O
The O
first O
place O
micro-averaged B-MetricName
accuracy I-MetricName
is O
used O
as O
evaluation O
metrics. O
The O
evaluation O
results O
are O
shown O
in O
Table O
3. O
KAM-BERT B-MethodName
consistently O
outperforms O
corresponding O
baselines O
on O
all O
tasks. O
It O
indicates O
that O
KAM-BERT B-MethodName
can O
generate O
better O

attention O
maps O
with O
semantic O
guidance O
. O
Query-Ad B-TaskName
Relevance I-TaskName
Query-ad O
relevance O
measures O
how O
relevant O
a O
search O
ad O
matches O
with O
a O
user O
's O
search O
query. O
Very O
often O
queries O
and O
ads O
have O
words O
with O
special O
meanings O
, O
which O
are O
not O
easily O
understood O
well O
by O
traditional O
NLP B-TaskName
techniques O
but O
can O
benefit O
from O
the O
knowledge-assisted O

mechanism O
proposed O
in O
this O
work. O
Besides O
, O
user O
queries O
and O
ads O
text O
often O
contain O
noises O
, O
so O
evaluation O
on O
query-ad O
relevance O
task O
would O
test O
our O
model O
's O
robustness O
and O
resistance O
of O
noise. O
We O
compare O
BERT B-MethodName
and O
KAM-BERT B-MethodName
on O
a O
large-scale O
internal O
dataset O
of O
a O
commercial O
search O
engine. O
As O
shown O
in O

Table O
5 O
, O
our O
model O
outperforms O
corresponding O
baselines O
by O
a O
large O
margin O
, O
which O
is O
statistically O
significant O
under O
95 B-HyperparameterValue
% I-HyperparameterValue
confidence B-HyperparameterName
interval. O
One O
thing O
to O
call O
out O
is O
that O
, O
although O
NER B-TaskName
and O
syntax B-TaskName
parsing I-TaskName
results O
are O
nosier O
comparing O
to O
the O
ones O
in O
academic O
datasets O
, O
we O
still O
have O
good O

improvements O
on O
this O
dataset. O
This O
indicates O
the O
way O
we O
combine O
those O
knowledge O
together O
makes O
our O
model O
more O
robust O
to O
noisy O
inputs O
. O
Model O
Analysis O
In O
this O
section O
, O
we O
explore O
the O
sensitivity O
of O
hyperparameter O
α B-HyperparameterName
, O
and O
then O
conduct O
ablation O
experiments O
on O
three O
types O
of O
added O
knowledge O
. O
Hyper-parameter O
Analysis O

The O
optimal O
α B-HyperparameterName
value O
after O
grid O
search O
is O
0.2 B-HyperparameterValue
, O
which O
means O
that O
the O
original O
attention O
maps O
still O
dominate O
the O
token O
relationships. O
We O
chose O
three O
tasks O
from O
different O
fields O
to O
do O
ablation O
study O
for O
α. B-HyperparameterName
Our O
model O
is O
KAM-BERT-Base B-MethodName
, O
and O
its O
performance O
is O
shown O
in O
Table O
4. O
In O
three O

different O
tasks O
, O
setting O
α B-HyperparameterName
to O
0.2 B-HyperparameterValue
achieves O
the O
best O
results. O
An O
intuitive O
understanding O
is O
that O
when O
α B-HyperparameterName
is O
small O
, O
external O
knowledge O
plays O
an O
unimportant O
role O
and O
can O
not O
participate O
in O
the O
entire O
training O
process O
well. O
With O
the O
gradual O
increase O
of O
α B-HyperparameterName
, O
the O
intervention O
of O
external O
knowledge O
on O

the O
attention O
map O
will O
increase O
, O
and O
the O
attention O
relationship O
in O
the O
original O
sequence O
will O
be O
gradually O
lost O
, O
resulting O
in O
the O
decline O
of O
model O
performance O
. O
Ablation O
Study O
For O
a O
comprehensive O
understanding O
of O
our O
model O
design O
, O
we O
conduct O
ablation O
study O
with O
the O
following O
settings O
in O
Table O
6. O
The O

average O
scores B-MetricValue
of O
all O
ablation O
experiments O
are O
better O
than O
BERT B-MethodName
, O
but O
are O
relatively O
worse O
than O
KAM-BERT B-MethodName
, O
demonstrating O
all O
the O
components O
are O
beneficial O
for O
the O
final O
performance. O
At O
the O
same O
time O
, O
we O
observe O
that O
after O
deleting O
the O
entity O
attention O
map O
, O
the O
score B-MetricName
of O
KAM-BERT B-MethodName
drops O
drastically O
from O

78.7 B-MetricValue
to O
77.7. B-MetricValue
This O
shows O
that O
the O
gain O
brought O
by O
entity O
information O
is O
the O
greatest. O
In O
addition O
, O
the O
convolution O
layer O
is O
indispensable O
for O
achieving O
a O
superior O
performance O
. O
Case O
Study O
In O
Figure O
3 O
, O
we O
visualize O
an O
example O
of O
query-ad B-TaskName
relevance I-TaskName
, O
where O
the O
query O
is O
" O
buy O
glipizide O

" O
and O
ad O
text O
is O
" O
Fred O
Meyer O
Pharmacy O
Near O
Me O
" O
. O
The O
darker O
color O
in O
the O
figure O
represents O
a O
higher O
attention B-MetricName
score. O
Figure O
3 O
( O
a O
) O
is O
the O
attention O
map O
of O
vanilla O
BERT B-MethodName
without O
adding O
explicit O
knowledge. O
When O
encountering O
rare O
words O
like O
" O
glipizide O
" O
, O
the O

self-attention O
mechanism O
can O
not O
do O
a O
good O
job O
to O
decide O
which O
terms O
should O
" O
glipizide O
" O
attend O
to. O
But O
in O
Figure O
3 O
( O
b O
) O
, O
the O
attention O
map O
of O
KAM-BERT B-MethodName
uses O
term O
correlations O
to O
learn O
that O
" O
glipizide O
" O
is O
a O
medicine O
, O
so O
it O
focuses O
on O
the O
medicinerelated O

tokens O
like O
" O
Pharmacy O
" O
and O
" O
antidiabetic O
" O
. O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
KAM-BERT B-MethodName
, O
a O
flexible O
and O
efficient O
approach O
to O
inject O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models. I-MethodName
Extensive O
experiments O
on O
GLUE B-MetricName
and O
LAMA B-DatasetName
benchmark O
show O
that O
our O
approach O
outperforms O
all O
BERT-Style B-MethodName
baselines O
and O
achieves O
new O
SOTA O

on O
QA B-TaskName
tasks O
, O
suggesting O
that O
our O
models O
indeed O
integrate O
knowledge O
in O
an O
effective O
manner O
and O
have O
good O
generalization O
ability. O
In O
future O
work O
, O
we O
hope O
to O
investigate O
more O
types O
of O
knowledge O
which O
can O
be O
effectively O
integrated O
in O
our O
framework O
. O

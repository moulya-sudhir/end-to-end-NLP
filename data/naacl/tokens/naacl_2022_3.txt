Learning Natural Language Generation with Truncated Reinforcement Learning
Learning for Language (TrufLL), an original approach to train conditional language models without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pretrained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual question generation tasks, for which we report positive results over performance and language metrics, which we then corroborate with a human evaluation. To our knowledge, it is the first approach that successfully learns a language generation policy without pre-training, using only reinforcement learning. 1   
Introduction Since the development of generic language models trained on massive unlabelled text corpora Brown et al., 2020), state-of-the art language processing systems rely on sequential transfer learning (Ruder, 2019). The pretrained Language Model (LM) is fine-tuned on the downstream task using a standard supervised learning (SL) 1 Code is available at https://github.com/ AMDonati/RL-NLP VQG, TrufLL truncates the vocabulary space by using a language model. Here, 'run,' and 'the' are syntactically incorrect and thus truncated. Yet, 'car' is not trimmed as the LM is not visually grounded. (right) In a VQG training loop, the agent generates a question given an image-answer pair, which is then fed to a VQA model predicting an expected answer. If both answers match, the agent is rewarded.
objective Peters et al., 2019). Yet, such an approach suffers from several issues : (i) catastrophic forgetting when a model forgets previously learned knowledge and overfits to target domains, (ii) computational inefficiency from fine-tuning billion-parameters networks, and (iii) the need of supervised datasets. Moreover, task-specific language models learned with SL suffer from well-studied text degeneration issues (Holtzman et al., 2019), such as the exposure bias , language biases (Saleh et al., 2020;, or a lack of diversity (Li et al., 2015).
On the other hand, text generation can be naturally framed as a sequential decision making problem, with the sequence of words seen as successive actions over a vocabulary. Thus, some researchers have recently focused on learning language models using instead Reinforcement Learning (RL) Das et al., 2017;Narasimhan et al., 2015). RL methods allow acquiring language through interactions within rich and diverse environments (Luketina et al., 2019), help understanding language acquisition and language pragmatics (Lazaridou et al., 2016;Bisk et al., 2020). "Reward is enough" (Silver et al., 2021) highlights the necessity of using RL for AI systems to acquire language in its full richness. Indeed, (i) language may be intertwined with other modalities of action and observation, (ii) the utility of language varies according to situations and behaviours, (iii) it is consequential and purposeful, and (iv) some linguistic problems are better solved dynamically, through experience (such as using a diplomatic tone in a speech.) In addition, RL allows optimizing a non-differentiable learning signal, hence handles more diverse objective functions, and also avoids some of the text degeneration issues previously mentioned. So far, RL-based text-generation tasks have relied on a pre-training phase to ease learning: the policy language model is trained with SL on the task dataset, before being fine-tuned with policy gradient methods (Sutton et al., 1999) on the task at hand. Those approaches often require human-labelled datasets. Besides, combining pre-training and fine-tuning phases either barely change the policy distribution, or induces language drift (Lazaridou et al., 2020;Lu et al., 2020b), i.e the generated language drifts semantically or syntactically from natural language.
In this paper, we aim at learning a conditional language model using RL without a pre-training phase, so that (i) we get free from datasets with human annotations, and (ii) we avoid the text generation flaws induced by the common methods. While appealing, such an approach requires overcoming the hurdle of the combinatorial language action space, a vocabulary usually containing more than 10,000 words. Yet, while large and discrete, a language action space contains a specific structure, made of all the syntactical and semantics rules of a given language. TrufLL leverages such structure to drive the exploration of the RL-based language agent during training. At each time step of the text generation process, TrufLL truncates its effective action space to a small subset of words provided by a pretrained task-agnostic language model. Such an approach injects a generic prior linguistic knowledge into the RL algorithm, is usable on tasks lacking in-domain labeled data, and can be easily transferred to new RL-based text generation tasks. Thus, TrufLL can be applied to any language generation task given a generic LM and a reward. We here evaluate it on two Visual Question Generation (VQG) tasks, the synthetic CLEVR dataset (Johnson et al., 2017), and the natural language VQAv2 dataset (Goyal et al., 2017). Unlike alternative RL without pre-training approaches, TrufLL manages to ask meaningful and valid questions on large vocabularies, exhibiting success rate and language metrics close to pretrain models with labeled data, while producing more original language.
TrufLL We here aim at making RL methods feasible in the language setting by dynamically reducing the action space, i.e., by restricting the language agent to select a word within a subset of the vocabulary at each time step. We detail below the action space's truncation model and the associated RL algorithm to learn the language agent.
Dynamic Vocabulary Truncation TrufLL combines two distinct language models, which share the same vocabulary V: a RL language agent π θ and a pretrained language model f LM . At each timestep t, TrufLL restricts the vocabulary space of the RL language agent with:
V − t ={w|w ∈V,g trunc (w|w <t )=1},
where g trunc is a truncation function based on f LM which either associates 0 or 1 with each word in the vocabulary given the past words w <t . From a language modelling perspective, the vocabulary space of the language agent is reduced from V to V − where |V − |≪|V|, with |•| the cardinal of a finite set. From a RL perspective, the RL agent follows a truncated policy π − θ which only samples actions over the subset V − . In practice, such a policy is computed using a masked softmax function over the truncated vocabulary V − t : π − θ (.|w <t ,c) = softmax(m * logits π θ (w <t ,c)) where m=1 when g trunc (w|w <t )=1 otherwise m=−∞.
Truncation Functions We here list the different truncation functions g trunc explored through the paper.
Top-k words: This function selects the k words with the highest probability given by f LM (.|w <t ):
g top(k) (w t |w <t ;k)=1 wt∈top(k)(f LM (.|w<t)) .
Probability threshold (α): This function only keeps words having a probability f LM (.|w <t ) greater than α:
g p th (α) (w t |w <t ;α)=1 f LM (wt|w<t)>α .
Top-p: This function is based on nucleus sampling (Holtzman et al., 2019), and it keeps the most likely words contained in a probability mass p of f LM (.|w <t ). Formally, we define V p t as: 
V p t =
g sample(k) (w t |w <t ;k)=1 wt∈{w i ∼f LM (.|w<t)i∈ 1,...,k } .
Only top(k) provides a fixed number of words at each time step. p th (α), top(p), and sample(k) have a dynamic truncation, whose size at t depends on the language model entropy.
Task-Specific vs. Generic LM We benchmark two types of language models for truncation. On the one hand, we use an external language model pretrained on a large task-agnostic language corpora. Such a model provides a generic linguistic prior to the RL agent exploration process, solely encoding syntactic and semantic information. On the other hand, we use a task-related language model pretrained on the supervised dataset associated with the task. Such a model provides a task-specific linguistic prior to the RL language agent, and captures language pragmatics. We emphasize that this paper aims at leveraging taskagnostic language models as they discard the need for task-specific data. For the sake of completeness, we also study the truncation with the task-related LM as an additional benchmark to assess our approach.
Experimental Setting We here list the experimental setting and detail the network and hyperparameters in Appendix A.4.
Visual Question Generation We showcase TrufLL on the task of Visual Question Generation (VQG) (Mostafazadeh et al., 2016), which is a form of Visual Jeopardy! ™ (Ferrucci, 2012). There, the language agent observes an image-answer pair and has to generate a question that results in a similar answer, as illustrated in Figure 1. Such a task presents multiple advantages. First, by combining vision, scene understanding and language generation, it requires high-level reasoning and exhibits a large spectrum of language difficulties. Secondly, the success criterion is naturally non-differentiable, hence a natural fit for RL methods. Such a criterion, unlike metrics based on ground-truth sentences, allows generating diverse grounded questions given an image-answer pair.
Formally, the initial context c is composed of the image-answer pair (I,A). The RL agent then generates a sequence of words w <t of maximum length T . We then provide the generated question to a pretrained VQA model. This model takes as inputs the image I, the generated question w <t and outputs a predicted answerÂ. Finally, the agent receives a reward r(w t ,w <t ,c) based on A andÂ.
Datasets We evaluate TrufLL on the CLEVR and VQAv2 datasets to simulate large-scale VQG datasets. The two datasets have been originally created for the task of Visual Question Answering (VQA), i.e. for multi-modal classification algorithms predicting an answer given an image-question pair.
CLEVR The CLEVR VQA dataset (Johnson et al., 2017) is made of template questions on synthetic images, which contain simple objects with four distinct properties (shape, material, color, size). The vocabulary contains 86 words and 28 potential answers, making it a valuable proof of concept for assessing TrufLL. Both language models are single-layer LSTMs (Hochreiter and Schmidhuber, 1997) with 512 units, and 512 word embedding dimension. The task-specific LM is trained over the full train dataset of CLEVR questions. The external language model is trained on the mixture of CLOSURE (Bahdanau et al., 2019) and CLEVR-Dialog (Kottur et al., 2019) datasets. Although those two datasets share the CLEVR vocabulary, their language distribution differs from vanilla CLEVR. Finally, we use a pretrained GT-Vector-NMN (Bahdanau et al., 2019) to compute the reward r(w t ,w <t ,c) = 1 A=Â,t=T −1 , where 1 is the indicator function.
VQAv2 The VQAv2 dataset (Goyal et al., 2017) is made of natural language and open-formed questions on images from the MS-Coco Dataset (Lin et al., 2014). It has a vocabulary of 14,810 words and 3,149 answers. The task-specific language model is a one-layer LSTM with 512 units and a 512 word embedding dimension, pretrained over the full training dataset of VQAv2 questions. The External Language Model is Open-AI's GPT-2 . The original language model outputs a probability distribution over 50,257 tokens, but we use a masked softmax function to restrict the probability distribution to the 14,810 tokens of the VQAv2 dataset. Unlike most NLP tasks relying on pretrained generic language models, we do not fine-tune it on the task dataset. Instead, we leverage the few-shot generalization capabilities of GPT-2, by feeding the language model with the prompt "Here are a few examples:" followed by 100 random questions q <100 from the dataset. The truncation is then based on the probability distribution f gpt2 LM (.|q <100 ,w <t ). Finally, we used a pretrained Vil-BERT to compute the reward (Lu et al., 2020a). Given the large number of answers, we use as reward a decreasing function of the rank of the reference answer rk(A): r(w t ,w <t ,c) = 1 rk(A)≤10,t=T −1 e −rk(A)/2 , as further explained in Appendix A.5.
In these two settings, we acknowledge that the task dataset is still used to train the VQA models. Please note that the VQA modules are only used to model the environment, i.e. to provide a positive/negative feedback to the agent. In other settings, TrufLL would still work if we replace the VQA model by any language interface: text-game (e.g. Zork), expert-systems, or humans. Here, we only use the VQG framework as a proof of concept that natural language can be learned through pure interaction given any task reward. Other language generation applications are discussed in Section 5.3.
Baselines In this paper, we aim to show that a RL language agent can be trained from scratch, i.e. without the usual pre-training phase by solely interacting with another language system, the VQA model, when supported by truncation methods. The truncation with the task-related LM is referred to as TrufLL (Task-LM), while the one with the External LM is referred as TrufLL (Ext-LM). We first emphasize the difficulty of training an RL language agent without a supervised pre-training phase through two baselines. We trained a simple on-policy PPO algorithm without any action space pruning, and refer to it as scratch. Then, we added a Kullback-Leibler (KL) regularization term to the loss, λ KL KL(π θ ||f LM ), with λ KL > 0, to incorporate language prior to the agent as in (Jaques et al., 2017(Jaques et al., , 2019. We refer to it as scratch + KL-task when distilling the task-specific language model, and scratch + KL-ext with the external language model. Finally, we include two baselines with a pre-training phase. We trained a language agent on the task-dataset with a log-likelihood objective, and refer to it as pretrain. Then, we fine-tune the pretrained language agent with PPO without truncation, and refer to it as pretrain + RL fine-tune. These two baselines should be viewed as gold standards as they rely on task-related data; additionally, pretrain + RL fine-tune is today the state-of-the-art method for learning RL-based LM.
Metrics and Evaluation Methods Evaluating text generation is an open-research problem in language literature. We decompose automatic language evaluation into three categories to assess different facets of language, and perform as well a human evaluation study.
Performance metrics. We measure the taskcompletion score or recall @ 1 which states whether the target answer A is the top answer of the VQA models, and the recall @ 5 (R@5), which assesses whether A is in the 5 top answers. These scores measure the task-solving abilities of the agent, but they are also conditioned by the VQA model abilities.
Language Metrics. First, we used n-grams metrics, BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015), to measure the similarity between the generated question and the reference questions in the evaluation set. While those scores can capture syntactic and semantic properties of language, they also fall short when dealing with open-form language, e.g. an identical answer may arise from two non-overlapping but syntactically correct questions. Thus, we also compute two metrics assessing the quality of the language independently of reference questions, the perplexity of the question given an external LM (ppl-e), and its perplexity given the task-related LM (ppl-t).
Diversity Metrics. We here estimate a self-BLEU (sBLEU) score (Zhang et al., 2017) over 10 questions generated on the same image-answer pair. Although such score detects potential mode collapse, i.e., when the language utters identical sequences of words, it also values babbling, i.e., outputting random words. We thus also measure the probability mass of the ten most frequent words (Choshen et al., 2020), and refer to it as peakiness (peak).
Human Evaluation. On the VQAv2 task, we also performed human evaluation by surveying 53 participants on the first 50 questions produced by some of the models at test time. The study (further detailed in Appendix C) is based on pairwise comparison of question samples produced by the concurrent algorithms according to four criteria. First, we evaluated the language quality of the question samples, by asking the participants to select the most syntactically and semantically correct question among the two samples of the questions pair. Secondly, we evaluated language grounding, i.e adequacy of the sample to the image-answer pair, by asking the participants to select the question most suitable given the two elements. Thirdly, we evaluated the language originality and diversity, by asking participants to select the question the most different from the dataset reference question. Finally, we evaluated the number of syntax errors by asking participants to tick the question if it is grammatically incorrect. Examples of questions asked during the study are included in the Appendix C.
Sampling methods for text generation When generating text from a trained language model, the quality and diversity of samples depend on the decoding algorithm (Zhang et al., 2020). We consider three text generation methods. greedy uses the argmax of the policy, while sampling uses the multinomial distribution. Finally, we sampled ten text sequences from the policy, and selected the one with the lowest perplexity according to the external language model, and refer to it as lm-ranking. This process has been used recently in Text-to-Image Generation tasks (Ramesh et al., 2021   5 Results
CLEVR results Quantitative performance: In Table 1, vanilla RL from scratch fails to have a decent performance even with synthetic language. Besides, adding a KL regularisation term does kick-start the learning process. Yet, as soon as we apply the dynamic truncation, TrufLL matches the pretrained baselines performance when using the external LM, and even outperforms them with the task-specific LM. In this synthetic VQG setting, TrufLL seems to be a viable and promising procedure to learn a RL language agent without a supervised training phase. Pretrained baselines have high language scores when assessed with datasetbased metrics, e.g BLEU or task-perplexity. Yet, they also remain close to the original dataset distribution with a medium external perplexity. Noticeably, TrufLL with the task-specific LM follows the same pattern. On the other hand, TrufLL with the external LM reports poor dataset-based language scores, while maintaining a low external perplexity. Therefore, TrufLL seems to correctly capture the language distribution of the initial LM. As the performance score is high when using an external LM, it suggests that our approach can learn a policy on a language task with-out the need of a task-related dataset. Less positively, TrufLL diversity metrics suggest potential mode collapse, with a high peakiness and self-BLEU score.
Qualitative performance: We display qualitative samples in Figure 2 and Appendix D. On the one hand, the pretrained baselines generate either a question inconsistent with the visual context, or which fails to answer the expected answer.  action space, while having a lower performance, yields to the most correct and diverse language, with higher language scores and a lower self-BLEU. A stochastic action space might be harder to explore efficiently for reaching good task-solving abilities, but might strengthen the agent language generation properties.
VQAv2 task In CLEVR, we observe that TrufLL seems a promising approach to learn a language policy without a supervised training phase, by solely interacting with another language system. We scale our approach to natural language with large vocabulary (15k tokens) through the VQAv2 dataset.
Quantitative performance: Table 3 reports the VQAv2 results, for which TrufLL and the baselines present a similar trend than on CLEVR. First, the scratch baselines keep failing to learn a valuable policy, with performance scores and n-grams metrics close to zero. Although TrufLL does not outperform the performance of the pretrained baselines anymore, it still leads to similar performances, and satisfactory language scores. The similarity between TrufLL (Task-LM) and TrufLL (Ext-LM) results suggests that the truncation approach is viable when using a generic LM whose original vocabulary distribution differs from the task. Interestingly, TrufLL displays a self-BLEU score similar to the pretrained baselines. This suggests that the poor diversity behavior observed on CLEVR is likely attributable to the small vocabulary and synthetic language distribution.
Qualitative performance: In Figure 2 and Appendix D, we display question samples for all models.
TrufLL and the pretrained baselines successfully generate a question giving the expected answer ("Black"), while the RL from scratch baselines fail, and even showcase degenerated language. Pretrained baselines tend to output a question closer to the reference question whereas TrufLL outputs original questions which differs from the VQA distribution, yet consistent with the context.
Human Evaluation: Figure 3 details the Human Evaluation results. Among the RL from scratch baselines, we selected scratch+KL-task as the only model producing sometimes meaningful questions. Yet, it fails to generate correct and grounded language; it is thus not a viable approach despite its diverse output. In line with the automatic metrics, the supervised baselines produce the best language, while being accurately grounded. Yet, they exhibit significantly less diversity with the reference language; this suggests in particular that pretrain+RL fails to go beyond the initial task-data distribution. Finally, unlike TrufLL (Task-LM) which suffers from syntactic errors, TrufLL (Ext-LM) produces language that qualitatively competes with pretrain models (53%), with a similar ratio of syntactic uncorrect samples. Although its questions are less grounded, they are diverse, which suggests that they follow a different distribution from the initial VQA dataset. It confirms that TrufLL (Ext-LM) could be an alternative approach as it has an excellent trade-off between language quality, diversity, and grounding.
Decoding procedure: In Table 4, we evaluate the text sampling procedures described in Section 4.5. While greedy decoding produces the best outcome for pretrained models, lm-ranking provides an excellent trade-off between task performance and language quality with RL-based methods. As PG solely optimizes the task success ratio, this may reduce overall language quality, the re-ranking thus retrieves the best syntactically sentences a posteriori.
Discussion Removing the truncation at evaluation with offpolicy RL. So far, TrufLL directly learns the truncated policy over the truncated vocabulary V −   each cell displays the proportion of questions chosen for the models in the row (bold) when compared to the concurrent model in the column. The table at the bottom displays the proportion of incorrect questions coming from each model among all incorrect samples. In all figures, bracket numbers indicates the model rank per criteria, from 1="best" to 5="worst".  2012). Formally, the off-policy PPO loss is defined by:
L off ppo (θ)=E π − θ min(ρ θ t A t ,clip(1−ϵ,ρ θ t ,1+ϵ)A t ) , whereρ θ t = π θ (at|st) π θ old (at|st) π θ old (at|st) π − θ old
(at|st) is the new ratio. 4 Table 5 displays the on-policy and off-policy results on both VQG tasks for TrufLL (task-LM), and is further detailed in Appendix B.3. We also monitor the probability mass of the policy attributed to the truncated action space (sumVA). The policy only samples words within the truncated action space when sumVA = 1, without needing the truncation. On CLEVR, the TrufLL off has lower -yet close -performance on language and task scores than TrufLL. As its sumVA ratios are very close to 1, the agent has learned to generalize over the full vocabulary. However, the approach does not manage to sufficiently scale to VQAv2. It could be improved with regularisation techniques and the use of TruFLL within state-of-the-art off-policy RL algorithms. We leave such possibilities to future works.  Additional experiments. We sweep over truncation hyper-parameters in Table 6 of Appendix B. In Table 8, we observe that rewarding an agent with a BLEU score is sub-optimal in both language and task scores on CLEVR. In VQA, we apply temperature scheduling on the LM to perform fine-grained truncations in Table 9 of B.2. Finally, we explore TrufLL with a pre-training phase in Table 10.
Generalization of the approach. TrufLL learns conditional language models able to solve specific Natural Language Generation tasks given a context c. For solving such tasks, it only requires the context, a reward function that scores the language generated by the RL agent with respect to the task, and eventually a few natural language demonstrations fed as input prompt to the generic language model used in the truncation algorithm. Hence, the method is transferable to a wide variety of NLG tasks, without requiring upfront large-scale labelled datasets. Additionally, the RL framework allows to optimize non-differentiable objectives, making TrufLL a natural choice to learn end-to-end task-oriented dialogs, such as Das et al., 2017). Other interesting tasks for TrufLL include the ones typically found in Vision and Language Representation Learning (Lu et al., 2020a), such as Image Captioning, Grounding Referring Expressions (generation of a referring expression over a specific bounding box of an image), Captionbased Image Retrieval (generation of a caption that discriminates an image between a set of images).
Reward functions for such tasks can be based on similarity scores between the generated language and the associated image or image region, which can be computed using pretrained language representations such as BERT (Devlin et al., 2019) or multi-modal pretrained systems such as ViLBERT (Lu et al., 2019). The context can be any kind of data structure (natural language, database, video, etc): if it is a linguistic input, TrufLL can be applied for instance to text summarization, paraphrase generation (with reward functions based on similarity scores between the context and the generated language) or text-based games (Ammanabrolu and Riedl, 2018).
6 Related work RL and NLP Tasks. Following (Singh et al., 2002;Lemon and Pietquin, 2007), recent RL-based taskoriented dialogues Das et al., 2017;Lewis et al., 2017;Narasimhan et al., 2015) have been developed, where the policy language model is generally pretrained with SL followed RL  (Yao et al., 2020) combines a pretrained language model to prune the action space with a Deep-Q network, aka DRNN ). Yet, its truncation language model remains fine-tuned on the RL dataset. Besides, CALM is only evaluated on a vocabulary of 697 tokens, and on 4-words action sequences.
Learning Language Models from scratch. (Ziegler et al., 2019;Garg et al., 2021) finetune pretrained GPT-2 models with RL for language generation tasks without task-related data, only using reward signals. Yet, they still face optimization and computational challenges (Parisotto et al., 2020).
Conclusion We proposed TrufLL, an original approach to learn a natural language generation (NLG) task using RL, without the usual pre-training phase requiring supervised datasets. To our knowledge, this is the first RL-based algorithm dedicated to learning a word-based text-generation task, which does not rely on a pre-training phase while scaling to large vocabularies. Although it comes with its limitations, the truncated RL algorithm provided by TrufLL gets free from labelled data in task-oriented language models, presents interesting language generation properties, and provides a generic and transferable method to learn any NLG problem.
A Dataset and training details A.1 Evaluation Metrics For the BLEU and METEOR scores, we used the NLTK 5 implementations with the smoothing function number 2 for the BLEU score. For the CIDEr score, we used the nlg-eval implementation 6 .
A.2 Answer filtering For each dataset, we remove yes and no question-answer pairs which frequency largely exceeds other answers, to avoid any bias in the question generation process, as usually done in the VQG litterature (Mostafazadeh et al., 2016).
A.3 Dataset split For CLEVR (resp. VQAv2), the RL language agent is trained for 50k (resp. 100k) episodes over the first 20k images (resp. all the images) of the training dataset, and is then evaluated on the first 5k (resp. 20k) images of the validation set. Besides, we uniformly sample the answer in the set of reference answers for each image to reduce the bias in the distribution of answers. Finally, questions are limited to 20 (resp. 10) words.
A.4 Language Agent Networks and Training For CLEVR (resp. VQAv2), we used a single-layer LSTM with 64 (resp. 256) units for the policy network. At every time step, the LSTM input is then the concatenation of the word embedding of dimension 32 (resp. 128), the answer embedding of dimension 32 (resp. 128), and the image representation. For CLEVR, the image representation is extracted from a pretrained ResNet50 and projected into a tensor of size (32,7,7) before being flattened. For VQAv2, the image representation is the average of 200 bounding box features of dimension 1048, extracted from a faster R- CNN (Ren et al., 2015). We optimize the full loss L=L P P O +αL V F +βL E with α=0.5, β =0.01 and a PPO clipping ratio ϵ=0.02 (resp. 0.01) for CLEVR (resp. VQAv2). We use Adam optimizer (Kingma and Ba, 2014) with a learning rate (lr) of 10 −3 for TrufLL and the scratch baseline, 10 −5 (resp. 10 −6 ) for RL algorithms with a pre-training phase on CLEVR (resp. VQAv2), and 5 * 10 −4 for models including a KL regularization term. We use a batch size (bs) of 128 for all models except the ones with KL regularization, for which we use a batch size of 64. Finally, for the RL from scratch baselines, we perform gradient clipping (gladclip) of 1 (resp. 5) for CLEVR and VQAv2.
Such hyper-parameters were selected, after conducting an extensive hyper-parameter search. The following values were tested: β ∈ {0.01, 0.02, 0.05, 0.1}, ϵ ∈ {0.01, 0.02, 0.05, 0.1, 0.5, 0.9}, lr ∈{10 −6 ,10 −5 ,10 −4 ,5 * 10 −4 ,10 −3 ,5 * 10 −3 ,10 −2 ,5 * 10 −2 }, gradclip ∈{None,1,5,10,100}, bs ∈{32,64,128}.
Additionally, we also tested for VQAv2 policy networks with 64, 256 and 1024 units, with respectively 32, 128 and 512 word embedding dimensions. We kept the network size giving the best performances, i.e. policy network of 256 units and 128 word embedding dimension.
A.5 Reward formula for VQAv2 In this section, we detail the reward function used for the VQAv2 task. r(w t ,w <t ,c)=1 rk(A)≤10,t=T −1 e −rk(A)/2 , with rk(A) the rank of the ground-truth answer given by the VQA model, when predicting the actual answer from the terminal state (c,w <T ). Formally, it is defined as:
rk(A)=rank(VQA(c,w <T )[A]),
with VQA(c,w <T ) the probability distribution given by the VQA model over the set of answers, and rank the function which ranks the probability of answer A within VQA(c,w <T ) probability distribution.
B Additional experiments B.1 CLEVR Table 6 displays the complete ablation on the truncation functions with parameters sweep. The 'sizeVA' variable indicates the average size of the truncated action space for each truncation function. Table 7 displays the ablation over the three decoding procedures defined in Section 4.5. Such an ablation presents a similar pattern than VQAv2 results described in section 5.2.
Finally, Table 8 reports CLEVR metrics when using the BLEU score as the reward. While on such a task TrufLL still exhibits promising language scores, the n-grams metrics remain lower than the pretrained baselines. This illustrates that using a language similarity score as a reward signal is much less interesting than a reward based on a task completion score.  
B.2 VQAv2 Temperature scheduling: On the CLEVR task, we observed that dynamic truncations outperform static ones such as top(k): indeed, they better take into account the inherent variability of the language structure at the sentence-level. When scaling up to the 15k words of the VQAv2 task, we also dynamically decrease the truncation size through training, by applying a decreasing temperature schedule on the language model. While temperature scaling (Bahdanau et al., 2015) is usually used at test time to control the smoothness of the language model distribution, temperature schedules during training of language models have been used in several settings (Jang et al., 2016;Zhang et al., 2018;Wang et al., 2020). Formally, f LM (w i |w <t ) distribution is computed as softmax(x i )=e −x i /τ / j e −x j /τ , with x j the LM logits and τ the temperature, which decreases from τ max to τ min by a factor T F every T u training step. In Table 9, both TrufLL (Task-LM) and TrufLL (Ext-LM) benefit slightly from truncation with a temperature schedule compared to a vanilla truncation. The former displays the best performance/language scores trade-off for the schedule "τ: 3 > 1. & T u =5,000", while the latter has the best metrics trade-off for "τ: 1.5 > 1. & T u =5,000". Finally, Figure 4 displays the evolution of the training return for TrufLL and the baselines. As expected, the pretrain+RL fine-tune baseline return does not evolve much, confirming that the policy distribution almost does not shift through the fine-tuning phase. The training curves of TrufLL present a steady increase in the return until reaching convergence, confirming that our approach, by guiding the exploration of the action space, provides a sufficient learning signal. On the other hand, the scratch+KL baselines stay stuck to a low training return. This suggests that the KL regularization term, while encouraging the policy distribution to resemble the language model distribution, fails to capture the task pragmatics, which requires generating a language that is visually grounded. 

Semantic Diversity in Dialogue with Natural Language Inference
Generating diverse, interesting responses to chitchat conversations is a problem for neural conversational agents. This paper makes two substantial contributions to improving diversity in dialogue generation. First, we propose a novel metric which uses Natural Language Inference (NLI) to measure the semantic diversity of a set of model responses for a conversation. We evaluate this metric using an established framework (Tevet and Berant, 2021) and find strong evidence indicating NLI Diversity is correlated with semantic diversity. Specifically, we show that the contradiction relation is more useful than the neutral relation for measuring this diversity and that incorporating the NLI model's confidence achieves state-of-the-art results. Second, we demonstrate how to iteratively improve the semantic diversity of a sampled set of responses via a new generation procedure called Diversity Threshold Generation, which results in an average 137% increase in NLI Diversity compared to standard generation procedures.
Introduction Dialogue models often struggle to produce engaging utterances in conversations, tending to generate responses which are common in the training data, such as "OK," "Yeah," or "I don't know" (Li et al., 2016). While these responses are appropriate for a wide variety of contexts, their over-production can result in a dull conversation (See et al., 2019).
An evaluation task has emerged that consists of measuring the diversity of chitchat model responses over a test set. While some past work uses human evaluation to measure model response diversity according to engagingness, specificity, or interestingness (Li et al., 2016;See et al., 2019;Ghandeharioun et al., 2019), several automated metrics have also been proposed to measure diversity of model responses. Some metrics measure lexical diversity, typically via n-gram overlap (Li Figure 1: Illustration of NLI Diversity using human responses from DailyDialog++. Contradictions are weighted by 1, entailments by -1, and neutrals by 0, so the score is (2 × 1) + (3 × 0) + (1 × −1) = 1. et al., 2016) or computing the BLEU score (Zhu et al., 2018) among model responses generated from the test set. Other past work attempts to measure semantic diversity via repurposing sentence similarity metrics (Tevet and Berant, 2021;Zhang et al., 2020a;Cer et al., 2017).
We propose a new metric aimed at measuring semantic diversity by leveraging a Natural Language Inference (NLI) model to score a set of multiple dialogue model responses for a single conversation, as illustrated in Figure 1. NLI is a three-way classification task to determine whether one sentence entails, contradicts, or is neutral toward a second sentence. We hypothesize that a diverse set of responses for a conversation captures contradictory ways one could respond, which can be measured by the NLI model. We aggregate the contradiction, neutral, and entailment predictions among pairs of responses from the set and combine the predictions into a new diversity metric, called NLI Diversity.
We additionally explore two modifications of NLI Diversity. First, because the neutral prediction may be indicative of diversity, we propose Neutral NLI Diversity, where neutral predictions are weighted the same as contradiction predictions. Second, since our Baseline NLI Diversity method does not take into account the confidence of the model's prediction, we propose Confidence NLI Diversity, which aggregates the probability mass of the model's predicted class instead of aggregating the number of predictions for each class.
We assess NLI Diversity using Tevet and Berant (2021)'s diversity metric evaluation framework, finding that NLI Diversity is highly correlated both with human judgments of diversity and with the diversity parameter, a gold standard diversity value used to generate the set of responses. Confidence NLI Diversity achieves state-of-the-art performance in terms of correlation with semantic diversity. Also, through an ablation study, we find positive, neutral, and negative correlations between human judgments and the number of contradiction, neutral, and entailment predictions, respectively.
We next explore the use of a dialogue model to generate a set of candidate responses with a minimum target level of semantic diversity, such as 10 Contradictions. Our new generation procedure, Diversity Threshold Generation, iteratively improves a set of model responses until this intended threshold is reached. If a set of sampled responses does not meet the intended threshold, the lowest-scoring response is thrown out and a new response is sampled until the diversity threshold is reached. We show this procedure results in a more diverse set of responses than the original sampled set, often with only a few resampled responses. Results of automated analysis shows relevancy is maintained from initial to final sets of responses.
In summary, our contributions are: • A novel diversity metric, NLI Diversity, evaluated using Tevet and Berant (2021) 
Measuring Model Response Diversity Traditionally, a model's diversity has been measured in terms of its predictions over the test set (Li et al., 2016), which we call Test Set Diversity. In this setup, the model predicts one response for each conversation in the test set (containing n conversations), resulting in n predictions. The diversity measure is computed over these n predictions, resulting in a score over the entire test set.
The notion of diversity we investigate, however, measures the model's ability to generate a set of responses for a single conversation Tevet and Berant, 2021), which we call Multi-Response Diversity. Instead of generating one response for each of the conversations in the test set, we evaluate a model's ability to generate m responses for each of the n conversations.
As shown by Tevet and Berant (2021), metrics which have been proposed in the Test Set Diversity setting can still be applied in the Multi-Response Diversity setting, however, by treating each set of m responses as its own "test set" and averaging over the n total sets.
Diversity Metrics Lexical diversity metrics measure differences in word choice, as opposed to diversity of content. Li et al. (2016) propose distinct-n, which measures the number of unique n-grams generated divided by the total number of n-grams generated in the Test Set Diversity setting. Some past work has applied this metric to the Multi-Response Diversity setting (Tevet and Berant, 2021). Cao and Clark (2017) propose examining the percent of unique responses over the test set. Other past work has proposed using BLEU score over a set of model responses in the Test Set Diversity setting (Zhu et al., 2018).
Semantic diversity metrics, on the other hand, compare diversity of the content present in each response. Many of these measures are adapted from semantic similarity scores, since lower similarity can indicate higher diversity (Tevet and Berant, 2021). BERTScore measures the similarity of BERT embeddings for each token in two sentences (Zhang et al., 2020a). Bert-STS assigns a score based on the semantic similarity of two sentences (Tevet and Berant, 2021). The Sent-BERT metric computes cosine similarity between BERT sentence embeddings (Reimers and Gurevych, 2019). Larson et al. (2019) propose identifying diverse paraphrases by identifying embedding outliers.
Other past work has used human evaluation to measure a model's diversity. Li et al. (2016) ask humans to choose the better of two responses based on specificity to the past conversation. See et al. (2019) ask humans to rank dialogue responses on a variety of factors, including interestingness and inquisitiveness. Tevet and Berant (2021) compare participants' ability to judge diversity of a set of responses in two ways: (i) by ranking one response as more diverse than a second response and (ii) by judging the diversity of a single response on a Likert scale, finding that participants were equally able to judge diversity in both conditions. They also find that human judges are better at distinguishing semantic diversity than lexical diversity.
Other past work has incorporated diversity metrics into the dialogue dataset creation pipeline. Stasaski et al. (2020) propose a method which measures the diversity of a crowdworker's contributions compared to a corpus, using that information to determine when to stop collecting data from the worker. This results in a more diverse dataset.
Evaluation of Diversity Metrics Tevet and Berant (2021) propose a framework to examine the reliability of diversity metrics. They propose the notion of a diversity parameter, which is used to generate a set of model responses, e.g., the p-value in nucleus sampling, which specifies the vocabulary probability distribution cutoff used to restrict sampling to the most-likely words whose combined likelihood ≥ p. If p is higher, the set of responses should have higher diversity, and viceversa. This diversity parameter is treated as a gold standard for a set of responses' diversity. Diversity metrics assign scores in the Multi-Response Diversity condition and are evaluated in terms of correlation to the diversity parameter. They further propose two datasets to evaluate diversity metrics: one which includes model responses and contains varying levels of lexical diversity and one which is human-created and maintains high lexical diversity to allow focused evaluation of semantic diversity.
Natural Language Inference Natural Language Inference is a task aimed at predicting whether one sentence contradicts, entails, or is neutral towards a second sentence. Models for NLI are typically trained using one of two datasets: Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) or Multi-Genre NLI (MNLI) (Williams et al., 2018). More recent datasets include FEVER (Thorne et al., 2018;Nie et al., 2019), adapted from a fact-checking dataset, and ANLI (Nie et al., 2020), collected in an adversarial human-in-the-loop procedure. With the rise of transformer architectures, models have achieved high performance on NLI tasks (Liu et al., 2019).
In a dialogue setting, NLI has been used to improve consistency between a persona and model responses over the course of a conversation by integrating an NLI-based reward into a reinforcement learning training procedure (Song et al., 2020).
To our knowledge, however, NLI has not been used to measure the diversity of model responses in either the Test Set Diversity or the Multi-Response Diversity setting.
Generating Diverse Sets of Hypotheses While work has only recently begun to explore the task of generating multiple dialogue responses to a conversation Tevet and Berant, 2021), past work has explored generating diverse sets of hypotheses in some other application areas. Carbonell and Goldstein (1998) explored using Maximal Mutual Relevance to reduce redundancy without sacrificing relevancy in document selection for summarization. Batra et al. (2012) proposed a greedy iterative algorithm to generate diverse, probable hypotheses for multiple vision tasks. Most related to our work is Gimpel et al. (2013), which applied Batra et al. (2012)'s approach to machine translation, generating a set of translations instead of a single translation. In contrast to Gimpel et al. (2013), by holding the sampling procedure constant throughout the iterative process, our method can explore the extent to which diversity can be increased without altering standard decoding practices.
NLI Diversity Metric We propose three diversity metrics in the Multi-Response Diversity setting which leverage the predictions of an NLI model. Two metrics (Baseline and Neutral) aggregate the NLI model's class predictions and one metric (Confidence) aggregates the weight of these predictions.
Baseline NLI Diversity We propose a new metric, called Baseline NLI Diversity, which uses an NLI model's predictions to measure diversity. More formally, for a given conversation, c, and a dialogue generation model M , a set of utterances u 1 , ..., u n is produced by the model. Each pair of utterances is compared in both directions using an NLI model,
N LI(u 1 , u 2 ), N LI(u 2 , u 1 ), ..., N LI(u n , u n−1 ).
The NLI model predicts a distribution over the three potential classes: contradiction, neutral, and entailment. We take the argmax over these classes, resulting in a list of NLI predictions,
N LI preds (N LI(u 1 , u 2 ), ..., N LI(u n−1 , u n )) of size n(n − 1).
To produce an overall diversity score for N LI preds (u 1 , ..., u n ), we assign each of these classes a value representing their diversity, denoted N LI score (N LI preds (u 1 , ..., u n )).
We hypothesize that larger numbers of entailment predictions found in a set of model-generated utterances is indicative of a lack of diversity; similarly, larger number of contradiction predictions is indicative of a larger amount of diversity. Because we want a higher value of N LI score to indicate higher diversity, we assign values as:
N LI score =      1 if contradiction 0 if neutral -1 if entailment
The sum of the N LI score values for the set of utterances results in the final NLI Diversity score, formally defined as:
Baseline N LIDiversity = u i ,u j ∈u 1 ,...,un N LI score (N LI pred (N LI(u i , u j ))
While the Baseline NLI Diversity metric aggregates all classes, we also investigate the separate number of entailment, contradiction, and neutral predictions in N LI preds , denoted # Entailment, # Contradiction, and # Neutral, respectively.
Neutral NLI Diversity Our primary hypothesis is that contradictions indicate diversity and entailments indicate lack of diversity. Because it is unclear what the role of neutrals might be, we explore a version of NLI Diversity which weights neutral and contradiction predictions as equally diverse. This metric is the same as Baseline NLI Diversity except the N LI score used to assign values is:
N LI score_neutral =      1 if contradiction 1 if neutral -1 if entailment
Confidence NLI Diversity Because the prior two NLI Diversity metrics do not incorporate the confidence of the NLI model's class predictions, we explore an additional metric which incorporates this value. Letting conf class (u 1 , u 2 ) represent the model's probability mass assigned to the predicted NLI class after sof tmax, the function is defined as:
N LI score_conf idence =      1 × conf con (u 1 , u 2 ) if contradiction 0 if neutral -1 × conf ent (u 1 , u 2 ) if entailment
Intuitively, instead of assigning a 1 value for a contradiction prediction, this metric assigns the probability of the contradiction class. Likewise, instead of a -1 for an entailment prediction, this metric assigns the negative probability mass of the entailment class.
Evaluation of NLI Diversity We evaluate NLI Diversity by computing the correlation between the metric and both human labels and diversity parameter labels. Below we first describe the models and data and then present the results of the evaluation.
Models We explore two NLI models: a Roberta-large model (Liu et al., 2019) Tevet and Berant (2021). Corresponding temperature parameter (higher is more diverse) or semantic and lexical diversity levels accompany each example. containing 300M parameters. We refer to these models as NLI Diversity -MNLI and NLI Diversity -Combined, respectively. We do not employ additional fine-tuning of these models.
Data There are two different English datasets released to evaluate diversity metrics in Tevet and Berant (2021): conTest and decTest, described in Table 1. The conTest dataset is human-created and captures content, or semantic, diversity independent of lexical diversity. Low-diversity examples in this dataset have high lexical diversity but low semantic diversity. This dataset was created by asking crowdworkers to generate sets of utterances with either low or high semantic diversity using varied language, in order to keep a high level of lexical diversity constant across both conditions.
The decTest dataset includes model-generated responses, with diversity controlled by a decoding parameter, such as a temperature parameter. The dataset can include duplicate responses, and does not attempt to mediate lexical diversity; therefore, low-diversity examples in this dataset may reflect low lexical as well as low semantic diversity.
While the original dataset includes multiple generation tasks, we evaluate on the dialogue task, respGen, which is drawn from Reddit conversations (Hashimoto et al., 2019) 3 . There are 200 conversations for each of conTest and decTest for the respGen task, with multiple responses for each conversation (5 for conTest, 10 for decTest).
Diversity Parameter Correlation The diversity parameter from Tevet and Berant (2021) represents either a parameter directly used to generate responses via a dialogue model, such as p in nucleus sampling, or a binary value indicating whether crowdworkers were instructed to generate a high-or low-diversity set of responses. A measure which is able to capture diversity will be positively correlated with this diversity parameter.
Table 2 shows Spearman's correlations between NLI Diversity and the diversity parameter. On the conTest semantic diversity dataset, Confidence NLI Diversity achieves the highest correlation of all metrics (0.62) and approaches human performance. Baseline NLI Diversity performs comparably to the top-performing automatic metric from Tevet and Berant (2021), at 0.59 correlation. We note the 95% confidence intervals overlaps between Baseline NLI Diversity, Confidence NLI Diversity, Sent-BERT, and human judgements, indicating a lack of significant differences (see Appendix A). Although Neutral NLI Diversity does relatively poorly on conTest (0.24), it is the highest-performing NLI metric on decTest (0.72), suggesting that incorporating neutral predictions may capture lexical instead of semantic diversity.
A histogram of Confidence NLI Diversity values for low and high semantic diversity sets of responses is shown in Figure 2. We note the lack of large overlap between the distributions of low and high semantic diversity data. In addition to  
Human Correlation In this subsection, we examine the NLI Diversity metric's correlation to the human annotations collected by Tevet and Berant (2021). Each set of responses in conTest and decTest is scored by 10 annotators from 1 (not diverse at all) to 5 (very diverse) with half-point increments. We compute correlation with respect to the averaged rating.
In addition to NLI Diversity, we explore the prediction counts for each category. We expect that a higher # Entailment value will be negatively correlated with diversity because the more pairs of responses that entail each other, the more similar the set of responses is. Similarly, we expect that a higher # Contradiction value will be positively correlated with diversity. Since the NLI Diversity metric incorporates both # Entailment and # Contradiction, we would expect this metric to be highly correlated with human judgments as well.   Spearmean's ρ rank correlation results between our metrics and the human diversity scores are shown in Table 3. The highest-performing correlation for lexical diversity is the Neutral NLI Diversity (0.69). The highest-performing semantic diversity correlation is Confidence NLI Diversity (0.64). Additionally, Baseline and Confidence NLI Diversity correlations are stronger when evaluating with the conTest dataset than the decTest dataset (an increase of 0.48 to 0.63 for Baseline MNLI and 0.41 to 0.64 for Confidence NLI), indicating these metrics are more correlated with human ratings of semantic diversity than lexical diversity.
Across both datasets, # Entailment is negatively correlated with diversity, # Neutral does not have a strong correlation, and # Contradiction is positively correlated, as hypothesized. This supports our motivation to use NLI as a diversity metric.
Diversity Threshold Generation We have verified that NLI Diversity is both able to capture semantic diversity and aligns with human judgements. We can additionally use NLI Diversity to define a straightforward desired diversity threshold, div thresh for a set of model-generated responses, u 1 , ..., u n . For example, we might intend there to be 10 Contradictions within the set. We propose a generation procedure, Diversity Threshold Generation, designed to iteratively increase the diversity of a set of responses for a conversation.
For a conversation, Diversity Threshold Generation begins by sampling n responses. We score the diversity of these responses using a diversity metric, div_metric(u 1 , ..., u n ). If the diversity score falls above div thresh , the process is finished.
If, however, the score falls below div thresh , we identify the model response which contributes least to the diversity score by calculating div_metric(u 1 , ..., u n−1 ) for each sub-group of model responses of size n − 1. We discard the model response not present in the highestscoring subgroup and resample a new response. We re-calculate div_metric(u 1 , ..., u n ) and if div_metric(u 1 , ..., u n ) > div thresh , the process finishes. We continue resampling until the maximum cutoff of S is reached.
Evaluation of Diversity Threshold Generation Method
Models and Datasets We experiment with two neural dialogue models, DialoGPT (700M parameters) (Zhang et al., 2020b) 4 and BlenderBot 1.0 (300M parameters) (Roller et al., 2021) 5 . We use the default Transformers implementation for each model (Wolf et al., 2020) and do not fine-tune them. Runtime was between 3 and 36 hours on one Titan-X GPU.
All experiments involve the dialogue model M generating 5 responses for each conversation. The maximum number of samples, S, is set to 20. All experiments are averaged over 10 trials for stability.
We evaluate each model on the development set of two public English conversational datasets : Dai-lyDialog++ (1,028 conversations) (Sai et al., 2020;Li et al., 2017) and EmpatheticDialogues (2,763 conversations) (Rashkin et al., 2019). DailyDia-log++ includes 5 human-written responses per conversation, allowing for multi-reference comparison. We split each EmpatheticDialogues conversation at a random turn (consistent for all experiments) for generation. Since BlenderBot supports up to 128 positional embeddings, we pass in the last 128 tokens of the conversation for this condition.
Metrics We evaluate three diversity metrics: two semantic diversity metrics, Baseline NLI Diversity (Section 3) and Sent-BERT (Reimers and Gurevych, 2019;Tevet and Berant, 2021), and one lexical diversity metric, distinct-n (Li et al., 2016;Tevet and Berant, 2021). For Sent-BERT, we compute the average negative cosine similarity between BERT sentence embeddings for each pair of responses. Like Tevet and Berant (2021), for distinct-n, we compute the average distinct n-grams from n ∈ 1, 2, 3, 4, 5.
Because Baseline NLI Diversity is more humaninterpretable than Confidence NLI Diversity, we use this version for experimentation. For all NLI Diversity experiments, div thresh is achieved when # Contradictions is greater than 10 out of a total of 20 pair-wise comparisons. For both Sent-BERT and distinct-n, however, we do not have a humanspecifiable threshold. We use empirical thresholds measured from the sets of 5 human responses for each conversation in DailyDialog++. We choose the 90th percentile for div thresh (0.98 and -0.179 for distinct-n and Sent-BERT, respectively).
We decode using nucleus sampling (p = 0.9), as it has been shown to increase response diversity (Holtzman et al., 2020). However our method could be applied with other decoding procedures.
In order to robustly evaluate Diversity Threshold Generation, we measure both (i) whether Diversity Threshold Generation is able to generate more diverse sets of responses than was originally sampled and (ii) whether the increased diversity comes at the expense of decreased relevancy of the responses.
Diversity Results We aim to measure whether the diversity of the 5 responses from M increases using Diversity Threshold Generation, compared to the initial 5 sampled responses. Diversity of the starting and ending sets of utterances is measured by Baseline NLI Diversity, distinct-n, or Sent-BERT. We also report the  number of sampled utterances required to reach div thresh . Results for Diversity Threshold Generation are shown in Table 4. For every condition, we see an increase from starting to ending diversity; for NLI Diversity, this results in an average 137% increase. For most conditions, distinct-n requires more samples than Sent-BERT and Baseline NLI Diversity.
We can use the results of Diversity Threshold Generation to probe differences in the models. In our experimental setup, DialoGPT generates more diverse utterances across all conditions than BlenderBot. The models change by similar proportions from starting to ending diversity using the NLI metric. However, the starting diversity for BlenderBot is far lower than DialoGPT; the negative value for BlenderBot indicates that a large number of entailment predictions were present in the starting response set.
We can also examine differences between the datasets. For instance, we observe lower starting diversities for the Empathetic Dialogues dataset than for DailyDialog++ for both models. Additionally, the number of samples required for Em-patheticDialogues is consistently higher than for DailyDialog++. This is likely because div thresh for both datasets was calculated using human responses from DailyDialog++, since EmpatheticDialogues does not include multiple human responses.
Sampled responses can be seen in Appendix B and results reporting the average overlap from starting to ending sets of responses is in Appendix C. Appendix D includes results using beam search instead of nucleus sampling, and Appendix E reports the stability of Diversity Threshold Generation.
Relevance Results Since past work has documented a tradeoff between diversity and relevancy , we also report results for the relevancy of the starting and ending sets of responses for Diversity Threshold Generation. We use two established relevancy metrics: BLEU Score (Papineni et al., 2002) 6 and BERTScore (Zhang et al., 2020a) 7 . We show results on DailyDialog++, which has multiple human-generated responses for comparison, which is more correlated to human judgements than single-reference evaluation (Gupta et al., 2019).
Results are shown in Table 5. The key takeaway is that the relevancy values remain virtually unchanged when using the Diversity Threshold Generation procedure, according to both BLEU score and BERTScore. The average percent difference is 0.08% for BertScore and 1.1% for BLEU.
Conclusion We propose a novel semantic diversity metric, NLI Diversity, which is highly correlated to human judgments. Confidence NLI Diversity achieves state-ofthe-art results on measuring semantic diversity. We propose Diversity Threshold Generation to incentivize production of diverse sets of responses for a conversation. This results in more diverse sets of responses than originally sampled for multiple models, datasets, and metrics while maintaining relevancy, and can also be used to investigate a model's ability to produce diverse responses.  
A Confidence Interval Analysis We perform experimentation using bootstrapping to determine confidence intervals for conTest correlations to the diversity parameter. We sample a dataset of 110 elements (50% of the original con-Test dataset's size) from conTest with replacement and compute corresponding Spearman's correlation values using the sampled dataset for Sent-BERT, Baseline NLI Diversity, Confidence NLI Diversity, and human judgements. We repeat this process 1,000 times for stability and calculate 95% Confidence Intervals. The full conTest correlation value plotted with these intervals can be seen in Figure 3. While the Confidence Interval values overlap between all 4 conditions, the Confidence NLI Diversity distribution closely matches the human distribution.
B Sampled Responses Table 7 shows randomly-sampled examples from the DailyDialog++ dataset, created using Diversity Threshold Generation with the DialoGPT model and NLI Diversity as the intended div_metric.
C Average Utterance Overlap We measure the number of utterances which occur in both the starting and ending sets of responses, called utterance overlap. A high utterance overlap represents a set of responses which did not need to be significantly changed to reach div thresh . For example, an utterance overlap of 4 indicates that only 1 response needed to be resampled (potentially multiple times) from the starting set to reach div thresh . Results are seen in Table 6. Keeping in mind that higher Average Overlap indicates less resampling was needed, we note higher overlap for DialoGPT than BlenderBot 1.0 (with the exception of distinct-n and EmpatheticDialogues).
D Beam Search We evaluate beam search's ability to generate diverse utterances using Diversity Threshold Generation for DailyDialog++ and NLI Diversity. To compare nucleus sampling to beam search, we generate 25 beams and consider these responses from most to least probable, i.e. if the 5 most likely beams do not satisfy the diversity threshold, we remove the lowest-scoring beam and replace it with the 6th most likely beam. We find the starting NLI Diversity for beam search is -5.05, the ending diversity is 5.35, and an average of 10.97 sampled utterances is required. While the NLI Diversity does improve from the starting to ending set of responses, beam search has a much lower ending diversity than nucleus sampling. While past work has confirmed that nucleus sampling is more lexically diverse than beam search using Self-BLEU (Holtzman et al., 2020), our results confirm that nucleus sampling is also able to generate more semantically diverse utterances.

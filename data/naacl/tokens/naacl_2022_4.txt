Language Model Augmented Monotonic Attention for Simultaneous Translation
The state-of-the-art adaptive policies for simultaneous neural machine translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge. In this work, we propose a framework to aid monotonic attention with an external language model to improve its decisions. Experiments on MuST-C English-German and English-French speech-to-text translation tasks show the future information from language model improves the state-of-the-art monotonic multi-head attention model further.
Introduction A typical application of simultaneous neural machine translation (SNMT) is conversational speech or live video caption translation. In order to achieve live translation, an SNMT model alternates between performing read from source sequence and write to target sequence. For a model to decide whether to read or write at certain moment, either a fixed or an adaptive read/write policy can be used.
Earlier approaches in simultaneous translation such as Ma et al. (2019a) and Dalvi et al. (2018) employ a fixed policy that alternate between read and write after the waiting period of k tokens. To alleviate possible long delay of fixed polices, recent works such as monotonic infinite lookback attention (MILk) (Arivazhagan et al., 2019), and monotonic multihead attention (MMA) (Ma et al., 2019c) developed flexible policies using monotonic attention (Raffel et al., 2017). * ⋆ Work done while at Samsung Research †
Equal contribution Figure 1: The finetuned XLM-RoBERTa language model predicts German words using the prefix as input.(Green: Correct, Red: Incorrect, Black: Neutral).
While these monotonic attention anticipates target words using only available prefix source and target sequence, human translators anticipate the target words using their language expertise (linguistic anticipation) as well as contextual information (extra-linguistic anticipation) (Vandepitte, 2001). Inspired by human translation experts, we aim to augment monotonic attention with future information using language models (LM) (Devlin et al., 2019;Conneau et al., 2019).
Integrating the external information effectively into text-to-text machine translation (MT) systems has been explored by several works (Khandelwal et al., 2020;Gulcehre et al., 2015Gulcehre et al., , 2017Stahlberg et al., 2018). Also, integrating future information implicitly into SNMT system during training is explored in Wu et al. (2020) by simultaneously training different wait-k SNMT systems. However, no previous works make use of explicit future information both during training and inference. To utilize explicit future information, we explored to integrate future information from LM directly into the output layer of the MMA model. However, it did not provide any improvements (refer to Appendix A), thus motivating us to explore a tighter integration of the LM information into SNMT model.
In this work, we explicitly use plausible future information from LM during training by transforming the monotonic attention mechanism. As shown in Figure 1, at each step, the LM takes the prefix target (and source, for cross-lingual LM) sequence and predicts the probable future information. We hypothesize that aiding the monotonic attention with this future information can improve MMA model's read/write policy, eventually leading to better translation with less delay. Several experiments on MuST-C (Di Gangi et al., 2019) English-German and English-French speech-to-text translation tasks with our proposed approach show clear improvements of latency-quality trade-offs over the state-of-the-art MMA models.
2 Monotonic Attention with Future Information Model
Monotonic Attention In simultaneous machine translation (SNMT) models, the probability of predicting the target token y i ∈ y depends on the partial source and target sequences (x ≤j ∈ x, y <i ∈ y). In sequence-tosequence based SNMT model, each target token y i is generated as follows:
h j = E(x ≤j )(1)
s i = D(y <i , c i = A(s i−1 , h ≤j ))(2)
y i = Output(s i )(3)
where E(.) and D(.) are the encoder and decoder layers, and c i is a context vector. In monotonic attention based SNMT, the context vector is computed as follows:
e i,j = M onotonicEnergy(s i−1 , h j ) (4) p i,j = Sigmoid(e i,j )(5)
z i,j ∼ Bernoulli(p i,j )(6)
When generating a target token y i , the decoder chooses whether to read/write based on Bernoulli selection probability p i,j . When z i,j = 1 (write), model sets t i = j, c i = h j and generates the target token y i . For z i,j = 0 (read), it sets t i = j + 1 and repeats Eq. 4 to 6. Here t i refers to the index of the encoder when decoder needs to produce the i th target token. Instead of hard alignment of c i = h j , Raffel et al. (2017) compute an expected alignment in a recurrent manner and propose a closed-form parallel solution. Arivazhagan et al. (2019) adopt monotonic attention into SNMT and later, Ma et al. (2019c) extend it to MMA to integrate it into the Transformer model (Vaswani et al., 2017).
Monotonic Attention with Future Information The monotonic attention described in Section 2.1 performs anticipation based only on the currently available source and target information. To augment this anticipation process using future information extracted using LMs, we propose the following modifications to the monotonic attention.
Future Representation Layer: At every decoding step i, the previous target token y i−1 is equipped with a plausible future tokenŷ i as shown in the Figure 2. Since the tokenŷ i comes from an LM possibly with a different tokenizer and vocabulary set, applying the model's tokenizer and vocabulary might split the tokenŷ i further into multiple sub-tokens
{ŷ 1 i ,ŷ 2 i , • • • ,ŷ m i }.
To get a single future token representations i ∈ R d from all the sub-tokens, we apply a sub-token summary layer:
s i = Γ({ŷ 1 i ,ŷ 2 i , • • • ,ŷ m i })(7)
The Γ represents a general sequence representation layer such as a Transformer encoder layer or a simple normalized sum of sub-token representations.
We enrichs i at every layer l of the decoder block by applying a residual feed-forward network.
s l i = F F N (ỹ l−1 i )(8)
Monotonic Energy Layer with Future Information: Despite the fact that we can add the plausible future information to the output layer (Appendix A) or append it to the target token representation y i−1 , the MMA read/write decisions happen in Eq. 4. Therefore, we integrates i into the Eq. 4 instead.
The integration is carried out by modifying Eq. 4 -Eq. 5. We compute the monotonic energy for future information using the enriched future token representations i available at each layer:
e i,j = M onotonicEnergy(s i , h j ) (9)
We integrate the future monotonic energy function into Eq. 5 as follows:
p i,j = Sigmoid(e i,j +ẽ i,j )(10)
After computingp i,j , we compute c i similar to MMA model. This way of integration of future information allows the model to condition the LM output usage on the input sequence. The model can control the relative weightage given to the LM output by varying theẽ i,j . In case of insufficient source information in the low latency regime, we expect the model's decision policy to rely more onẽ i,j .
Inference: During inference, the start token does not contain any plausible information. After predicting the first target token, for every subsequent prediction of target token y i , we invoke the LM to predict the next plausible future token and integrate this new information into Eq. 10.
Experiments and Results 
Experimental Settings Datasets and Metrics: We conduct our experiments on the MuST-C English(En)-German(De) and English(En)-French(Fr) speech-to-text (ST) translation task. The speech sequence is represented using 80-dimensional log-mel filter bank features. The target sequence is represented as subwords using a SentencePiece (Kudo and Richardson, 2018) model with a unigram vocabulary of size 10,000. We evaluate the performance of the models on both the latency and quality aspects. We use Average Lagging(AL) as our latency metric and case-sensitive detokenized SacreBLEU (Post, 2018) to measure the translation quality, similar to (Ma et al., 2020). The best models are chosen based on the dev set results and reported results are from the MuST-C test (tst-COMMON) sets.
Language Models We use two language models to train our proposed modified MMA model. Firstly, we use the pretrained XLM-RoBERTa (Conneau et al., 2019) model from Huggingface Transformers 1 model repository. Since the LM output can be very open-ended and might not directly suit/cater to our task and dataset, we finetune the head of the model using the MuST-C target text data for each task.
We also train a smaller language model (SLM), which contains 6 Transformer decoder layers, 512 hidden-states and 24M parameters. We use the MuST-C data along with additional data augmentation to reduce overfitting. The SLM helps to remove the issues related to vocabulary mismatch as discussed in the Section 2.2.
Implementation Details: Our base model is adopted from Ma et al. (2020). We use a predecision ratio of 7 , which means that the simultaneous read/write decisions are made after every seven encoder states. We use λ or λ latency to refer to the hyperparameter corresponding to the weighted average(λ avg ) in MMA. The values of this hyperparameter λ are chosen from the set {0.01, 0.05, 0.1}. The Γ layer in Eq. 7 computes the normalized sum of the sub-token representations. For SLM, it simply finds the embedding since it shares the same vocabulary set. All the models are trained on a NVIDIA v100 GPU with update_f req set to 8.
Simultaneous Translation Models: Even though future information can be integrated explicitly into the fixed policy approaches such as Wait-K (Ma et al., 2019b), we choose monotonic attention as our baseline due to its superior performance (Arivazhagan et al., 2019;Ma et al., 2019c). We train a baseline based on Ma et al. (2020) work, called as MMA model. The MMA model encoder and decoder embedding dimensions are set to 392, whereas our proposed model's encoder and decoder embeddings are set to 256 to have similar parameters (≈ 39M ) for a fair comparison. We train two models using the 
Results We first analyze how the LM predictions are being utilized by the our model. In order to measure the relative weight given to model's internal states versus the predictions from the LM, we compare the norm of the monotonic energies corresponding to the LM predictions e pred (Eq. 9) and the previous output tokens e output (Eq. 4). Let us define LM prediction weight as follows:
LM pw = ∥e pred ∥ ∥e output ∥ (11)
In Figure 3, we plot the variation of LM pw (averaged) vs. λ. We use two additional values of λ ∈ {0.005, 0.001} to obtain this plot. We can observe that as the latency requirements become more and more strict, the model starts to give more weightage to the predictions coming from the LM. This shows that the model learns to utilize the information coming from LM predictions based on latency requirements. Next, we discuss the performance improvements obtained from our proposed approach. By varying the λ, we train separate models for different latency regimes. Moreover, the quality and latency for a particular model can also be varied by controlling the speech segment size during the inference. Speech segment size or step size refers to the duration of speech (in ms) processed corresponding to each read decision. We vary these hyperparameters for all the three models, namely MMA, MMA-XLM and MMA-SLM.
The BLEU-AL curves for all the models have been provided in Figure 4 and BLEU-AL numbers for all models are included in Appendix F for reference. We vary the step sizes in intervals of 80ms from 120 ms to 520 ms in order to get performances corresponding to different latency regimes. We can observe that the LM-based models using both XLM and SLM provide a significant performance improvement over the baseline MMA model. We observe improvements in the range of 1-2 BLEU scores consistently across all the latency regimes (λ = 0.1, 0.05, 0.01). The MMA using SLM language model performs slightly better than MMA using XLM language model. This is due to SLM's higher accuracy on the next token prediction task as compared to XLM, 30.15% vs. A LM at MMA Output Layer
We explored a naive approach of integrating LM information into the MMA. In this approach, we integrate the future information obtained from the LM directly into the output layer of the MMA model. We refer to this experiment as 'LM Rescoring(LMR)', and the corresponding model is called MMA-LMR. As observed in Figure 5, MMA-LMR has inferior performance compared to the MMA model. Since the LM information integration is only done at the output layer of the model, the MMA model cannot easily discard the incorrect information from LM. This motivates us to tightly integrate the LM information into the simultaneous model.
B Language Models As mentioned earlier, we train two different language models (LMs) and use them to improve the anticipation in monotonic attention based Simultaneous models.
B.1 XLM-Roberta(XLM-R) XLM-R Large model 2 was trained on the 100 languages CommonCrawl corpora total size of 2.5TB with 550M parameters from 24 layers, 1024 hidden states, 4096 feed-forward hidden-states, and 16 heads. Total number of parameters is 558M. We finetune the head of the XLM-R LM model using the Masked Language Modeling objective which accounts for 0.23% of the total model parameters, i.e., 1.3M parameters.
B.2 Smaller Language Model Since the LM predictions are computed serially during inference, the time taken to compute the 2 https://huggingface.co/xlm-roberta-large LM token serves as a bottleneck to the latency requirements. To reduce the LM computation time, we train a smaller Language Model (SLM) from scratch using the Causal Language Modeling objective. SLM is composed of 6 Transformer decoder blocks, 512 hidden-states, 2048 feed-forward hidden-states & 8 attention heads. It alleviates the need for the sub-token summary layer since it shares the vocabulary and tokenization with the MMA models. The train examples are at the sentence level, rather than forming a block out of multiple sentences(which is the usual case for Language Models).
Since the target texts contain lesser than 250k examples, we use additional data augmentation techniques to upsample the target data. We also use additional data to avoid overfitting on the MuST-C target text. Details have been provided in B.2.1.
B.2.1 Data Augmentation Up-Sampling: To boost the LM performance and mitigate overfitting, we use contextual data augmentation (Kobayashi, 2018) to upsample the MuST-C target text data by substituting and inserting words based on LM predictions. We use the NLPAUG 3 package to get similar words based on contextual embeddings. From the Hugging Face Repository, we use two different pretrained BERT (Devlin et al., 2019)  We observe that both upsampling and data augmentation help us to reduce the overfitting on the MuST-C dev set.
B.3 Token Prediction For each output token, the LM prediction is obtained by feeding the prefix upto that token to the LM model. These predictions are pre-computed for training and validation sets. This ensures parallelization and avoids the overhead to run the LM simultaneously during the training process. During  inference, the LM model is called every time a new output token is written.
C Dataset The MuST-C dataset comprises of English TED talks, the translations and transcriptions have been aligned with the speech at sentence level. Dataset statistics have been provided in the Table 1.
D Effect of LM Size on Latency-Quality We train several SLM models with varying sizes in our experiments and choose the best model based on the top-1 accuracy. As we increase the number of layers in the LM model from 2 to 4 to 6 layers, the SLM and the proposed MMA with future information models have shown performance improvements. However, increasing the number of layers greater than 6 does not yield any performance improvements. We also notice this degradation of performance with the XLM model while varying the number of hidden layers in the LM head.
E Training Details We follow the training process similar to Ma et al. (2020) training process. We train an English ASR model using the source speech data. Next, we train a simultaneous model without the latency loss (setting λ latency = 0) after initializing the encoder from the English ASR model. After this step, we finetune the simultaneous model for different λs. This training process is repeated for all the reported models and for each task. The details regarding the hyperparameters for the model have been provided in Table 2.
F BLEU-AL Numbers As mentioned in the results section of the main paper, we vary the latency weight hyperparameter (λ) to train different models to obtain different latency regimes. We also vary the step-size/speech segment size during inference. In total, we obtain 18 different data points corresponding to each model. In Table 3, we compare the results obtained using MMA, MMA-XLM and MMA-SLM under similar hyperparameter settings. It will help the reader to quantify the benefits obtained from our proposed approach.

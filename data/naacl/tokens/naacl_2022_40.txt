Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization
It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small amount of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pretraining by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at https://github.com/ fanolabs/isoIntentBert-main.
Introduction Intent detection is a core module of task-oriented dialogue systems. Training a well-performing intent classifier with only a few annotations, i.e., few-shot intent detection, is of great practical value. Recently, this problem has attracted considerable attention (Vulić et al., 2021;Zhang et al., b;Dopierre et al., b) but remains a challenge.
To tackle few-shot intent detection, earlier works employ induction network (Geng et al., 2019), generation-based methods (Xia et al., a), metric learning (Nguyen et al., 2020), and selftraining (Dopierre et al., b), to design sophisticated algorithms. Recently, pre-trained language models (PLMs) have emerged as a simple yet promising solution to a wide spectrum of natural language processing (NLP) tasks, triggering the surge of PLM- * Corresponding author.
based solutions for few-shot intent detection Zhang et al., a,b;Vulić et al., 2021;Zhang et al., b), which typically fine-tune PLMs on conversation data.
A PLM-based fine-tuning method (Zhang et al., a), called IntentBERT, utilizes a small amount of labeled utterances from public intent datasets to fine-tune PLMs with a standard classification task, which is referred to as supervised pre-training. Despite its simplicity, supervised pre-training has been shown extremely useful for few-shot intent detection even when the target data and the data used for fine-tuning are very different in semantics. However, as will be shown in Section 3.2, IntentBERT suffers from severe anisotropy, an undesirable property of PLMs Ethayarajh, 2019;Li et al., 2020).
Anisotropy is a geometric property that semantic vectors fall into a narrow cone. It has been identified as a crucial factor for the sub-optimal performance of PLMs on a variety of downstream tasks Arora et al., b;Cai et al., 2020;Ethayarajh, 2019), which is also known as the representation degeneration problem (Gao et al., a). Fortunately, isotropization techniques can be applied to adjust the embedding space and yield significant performance improvement in many tasks Rajaee and Pilehvar, 2021a).
Hence, this paper aims to answer the question:
• Can we improve supervised pre-training via isotropization for few-shot intent detection?
Many isotropization techniques have been developed based on transformation Huang et al., 2021), contrastive learning (Gao et al., b), and top principal components elimination (Mu and Viswanath, 2018). However, these methods are designed for off-the-shelf PLMs. When applied on PLMs that have been fine-tuned on some NLP task such as semantic textual similarity or intent classification, they may introduce an adverse effect, CL-Reg and Cor-Reg are designed to regularize SPT and increase the isotropy of the feature space, which leads to better performance on few-shot intent detection.
as observed in Rajaee and Pilehvar (2021c) and our pilot experiments. In this work, we propose to regularize supervised pre-training with isotropic regularizers. As shown in Fig. 1, we devise two regularizers, a contrastive-learning-based regularizer (CL-Reg) and a correlation-matrix-based regularizer (Cor-Reg), each of which can increase the isotropy of the feature space during supervised training. Our empirical study shows that the regularizers can significantly improve the performance of standard supervised training, and better performance can often be achieved when they are combined.
The contributions of this work are three-fold:
• We present the first study on the isotropy property of PLMs for few-shot intent detection, shedding light on the interaction of supervised pre-training and isotropization.
• We improve supervised pre-training by devising two simple yet effective regularizers to increase the isotropy of the feature space.
• We conduct a comprehensive evaluation and analysis to validate the effectiveness of the proposed approach.
2 Related Works
Few-shot Intent Detection With a surge of interest in few-shot learning (Finn et al., 2017;Vinyals et al., 2016;Snell et al., 2017), few-shot intent detection has started to receive attention. Earlier works mainly focus on model design, using capsule network (Geng et al., 2019), variational autoencoder (Xia et al., a), or metric functions (Yu et al., 2018;Nguyen et al., 2020). Recently, PLMs-based methods have shown promising performance in a variety of NLP tasks and become the model of choice for few-shot intent detection. Zhang et al. (c) cast few-shot intent detection into a natural language inference (NLI) problem and fine-tune PLMs on NLI datasets. Zhang et al. (b) propose to fine-tune PLMs on unlabeled utterances by contrastive learning. Zhang et al. (a) leverage a small set of public annotated intent detection benchmarks to fine-tune PLMs with standard supervised training and observe promising perfor-mance on cross-domain few-shot intent detection.
Meanwhile, the study of few-shot intent detection has been extended to other settings including semisupervised learning (Dopierre et al., b,a), generalized setting (Nguyen et al., 2020), multi-label classification (Hou et al., 2021), and incremental learning (Xia et al., b). In this work, we consider standard few-shot intent detection, following the setup of Zhang et al. (a) and aiming to improve supervised pre-training with isotropization.
Further Pre-training PLMs with Dialogue Corpora Recent works have shown that further pre-training off-the-shelf PLMs using dialogue corpora (Henderson et al., b;Peng et al., 2020Peng et al., , 2021 are beneficial for task-oriented downstream tasks such as intent detection. Specifically, TOD-BERT  conducts self-supervised learning on diverse task-oriented dialogue corpora. ConvBERT (Mehri et al., 2020) is pre-trained on a 700 million open-domain dialogue corpus. Vulić et al. (2021) propose a two-stage procedure: adaptive conversational fine-tuning followed by task-tailored conversational fine-tuning. In this work, we follow Zhang et al. (a) to further pre-train PLMs using a small amount of labeled utterances from public intent detection benchmarks.
Measuring isotropy Following Mu and Viswanath (2018); Biś et al. (2021), we adopt the following measurement of isotropy:
I(V) = min c ∈ C Z(c, V) max c ∈ C Z(c, V) ,(1)
where V ∈ R N ×d is the matrix of stacked embeddings of N utterances (note that the embeddings have zero mean), C is the set of unit eigenvectors of V ⊤ V, and Z(c, V) is the partition function (Arora et al., b) defined as:
Z(c, V) = N i=1 exp c ⊤ v i ,(2)
where v i is the i th row of V. I(V) ∈ [0, 1], and 1 indicates perfect isotropy.
Fine-tuning Leads to Anisotropy To observe the impact of fine-tuning on isotropy, we follow IntentBERT (Zhang et al., a) to fine-tune BERT (Devlin et al., 2019) with standard supervised training on a small set of an intent detection benchmark OOS (Larson et al., 2019) (details are given in Section 4.1). We then compare the isotropy of the original embedding space (BERT) and the embedding space after fine-tuning (IntentBERT) on target datasets. As shown in Table 1, after finetuning, the isotropy of the embedding space is notably decreased on all datasets. Hence, it can be seen that fine-tuning may render the feature space more anisotropic. 
Isotropization after Fine-tuning May Have an Adverse Effect To examine the effect of isotropization on a finetuned model, we apply two strong isotropization techniques to IntentBERT: dropout-based contrastive learning (Gao et al., b) and whitening transformation . The former fine-tunes PLMs in a contrastive learning manner 1 , while the latter transforms the semantic feature space into an isotropic space via matrix transformation. These methods have been demonstrated highly effective (Gao et al., b; when applied to off-the-shelf PLMs, but things are different when they are applied to fine-tuned models. As shown in Fig. 2, contrastive learning improves isotropy, but it significantly lowers the performance on two benchmarks. As for whitening transformation, it has inconsistent effects on the two datasets, as shown in Fig. 3. It hurts the performance on HWU64 (Fig. 3a) but yields better results on BANKING77 (Fig. 3b), while producing nearly perfect isotropy on both. The above observations indicate that isotropization may hurt fine-tuned models, which echoes the recent finding of Rajaee and Pilehvar.
Method The pilot experiments reveal the anisotropy of a PLM fine-tuned on intent detection tasks and the 1 We refer the reader to the original paper for details.  Whitening transformation leads to perfect isotropy but has inconsistent effects on the performance.
challenge of applying isotropization techiniques on the fine-tuned model. In this section, we propose a joint fine-tuning and isotropization framework. Specifically, we propose two regularizers to make the feature space more isotropic during fine-tuning. Before presenting our method, we first introduce supervised pre-training.
Supervised Pre-training for Few-shot Intent Detection Few-shot intent detection targets to train a good intent classifier with only a few labeled data D target = {(x i , y i )} Nt , where N t is the number of labeled samples in the target dataset, x i denotes the i th utterance, and y i is the label.  x i is the i th utterance in a batch of size 3. In (a), x i is fed to the PLM twice with built-in dropout to produce two different representations of x i : h i and h + i . Positive and negative pairs are then constructed for each x i . For example, h 1 and h + 1 form a positive pair for x 1 , while h 1 and h + 2 , and h 1 and h + 3 , form negative pairs for x 1 . In (b), the correlation matrix is estimated from h i , feature vectors generated by the PLM, and is regularized towards the identity matrix. can work well when the label spaces of D source and D target are disjoint.
Specifically, the pre-training is conducted by attaching a linear layer (as the classifier) on top of the utterance representation generated by the PLM: 
p(y|h i ) = softmax (Wh i + b) ∈ R L , (3
)
where h i ∈ R d is
Regularizing Supervised Pre-training with Isotropization To mitigate the anisotropy of the PLM fine-tuned by supervised pre-training, we propose a joint training objective by adding a regularization term L reg for isotropization:
L = L ce (D source ; θ) + λL reg (D source ; θ), (5
)
where λ is a weight parameter. The aim is to learn intent detection skills while maintaining an appropriate degree of isotropy. We devise two different regularizers introduced as follows.
Contrastive-learning-based Regularizer. Inspired by the recent success of contrastive learning in mitigating anisotropy (Yan et al., 2021;Gao et al., b), we employ the dropout-based contrastive learning loss used in Gao et al. (b) as the regularizer:
L reg = − 1 N b N b i log e sim(h i ,h + i )/τ N b j=1 e sim(h i ,h + j )/τ .(6)
In particular, h i ∈ R d and h + i ∈ R d are two different representations of utterance x i generated by the PLM with built-in standard dropout (Srivastava et al., 2014), i.e., x i is passed to the PLM twice with different dropout masks to produce h i and h + i . sim(h 1 , h 2 ) denotes the cosine similarity between h 1 and h 2 . τ is the temperature parameter. N b is the batch size. Since h i and h + i represent the same utterance, they form a positive pair. Similarly, h i and h + j form a negative pair, since they represent different utterances. An example is given in Fig. 4a. By minimizing the contrastive loss, positive pairs are pulled together while negative pairs are pushed away, which in theory enforces an isotropic feature space (Gao et al., b). In Gao et al. (b), the contrastive loss is used as the single objective to fine-tune off-the-shelf PLMs in an unsupervised manner, while in this work we use it jointly with supervised pre-training to fine-tune PLMs for fewshot learning.
Correlation-matrix-based Regularizer. The above regularizer enforces isotropization implicitly.
Here, we propose a new regularizer that explicitly enforces isotropization. The perfect isotropy is characterized by zero covariance and uniform variance Zhou et al., 2021), i.e., a covariance matrix with uniform diagonal elements and zero non-diagonal elements. Isotropization can be achieved by endowing the feature space with such statistical property. However, as will be shown in Section 5.3, it is difficult to determine the appropriate scale of variance. Therefore, we base the regularizer on correlation matrix :
L reg = ∥Σ − I∥,(7)
where ∥•∥ denotes Frobenius norm, I ∈ R d×d is the identity matrix, Σ ∈ R d×d is the correlation matrix with Σ ij being the Pearson correlation coefficient between the i th dimension and the j th dimension.
As shown in Fig. 4b, Σ is estimated with utterances in the current batch. By pushing the correlation matrix towards the identity matrix during training, we can learn a more isotropic feature space. Moreover, the proposed two regularizers can be used together as follows:
L = L ce (D source ; θ) + λ 1 L cl (D source ; θ) +λ 2 L cor (D source ; θ),(8)
where λ 1 and λ 2 are the weight parameters, and L cl and L cor denote CL-Reg and Cor-Reg, respectively. Our experiments show that better performance is often observed when they are used together.
Experimental Setup Datasets.   Gaming". HWU64 (Liu et al., 2019a) is a largescale dataset containing 21 domains. Dataset statistics are summarized in Table 3.
Our Method. Our method can be applied to fine-tune any PLM. We conduct experiments on two popular PLMs, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b). For both of them, the embedding of [CLS] is used as the utterance representation in Eq. 3. We employ logistic regression as the classifier. We select the hyperparameters λ, λ 1 , λ 2 , and τ by validation. The best hyperparameters are provided in Table 4.  Baselines. We compare our method to the following baselines.
First, for BERT-based methods, BERT-Freeze freezes BERT; CON-VBERT (Mehri et al., 2020), TOD-BERT , and DNNC-BERT (Zhang et al., c) further pre-train BERT on conversational corpus or natural language inference tasks. USE-ConveRT Casanueva et al., 2020) 5: 5-way few-shot intent detection using BERT. We report the mean and standard deviation of our methods and IntentBERT variants. CL-Reg, Cor-Reg, and CL-Reg + CorReg denote supervised pre-training regularized by the corresponding regularizer. The top 3 results are highlighted. ¶ denotes results from (Zhang et al., a   Finally, to show the superiority of the joint finetuning and isotropization, we compare our method against whitening transformation . BERT-White and RoBERTa-White apply the transformation to BERT and RoBERTa, respectively. IntentBERT-White and IntentRoBERTa-White apply the transformation to IntentBERT-ReImp and IntentRoBERTa, respectively. All baselines use logistic regression as classifier except DNNC-BERT and DNNC-RoBERTa, wherein we follow the original work 2 to train a pairwise encoder for nearest neighbor classification.
Training Details. We use PyTorch library and Python to build our model. We employ Hugging Face implementation 3 of bert-base-uncased and roberta-base. We use Adam (Kingma and Ba, 2015) as the optimizer with learning rate of 2e − 05 and weight decay of 1e − 03. The model is trained with Nvidia RTX 3090 GPUs. The training is early stopped if no improvement in validation accuracy is observed for 100 steps. The same set of random seeds, {1, 2, 3, 4, 5}, is used for IntentBERT-ReImp, IntentRoBERTa, and our method.
Evaluation. The baselines and our method are evaluated on C-way K-shot tasks. For each task, we randomly sample C classes and K examples per class. The C ×K labeled examples are used to train the logistic regression classifier. Note that we do not further fine-tune the PLM using the labeled data of the task. We then sample another 5 examples per class as queries. Fig. 1 gives an example with C = 2 and K = 1. We report the averaged accuracy of 500 tasks randomly sampled from D target .
Main Results The main results are provided in Table 5 ( BERTbased) and Table 6 (RoBERTa-based). The following observations can be made. First, our proposed regularized supervised pre-training, with either CL-Reg or Cor-Reg, consistently outperforms all the baselines by a notable margin in most cases, indicating the effectiveness of our method. Our method also outperforms whitening transformation, demonstrating the superiority of the proposed joint finetuning and isotropization framework. Second, Cor-Reg slightly outperforms CL-Reg in most cases, showing the advantage of enforcing isotropy explicitly with the correlation matrix. Finally, CL-Reg and Cor-Reg show a complementary effect in many cases, especially on BANKING77. The above observations are consistent for both BERT and RoBERTa. It can be also seen that higher performance is often attained with RoBERTa.  The observed improvement in performance comes with an improvement in isotropy. We report the change in isotropy by the proposed regularizers in Table 7. It can be seen that both regularizers and their combination make the feature space more isotropic compared to IntentBERT-ReImp that only uses supervised pre-training. In addition, in general, Cor-Reg can achieve better isotropy than CL-Reg.
Ablation Study and Analysis Moderate isotropy is helpful. To investigate the relation between the isotropy of the feature space and the performance of few-shot intent detection, we tune the weight parameter λ of Cor-Reg to increase the isotropy and examine the performance. As shown in Fig. 5, a common pattern is observed: the best performance is achieved when the isotropy is moderate. This observation indicates that it is important to find an appropriate trade-off between learning intent detection skills and learning an insotropic feature space. In our method, we select the appropriate λ by validation. Correlation matrix is better than covariance matrix as regularizer. In the design of Cor-Reg (Section 4.2), we use the correlation matrix, rather than the covariance matrix, to characterize isotropy, although the latter contains more informationvariance. The reason is that it is difficult to determine the proper scale of the variances. Here, we conduct experiments using the covariance matrix, by pushing the non-diagonal elements (covariances) towards 0 and the diagonal elements (variances) towards 1, 0.5, or the mean value, which are denoted by Cov-Reg-1, Cov-Reg-0.5, and Cov-Reg-mean respectively in Table 8. It can be seen that all the variants perform worse than Cor-Reg.
Our method is complementary with batch normalization. Batch normalization (Ioffe and Szegedy, 2015)   anisotropy problem via normalizing each dimension with unit variance. We find that combining our method with batch normalization yields better performance, as shown in Table 9.  The performance gain is not from the reduction in model variance. Regularization techniques such as L1 regularization (Tibshirani, 1996) andL2 regularization (Hoerl andKennard, 1970) are often used to improve model performance by reducing model variance. Here, we show that the performance gain of our method is ascribed to the improved isotropy (Table 7) rather than the reduction in model variance. To this end, we compare our method against L2 regularization with a wide range of weights, and it is observed that reducing model variance cannot achieve comparable performance to our method, as shown in Fig. 6. The computational overhead is small. To analyze the computational overheads incurred by CL-Reg and Cor-Reg, we decompose the duration of one epoch of our method using the two regularizers jointly. As shown in Fig. 7, the overheads of CL-Reg and Cor-Reg are small, only taking up a small portion of the time.  
Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. This research was supported by the grants of HK ITF UIM/377 and PolyU DaSAIL project P0030935 funded by RGC.

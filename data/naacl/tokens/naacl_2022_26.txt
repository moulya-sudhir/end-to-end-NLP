TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation
Many scientific papers such as those in arXiv and PubMed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstractlike summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form, such as in scientific documents, such summary is not able to go beyond the general and coarse overview and provide salient information from the source document. The recent interest to tackle this problem motivated curation of scientific datasets, arXiv-Long and PubMed-Long, containing human-written summaries of 400-600 words, hence, providing a venue for research in generating long/extended summaries. Extended summaries facilitate a faster read while providing details beyond coarse information. In this paper, we propose TSTR, an extractive summarizer that utilizes the introductory information of documents as pointers to their salient information. The evaluations on two existing large-scale extended summarization datasets indicate statistically significant improvement in terms of ROUGE and average ROUGE (F1) scores (except in one case) as compared to strong baselines and state-of-theart. Comprehensive human evaluations favor our generated extended summaries in terms of cohesion and completeness.
Introduction Over the past few years, summarization task has witnessed a huge deal of progress in extractive (Nallapati et al., 2017;Liu and Lapata, 2019;Yuan et al., 2020;Cui et al., 2020;Jia et al., 2020;Feng et al., 2018) and abstractive (See et al., 2017;Gehrmann et al., 2018;Zhang et al., 2019;Tian et al., 2019;Zou et al., 2020) [Introductory] Neural machine translation (@xcite), directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance (@xcite […] Motivated by recent success in unsupervised cross-lingual embeddings (@xcite), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (@xcite) […] Although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure. […] For each language, the encoder and its corresponding decoder perform an AE, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations. Experimental results show that the proposed approach consistently achieves great success.
Related Work Summarizing scientific documents has gained a huge deal of attention from researchers, although it has been studied for decades. Neural efforts in scientific text have used specific characteristics of papers such as discourse structure Xiao and Carenini, 2019) and citation information (Qazvinian and Radev, 2008;Goharian, 2015, 2018) to aid summarization model. While prior work has mostly covered the generation of shorter-form summaries (approx. 200 terms), generating extended summaries of roughly 600 terms for long-form source documents such as scientific papers has been motivated very recently (Chandrasekaran et al., 2020). The proposed models for the extended summary generation task include jointly learning to predict sentence importance and sentence section to extract top sentences (Sotudeh et al., 2020); utilizing section-contribution computations to pick sentences from important section for forming the final summary (Ghosh Roy et al., 2020); identifying salient sections for generating abstractive summaries (Gidiotis et al., 2020); ensembling of extraction and abstraction models to form final summary (Ying et al., 2021); an extractive model with TextRank algorithm equipped with BM25 as similarity function (Kaushik et al., 2021); and incorporating sentences embeddings into graph-based extractive summarizer in an unsupervised manner (Ramirez-Orta and Milios, 2021). Unlike these works, we do not exploit any sectional nor citation information in this work. To the best of our knowledge, we are the first at proposing the novel method of utilizing introductory information of the scientific paper to guide the model to learn to generate summary from the salient and related information.
3 Background: Contextualized language models for summarization
Contextualized language models such as BERT (Devlin et al., 2019), and ROBERTA  have achieved state-of-the-art performance on a variety of downstream NLP tasks including text summarization. Liu and Lapata (2019) were the first to fine-tune a contextualized language model (i.e., BERT) for the summarization task. They proposed BERTSUM -a fine-tuning scheme for text summarization-that outputs the sentence representations of the source document (we use the term source and source document interchangeably, referring to the entire document). The BERT-SUMEXT model, which is built based on BERT-SUM, was proposed for the extractive summarization task. It utilizes the representations produced by BERTSUM, passes them through Transformers encoder (Vaswani et al., 2017), and finally uses a linear layer with Sigmoid function to compute copying probabilities for each input sentence. Formally, let l 1 , l 2 , ..., l n be the binary tags over the source sentences x = {sent 1 , sent 2 , ..., sent n } of a long document, in which n is the number of sentences in the paper. The BERTSUMEXT network runs over the source documents as follows (Eq. 1),
h b = BertSum(x) h = Encoder t (h b ) p = σ(W o h + b o ) (1)
where h b and h are the representations of source sentences encoded by BERTSUM and Trasformers encoder, respectively. W o and b o are trainable parameters, and p is the probability distribution over the source sentences, signifying extraction copy likelihood. The goal of this network is to train a network that can identify the positive sets of sentences as the summary. To prevent the network from selecting redundant sentences, BERTSUM uses Trigram Blocking (Liu and Lapata, 2019) for sentence selection in inference time. We refer the reader to the main paper for more details.
TSTR: Intro-guided Summarization In this section, we describe our methodology to tackle the extended summary generation task. Our approach exploits the introductory information 3 .
3 Introductory information is defined in Section 1  of the paper as pointers to salient sentences within it, as shown in Figure 2. It is ultimately expected that the extractive summarizer is guided to pick salient sentences across the entire paper.
The detailed illustration of our model is shown in Figure 3. To aid the extractive summarization model (i.e., right-hand box in Figure 3) which takes in source sentences of a scientific paper, we utilize an additional BERTSUM encoder called Introductory encoder (left-hand box in Fig. 3) that receives x intro = {sent 1 , sent 2 , ..., sent m }, with m being the number of sentences in introductory section. The aim of adding second encoder in this framework is to identify the clues in the introductory section which point to the salient supplementary sentences 4 . The BERTSUM network computes the extraction probabilities for introductory sentences as follow (same way as in Eq. 1),
h b = BertSum(x intro ) h = Encoder t (h b ) p = σ(W jh + b j ) (2)
in whichh b , andh are the introductory sentence representations by BERTSUM, Transformers encoder, respectively.p is the introductory sentence extraction probabilities. W j and b j are trainable matrices.
After identifying salient introductory sentences, the representations associated with them are retrieved using a pooling function and further used to guide the first task (i.e., right-hand side in Figure 3) as follows, where Select(•) is a function that takes in all introductory sentence representations (i.e.,h), and introductory sentence probabilitiesp. It then outputs the representations associated with top k introductory sentences, sorted byp. To extract top introductory sentences, we first sorth vectors based on their computed probabilitiesp and then we pick up top k hidden vectors (i.e.,h top ) that has the highest probability. MLP 1 is a multi-layer perceptron that takes in concatenated vector of top introductory sentences and projects it into a new vector calledĥ.
h top = Select(h,p, k) h = MLP 1 (h top )(3)
At the final stage, we concatenate the transformed introductory top sentence representations (i.e.,ĥ) with each source sentence representations from Eq. 1 (i.e., h i where i shows the ith paper sentence) and process them to produce a resulting vector r which is intro-aware source sentence hidden representations. After processing the resulting vector through a linear output layer (with W z and b z as trainable parameters), we obtain final introaware sentence extraction probabilities (i.e., p) as follows,
r = MLP 2 (h i ;ĥ) p = σ(W z r + b z ) (4)
in which MLP 2 is a multi-layer perceptron, influencing the knowledge from introductory sentence extraction task (i.e., t 2 ) into the source sentence extraction task (i.e., t 1 ). We train both tasks through our end-to-end system jointly as follows,
ℓ total = (α)ℓ t 1 + (1 − α)ℓ t 2(5)
where ℓ t 1 , and ℓ t 2 are the losses computed for introductory sentence extraction and source sentence extraction tasks, α is the regularizing parameter that balances the learning process between two tasks, and ℓ total is the total computed loss that is optimized during the training.
Experimental Setup In this section, we explain the datasets, baselines, and preprocessing and training parameters.
Dataset We use two publicly available scientific extended summarization datasets (Sotudeh et al., 2021).
-arXiv-Long:
A set of arXiv scientific papers containing papers from various scientific domains such as physics, mathematics, computer science, quantitative biology. arXiv-Long is intended for extended summarization task and was filtered from a larger dataset i.e., arXiv  for the summaries of more than 350 tokens. The ground-truth summaries (i.e., abstract) are long, with the average length of 574 tokens. It contains 7816 (train), 1381 (validation), and 1952 (test) papers.
-PubMed-Long: A set of biomedical scientific papers from PubMed with average summary length of 403 tokens. This dataset contains 79893 (train), 4406 (validation), and 4402 (test) scientific papers. 
Baselines We compare our model with two strong non-neural systems, and four state-of-the-art neural summarizers. We use all of these baselines for the purpose of extended summary generation whose documents hold different characteristics in length, writing style, and discourse structure as compared to documents in the other domains of summarization.
-LSA (Steinberger and Jezek, 2004): an extractive vector-based model that utilizes Singular Value Decomposition (SVD) to find the semantically important sentences.
-LEXRANK (Erkan and Radev, 2004): a widely adopted extractive summarization baseline that utilizes a graph-based approach based on eigenvector centrality to identify the most salient sentences.
-BERTSUMEXT (Liu and Lapata, 2019): a contextualized summarizer fine-tuned for summarization task, which encodes input sentence representations, and then processes them through a multi-layer Transformers encoder to obtain document-level sentence representation. Finally, a linear output layer with Sigmoid activation function outputs a probability distribution over each input sentence, denoting the extent to which they are probable to be extracted.
-BERTSUMEXT-INTRO (Liu and Lapata, 2019): a BERTSUMEXT model that only runs on the introductory sentences as the input, and extracts the salient introductory sentences as the summary.
-BERTSUMEXTMULTI (Sotudeh et al., 2021): an extension of the BERTSUMEXT model that incorporates an additional linear layer with Sigmoid classifier to output a probability distribution over a fixed number of pre-defined sections that an input sentence might belong to. The additional network is expected to predict a single section for an input sentence and is trained jointly with BERTSUMEXT module (i.e., sentence extractor).
-BART (Lewis et al., 2020): a state-of-the-art abstractive summarization model that makes use of pretrained encoder and decoder. BART can be thought of as an extension of BERTSUM in which merely encoder is pre-trained, but decoder is trained from scratch. While our model is an extractive one, at the same time, we find it of value to measure the abstractive model performance in the extended summary generation task.
Preprocessing, parameters, labeling, and implementation details We used the open implementation of BERT-SUMEXT with default parameters 5 . To implement the non-neural baseline models, we utilized Sumy python package 6 . Longformer model (Beltagy et al., 2020) is utilized as our contextualized language model for running all the models due to its efficacy at processing long documents. For our model, the cross-entropy loss function is set for two tasks (i.e., t 1 : source sentence extraction and t 2 : introductory sentences extraction in Figure 3) and the model is optimized through multi-tasking approach as discussed in Section 3. The model with the highest ROUGE-2 on validation set is selected for inference. The validation is performed every 2k training steps. α (in Eq. 5) is set to be 0.5 (empirically determined). Our model includes 474M trainable parameters, trained on dual GeForce GTX 1080Ti GPUs for approximately a week. We use k = 5 for arXiv-Long, k = 8 for PubMed-Long datasets (Eq. 3). We make our model implementation as well as sample summaries publicly available to expedite ongoing research in this direction 7 . A two-stage labeling approach was employed to identify ground-truth introductory and nonintroductory sentences. In the first stage, we used a greedy labeling approach (Liu and Lapata, 2019) to label sentences within the first section of a given paper (i.e., labeling introductory sentences) with respect to their ROUGE overlap 8 with the groundtruth summary (i.e., abstract). In the second stage, the same greedy approach was exploited over the rest of sentences (i.e., non-introductory) 9 with regard to their ROUGE overlap with the identified introductory sentences in the first stage. Our choice of ROUGE-2 and ROUGE-L is based on the fact that these express higher similarity with human judgments (Cohan and Goharian, 2016). We continued the second stage until a fixed length of the summary was reached. Specifically, the fixed length of positive labels is set to be 15 for arXiv-Long, and 20 for PubMed-Long datasets as these achieved the highest oracle ROUGE scores in our experiments.
Results 
Experimental evaluation The recent effort in extended summarization and its shared task of LongSumm (Chandrasekaran et al., 2020) used average ROUGE (F1) to rank the participating systems, in addition to commonly-used ROUGE-N scores. Table 2 shows the performance of the participated systems on the blind test set. As shown, BERTSUMEXTMULTI model outperforms other models by a large margin (i.e., with relative improvements of 6% and 3% on ROUGE-1 and average ROUGE(F1), respectively); hence, we use the best-performing in terms of F1 (i.e., BERTSUMEXTMULTI model) in our experiments. Tables. 1 presents our results on the test sets of arXiv-Long and PubMed-Long datasets, respectively. As observed, our model statistically significantly outperforms the state-of-the-art systems on both datasets across most of the ROUGE vari-ants, except ROUGE-L on PubMed-Long. The improvements gained by our model validates our hypothesis that incorporating the salient introductory sentence representations into the extractive summarizer yields a promising improvement. Two nonneural models (i.e., LSA and LEXRANK) underperform the neural models, as expected. Comparing the abstractive model (i.e., BART) with extractive neural ones (i.e., BERTSUMEXT and BERT-SUMEXTMULTI), we see that while there is relatively a smaller gap in terms of ROUGE-1, the gap is larger for ROUGE-2, and ROUGE-L. Interestingly, in the case of BART, we found that generating extended summaries is rather challenging for abstractive summarizers. Current abstractive summarizers including BART have difficulty in abstracting very detailed information, such as numbers, and quantities, which hurts the faithfulness of the generated summaries to the source. This behavior has a detrimental effect, specifically, on ROUGE-2 and ROUGE-L as their high correlation with human judgments in terms of faithfulness has been shown (Pagnoni et al., 2021). Comparing the extractive BERTSUMEXT and BERTSUMEXT-MULTI models, while BERTSUMMULTIEXT is expected to outperfom BERTSUMEXT, it is observed that they perform almost similarly, with small (i.e., insignificant) improved metrics. This might be due to the fact that BERTSUMEXTMULTI works out-of-the-box when a handful amount of sentences are sampled from diverse sections to form the oracle summary as also reported by its authors. However, when labeling oracle sentences in our framework (i.e., Intro-guided labeling), there is no guarantee that the final set of oracle sentences are labeled from diverse sections. Overall, our model achieves about 1.4%, 2.4%, 3.5% (arXiv-Long), and 1.0%, 2.5%, 1.3% (PubMed-Long) improvements across ROUGE score variants; and 2.2% (arXiv-Long), 1.4% (PubMed-Long) improvements over F1, compared to the neural baselines (i.e., BERTSUMEXT and BERT-SUMEXTMULTI). While comparing our model with BERTSUMEXT-INTRO, we see the vital effect of adding second encoder at finding supplementary sentences across non-introductory sections, where our model gains relative improvements of 9.62%-26.26%-16.09% and 9.40%-5.27%-9.99% for ROUGE-1, ROUGE-2, ROUGE-L on arXiv-Long and PubMed-Long, respectively. In fact, the sentences that are picked as summary from the in-   troduction section are not comprehensive as such they are clues to the main points of the paper. The other important sentences are picked from the supplementary parts (i.e., non-introductory) of the paper.
Human evaluation While our model statistically significantly improves upon the state-of-the-art baselines in terms of ROUGE scores, a few works have reported the low correlation of ROUGE with human judgments (Liu and Liu, 2008;Cohan and Goharian, 2016;Fabbri et al., 2021). In order to provide insights into why and how our model outperforms the bestperforming baselines, we perform a manual analysis of our system's generated summaries, BERT-SUMEXT's, and BERTSUMEXTMULTI's. For the sake of evaluation, two annotators were asked to manually evaluate two sets of 40 papers' groundtruth abstracts (40 for arXiv-Long, and 40 for PubMed-Long) with their generated extended summaries (baselines' and ours) to gain insights into qualities of each model. Annotators were Electrical Engineering and Computer Science PhD students and familiar with principles of reading scientific papers. Samples were randomly selected from the test set, one from each 40 evenly-spaced bins sorted by the difference of ROUGE-L between two experimented systems.
The evaluations were performed according to two metrics: (1) Cohesion: whether the ordering of sentences in summary is cohesive, namely sentences entail each other. (2) Completeness: whether the summary covers all salient information provided in the ground-truth summary. To prevent bias in selecting summaries, the ordering of systemgenerated summaries were shuffled such that it could not be guessed by the annotators. Annotators were asked to specify if the first system-generated summary wins/loses or ties with the second systemgenerated summary in terms of qualitative metrics. It has to be mentioned that since our model is purely extractive, it does not introduce any fact that is unfaithful to the source.
Our human evaluation results along with Cohen's kappa (Cohen, 1960) inter-rater agreements are shown in Table 3 (agr. column). As shown, our system's generated summaries improve completeness and cohesion in over 40% for most of the cases (6 out of 8 for win cases 10 ). Specifically, when comparing with BERTSUMEXT, we see that 68%, 80% (arXiv-Long); and 60%, 66% (PubMed-Long) of sampled summaries are at least as good as or better than the corresponding baseline's generated summaries in terms of cohesion and completeness, respectively. Overall, across two metrics for BERTSUMEXT and BERTSUMEXT-MULTI, we gain relative improvements over the baselines: 25.6%, 19.0% (cohesion), and 56.5%,  [Introductory] The objective of the work presented here is to study the mechanism of radiative line driving and the corresponding properties of the winds of possible generations of very massive stars at extremely low metallicities and to investigate the principal influence of these winds on ionizing fluxes and observable ultraviolet spectra.
[" # ] The basic new element of this approach, needed in the domain of The purpose of this first study is to provide an estimate about the strengths of stellar winds at very low metallicity for very massive hot stars in a mass range roughly between 100 to 300 m@xmath3.
[" ) ] With our new approach to describe line driven stellar winds at extremely low metallicity we were able to make first predictions of stellar wind properties, ionizing fluxes and synthetic spectra of a possible population of very massive stars in this range of metallicity.  46.7% (completeness) on arXiv-Long; and 23.1%, 13.5% (cohesion), and 27.7%, 21.9% (completeness) on PubMed-Long. 11 These improvements, qualitatively evaluated by the human annotators, show the promising capability of our purposed model in generating improved extended summaries which are more preferable than the baselines'. We observe a similar improvement trend when comparing our summaries with BERTSUMEXTMULTI, where 66%, 77% (arXiv-Long); and 58%, 58% (PubMed-Long) of our summaries are as good as or better than the baseline's in terms of cohesion and completeness. Looking at the Cohen's inter-rater agreement, the correlation scores fall into "moder- 11 Relative improvement of win rate over lose rate.
ate" agreement range according to the interpretation of Cohen's kappa range (McHugh, 2012).

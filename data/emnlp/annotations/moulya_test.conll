Zero-Shot B-TaskName
Text I-TaskName
Classification I-TaskName
with O
Self-Training O
Recent O
advances O
in O
large O
pretrained O
language O
models O
have O
increased O
attention O
to O
zero-shot B-TaskName
text I-TaskName
classification. I-TaskName
In O
particular O
, O
models O
finetuned O
on O
natural O
language O
inference O
datasets O
have O
been O
widely O
adopted O
as O
zero-shot O
classifiers O
due O
to O
their O
promising O
results O
and O
offthe-shelf O
availability. O
However O
, O
the O
fact O
that O

such O
models O
are O
unfamiliar O
with O
the O
target O
task O
can O
lead O
to O
instability O
and O
performance O
issues. O
We O
propose O
a O
plug-and-play O
method O
to O
bridge O
this O
gap O
using O
a O
simple O
self-training O
approach O
, O
requiring O
only O
the O
class O
names O
along O
with O
an O
unlabeled O
dataset O
, O
and O
without O
the O
need O
for O
domain O
expertise O
or O
trial O

and O
error. O
We O
show O
that O
fine-tuning O
the O
zero-shot O
classifier O
on O
its O
most O
confident O
predictions O
leads O
to O
significant O
performance O
gains O
across O
a O
wide O
range O
of O
text O
classification O
tasks O
, O
presumably O
since O
self-training O
adapts O
the O
zero-shot O
model O
to O
the O
task O
at O
hand O
. O
Introduction O
Large O
language O
models O
have O
revolutionized O
the O
field O
of O

natural O
language O
processing O
, O
leading O
to O
great O
leaps O
in O
performance O
across O
the O
NLP O
task O
landscape O
( O
Devlin O
et O
al. O
, O
2018 O
; O
Raffel O
et O
al. O
, O
2020 O
; O
Brown O
et O
al. O
, O
2020 O
) O
. O
The O
pretrain-finetune O
paradigm O
has O
led O
to O
a O
significant O
reduction O
in O
the O
amount O
of O
labeled O
data O

required O
for O
obtaining O
high O
performance O
on O
downstream O
tasks. O
However O
, O
the O
need O
to O
collect O
labeled O
examples O
for O
each O
target O
task O
remains O
an O
obstacle O
, O
limiting O
the O
usage O
of O
language O
models O
in O
practice O
, O
at O
scale. O
Thus O
, O
the O
more O
ambitious O
vision O
of O
a O
generalpurpose O
zero-shot O
model O
-one O
that O
can O
tackle O

many O
different O
tasks O
without O
requiring O
labeled O
data O
-has O
become O
an O
enticing O
goal O
for O
the O
community. O
This O
notion O
is O
increasingly O
gaining O
attention O
, O
with O
recent O
works O
suggesting O
new O
paradigms O
that O
aim O
to O
utilize O
the O
language O
understanding O
capabilities O
of O
large O
models O
for O
the O
zero-shot O
scenario. O
* O
These O
authors O
contributed O
equally O
to O
this O

work O
. O
In O
their O
pioneering O
work O
on O
more O
generalpurpose O
zero-shot O
models O
, O
Yin O
et O
al. O
( O
2019 O
) O
propose O
to O
formulate O
text O
classification O
tasks O
as O
a O
textual O
entailment O
problem O
( O
Dagan O
et O
al. O
, O
2005 O
) O
. O
This O
mapping O
enables O
using O
a O
model O
trained O
on O
natural O
language O
inference O
( O
NLI O

) O
as O
a O
zero-shot O
text O
classifier O
for O
a O
wide O
variety O
of O
unseen O
downstream O
tasks. O
The O
underlying O
idea O
is O
fairly O
intuitive. O
To O
determine O
if O
a O
particular O
text O
should O
be O
assigned O
to O
, O
e.g. O
, O
the O
" O
sports O
" O
class O
or O
the O
" O
politics O
" O
class O
, O
one O
constructs O
sentences O
such O
as O

" O
This O
text O
is O
about O
sports O
" O
and O
" O
This O
text O
is O
about O
politics O
" O
, O
respectively O
; O
the O
model O
prediction O
as O
to O
which O
one O
is O
most O
entailed O
by O
the O
original O
text O
can O
then O
be O
used O
to O
determine O
the O
predicted O
class O
label. O
Similarly O
, O
some O
recent O
works O
have O
tried O
to O

map O
even O
more O
varied O
types O
of O
NLP O
tasks O
into O
a O
unified O
cross-task O
format O
( O
Wei O
et O
al. O
, O
2022 O
; O
Zhong O
et O
al. O
, O
2021 O
; O
Bragg O
et O
al. O
, O
2021 O
; O
Sanh O
et O
al. O
, O
2022 O
) O
. O
Such O
unified O
task O
formats O
enable O
" O
meta-tuning O
" O
a O
model O
using O

existing O
labeled O
data O
from O
different O
tasks. O
By O
teaching O
the O
model O
to O
solve O
the O
broader O
" O
meta-task O
" O
, O
it O
is O
then O
able O
to O
cope O
with O
a O
wide O
variety O
of O
unseen O
tasks O
at O
inference O
time O
. O
While O
zero-shot O
models O
hold O
great O
promise O
by O
eliminating O
the O
burden O
of O
collecting O
task-specific O
labeled O
data O

, O
they O
often O
still O
come O
at O
the O
cost O
of O
providing O
mediocre O
performance O
compared O
to O
models O
trained O
in O
the O
conventional O
supervised O
learning O
paradigm. O
Thus O
, O
improving O
the O
prediction O
performance O
of O
zero-shot O
models O
is O
of O
great O
practical O
importance. O
One O
of O
the O
simplest O
and O
most O
effective O
approaches O
for O
improving O
performance O
of O
classifiers O
is O

self-training O
( O
Scudder O
, O
1965 O
) O
. O
In O
this O
paradigm O
, O
a O
model O
's O
own O
predictions O
on O
unlabelled O
data O
are O
leveraged O
for O
creating O
pseudo-labels O
, O
which O
are O
then O
used O
for O
further O
training O
the O
model O
. O
In O
the O
original O
setting O
of O
self-training O
, O
some O
labeled O
data O
is O
available O
for O
training O
an O

initial O
classifier O
, O
and O
the O
predictions O
of O
the O
classifier O
on O
unlabeled O
data O
are O
used O
for O
data O
augmentation O
( O
Van O
En- O
gelen O
and O
Hoos O
, O
2020 O
) O
. O
More O
recently O
, O
the O
use O
of O
self-training O
has O
been O
extended O
to O
the O
scenario O
of O
unsupervised O
domain O
adaptation O
, O
where O
labeled O
data O
is O
available O

only O
for O
a O
source O
domain O
, O
and O
only O
unlabeled O
data O
is O
available O
for O
the O
target O
domain O
( O
e.g. O
, O
Du O
et O
al. O
, O
2021 O
; O
Zou O
et O
al. O
, O
2019 O
) O
. O
Here O
, O
we O
aim O
to O
study O
self-training O
as O
a O
method O
for O
improving O
general-purpose O
zero-shot O
models O
, O
by O
adapting O

them O
to O
the O
task O
at O
hand. O
Given O
the O
distinct O
properties O
of O
such O
models O
, O
applying O
selftraining O
in O
this O
scenario O
is O
not O
trivial O
and O
poses O
unique O
challenges. O
Our O
approach O
can O
be O
viewed O
as O
a O
further O
extension O
of O
self-training O
-from O
unsupervised O
domain-adaptation O
to O
unsupervised O
taskadaptation O
, O
where O
only O
unlabeled O
data O
is O
available O

for O
the O
target O
task O
. O
As O
prominent O
representatives O
of O
general-purpose O
zero-shot O
models O
, O
in O
this O
work O
we O
focus O
on O
NLIbased O
models O
( O
Yin O
et O
al. O
, O
2019 O
) O
, O
which O
are O
increasingly O
being O
utilized O
for O
zero-shot B-TaskName
classification I-TaskName
( O
Davison O
, O
2020 O
; O
Sainz O
and O
Rigau O
, O
2021 O
; O
Basile O
et O

al. O
, O
2021 O
) O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
work O
that O
explores O
self-training O
in O
the O
context O
of O
general-purpose O
zero-shot O
models. O
We O
release O
our O
code O
1 O
, O
including O
access O
to O
all O
datasets O
, O
and O
an O
associated O
automatic O
evaluation O
framework O
, O
aiming O
to O
facilitate O
further O

research O
along O
the O
lines O
explored O
here. O
. O
This O
procedure O
can O
be O
repeated O
to O
obtain O
M O
′′ O
, O
and O
so O
on O
, O
in O
an O
iterative O
manner O
. O
Next O
, O
we O
describe O
the O
motivation O
of O
applying O
self-training O
to O
zero-shot O
text O
classifiers O
, O
and O
the O
details O
of O
our O
approach O
. O
Motivation O
We O
hypothesize O

that O
self-training O
brings O
forth O
unique O
benefits O
to O
general-purpose O
zero-shot O
models O
, O
going O
beyond O
data O
augmentation O
and O
the O
exposure O
to O
the O
target O
domain O
. O
A O
zero-shot O
model O
, O
as O
implied O
by O
its O
name O
, O
has O
never O
been O
directly O
exposed O
to O
the O
task O
it O
should O
perform. O
Moreover O
, O
one O
should O
expect O
significant O

differences O
between O
the O
characteristics O
and O
distributions O
of O
the O
task O
( O
s O
) O
used O
to O
create O
the O
generalpurpose O
model O
, O
and O
those O
of O
the O
downstream O
task. O
Self-training O
may O
help O
bridge O
this O
gap O
, O
by O
adapting O
the O
model O
to O
the O
properties O
of O
the O
target O
task O
. O
Specifically O
, O
for O
NLI-based O
classification O
models O

( O
Yin O
et O
al. O
, O
2019 O
) O
, O
which O
are O
at O
the O
focus O
of O
this O
work O
, O
self-training O
may O
provide O
two O
important O
benefits O
, O
discussed O
next O
. O
Exposure O
to O
class O
names. O
As O
a O
language O
model O
, O
the O
zero-shot O
model O
presumably O
embodies O
some O
knowledge O
about O
the O
meaning O
of O
the O
target O
class O

names O
, O
considering O
each O
class O
name O
independently O
; O
however O
, O
chances O
are O
it O
has O
never O
been O
trained O
to O
consider O
their O
potential O
interactions. O
Pseudolabeled O
examples O
, O
obtained O
via O
self-training O
, O
can O
force O
the O
zero-shot O
model O
to O
contrast O
the O
class O
names O
with O
one O
another O
, O
and O
to O
learn O
more O
subtle O
distinctions O
that O

will O
be O
required O
at O
test O
time. O
As O
a O
simple O
example O
, O
'guilt O
' O
and O
'shame O
' O
may O
often O
be O
considered O
synonyms O
, O
but O
represent O
two O
distinct O
classes O
in O
one O
of O
our O
datasets. O
Explicit O
exposure O
to O
even O
weakly O
labeled O
data O
is O
presumably O
essential O
to O
learn O
such O
distinctions O
. O
Exposure O
to O
the O

task O
and O
template O
/ O
prompt. O
Entailment-based O
models O
are O
originally O
trained O
on O
general O
NLI O
datasets O
, O
which O
aim O
to O
capture O
a O
broad O
and O
diverse O
range O
of O
textual O
entailment O
instances. O
Utilizing O
these O
models O
for O
text O
classification O
implies O
that O
they O
should O
focus O
on O
a O
narrower O
set O
of O
entailment O
types O
, O
namely O
those O
that O

map O
to O
the O
text O
classification O
problem O
under O
consideration. O
Moreover O
, O
the O
application O
of O
these O
models O
as O
zero-shot O
classifiers O
involves O
the O
use O
of O
generic O
hypothesis O
templates O
that O
aim O
to O
formulate O
the O
downstream O
classification O
task O
in O
terms O
of O
textual O
entailmente.g. O
, O
" O
This O
text O
is O
about O
X O
" O
. O
Both O
the O
relevant O

entailment O
sub-types O
, O
and O
the O
generic O
templates O
used O
at O
test O
time O
, O
are O
presumably O
not O
common O
in O
the O
data O
used O
to O
train O
the O
model. O
Thus O
, O
self-training O
exposes O
the O
model O
to O
the O
specific O
hypothesis O
template O
that O
will O
be O
used O
for O
text O
classification O
, O
as O
well O
as O
to O
the O
underlying O
distribution O

of O
text O
classification O
entailment O
problems O
it O
will O
need O
to O
face O
. O
Our O
approach O
We O
consider O
an O
entailment-based O
zero-shot O
model O
, O
M O
, O
for O
a O
multi-class O
classification O
task O
, O
with O
a O
set O
of O
target O
class O
names O
C. O
Yin O
et O
al. O
( O
2019 O
) O
proposed O
to O
map O
the O
text O
classification O
task O
into O

an O
entailment O
task O
, O
as O
depicted O
in O
Fig. O
1. O
Specifically O
, O
a O
target O
text O
, O
t O
, O
is O
taken O
as O
the O
premise. O
For O
every O
class O
c O
∈ O
C O
, O
a O
hypothesis O
is O
constructed O
from O
a O
template O
such O
as O
" O
This O
example O
is O
c O
" O
( O
e.g. O
, O
" O
This O
example O

is O
joy O
" O
, O
or O
" O
This O
example O
is O
anger O
" O
) O
. O
The O
entailment O
model O
is O
presented O
with O
t O
and O
a O
set O
of O
hypotheses O
that O
correspond O
to O
the O
different O
classes. O
The O
class O
whose O
hypothesis O
receives O
the O
top O
entailment O
score O
is O
predicted O
as O
the O
label O
for O
t O
( O
see O
Fig. O

1 O
) O
. O
We O
further O
assume O
a O
collection O
of O
unlabeled O
examples O
U O
is O
available. O
Following O
the O
entailment O
approach O
, O
we O
generate O
pseudo-labeled O
examples O
from O
U O
based O
on O
the O
predictions O
given O
by O
M O
. O
First O
, O
for O
each O
u O
∈ O
U O
and O
each O
class O
name O
c O
∈ O
C O
we O
obtain O
S O

uc O
, O
the O
confidence O
score O
for O
u O
entailing O
the O
hypothesis O
constructed O
for O
c O
( O
entailment O
score O
in O
Fig. O
1 O
) O
. O
In O
other O
words O
, O
S O
uc O
represents O
the O
confidence O
of O
assigning O
u O
to O
c O
. O
Selecting O
positive O
examples O
Our O
goal O
is O
to O
collect O
for O
each O
class O
c O
, O
a O

set O
of O
n O
pseudo-labeled O
positive O
examples O
in O
U O
. O
As O
common O
in O
self-training O
, O
we O
aim O
to O
focus O
on O
the O
most O
confident O
predictions. O
We O
follow O
a O
" O
Best-versus-Second-Best O
" O
approach O
, O
as O
in O
Slonim O
et O
al. O
( O
2011 O
) O
. O
To O
that O
end O
, O
we O
first O
consider O
all O
examples O
in O

U O
for O
which O
c O
obtained O
the O
top O
entailment O
score O
, O
i.e. O
, O
S O
uc O
> O
S O
uc O
′ O
, O
∀c O
′ O
̸ O
= O
c. O
Next O
, O
we O
focus O
our O
attention O
on O
examples O
that O
maximize O
the O
delta O
between O
the O
top O
ranked O
class O
and O
the O
second O
highest O
class O
( O
in O
Fig. O
1 O

, O
the O
delta O
is O
between O
the O
entailment O
score O
for O
joy O
and O
guilt O
) O
. O
Loosely O
speaking O
, O
such O
examples O
correspond O
to O
points O
farthest O
from O
the O
decision O
boundary O
, O
and O
thus O
the O
points O
that O
the O
model O
is O
most O
certain O
about. O
Assuming O
c O
and O
c O
′ O
are O
the O
top-ranked O
and O
second-ranked O
classes O

for O
u O
, O
respectively O
, O
let O
δ O
uc O
= O
S O
uc O
− O
S O
uc O
′ O
. O
Next O
, O
for O
a O
given O
class O
c O
, O
we O
sort O
all O
examples O
in O
U O
for O
which O
c O
was O
the O
top-ranked O
class O
by O
δ O
uc O
in O
a O
decreasing O
order O
, O
and O
select O
the O
top O
n O

examples O
as O
the O
positive O
examples O
for O
class O
c O
. O
In O
order O
to O
utilize O
these O
pseudo-labeled O
examples O
as O
training O
examples O
for O
the O
entailment O
model O
M O
, O
we O
use O
a O
similar O
transformation O
to O
the O
one O
described O
above O
-the O
example O
is O
taken O
as O
the O
premise O
, O
the O
class O
name O
is O
incorporated O
into O
the O

hypothesis O
template O
, O
and O
the O
premise-hypothesis O
pair O
is O
assigned O
the O
entail O
pseudo-label O
. O
Selecting O
negative O
examples O
To O
train O
the O
entailment O
model O
to O
contrast O
between O
classes O
we O
need O
to O
generate O
negative O
entailment O
examples O
, O
with O
the O
contradict O
pseudo-label. O
For O
that O
, O
we O
examine O
four O
approaches O
: O
Contrast-random O
For O
each O
entail O
pair O

for O
a O
hypothesis O
based O
on O
c O
, O
add O
a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
at O
random O
with O
another O
class O
. O
Contrast-closest O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O

a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
the O
class O
receiving O
the O
second-highest O
entailment O
score O
for O
this O
premise O
( O
guilt O
in O
the O
example O
of O
Fig. O
1 O
) O
. O
Contrast-furthest O
For O
each O
entail O
pair O
for O

a O
hypothesis O
based O
on O
c O
, O
add O
a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
the O
class O
receiving O
the O
lowest O
entailment O
score O
for O
this O
premise O
( O
anger O
in O
the O
example O
of O
Fig. O
1 O
) O

. O
Contrast-all O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O
|C| O
− O
1 O
pairs O
with O
the O
contradict O
label O
, O
all O
with O
same O
premise O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
each O
of O
the O
other O
target O
class O
c O
′ O
̸ O
= O
c. O
Note O
that O
for O

datasets O
with O
a O
large O
number O
of O
classes O
, O
this O
setting O
significantly O
increases O
the O
size O
of O
the O
training O
set O
, O
and O
correspondingly O
the O
run O
time O
. O
The O
full O
training O
data O
, O
including O
both O
entail O
and O
contradict O
pseudo-labeled O
examples O
, O
is O
used O
to O
fine-tune O
the O
general O
entailment O
model O
M O
, O
yielding O
an O

entailment O
zero-shot O
model O
M O
′ O
that O
has O
been O
adapted O
to O
the O
target O
task. O
We O
continue O
this O
procedure O
in O
iterations O
: O
we O
generate O
a O
new O
pseudolabeled O
dataset O
based O
on O
the O
predictions O
of O
M O
′ O
, O
which O
is O
then O
fine-tuned O
to O
generate O
M O
′′ O
, O
and O
so O
forth O
. O
Balancing O
noise O
and O

informativeness O
with O
token O
masking O
Self-training O
relies O
on O
a O
delicate O
balance. O
On O
the O
one O
hand O
, O
the O
pseudo-labels O
are O
noisy. O
Training O
on O
noisy O
data O
may O
lead O
to O
overconfidence O
and O
propagation O
of O
errors O
( O
Zou O
et O
al. O
, O
2019 O
) O
. O
Therefore O
, O
a O
standard O
self-training O
practice O
is O
to O
take O
the O
most O

confident O
predictions O
, O
which O
are O
presumed O
to O
be O
less O
noisy. O
On O
the O
other O
hand O
, O
the O
most O
confident O
examples O
are O
more O
likely O
to O
be O
the O
easy O
and O
less O
informative O
ones O
, O
and O
thus O
less O
useful O
for O
training O
( O
Hajmohammadi O
et O
al. O
, O
2015 O
; O
Mukherjee O
and O
Awadallah O
, O
2020 O
) O

. O
With O
zero-shot O
models O
, O
this O
trade-off O
becomes O
even O
more O
pronounced. O
As O
these O
models O
were O
not O
trained O
on O
the O
target O
task O
, O
the O
pseudo-labels O
that O
are O
obtained O
from O
their O
predictions O
are O
likely O
to O
be O
noisier O
than O
those O
obtained O
from O
a O
model O
trained O
on O
some O
labeled O
data. O
Thus O
, O
with O
zero-shot O

models O
we O
are O
compelled O
to O
raise O
the O
confidence O
bar O
in O
order O
to O
obtain O
pseudo-labels O
of O
reasonable O
quality O
, O
which O
in O
turn O
may O
focus O
the O
training O
on O
the O
easy O
and O
thus O
less O
informative O
examples O
. O
To O
increase O
the O
informativeness O
of O
the O
selected O
examples O
, O
we O
apply O
the O
following O
heuristic O
: O
in O

each O
example O
we O
identify O
the O
token O
which O
is O
the O
most O
similar O
to O
the O
positive O
class O
name O
assigned O
to O
this O
example O
, O
and O
mask O
it. O
In O
the O
example O
of O
Fig. O
1 O
, O
the O
word O
thrilled O
will O
be O
masked O
when O
this O
example O
is O
used O
as O
a O
positive O
or O
as O
a O
negative O
example O

for O
the O
class O
" O
joy O
" O
. O
By O
masking O
those O
most O
similar O
tokens O
, O
the O
selected O
examples O
become O
more O
challenging O
, O
and O
the O
model O
is O
forced O
to O
rely O
on O
other O
signalse.g. O
, O
in O
Fig. O
1 O
, O
on O
the O
understanding O
that O
the O
event O
of O
a O
paper O
getting O
accepted O
to O
a O
conference O

is O
a O
joyful O
one O
. O
Datasets O
and O
Tasks O
We O
experiment O
with O
8 O
datasets O
representing O
a O
variety O
of O
text O
classification O
tasks O
: O
20 B-DatasetName
newsgroup I-DatasetName
( O
Lang O
, O
1995 O
) O
, O
AG B-DatasetName
's I-DatasetName
news I-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
) O
, O
Amazon B-DatasetName
reviews I-DatasetName
( O
McAuley O
and O
Leskovec O
, O
2013 O
) O
, O

DBPedia B-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
) O
, O
GoEmotions B-DatasetName
( O
Demszky O
et O
al. O
, O
2020 O
) O
, O
IMDB B-DatasetName
reviews I-DatasetName
( O
Maas O
et O
al. O
, O
2011 O
) O
, O
ISEAR B-DatasetName
( O
Shao O
et O
al. O
, O
2015 O
) O
, O
and O
Yahoo B-DatasetName
! I-DatasetName
Answers I-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
names O
were O
used O

to O
describe O
the O
target O
labels O
for O
zero-shot O
inference O
2 O
. O
We O
report O
results O
on O
the O
test O
set O
of O
each O
dataset O
( O
the O
labels O
of O
the O
train O
sets O
were O
not O
used O
as O
there O
is O
no O
training O
in O
our O
method O
) O
; O
preliminary O
experiments O
were O
conducted O
on O
separate O
development O
sets. O
Since O
we O

aim O
for O
a O
practical O
setting O
with O
lower O
computational O
costs O
, O
we O
limit O
the O
size O
of O
our O
unlabeled O
set O
U O
to O
a O
maximum O
of O
10K O
examples O
sampled O
from O
the O
full O
training O
set O
of O
each O
dataset. O
For O
details O
on O
the O
dataset O
sizes O
, O
task O
types O
, O
and O
label O
names O
, O
see O
App. O

A O
. O
Zero-Shot O
Models O
We O
evaluate O
3 O
off-the-shelf O
entailment O
models O
, O
trained O
on O
the O
MNLI B-DatasetName
( O
Williams O
et O
al. O
, O
2018 O
) O
dataset O
: O
roberta-large-mnli B-MethodName
, O
deberta-large-mnlizero-cls B-MethodName
, O
and O
bart-large-mnli. B-MethodName
To O
infer O
zero-shot O
predictions O
from O
these O
models O
with O
respect O
to O
the O
target O
labels O
we O
rely O
on O
the O
dedicated O
zero-shot B-TaskName
classification I-TaskName

pipeline I-TaskName
from O
the O
Hugging O
Face O
Transformers O
library O
3 O
, O
using O
the O
default O
hypothesis O
template O
" O
This O
example O
is O
[ O
] O
. O
" O
. O
Implementation O
Details O
Each O
experiment O
is O
repeated O
5 O
times O
, O
with O
each O
repetition O
using O
a O
different O
random O
seed. O
All O
models O
are O
fine-tuned O
for O
one B-HyperparameterValue
epoch B-HyperparameterName
with O
a O
learning B-HyperparameterName

rate I-HyperparameterName
of O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
using O
the O
AdamW O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
and O
cross O
entropy O
loss O
to O
optimize O
the O
models. O
A O
single O
NVIDIA O
A100 O
GPU O
was O
used O
for O
fine-tuning O
and O
inference. O
We O
base O
our O
implementation O
on O
Hugging O
Face O

Transformers O
( O
Wolf O
et O
al. O
, O
2019 O
) O
mask O
a O
token O
in O
the O
example O
texts O
based O
on O
similarity O
to O
the O
predicted O
class O
names. O
For O
each O
example O
we O
extract O
the O
GloVe O
( O
Pennington O
et O
al. O
, O
2014 O
) O
representations O
for O
each O
token O
in O
the O
example O
text O
, O
and O
for O
the O
predicted O

class O
name. O
Where O
the O
class O
name O
is O
an O
ngram O
, O
we O
average O
over O
its O
unigrams. O
Representations O
are O
extracted O
using O
the O
en-core-web-lg O
model O
from O
the O
spacy O
library O
, O
after O
removing O
punctuation O
and O
stopwords O
. O
As O
illustrated O
in O
Fig. O
2 O
, O
for O
each O
example O
, O
we O
select O
the O
token O
with O
the O
largest O

GloVe O
similarity O
to O
the O
class O
name. O
This O
token O
is O
then O
masked O
from O
the O
text O
by O
replacing O
it O
with O
the O
model O
's O
special O
unknown O
token O
( O
< O
unk O
> O
for O
RoBERTa O
and O
BART O
, O
[ O
UNK O
] O
for O
DeBERTa O
) O
. O
Experimental O
Results O
We O
set O
n B-HyperparameterName
, O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName

examples I-HyperparameterName
per I-HyperparameterName
class I-HyperparameterName
, O
to O
be O
1 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
unlabeled I-HyperparameterValue
set I-HyperparameterValue
U I-HyperparameterValue
( O
i.e. O
, O
n O
= O
100 O
for O
a O
U O
of O
size O
10k O
) O
. O
For O
each O
dataset O
and O
zero-shot O
model O
, O
we O
perform O
two B-HyperparameterValue
iterations I-HyperparameterValue
of O
self-training. B-HyperparameterName
4 O
We O
test O
4 O
settings O
of O
adding O
contradict O
examples O
as O

described O
in O
Section O
2.2.2 O
. O
Classification B-MetricName
accuracy I-MetricName
before O
and O
after O
selftraining O
for O
all O
models O
using O
the O
Contrast-random O
setting O
is O
shown O
in O
Table O
1. O
The O
results O
demonstrate O
a O
clear O
and O
significant O
benefit O
to O
the O
self-training O
process O
, O
across O
all O
models O
and O
datasets. O
Significance O
was O
tested O
with O
paired O
t-tests O
to O
compare O
accuracy O

with O
and O
without O
self-training O
, O
pooling O
together O
all O
datasets O
and O
seeds O
for O
each O
of O
the O
three O
models O
. O
Fig. O
3 O
compares O
the O
4 O
settings O
of O
selecting O
negative O
examples. O
As O
can O
be O
seen O
, O
among O
the O
four O
settings O
, O
Contrast-furthest O
yields O
the O
poorest O
re-sults. O
A O
possible O
explanation O
is O
that O
in O
this O

setting O
the O
negative O
examples O
are O
too O
trivial O
for O
the O
model. O
Contrast-closest O
yields O
better O
results O
, O
probably O
because O
the O
negative O
examples O
in O
this O
setting O
are O
more O
difficult O
and O
thus O
more O
informative O
for O
the O
model. O
However O
, O
for O
this O
same O
reason O
, O
these O
pseudo O
labeled O
examples O
are O
expected O
to O
suffer O
from O
the O

largest O
noise. O
The O
best O
performing O
settings O
are O
Contrast-all O
and O
Contrast-random O
, O
which O
represent O
a O
better O
balance O
between O
the O
informativeness O
of O
the O
negative O
examples O
and O
their O
noise O
level. O
Taking O
the O
computational O
costs O
into O
account O
, O
Contrast-random O
emerges O
as O
the O
preferred O
setting O
. O
The O
contribution O
of O
token O
masking O
One O
component O
of O
our O

approach O
, O
which O
aims O
at O
increasing O
the O
informativeness O
of O
pseudo-labeled O
examples O
, O
is O
the O
masking O
of O
the O
token O
closest O
to O
the O
class O
name O
( O
§3.4 O
) O
. O
We O
examine O
the O
performance O
of O
self-training O
without O
the O
masking O
procedure. O
A O
comparison O
of O
classification O
accuracy O
with O
and O
without O
applying O
token O
masking O
is O
shown O

in O
Table O
2. O
Overall O
, O
applying O
token O
masking O
does O
provide O
a O
performance O
gainas O
confirmed O
by O
a O
paired O
t-test O
( O
p O
= O
3×10 O
−4 O
, O
pooling O
together O
all O
models O
, O
datasets O
and O
seeds O
) O
. O
However O
, O
as O
can O
be O
seen O
in O
Table O
2 O
, O
results O
do O
differ O
across O
models O
and O

datasets. O
For O
instance O
, O
masking O
affords O
more O
consistent O
benefits O
in O
the O
case O
of O
the O
RoBERTa B-MethodName
entailment I-MethodName
model I-MethodName
, O
and O
has O
a O
more O
pronounced O
effect O
in O
the O
case O
of O
the O
ISEAR B-DatasetName
dataset O
. O
The O
beneficial O
effect O
of O
masking O
raises O
the O
question O
of O
whether O
the O
pseudo-labeled O
train O
set O
, O
i.e. O
, O
the O

model O
's O
most O
confident O
predictions O
, O
are O
trivial O
examples O
that O
could O
just O
as O
easily O
have O
been O
obtained O
using O
a O
simple O
heuristic. O
To O
test O
this O
, O
we O
construct O
an O
alternative O
pseudo-labeled O
set O
that O
is O
based O
on O
a O
token-level O
heuristic O
rather O
than O
the O
model O
predictions. O
This O
example O
selection O
method O
had O
a O
substantial O

negative O
effect O
on O
performance O
( O
see O
App. O
B O
for O
details O
) O
. O
Cross-task O
effects O
A O
recurring O
question O
in O
transfer O
learning O
is O
whether O
fine-tuning O
on O
a O
task O
T O
i O
can O
translate O
to O
benefits O
on O
another O
task O
, O
T O
j O
( O
e.g. O
, O
Phang O
et O
al. O
, O
2018 O
; O
Aghajanyan O
et O
al. O

, O
2021 O
) O
. O
It O
is O
thus O
interesting O
to O
study O
this O
question O
in O
the O
present O
context O
of O
self-training O
zero-shot O
classifiers. O
In O
other O
words O
, O
can O
exposure O
to O
pseudo-labels O
for O
task O
T O
i O
improve O
zero-shot O
performance O
on O
task O
T O
j O
. O
One O
aspect O
that O
is O
specific O
to O
our O
scenario O
is O
that O

fine-tuning O
with O
pseudo-labels O
on O
T O
i O
exposes O
the O
model O
to O
the O
task O
template O
which O
is O
also O
used O
for O
T O
j O
. O
To O
explore O
this O
question O
, O
each O
model O
that O
was O
self-trained O
on O
T O
i O
is O
evaluated O
over O
all O
the O
other O
datasets O
as O
well. O
Fig. O
4 O
depicts O
the O
cross-task O
effects O
of O

self-training O
on O
performance. O
This O
analysis O
reveals O
that O
self-training O
on O
a O
different O
task O
can O
be O
beneficial O
or O
harmful. O
The O
topical O
datasets O
( O
20 B-DatasetName
newsgroup I-DatasetName
, O
AG B-DatasetName
's I-DatasetName
news I-DatasetName
, O
DBPedia B-DatasetName
, O
and O
Yahoo B-DatasetName
! I-DatasetName
answers I-DatasetName
) O
appear O
to O
be O
beneficial O
to O
each O
other O
; O
as O
do O
the O
two O
emotion O
classification O
datasets O

( O
GoEmotions B-DatasetName
and O
ISEAR B-DatasetName
) O
. O
In O
contrast O
, O
self-training O
on O
sentiment O
data O
( O
Amazon B-DatasetName
, O
IMDB B-DatasetName
) O
leads O
to O
significant O
degradation O
in O
results O
on O
the O
emotion O
datasets. O
Possibly O
, O
this O
is O
related O
to O
particular O
characteristics O
of O
the O
reviews O
domain O
, O
along O
with O
the O
sharp O
binary O
distinction O
between O
positive O
and O

negative O
sentiment O
, O
as O
opposed O
to O
the O
subtle O
nuances O
that O
are O
necessary O
to O
distinguish O
between O
different O
types O
of O
emotions. O
6 O
Related O
Work O
Self-training O
( O
Scudder O
, O
1965 O
) O
has O
a O
long O
history O
as O
a O
method O
for O
semi-supervised O
learning O
, O
where O
predictions O
of O
a O
supervised O
classifier O
on O
unlabeled O
examples O
are O
used O

as O
pseudo-labels O
to O
augment O
the O
amount O
of O
training O
data. O
This O
approach O
has O
successfully O
been O
applied O
to O
a O
wide O
range O
of O
machine O
learning O
problems O
( O
Van O
Engelen O
and O
Hoos O
, O
2020 O
) O
. O
Many O
variations O
of O
self-training O
have O
been O
put O
forth O
, O
varying O
in O
terms O
of O
the O
selection O
strategy O
of O
samples O

to O
pseudo-label O
, O
the O
amount O
-and O
characteristicsof O
the O
models O
involved O
in O
the O
procedure O
, O
and O
other O
specific O
design O
decisions O
( O
Triguero O
et O
al. O
, O
2015 O
) O
. O
From O
a O
more O
theoretical O
standpoint O
, O
previous O
works O
( O
Lee O
et O
al. O
, O
2013 O
) O
have O
described O
selftraining O
as O
somewhat O
equivalent O
to O
entropy O

minimization O
( O
Grandvalet O
and O
Bengio O
, O
2004 O
) O
, O
in O
that O
it O
modifies O
the O
model O
's O
decision O
boundaries O
by O
driving O
the O
model O
to O
make O
more O
confident O
predictions O
. O
Aiming O
for O
more O
general-purpose O
models O
, O
that O
can O
achieve O
cross-task O
generalization O
and O
perform O
in O
a O
zero-shot O
scenario O
, O
recent O
works O
have O
proposed O

different O
strategies O
for O
mapping O
a O
range O
of O
NLP O
tasks O
into O
a O
generic O
and O
unified O
framework. O
Yin O
et O
al. O
( O
2019 O
) O
suggest O
the O
textual O
entailment O
paradigm O
as O
one O
that O
can O
encompass O
different O
types O
of O
text O
classification O
tasks. O
Zhong O
et O
al. O
( O
2021 O
) O
map O
classification O
tasks O
to O
a O
question-answering O
format O

, O
where O
each O
class O
is O
formulated O
as O
a O
question O
and O
given O
as O
a O
prompt O
, O
and O
the O
decoder O
probabilities O
of O
the O
Yes O
and O
No O
tokens O
correspond O
to O
a O
positive O
or O
negative O
prediction O
for O
the O
class. O
They O
also O
propose O
a O
" O
meta-tuning O
" O
paradigm O
, O
where O
labeled O
data O
for O
different O
tasks O

-formulated O
in O
terms O
of O
the O
unified O
task O
format O
-is O
utilized O
in O
order O
to O
teach O
the O
model O
how O
to O
solve O
the O
generic O
" O
meta-task O
" O
, O
and O
thus O
better O
cope O
with O
unseen O
tasks O
at O
test O
time. O
By O
opting O
for O
a O
generic O
cross-task O
format O
of O
natural O
language O
instructions O
, O
Wei O
et O
al. O

( O
2022 O
) O
and O
Sanh O
et O
al. O
( O
2022 O
) O
extend O
this O
notion O
even O
further O
, O
where O
meta-tuning O
on O
multiple O
types O
of O
NLP O
tasks O
enables O
zero-shot O
prediction O
even O
on O
tasks O
of O
a O
very O
different O
nature O
from O
those O
seen O
during O
training O
. O
In O
the O
present O
work O
we O
explore O
the O
intersection O
of O

these O
two O
threads O
-namely O
, O
self-training O
and O
general O
purpose O
zero-shot O
models O
, O
while O
focusing O
on O
zero-shot O
text O
classifiers O
that O
use O
the O
entailment O
paradigm O
, O
and O
on O
a O
scenario O
where O
only O
unlabeled O
data O
is O
available. O
Ye O
et O
al. O
( O
2020 O
) O
apply O
self-training O
to O
text O
classification O
in O
order O
to O
transfer O
to O

unseen O
classes O
for O
which O
there O
is O
no O
labeled O
data O
, O
and O
propose O
a O
reinforcement O
learning O
method O
for O
selecting O
examples O
to O
pseudo-label. O
This O
scenario O
differs O
substantially O
from O
ours O
in O
that O
self-training O
is O
not O
applied O
to O
an O
existing O
general-purpose O
zero-shot O
model. O
In O
addition O
, O
they O
deal O
with O
a O
setting O
where O
labeled O
data O

for O
some O
of O
the O
target O
classes O
is O
available O
. O
Like O
the O
present O
work O
, O
Zhou O
et O
al. O
( O
2022 O
) O
also O
aim O
to O
improve O
existing O
general-purpose O
zero-shot O
learners O
by O
utilizing O
unlabeled O
data. O
Starting O
from O
T0 O
, O
the O
prompt-based O
zero-shot O
learner O
from O
Sanh O
et O
al. O
( O
2022 O
) O
, O
they O
use O

unlabeled O
texts O
to O
apply O
a O
prompt O
consistency O
loss O
: O
an O
example O
is O
fed O
into O
the O
model O
multiple O
times O
, O
each O
time O
in O
the O
context O
of O
a O
different O
-but O
synonymous O
-task O
prompt O
; O
then O
, O
the O
model O
is O
trained O
to O
assign O
similar O
predictions O
across O
differently-phrased O
prompts O
( O
Zhou O
et O
al. O
, O

2022 O
) O
. O
Thus O
, O
whereas O
we O
explore O
improving O
a O
generalpurpose O
model O
using O
a O
form O
of O
self-training O
, O
they O
do O
so O
using O
a O
variation O
on O
the O
paradigm O
of O
consistency O
training O
( O
Xie O
et O
al. O
, O
2020 O
) O
. O
Some O
works O
attempt O
to O
improve O
the O
performance O
of O
general-purpose O
models O
within O
a O

few-shot O
scenario. O
For O
example O
, O
Basile O
et O
al. O
( O
2021 O
) O
experiment O
with O
entailment-based O
classifiers. O
They O
show O
that O
compared O
to O
a O
standard O
pre-trained O
language O
model O
, O
off-the-shelf O
entailment O
models O
require O
less O
labeled O
examples O
for O
fine-tuning O
to O
reach O
reasonable O
performance O
on O
an O
emotion O
classification O
task O
. O
Discussion O
In O
this O
paper O
we O

look O
at O
the O
applicability O
of O
selftraining O
for O
adapting O
a O
general-purpose O
zero-shot O
model O
, O
focusing O
on O
the O
scenario O
of O
entailmentbased O
models. O
We O
opted O
for O
this O
specific O
setting O
due O
to O
the O
high O
accessibility O
of O
these O
off-the-shelf O
models. O
In O
other O
words O
, O
given O
that O
these O
models O
are O
readily O
available O
for O
use O
, O
we O

ask O
whether O
selftraining O
provides O
a O
straightforward O
way O
for O
practitioners O
to O
adapt O
the O
general O
model O
for O
their O
downstream O
task O
, O
using O
only O
a O
modest O
collection O
of O
unlabeled O
data. O
We O
show O
that O
in O
this O
setting O
self-training O
does O
indeed O
provide O
value O
, O
delivering O
significant O
performance O
gains O
for O
text O
classification O
. O
The O
notion O
of O

using O
self-training O
as O
a O
tool O
to O
adapt O
a O
general-purpose O
zero-shot O
model O
is O
not O
specific O
to O
entailment O
models O
, O
nor O
is O
it O
limited O
to O
classification O
tasks. O
Thus O
, O
a O
major O
avenue O
for O
future O
work O
would O
be O
to O
explore O
this O
combination O
on O
models O
that O
rely O
on O
different O
kinds O
of O
mapping O
functions O
or O

" O
meta-tasks O
" O
for O
formulating O
downstream O
tasks O
within O
a O
generic O
cross-task O
framework O
( O
Wei O
et O
al. O
, O
2022 O
; O
Zhong O
et O
al. O
, O
2021 O
; O
Bragg O
et O
al. O
, O
2021 O
; O
Sanh O
et O
al. O
, O
2022 O
) O
. O
Zero-shot B-TaskName
text I-TaskName
classification I-TaskName
is O
recently O
drawing O
much O
attention O
, O
with O
prominent O
recent O

works O
showing O
promising O
results O
using O
different O
kinds O
of O
iterative O
approaches O
( O
Meng O
et O
al. O
, O
2020 O
; O
. O
Such O
approaches O
build O
their O
zero-shot O
classi-fiers O
from O
scratch O
-and O
therefore O
typically O
require O
larger O
unlabeled O
datasets O
to O
perform O
well O
-whereas O
we O
aim O
to O
utilize O
and O
build O
on O
the O
knowledge O
contained O
in O
general-purpose O
zero-shot O

classifiers. O
Exploring O
ways O
to O
combine O
these O
differing O
approaches O
is O
left O
for O
future O
work O
. O
Importantly O
, O
while O
our O
method O
does O
assume O
the O
existence O
of O
a O
collection O
of O
unlabeled O
examples O
, O
our O
results O
show O
that O
an O
order O
of O
10K O
examples O
is O
sufficient O
to O
benefit O
from O
self-training. O
Moreover O
, O
the O
cross-task O
effects O

in O
section O
5.2 O
demonstrate O
that O
even O
unlabeled O
examples O
from O
a O
similar O
domain O
and O
/ O
or O
task O
may O
be O
useful O
in O
adapting O
the O
generalpurpose O
model O
for O
the O
downstream O
task. O
Determining O
the O
exact O
conditions O
under O
which O
self-training O
is O
useful O
for O
adaptation O
across O
tasks O
is O
a O
matter O
for O
future O
study. O
Moreover O
, O
it O

would O
be O
interesting O
to O
explore O
the O
effects O
of O
self-training O
on O
multiple O
datasets O
, O
akin O
to O
works O
on O
supervised O
multi-task O
fine-tuning O
( O
e.g. O
, O
Aghajanyan O
et O
al. O
, O
2021 O
) O
. O
In O
our O
work O
, O
we O
select O
examples O
for O
pseudolabeling O
based O
solely O
on O
model O
confidence O
( O
see O
§2.2 O
) O
. O
Some O

self-training O
works O
opt O
for O
more O
balanced O
approaches O
for O
example O
selection O
, O
aiming O
for O
a O
more O
diverse O
and O
/ O
or O
more O
informative O
set O
of O
examples O
( O
e.g. O
, O
Hajmohammadi O
et O
al. O
, O
2015 O
; O
Mukherjee O
and O
Awadallah O
, O
2020 O
) O
. O
It O
would O
be O
interesting O
to O
explore O
such O
questions O
in O
the O

zero-shot O
scenario. O
In O
addition O
, O
in O
Section O
2.2 O
we O
describe O
our O
method O
to O
select O
confident O
examples O
, O
namely O
by O
looking O
at O
the O
maximal O
delta O
between O
the O
highest O
and O
second O
highest O
prediction O
scores. O
Other O
alternatives O
for O
choosing O
confident O
examples O
, O
e.g. O
, O
by O
looking O
at O
the O
entropy O
across O
classes O
, O
could O

be O
tested O
as O
well O
. O
To O
conclude O
, O
the O
method O
we O
proposed O
in O
this O
paper O
can O
boost O
the O
performance O
of O
entailmentbased B-TaskName
zero-shot I-TaskName
text I-TaskName
classifiers I-TaskName
, O
with O
little O
effort O
and O
a O
modest O
amount O
of O
domain O
data. O
This O
can O
prove O
useful O
to O
the O
many O
practitioners O
who O
benefit O
from O
the O
practicality O
and O
accessibility O

of O
these O
models O
. O
Limitations O
Our O
focus O
here O
is O
on O
off-the-shelf O
models O
that O
are O
highly O
accessible O
-and O
thus O
potentially O
usefulfor O
practitioners. O
Nevertheless O
, O
these O
models O
are O
quite O
large O
, O
and O
thus O
carry O
a O
non-negligible O
computational O
footprint. O
For O
instance O
, O
inferring O
on O
10K O
unlabeled O
samples O
does O
require O
a O
GPU O
, O
limiting O

access O
to O
such O
approaches O
and O
models O
in O
practice O
. O
Our O
work O
is O
empirical O
in O
nature. O
As O
such O
, O
we O
report O
experimental O
results O
, O
with O
no O
theoretical O
guar-antees O
, O
and O
one O
should O
recognize O
the O
existence O
of O
exceptions. O
In O
addition O
, O
our O
experimental O
results O
are O
for O
relatively O
standard O
academic O
benchmarks O
for O
text O

classification. O
Real-world O
datasets O
, O
especially O
in O
specific O
domains O
such O
as O
legal O
and O
healthcare O
, O
may O
pose O
additional O
challenges. O
The O
practical O
value O
of O
our O
approach O
in O
these O
cases O
is O
yet O
to O
be O
seen O
. O
We O
formulate O
and O
test O
our O
approach O
in O
the O
scenario O
where O
each O
example O
should O
be O
assigned O
to O
exactly O

one O
class. O
Applying O
our O
method O
to O
the O
multi-label O
classification O
scenario O
might O
not O
be O
straightforward O
, O
and O
may O
require O
different O
ways O
of O
selecting O
examples O
for O
the O
pseudo-labeled O
set O
. O
Finally O
, O
the O
large O
scale O
of O
our O
experiments O
places O
a O
non-trivial O
burden O
on O
trying O
to O
replicate O
our O
results. O
Moreover O
, O
the O
off-the-shelf O

models O
used O
in O
our O
experiments O
are O
not O
guaranteed O
to O
be O
hosted O
publicly O
in O
the O
future O
. O
B O
Heuristic-based O
selection O
As O
stated O
in O
section O
5.1 O
, O
we O
experiment O
with O
constructing O
an O
alternative O
pseudo-labeled O
train O
set O
that O
is O
based O
on O
a O
token-level O
heuristic. O
In O
this O
method O
, O
the O
examples O
are O
chosen O
based O

on O
GloVe-based O
similarity O
to O
the O
class O
names. O
First O
, O
for O
each O
example O
and O
class O
we O
calculate O
a O
" O
GloVe-to-closest-token O
" O
score O
, O
which O
is O
the O
similarity O
between O
the O
class O
and O
the O
closest O
token O
in O
the O
example O
, O
following O
a O
similar O
protocol O
as O
that O
for O
finding O
tokens O
to O
mask O
( O
cf. O

3.4 O
) O
. O
Then O
, O
for O
each O
class O
c O
we O
construct O
a O
list O
of O
size O
n O
of O
the O
top O
candidates O
: O
we O
take O
the O
examples O
where O
the O
" O
GloVe-to-closest-token O
" O
score O
was O
highest O
for O
c O
; O
these O
examples O
are O
sorted O
by O
the O
difference O
between O
the O
" O
GloVe-to-closest-token O
" O
score O
for O

c O
and O
for O
the O
class O
with O
the O
second O
highest O
score O
, O
and O
the O
top O
n O
examples O
are O
selected. O
We O
apply O
masking O
for O
the O
selected O
examples O
using O
the O
same O
protocol O
we O
use O
in O
the O
self-training O
approach. O
Fig. O
5 O
compares O
this O
approach O
to O
a O
single O
iteration O
of O
self-training. O
As O
can O
be O
seen O

, O
for O
some O
datasets O
this O
pseudo-labeling O
approach O
does O
improve O
the O
zero-shot O
classifier O
, O
yet O
, O
the O
results O
are O
not O
consistent O
across O
datasets O
and O
in O
4 O
of O
the O
datasets O
applying O
this O
approach O
results O
in O
a O
lower O
accuracy B-MetricName
compared O
to O
the O
zero-shot O
classifier O
. O


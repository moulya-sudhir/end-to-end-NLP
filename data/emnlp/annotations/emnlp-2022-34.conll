Unsupervised O
Boundary-Aware O
Language O
Model O
Pretraining O
for O
Chinese B-TaskName
Sequence I-TaskName
Labeling I-TaskName
Chinese O
language O
processing O
tasks, O
such O
as O
word O
segmentation, O
part-of-speech O
tagging, O
and O
named O
entity O
recognition. O
Previous O
studies O
usually O
resorted O
to O
the O
use O
of O
a O
high-quality O
external O
lexicon, O
where O
lexicon O
items O
can O
offer O
explicit O
boundary O
information. O
However, O
to O
ensure O
the O
quality O
of O
the O
lexicon, O
great O
human O
effort O
is O
always O
necessary, O
which O
has O
been O
generally O
ignored. O
In O
this O
work, O
we O
suggest O
unsupervised O
statistical O
boundary O
information O
instead, O
and O
propose O
an O
architecture O
to O
encode O
the O
information O
directly O
into O
pre-trained O
language O
models, O
resulting O
in O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
). O
We O
apply O
BABERT B-MethodName
for O
feature O
induction O
of O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Experimental O
results O
on O
ten O
benchmarks O
of O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
demonstrate O
that O
BABERT B-MethodName
can O
provide O
consistent O
improvements O
on O
all O
datasets. O
In O
addition, O
our O
method O
can O
complement O
previous O
supervised O
lexicon O
exploration, O
where O
further O
improvements O
can O
be O
achieved O
when O
integrated O
with O
external O
lexicon O
information. O
Introduction O
The O
representative O
sequence O
labeling O
tasks O
for O
the O
Chinese O
language, O
such O
as O
word B-TaskName
segmentation I-TaskName
, O
partof-speech B-TaskName
(POS) I-TaskName
tagging I-TaskName
and O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
(Emerson, O
2005;Jin O
and O
Chen, O
2008), O
have O
been O
inclined O
to O
be O
performed O
at O
the O
characterlevel O
in O
an O
end-to-end O
manner O
(Shen O
et O
al., O
2016). O
The O
paradigm, O
naturally, O
is O
standard O
to O
Chinese B-TaskName
word I-TaskName
segmentation I-TaskName
( O
CWS B-TaskName
), O
while O
for O
Chinese B-TaskName
POS I-TaskName
tagging I-TaskName
and O
NER B-TaskName
, O
it O
can O
better O
help O
reduce O
the O
error O
propagation O
(Sun O
and O
Uszkoreit, O
2012;Yang O
et O
al., O
2016;Liu O
et O
al., O
2019a) O
compared O
with O
word-based O
counterparts O
by O
straightforward O
modeling. O
Recently, O
all O
the O
above O
tasks O
have O
reached O
stateof-the-art O
performances O
with O
the O
help O
of O
BERTlike O
pre-trained O
language O
models O
(Yan O
et O
al., O
2019;Meng O
et O
al., O
2019). O
The O
BERT O
variants, O
such O
as O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021), O
ERNIE B-MethodName
, O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
NEZHA B-MethodName
, O
etc., O
further O
improve O
the O
vanilla O
BERT O
by O
either O
using O
external O
knowledge O
or O
larger-scale O
training O
corpus. O
The O
improvements O
can O
also O
benefit O
character-level B-TaskName
Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Notably, O
since O
the O
output O
tags O
of O
all O
these O
character-level B-TaskName
Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
involve O
identifying O
Chinese O
words O
or O
entities O
(Zhang O
and O
Yang, O
2018;Yang O
et O
al., O
2019), O
prior O
boundary O
knowledge O
could O
be O
highly O
helpful O
for O
them. O
A O
number O
of O
studies O
propose O
the O
integration O
of O
an O
external O
lexicon O
to O
enhance O
their O
baseline O
models O
by O
feature O
representation O
learning O
(Jia O
et O
al., O
2020;Tian O
et O
al., O
2020a;. O
Moreover, O
some O
works O
suggest O
injecting O
similar O
resources O
into O
the O
pre-trained O
BERT O
weights. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
the O
representatives, O
which O
leverage O
an O
external O
lexicon O
for O
masked O
word O
prediction O
in O
Chinese O
BERT. O
The O
lexicon-based O
methods O
have O
indeed O
achieved O
great O
success O
for O
boundary O
integration. O
However, O
there O
are O
two O
major O
drawbacks. O
First, O
the O
lexicon O
resources O
are O
always O
constructed O
manually O
(Zhang O
and O
Yang, O
2018;Diao O
et O
al., O
2020;Jia O
et O
al., O
2020;, O
which O
is O
expensive O
and O
time-consuming. O
The O
quality O
of O
the O
lexicon O
is O
critical O
to O
our O
tasks. O
Second, O
different O
tasks O
as O
well O
as O
different O
domains O
require O
different O
lexicons O
(Jia O
et O
al., O
2020;. O
A O
well-studied O
lexicon O
for O
word O
segmentation O
might O
be O
inappropriate O
for O
NER, O
and O
a O
lexicon O
for O
news O
NER O
might O
also O
be O
problematic O
for O
finance O
NER. O
The O
two O
drawbacks O
can O
be O
due O
to O
the O
supervised O
characteristic O
of O
these O
lexicon-based O
enhancements. O
Thus, O
it O
is O
more O
desirable O
to O
offer O
boundary O
information O
in O
an O
unsupervised O
manner. O
In O
this O
paper, O
we O
propose O
an O
unsupervised O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
), O
which O
is O
achieved O
by O
fully O
exploring O
the O
potential O
of O
statisti-cal O
features O
mined O
from O
a O
large-scale O
raw O
corpus. O
We O
extract O
a O
set O
of O
N-grams O
(a O
predefined O
fixed O
N) O
no O
matter O
they O
are O
valid O
words O
or O
entities, O
and O
then O
calculate O
their O
corresponding O
unsupervised O
statistical O
features, O
which O
are O
mostly O
related O
to O
boundary O
information. O
We O
inject O
the O
boundary O
information O
into O
the O
internal O
layer O
of O
a O
pre-trained O
BERT, O
so O
that O
our O
final O
BABERT B-MethodName
model O
can O
approximate O
the O
boundary O
knowledge O
softly O
by O
using O
inside O
representations. O
The O
BABERT B-MethodName
model O
has O
no O
difference O
from O
the O
original O
BERT, O
so O
that O
we O
can O
use O
it O
in O
the O
same O
way O
as O
the O
standard O
BERT O
exploration. O
We O
conduct O
experiments O
on O
three O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
to O
demonstrate O
the O
effectiveness O
of O
our O
proposed O
method. O
Experimental O
results O
show O
that O
our O
approach O
can O
significantly O
outperform O
other O
Chinese O
pre-trained O
language O
models. O
In O
addition, O
compared O
with O
supervised O
lexicon-based O
methods, O
BABERT B-MethodName
obtains O
competitive O
results O
on O
all O
tasks O
and O
achieves O
further O
improvements O
when O
integrated O
with O
external O
lexicon O
knowledge. O
We O
also O
conduct O
extensive O
analyses O
to O
understand O
our O
method O
comprehensively. O
The O
pre-trained O
model O
and O
code O
are O
publicly O
available O
at O
http://github.com/modelscope/ O
adaseq/examples/babert. O
Our O
contributions O
in O
this O
paper O
include O
the O
following: O
1) O
We O
design O
a O
method O
to O
encode O
unsupervised O
statistical O
boundary O
information O
into O
boundary-aware O
representation, O
2) O
propose O
a O
new O
pre-trained O
language O
model O
called O
BABERT B-MethodName
as O
a O
boundary-aware O
extension O
for O
BERT, O
3) O
verify O
BABERT B-MethodName
on O
ten O
benchmark O
datasets O
of O
three O
Chinese O
sequence O
labeling O
tasks. O
Related O
Work O
In O
the O
past O
decades, O
machine O
learning O
has O
achieved O
good O
performance O
on O
sequence O
labeling O
tasks O
with O
statistical O
information O
(Bellegarda, O
2004;Low O
et O
al., O
2005;Bouma, O
2009). O
Recently, O
neural O
models O
have O
led O
to O
state-of-the-art O
results O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Lample O
et O
al., O
2016;Ma O
and O
Hovy, O
2016;Chiu O
and O
Nichols, O
2016). O
In O
addition, O
the O
presence O
of O
language O
representation O
models O
such O
as O
BERT O
(Devlin O
et O
al., O
2019) O
has O
led O
to O
impressive O
improvements. O
In O
particular, O
many O
variants O
of O
BERT O
are O
devoted O
to O
integrating O
boundary O
information O
into O
BERT O
to O
improve O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Diao O
et O
al., O
2020;Jia O
et O
al., O
2020;. O
Statistical O
Machine O
Learning O
Statistical O
information O
is O
critical O
for O
sequence O
labeling. O
Previous O
works O
attempt O
to O
count O
such O
information O
from O
large O
corpora O
in O
order O
to O
combine O
it O
with O
machine O
learning O
methods O
for O
sequence O
labeling O
(Bellegarda, O
2004;Liang, O
2005;Bouma, O
2009). O
Peng O
et O
al. O
(2004) O
attempts O
to O
conduct O
sequence O
labeling O
by O
CRF O
and O
a O
statistical-based O
new O
word O
discovery O
method. O
Low O
et O
al. O
(2005) O
introduce O
a O
maximum O
entropy O
approach O
for O
sequence O
labeling. O
Liang O
(2005) O
utilizes O
unsupervised O
statistical O
information O
in O
Markov O
models, O
and O
gets O
a O
boost O
on O
Chinese O
NER O
and O
CWS. O
Pre-trained O
Language O
Model O
Pre-trained O
language O
model O
is O
a O
hot O
topic O
in O
natural O
language O
processing O
(NLP) O
communities O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019b;Clark O
et O
al., O
2020;Diao O
et O
al., O
2020; O
and O
has O
been O
extensively O
studied O
for O
Chinese O
sequence O
labeling. O
For O
instance, O
TENER O
(Yan O
et O
al., O
2019) O
adopts O
Transformer O
encoder O
to O
model O
characterlevel O
features O
for O
Chinese O
NER. O
Glyce O
(Meng O
et O
al., O
2019) O
uses O
BERT O
to O
capture O
the O
contextual O
representation O
combined O
with O
glyph O
embeddings O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Lexicon-based O
Methods O
In O
recent O
studies, O
lexicon O
knowledge O
has O
been O
applied O
to O
improve O
model O
performance. O
There O
are O
two O
mainstream O
categories O
to O
the O
work O
of O
lexicon O
enhancement. O
The O
first O
aims O
to O
enhance O
the O
original O
BERT O
with O
implicit O
boundary O
information O
by O
using O
the O
multi-granularity O
word O
masking O
mechanism. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
representatives O
of O
this O
category, O
which O
propose O
to O
mask O
tokens, O
entities, O
and O
phrases O
as O
the O
mask O
units O
in O
the O
masked O
language O
modeling O
(MLM) O
task O
to O
learn O
the O
coarse-grained O
lexicon O
information O
during O
pre-training. O
ERNIE-Gram B-MethodName
, O
an O
extension O
of O
ERNIE, O
utilizes O
statistical O
boundary O
information O
for O
unsupervised O
word O
extraction O
to O
support O
masked O
word O
prediction, O
The O
second O
category, O
which O
includes O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
EEBERT B-MethodName
(Jia O
et O
al., O
2020), O
and O
LEBERT B-MethodName
, O
exploits O
the O
potential O
of O
directly O
injecting O
lexicon O
information O
into O
BERT O
via O
extra O
modules, O
leading O
to O
better O
performance O
but O
is O
limited O
in O
predefined O
external O
knowledge. O
Our O
work O
follows O
the O
first O
line O
of O
work, O
most O
similar O
to O
ERNIE-Gram B-MethodName
. O
However, O
different O
from O
ERNIE-Gram B-MethodName
, O
we O
do O
not O
discretize O
the O
real-valued O
statistical O
information O
ex- O
tracted O
from O
corpus, O
but O
adopt O
a O
regression O
manner O
to O
leverage O
the O
information O
fully. O
PMI O
LRE O
1 O
2 O
… O
… O
−1 O
MLM O
Loss O
ℒ O
MLM O
MSE O
Loss O
ℒ O
BA O
(c). O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Input O
Sentence O
Raw O
Corpus O
N-gram O
Statistical O
Dictionary O
Contextual O
N-gram O
Sets O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
N-gram O
Set O
of O
N-gram O
Set O
of O
+1 O
… O
+ O
−1 O
• O
• O
• O
• O
• O
• O
−1 O
− O
/2 O
… O
+ O
/2 O
• O
• O
• O
• O
• O
• O
+1 O
+2 O
− O
+1 O
… O
−1 O
+1 O
N-gram O
Set O
1 O
of O
1 O
Method O
Figure O
1 O
shows O
the O
overall O
architecture O
of O
our O
unsupervised O
boundary-aware O
pre-trained O
language O
model, O
which O
mainly O
consists O
of O
three O
components: O
1) O
boundary O
information O
extractor O
for O
unsupervised O
statistical O
boundary O
information O
mining, O
2) O
boundary-aware O
representation O
to O
integrate O
statistical O
information O
at O
the O
character-level, O
and O
3) O
boundary-aware B-MethodName
BERT I-MethodName
learning O
which O
injects O
boundary O
knowledge O
into O
the O
internal O
layer O
of O
BERT. O
In O
this O
section, O
we O
first O
focus O
on O
the O
details O
of O
the O
above O
components, O
and O
then O
introduce O
the O
fine-tuning O
method O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Boundary O
Information O
Extractor O
Statistical O
boundary O
information O
has O
been O
shown O
with O
a O
positive O
influence O
on O
a O
variety O
of O
Chinese O
NLP O
tasks O
(Song O
and O
Xia, O
2012;Higashiyama O
et O
al., O
2019;Ding O
et O
al., O
2020;. O
We O
follow O
this O
line O
of O
work, O
designing O
a O
boundary O
information O
extractor O
to O
mine O
statistical O
information O
from O
a O
large O
raw O
corpus O
in O
an O
unsupervised O
way. O
The O
overall O
flow O
of O
the O
extractor O
includes O
two O
steps: O
I) O
First, O
we O
collect O
all O
N-grams O
from O
the O
raw O
corpus O
to O
build O
a O
dictionary O
N O
, O
in O
which O
we O
count O
the O
frequencies O
of O
each O
N-gram O
and O
filter O
out O
the O
low O
frequencies O
items; O
II) O
second, O
considering O
that O
word O
frequency O
is O
insufficient O
for O
representing O
the O
flexible O
boundary O
relation O
in O
the O
Chinese O
context, O
we O
further O
compute O
two O
unsupervised O
indicators O
which O
can O
capture O
most O
of O
the O
boundary O
information O
in O
the O
corpus. O
In O
the O
following, O
we O
will O
describe O
these O
two O
indicators O
in O
detail. O
Pointwise O
Mutual O
Information O
(PMI) O
Given O
an O
N-gram, O
we O
split O
it O
into O
two O
sub-strings O
and O
compute O
the O
mutual O
information O
(MI) O
between O
them O
as O
a O
candidate. O
Then, O
we O
enumerate O
all O
sub-string O
pairs O
and O
choose O
the O
minimum O
MI O
as O
the O
overall O
PMI O
to O
estimate O
the O
tightness O
of O
the O
N-gram. O
Let O
g O
= O
{c O
1 O
...c O
m O
} O
be O
an O
N-gram O
that O
consists O
of O
m O
characters, O
we O
calculate O
PMI O
using O
this O
formula: O
PMI(g) O
= O
min O
i∈[1:m−1] O
{ O
p(g) O
p(c1...ci) O
• O
p(ci+1...cm) O
}, O
(1) O
where O
p(•) O
denotes O
the O
probability O
over O
the O
corpus. O
Note O
that, O
when O
m O
= O
1, O
the O
corresponding O
PMI O
is O
constantly O
equal O
to O
1. O
The O
higher O
PMI O
indicates O
that O
the O
N-gram O
(e.g., O
"贝克汉姆 O
(Beckham)") O
has O
a O
similar O
occurrence O
probability O
to O
the O
sub-string O
pair O
(e.g., O
"贝克 O
(Beck)" O
and O
"汉姆 O
(Ham)"), O
leading O
to O
a O
higher O
association O
between O
internal O
sub-string O
pairs, O
which O
makes O
the O
N-gram O
more O
likely O
to O
be O
a O
word/entity. O
In O
contrast, O
a O
lower O
PMI O
means O
the O
Ngram O
(e.g., O
"克汉(Kehan)") O
is O
possibly O
an O
invalid O
word/entity. O
Left O
and O
Right O
Entropy O
(LRE) O
Given O
an O
Ngram O
g, O
we O
first O
collect O
a O
left-adjacent O
character O
set O
S O
l O
m O
= O
{c O
l O
1 O
, O
..., O
c O
l O
n O
l O
} O
with O
n O
l O
characters. O
Then, O
we O
utilize O
the O
conditional O
probability O
between O
g O
and O
its O
left O
adjacent O
characters O
in O
S O
l O
m O
to O
compute O
the O
left O
entropy O
(LE), O
which O
measures O
sufficient O
boundary O
information. O
LE O
can O
be O
defined O
as: O
LE(g) O
= O
− O
n O
l O
i O
p(c O
l O
i O
g|g) O
log O
p(c O
l O
i O
g|g).(2) O
Similar O
to O
LE, O
we O
further O
collect O
a O
right O
adjacent O
set O
S O
r O
m O
= O
{c O
r O
1 O
, O
..., O
c O
r O
nr O
} O
with O
n O
r O
characters O
to O
calculate O
the O
right O
entropy O
(RE) O
for O
the O
N-gram O
g: O
RE(g) O
= O
− O
nr O
i O
p(gc O
r O
i O
|g) O
log O
p(gc O
r O
i O
|g).(3) O
Intuitively, O
LRE O
represents O
the O
abundance O
of O
neighboring O
characters O
for O
the O
N-gram. O
With O
a O
lower O
LRE, O
the O
N-gram O
(e.g., O
"汉姆 O
") O
has O
a O
more O
fixed O
context, O
indicating O
it O
is O
more O
likely O
to O
be O
a O
part O
of O
a O
phrase O
or O
entity. O
Conversely, O
the O
N-gram O
with O
a O
higher O
LRE O
(e.g., O
"贝克汉姆") O
will O
interact O
more O
with O
context, O
which O
prefers O
to O
be O
an O
independent O
word O
or O
phrase. O
Finally, O
we O
utilize O
PMI O
and O
LRE O
to O
measure O
the O
flexible O
boundary O
relations O
in O
the O
Chinese O
context, O
and O
then O
update O
each O
N-gram O
in O
N O
with O
the O
unsupervised O
statistical O
indicators O
above. O
Boundary-Aware O
Representation O
By O
using O
the O
boundary O
information O
extractor, O
we O
can O
obtain O
an O
N-gram O
dictionary O
N O
with O
unsupervised O
statistical O
boundary O
information. O
Unfortunately, O
since O
the O
context O
independence O
and O
the O
high O
relevance O
to O
N-gram, O
previous O
works O
(Ding O
et O
al., O
2020; O
use O
such O
statistical O
features O
for O
word O
extraction O
only, O
which O
ignore O
the O
potential O
of O
statistical O
boundary O
information O
in O
representation O
learning. O
To O
alleviate O
this O
problem, O
we O
propose O
boundary-aware O
representation, O
a O
highly O
extensible O
method, O
to O
fully O
benefit O
from O
the O
statistical O
boundary O
information O
for O
representation O
learning. O
To O
achieve O
boundary-aware O
representation, O
we O
first O
build O
contextual O
N-gram O
sets O
from O
the O
sentence. O
As O
shown O
in O
Figure O
1 O
(b), O
given O
a O
sentence O
x O
= O
{c O
1 O
, O
c O
2 O
, O
..., O
c O
n O
} O
with O
n O
characters O
and O
the O
maximum O
N-gram O
length O
N O
, O
we O
extract O
all O
N-grams O
that O
include O
c O
i O
as O
the O
contextual O
N-gram O
set O
S O
c O
i O
= O
{c O
i O
, O
c O
i O
c O
i+1 O
, O
• O
• O
• O
, O
c O
i−N O
+1 O
...c O
i O
} O
for O
char- O
acter O
c O
i O
. O
Then, O
we O
design O
a O
composition O
method O
to O
integrate O
the O
statistical O
features O
of O
N-grams O
in O
S O
c O
i O
by O
using O
specific O
conditions O
and O
rules, O
aiming O
to O
avoid O
the O
sparsity O
and O
contextual O
independence O
limitations O
of O
statistical O
information. O
Concretely, O
we O
divide O
the O
information O
composition O
method O
into O
PMI O
and O
entropy O
representation. O
First, O
we O
concatenate O
the O
PMI O
of O
all O
N-grams O
in O
S O
c O
i O
to O
generate O
PMI O
representation: O
e O
p O
i O
=PMI( O
ci O
) O
⊕PMI( O
ci O
ci+1) O
⊕ O
PMI(ci−1 O
ci O
) O
⊕PMI( O
ci O
ci+1ci+2) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−2ci−1 O
ci O
) O
• O
• O
• O
• O
• O
• O
⊕PMI( O
ci O
...ci+N−1) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−N+1... O
ci O
),(4) O
where O
e O
p O
i O
∈ O
R O
a O
, O
and O
a O
= O
1+2+• O
• O
•+N O
is O
the O
number O
of O
the O
N-grams O
that O
contain O
c O
i O
. O
Note O
that O
the O
position O
of O
each O
N-gram O
is O
fixed O
in O
PMI O
representation. O
We O
strictly O
follow O
the O
order O
of O
N-gram O
length O
and O
the O
position O
of O
c O
i O
in O
N-gram O
to O
concatenate O
their O
corresponding O
PMI, O
ensuring O
that O
the O
position O
and O
context O
information O
can O
be O
encoded O
into O
e O
p O
i O
. O
Entropy O
representation O
focuses O
on O
the O
contextual O
interactions O
of O
each O
character. O
When O
c O
i O
is O
the O
border O
of O
N-grams O
in O
S O
c O
i O
, O
we O
separately O
aggregate O
the O
LE O
and O
RE O
as O
left O
and O
right O
entropy O
representation: O
e O
le O
i O
=LE( O
ci O
) O
⊕ O
LE( O
ci O
ci+1) O
⊕ O
• O
• O
• O
⊕ O
LE( O
ci O
...ci+N−1), O
e O
re O
i O
=RE( O
ci O
) O
⊕ O
RE(ci−1 O
ci O
) O
⊕ O
• O
• O
• O
⊕ O
RE(ci−N+1... O
ci O
),(5) O
where O
e O
le O
i O
∈ O
R O
b O
, O
e O
re O
i O
∈ O
R O
b O
, O
and O
b O
= O
N O
1 O
is O
the O
number O
of O
integrated O
N-grams. O
Similar O
to O
PMI O
representation, O
the O
position O
of O
each O
N-gram O
in O
e O
le O
i O
and O
e O
le O
i O
is O
fixed O
and O
symmetric. O
Therefore, O
the O
boundary-aware O
representation O
e O
i O
of O
c O
i O
can O
be O
formalized O
as: O
e O
i O
= O
e O
le O
i O
⊕ O
e O
p O
i O
⊕ O
e O
re O
i O
,(6) O
where O
e O
i O
∈ O
R O
a+2b O
. O
Finally, O
by O
composing O
multigranularity O
statistical O
boundary O
information O
in O
a O
specific O
order, O
we O
are O
able O
to O
obtain O
the O
boundaryaware O
representation, O
which O
explicitly O
contains O
the O
boundary O
and O
context O
information. O
Figure O
2 O
shows O
an O
example O
of O
the O
boundaryaware O
representation. O
Given O
a O
sentence O
"南 O
京 O
市 O
长 O
江 O
大 O
桥 O
(Nanjing O
Yangtze O
River O
Bridge)" O
and O
a O
maximum O
N-gram O
length O
N O
= O
3, O
we O
first O
build O
a O
contextual O
N-gram O
set O
for O
the O
character O
"长 O
(Long)". O
Then, O
we O
integrate O
the O
PMI O
of O
all O
N-grams O
in O
a O
specific O
order O
(from O
N-gram O
"长" O
to O
"京市长 O
(Mayor O
of O
Jing)") O
to O
compute O
PMI O
representation. O
Furthermore, O
left O
and O
right O
entropy O
representations O
are O
also O
calculated O
in O
a O
particular O
order O
(from O
Ngram O
"长" O
to O
"长江大 O
(Yangtze O
River O
Big)" O
and O
"京市长", O
respectively). O
Finally, O
we O
concatenate O
the O
above O
features O
to O
produce O
the O
overall O
boundaryaware O
representation O
of O
the O
character O
"长". O
Contextual O
N-grams O
Set O
of O
"长" O
长 O
Long O
长江 O
Yangtze O
River O
+1 O
市长 O
Mayor O
−1 O
京市长 O
Mayor O
of O
Jing O
−2 O
−1 O
长江大 O
Yangtze O
River O
Big O
+1 O
+2 O
PMI O
Representation O
市长江 O
Mayor O
Jiang O
−1 O
+1 O
RE O
Representation O
LE O
Representation O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
⊕ O
Boundary-Aware O
Representation O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Boundary-aware B-MethodName
BERT I-MethodName
is O
a O
variant O
of O
BERT, O
enhanced O
with O
boundary O
information O
simply O
and O
effectively. O
In O
this O
subsection, O
we O
describe O
how O
the O
boundary O
information O
can O
be O
integrated O
into O
BERT O
during O
pre-training O
by O
boundary-aware O
learning. O
Boundary-Aware O
Objective O
As O
mentioned O
in O
Section O
3.2, O
given O
a O
sentence O
x O
with O
characterlength O
n, O
we O
can O
compute O
the O
corresponding O
boundary-aware O
representation O
E O
= O
{e O
1 O
, O
..., O
e O
n O
}. O
Then, O
we O
transfer O
the O
BERT O
feature O
into O
the O
boundary O
information O
space O
and O
approximate O
it O
to O
E O
for O
boundary-aware O
learning. O
Moreover, O
shows O
that O
encoding O
basic O
lexical O
knowledge O
in O
the O
shallow O
BERT O
layers O
is O
a O
more O
effective O
approach. O
Hence, O
we O
use O
the O
hidden O
features O
H O
l O
= O
{h O
l O
1 O
, O
..., O
h O
l O
n O
} O
of O
the O
l-th O
shallow O
layer O
to O
achieve O
the O
boundary-aware O
objective: O
L O
BA O
= O
n O
i O
MSE(W O
B O
h O
l O
i O
, O
e O
i O
),(7) O
where O
MSE(•) O
denotes O
the O
mean O
square O
error O
loss. O
W O
B O
is O
a O
trainable O
matrix O
used O
to O
project O
BERT O
representation O
into O
boundary O
information O
space. O
Previous O
classification-based O
word-level O
masking O
methods O
use O
statistical O
information O
as O
thresholds O
to O
filter O
valid O
words O
for O
masked O
word O
prediction. O
Unlike O
the O
above O
works, O
we O
softly O
utilize O
such O
information O
in O
a O
regression O
manner, O
avoiding O
possible O
errors O
in O
empirically O
filtering O
valid O
tags, O
thereby O
fully O
exploring O
the O
potential O
of O
this O
information. O
Jia O
et O
al. O
(2020) O
and O
Gao O
and O
Callan O
(2021), O
we O
opt O
to O
initialize O
our O
model O
with O
a O
pre-trained O
BERT O
model O
released O
by O
Google O
2 O
and O
randomly O
initialize O
the O
other O
parameters, O
alleviating O
the O
enormous O
cost O
of O
train-ing O
BABERT B-MethodName
from O
scratch. O
In O
particular, O
we O
discard O
the O
next O
sentence O
prediction O
task O
during O
pretraining, O
which O
is O
confirmed O
to O
be O
not O
essential O
for O
the O
pre-trained O
language O
models O
(Lan O
et O
al., O
2020;Liu O
et O
al., O
2019b). O
The O
total O
pre-training O
loss O
of O
BABERT B-MethodName
can O
be O
formalized O
as: O
Pre-training O
Following O
L O
pre O
= O
L O
MLM O
+ O
L O
BA O
,(8) O
where O
L O
MLM O
is O
the O
standard O
objective O
of O
MLM O
task. O
Fine-tuning O
for O
Sequence O
Labeling O
Straightforward O
Fine-tuning O
As O
shown O
in O
Figure O
1 O
(c), O
because O
BABERT B-MethodName
has O
the O
same O
architecture O
as O
BERT, O
we O
can O
adopt O
the O
identical O
procedure O
that O
BERT O
uses O
for O
fine-tuning, O
where O
the O
output O
of O
BABERT O
can O
be O
used O
as O
the O
contextual O
character O
representation O
for O
sequence O
labeling. O
Concretely, O
given O
a O
sequence O
labeling O
dataset O
D O
= O
{(x O
j O
, O
y O
j O
)} O
N O
j=1 O
, O
where O
y O
j O
is O
the O
label O
sequence O
of O
x O
j O
, O
we O
utilize O
the O
output O
of O
BABERT B-MethodName
and O
a O
CRF O
layer O
to O
calculate O
the O
sentence-level O
output O
probability O
p(y O
j O
|x O
j O
), O
which O
is O
exactly O
the O
same O
as O
. O
The O
negative O
log-likelihood O
loss O
for O
training O
can O
be O
defined O
as: O
L O
sq O
= O
− O
N O
j O
log O
p(y O
j O
|x O
j O
),(9) O
At O
the O
inference O
stage, O
we O
use O
the O
Viterbi O
algorithm O
(Viterbi, O
1967) O
to O
generate O
the O
final O
label O
sequence. O
Combining O
with O
Supervised O
Lexicon O
Features O
We O
can O
naturally O
combine O
BABERT B-MethodName
with O
other O
supervised O
lexicon-based O
methods O
because O
of O
the O
unsupervised O
setting O
of O
BABERT B-MethodName
. O
To O
this O
end, O
we O
propose O
a O
lexicon-enhanced B-MethodName
BABERT I-MethodName
( O
BABERT-LE B-MethodName
) O
for O
the O
fine-tuning O
stage, O
which O
utilizes O
the O
lexicon O
adapter O
proposed O
by O
to O
incorporate O
external O
lexicon O
knowledge O
into O
BABERT B-MethodName
feature: O
h O
i O
= O
LA(h O
i O
, O
S O
lex O
i O
),(10) O
where O
LA(•) O
is O
the O
lexicon O
adapter, O
S O
lex O
i O
is O
a O
set O
of O
related O
N-gram O
embeddings O
of O
character O
c O
i O
, O
andĥ O
i O
is O
the O
lexicon-enhanced O
version O
of O
original O
BERT O
feature O
h O
i O
. O
We O
apply O
the O
lexicon O
adapter O
after O
the O
l-th O
layer O
to O
be O
consistent O
with O
boundary-aware O
learning. O
Finally, O
BABERT-LE B-MethodName
performs O
a O
similar O
fine-tuning O
procedure O
as O
BABERT B-MethodName
for O
training: O
L O
lex O
= O
− O
N O
j O
log O
p(yj|xj, O
[S O
lex O
1 O
, O
..., O
S O
lex O
n O
j O
]).(11) O
4 O
Experiments O
Datasets O
Following O
previous O
works O
(Devlin O
et O
al., O
2019;, O
we O
draw O
the O
mixed O
corpus O
of O
Chinese B-DatasetName
Wikipedia I-DatasetName
3 O
and O
Baidu B-DatasetName
Baike I-DatasetName
4 O
as O
our O
pretraining O
corpus, O
which O
contains O
3B O
tokens O
and O
62M O
sentences. O
To O
further O
confirm O
the O
effectiveness O
of O
our O
proposed O
method O
for O
Chinese O
sequence O
labeling, O
we O
evaluate O
BABERT B-MethodName
on O
ten O
benchmark O
datasets O
of O
three O
representative O
tasks: O
Chinese O
Word O
Segmentation O
We O
use O
three O
CWS O
benchmarks O
to O
evaluate O
our O
BABERT. O
Penn B-DatasetName
Chinese I-DatasetName
TreeBank I-DatasetName
version I-DatasetName
6.0 I-DatasetName
( O   
CTB6 B-DatasetName
) O
is O
from O
Xue O
et O
al. O
(2005), O
and O
MSRA B-DatasetName
and O
PKU B-DatasetName
are O
from O
SIGHAN O
2005 O
Bakeoff O
(Emerson, O
2005). O
Part-Of-Speech O
Tagging O
For O
Chinese O
POS O
tagging, O
we O
conduct O
experiments O
on O
CTB6 B-DatasetName
(Xue O
et O
al., O
2005) O
and O
the O
Chinese O
part O
of O
Universal B-DatasetName
Dependencies I-DatasetName
( O
UD B-DatasetName
) O
(Nivre O
et O
al., O
2016). O
The O
UD B-DatasetName
dataset O
uses O
two O
different O
POS O
tagsets, O
which O
are O
universal O
and O
language-specific O
tagsets. O
We O
follow O
Shao O
et O
al. O
(2017), O
referring O
to O
the O
corpus O
with O
the O
two O
tagsets O
as O
UD1 O
and O
UD2, O
respectively. O
Named O
Entity O
Recognition O
For O
the O
Chinese O
NER O
task, O
we O
conduct O
experiments O
on O
OntoNotes B-DatasetName
4.0 I-DatasetName
( O
Onto4 B-DatasetName
) O
(Weischedel O
et O
al., O
2011) O
and O
News B-DatasetName
datasets O
(Jia O
et O
al., O
2020), O
both O
of O
which O
are O
from O
the O
standard O
newswire O
domain. O
Moreover, O
we O
evaluate O
BABERT B-MethodName
in O
the O
internet O
novel O
(Book) O
and O
financial O
report O
(Finance) O
domains O
(Jia O
et O
al., O
2020) O
to O
further O
verify O
the O
robustness O
of O
our O
method. O
The O
statistics O
of O
the O
benchmark O
datasets O
are O
shown O
in O
Table O
1. O
For O
a O
fair O
comparison, O
we O
split O
these O
datasets O
into O
training, O
development, O
and O
test O
sections O
following O
previous O
works O
(Jia O
et O
al., O
2020;. O
Note O
that O
MSRA B-DatasetName
, O
PKU B-DatasetName
, O
and O
Finance B-DatasetName
do O
not O
have O
development O
sections. O
Therefore, O
we O
randomly O
select O
10% O
instances O
from O
the O
training O
set O
as O
the O
development O
set O
for O
these O
datasets. O
Experimental O
Settings O
Hyperparameters O
During O
pre-training, O
we O
use O
the O
hyperparameters O
of O
BERT B-MethodName
BASE I-MethodName
to O
initialize O
BABERT B-MethodName
and O
Adam O
(Kingma O
and O
Ba, O
2014) O
for O
optimizing. O
The O
number O
of O
BERT O
layers B-HyperparameterName
L B-HyperparameterName
is O
12 B-HyperparameterValue
, O
with O
12 B-HyperparameterValue
self-attention B-HyperparameterName
heads I-HyperparameterName
, O
768 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
hidden B-HyperparameterName
states I-HyperparameterName
, O
and O
64 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
each B-HyperparameterName
head I-HyperparameterName
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
1e-4 B-HyperparameterValue
with O
a O
warmup B-HyperparameterName
ratio I-HyperparameterName
of O
0.1 B-HyperparameterValue
, O
and O
the O
max B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
input I-HyperparameterName
sequence I-HyperparameterName
is O
512 B-HyperparameterValue
. O
To O
extract O
unsupervised O
boundary O
information, O
we O
set O
the O
maximum B-HyperparameterName
N-gram I-HyperparameterName
length I-HyperparameterName
N B-HyperparameterName
to O
4 B-HyperparameterValue
5 I-HyperparameterValue
and O
the O
frequency B-HyperparameterName
filtering I-HyperparameterName
threshold I-HyperparameterName
to O
50 B-HyperparameterValue
. O
Then O
we O
use O
the O
3-th O
BERT O
layer O
to O
compute O
boundary-aware O
objective. O
BABERT B-MethodName
has O
no O
extra O
modules, O
which O
is O
why O
the O
parameter O
size O
and O
model O
architecture O
are O
the O
same O
as O
those O
of O
BERT B-MethodName
BASE I-MethodName
. O
Finally, O
we O
train O
the O
BABERT B-MethodName
on O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
with O
32GB O
memory. O
For O
Chinese O
sequence O
labeling, O
we O
empirically O
set O
hyperparameters O
based O
on O
previous O
studies O
(Jia O
et O
al., O
2020; O
and O
preliminary O
experiments. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
32 B-HyperparameterValue
, O
the O
max B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
fixed O
to O
2e-5 B-HyperparameterValue
. O
Baselines O
To O
verify O
the O
effectiveness O
of O
our O
proposed O
BABERT B-MethodName
, O
we O
build O
systems O
on O
the O
following O
methods O
to O
conduct O
fair O
comparisons: O
• O
BERT O
is O
the O
Chinese O
version O
BERT B-MethodName
BASE I-MethodName
model O
released O
by O
Google. O
• O
BERT-wwm B-MethodName
performs O
segmentation O
on O
the O
corpus O
and O
further O
conduct O
word-level O
masking O
in O
pre-training O
(Cui O
et O
al., O
2021). O
• O
ERNIE B-MethodName
is O
an O
extension O
of O
BERT, O
which O
leverages O
external O
lexicons O
for O
word-level O
masking O
. O
• O
ERNIE-Gram B-MethodName
is O
an O
extension O
of O
ERNIE B-MethodName
, O
which O
alleviates O
the O
limitations O
of O
external O
lex- O
icons O
by O
using O
statistical O
information O
for O
entity O
and O
phrase O
extraction O
. O
• O
ZEN B-MethodName
uses O
an O
extra O
N-gram O
encoder O
to O
integrate O
external O
lexicon O
knowledge O
into O
BERT O
during O
pre-training O
(Diao O
et O
al., O
2020). O
• O
NEZHA B-MethodName
leverages O
functional O
relative O
positional O
encoding, O
supervised O
word-level O
masking O
strategy, O
and O
enormous O
training O
data O
6 O
to O
enhance O
vanilla O
BERT O
. O
• O
BERT-LE B-MethodName
is O
a O
lexicon-enhanced O
BERT O
, O
which O
introduces O
a O
lexicon O
adapter O
between O
BERT O
layers O
to O
incorporate O
external O
lexicon O
embeddings. O
We O
strictly O
follow. O
to O
reimplement O
it O
with O
open-source O
word O
embeddings O
7 O
Main O
Results O
The O
overall O
Chinese O
sequence O
labeling O
results O
are O
shown O
in O
Table O
2. O
We O
report O
the O
F1 B-MetricName
-score O
of O
the O
test O
datasets O
on O
CWS B-TaskName
, O
POS B-TaskName
, O
and O
NER B-TaskName
tasks. O
Here, O
we O
first O
compare O
our O
BABERT B-MethodName
with O
various O
Chinese O
pre-trained O
language O
models O
to O
evaluate O
its O
effectiveness. O
Then, O
we O
compare O
BABERT-LE B-MethodName
with O
other O
supervised O
lexicon-based O
methods O
to O
show O
the O
potential O
of O
BABERT B-MethodName
in O
combining O
with O
external O
lexicon O
knowledge. O
6 O
NEZHA B-MethodName
uses O
three O
large O
corpora, O
including O
Chinese B-DatasetName
Wikipedia I-DatasetName
, O
Baidu B-DatasetName
Baike I-DatasetName
, O
and O
Chinese B-DatasetName
News I-DatasetName
, O
which O
contain O
11B O
tokens O
and O
are O
four O
times O
more O
than O
us. O
7 O
https://ai.tencent.com/ailab/nlp/en/embedding.html O
First, O
we O
examine O
the O
F1 B-MetricName
values O
of O
the O
BERT B-MethodName
baseline. O
As O
shown, O
BERT B-MethodName
obtains O
comparable O
results O
on O
all O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks, O
which O
is O
similar O
to O
that O
of O
Diao O
et O
al. O
(2020), O
Tian O
et O
al. O
(2020a) O
and O
. O
BABERT B-MethodName
significantly O
outperforms O
BERT B-MethodName
, O
resulting O
in O
an O
increase O
of O
90.47 O
− O
89.80 O
= O
0.67 B-MetricValue
on O
average. O
This O
observation O
clearly O
indicates O
the O
advantage O
of O
introducing O
boundary O
information O
into O
BERT O
pre-training. O
Compared O
with O
various O
BERT O
extensions, O
our O
BABERT B-MethodName
can O
achieve O
competitive O
performances O
as O
a O
whole. O
First, O
in O
comparison O
with O
BERT-wwm B-MethodName
, O
ERNIE B-MethodName
, O
ERNIE-gram B-MethodName
, O
and O
ZEN B-MethodName
, O
which O
leverage O
external O
lexicons O
that O
include O
high-frequency O
words O
for O
pre-training, O
BABERT B-MethodName
outperforms O
all O
of O
them O
by O
averaging O
0.54+0.41+0.40+0.57 O
4 O
= O
0.48 B-MetricValue
point, O
and O
achieves O
top O
scores O
on O
eight O
of O
the O
ten O
benchmarks. O
This O
result O
is O
consistent O
with O
our O
intuition O
that O
directly O
exploiting O
a O
supervised O
lexicon O
can O
only O
achieve O
good O
performance O
in O
specific O
tasks, O
indicating O
the O
limitation O
of O
these O
methods O
when O
the O
chosen O
lexicon O
is O
incompatible O
with O
the O
target O
tasks. O
Second, O
we O
find O
that O
BABERT B-MethodName
surpasses O
NEZHA B-MethodName
in O
the O
average O
F1 B-MetricName
values, O
indicating O
that O
the O
boundary O
information O
is O
more O
critical O
than O
the O
data O
scale O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Then, O
we O
compare O
our O
method O
with O
supervised O
lexicon-based O
methods. O
Lattice-based O
methods O
(Zhang O
and O
Yang, O
2018;Yang O
et O
al., O
2019) O
2020a,b) O
designs O
an O
external O
memory O
network O
after O
the O
BERT O
encoder O
to O
incorporate O
lexicon O
knowledge. O
EEBERT B-MethodName
(Jia O
et O
al., O
2020) O
builds O
entity O
embeddings O
from O
the O
corpus O
and O
further O
utilizes O
them O
in O
the O
multi-head O
attention O
mechanism. O
The O
results O
are O
shown O
in O
Table O
2 O
(II). O
All O
the O
above O
methods O
lead O
to O
significant O
improvements O
over O
the O
base O
BERT O
model, O
which O
shows O
the O
effectiveness O
of O
external O
lexicon O
knowledge. O
Moreover, O
BABERT B-MethodName
can O
achieve O
comparable O
performance O
with O
the O
above O
methods, O
which O
further O
demonstrates O
the O
potential O
of O
our O
unsupervised O
manner. O
BABERT B-MethodName
learns O
boundary O
information O
from O
unsupervised O
statistical O
features O
with O
vanilla O
BERT, O
which O
means O
it O
has O
excellent O
scalability O
to O
fuse O
with O
other O
BERT-based O
supervised O
lexicon O
models. O
As O
shown, O
we O
can O
see O
that O
our O
BABERT-LE B-MethodName
achieves O
further O
improvements O
and O
state-of-the-art O
performances O
on O
all O
tasks, O
showing O
the O
advantages O
of O
our O
unsupervised O
setting O
and O
boundary-aware O
learning. O
Interestingly, O
compared O
with O
MEM-ZEN B-MethodName
, O
BABERT-LE B-MethodName
has O
larger O
improvements O
over O
their O
corresponding O
baselines. O
One O
reason O
might O
be O
that O
both O
ZEN B-MethodName
and O
the O
memory O
network O
module O
exploits O
supervised O
lexicons, O
which O
leads O
to O
a O
duplication O
of O
introduced O
knowledge. O
Analysis O
In O
this O
subsection, O
we O
conduct O
detailed O
experimental O
analyses O
for O
an O
in-depth O
comprehensive O
understanding O
of O
our O
method. O
Few-Shot O
Setting O
To O
further O
verify O
the O
effectiveness O
of O
BABERT B-MethodName
, O
we O
conduct O
experiments O
under O
the O
few-shot O
setting, O
where O
we O
randomly O
sample O
10, O
50, O
and O
100 O
instances O
of O
the O
original O
training O
data O
from O
PKU B-DatasetName
( O
CWS B-TaskName
) O
and O
Onto4 B-DatasetName
( O
NER B-TaskName
). O
For O
fair O
comparisons, O
we O
compare O
BABERT B-MethodName
with O
the O
pretrained O
language O
models O
without O
external O
supervised O
knowledge. O
The O
results O
are O
presented O
in O
Table O
3. O
As O
the O
size O
of O
training O
data O
is O
reduced, O
the O
adding O
T-test O
does O
not O
bring O
further O
improvements. O
One O
possible O
reason O
is O
that O
the O
T-test O
is O
essentially O
similar O
to O
the O
entropy O
measure O
of O
2-grams, O
which O
has O
already O
been O
injected O
into O
our O
BABERT B-MethodName
model. O
Boundary O
Information O
Encoding O
Layer O
Previous O
works O
(Jawahar O
et O
al., O
2019; O
exploit O
the O
fact O
that O
different O
BERT O
layers O
would O
generate O
different O
concept O
representations. O
The O
shallow O
BERT O
layers O
are O
more O
likely O
to O
capture O
basic O
lexicon O
information, O
while O
the O
top O
layers O
focus O
on O
the O
semantic O
representation. O
We O
empirically O
set O
l O
in O
{1, O
3, O
6, O
12} O
to O
explore O
the O
effect O
of O
computing O
boundary-aware O
loss O
by O
the O
hidden O
features O
H O
l O
of O
different O
BERT O
layers O
on O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Table O
5 O
shows O
the O
results. O
We O
can O
see O
that O
the O
best O
F1 B-MetricName
-score O
can O
be O
achieved O
when O
l O
= O
3 O
on O
all O
datasets, O
which O
indicates O
that O
the O
BABERT B-MethodName
still O
needs O
sufficient O
parameters O
to O
learn O
the O
basic O
boundary O
information. O
Interestingly, O
the O
BABERT B-MethodName
performs O
poorly O
when O
l O
= O
12, O
which O
might O
be O
due O
to O
a O
conflict O
between O
the O
MLM O
loss O
and O
our O
boundary-aware O
regression O
loss O
during O
pretraining. O
Qualitative O
Analysis O
To O
explore O
how O
BABERT B-MethodName
improves O
the O
performance O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
, O
we O
conduct O
qualitative O
analysis O
on O
the O
News B-DatasetName
test O
dataset, O
which O
consists O
of O
four O
different O
subdomains, O
namely O
game O
(GAM), O
entertainment O
(ENT), O
lottery O
(LOT) O
and O
finance O
(FIN). O
The O
results O
are O
shown O
in O
Table O
6. O
We O
can O
see O
that O
compared O
with O
other O
pre-trained O
language O
models, O
BABERT B-MethodName
can O
obtain O
consistent O
improvement O
in O
all O
domains O
with O
unsupervised O
statistical O
boundary O
information, O
while O
the O
other O
models O
only O
improve O
performance O
on O
specific O
domains. O
Moreover, O
as O
shown O
in O
Table O
7, O
we O
also O
give O
an O
example O
from O
the O
game O
domain O
to O
further O
demonstrate O
the O
effectiveness O
of O
our O
method. O
BABERT B-MethodName
is O
the O
only O
model O
that O
correctly O
recognizes O
all O
entities. O
In O
particular, O
the O
prediction O
of O
BABERT B-MethodName
for O
the O
entity O
"WCG2011 O
org O
" O
indicates O
the O
potential O
of O
boundary O
information. O


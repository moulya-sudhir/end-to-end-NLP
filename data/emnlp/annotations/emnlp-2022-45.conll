-DOCSTART-
Crossmodal-3600 B-DatasetName
: O
A O
Massively O
Multilingual O
Multimodal O
Evaluation O
Dataset O

Research O
in O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
has O
been O
severely O
hampered O
by O
a O
lack O
of O
high-quality O
evaluation O
datasets. O
In O
this O
paper O
we O
present O
the O
Crossmodal-3600 B-DatasetName
dataset I-DatasetName
( O
XM3600 B-DatasetName
in O
short O
) O
, O
a O
geographically-diverse O
set O
of O
3600 O
images O
annotated O
with O
humangenerated O
reference O
captions O
in O
36 O
languages. O
The O
images O
were O
selected O
from O
across O
the O
world O
, O
covering O
regions O
where O
the O
36 O
languages O
are O
spoken O
, O
and O
annotated O
with O
captions O
that O
achieve O
consistency O
in O
terms O
of O
style O
across O
all O
languages O
, O
while O
avoiding O
annotation O
artifacts O
due O
to O
direct O
translation. O
We O
apply O
this O
benchmark O
to O
model O
selection O
for O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
and O
show O
strong O
correlation O
results O
with O
human O
evaluations O
when O
using O
XM3600 B-DatasetName
as O
golden O
references O
for O
automatic O
metrics O
. O

Introduction O
Image B-TaskName
captioning I-TaskName
is O
the O
task O
of O
automatically O
generating O
a O
fluent O
natural O
language O
description O
for O
a O
given O
image. O
This O
task O
is O
important O
for O
enabling O
accessibility O
for O
visually O
impaired O
users O
, O
and O
is O
a O
core O
task O
in O
multimodal O
research O
encompassing O
both O
vision O
and O
language O
modeling. O
However O
, O
datasets O
for O
this O
task O
are O
primarily O
available O
in O
English O
( O
Young O
et O
al. O
, O
2014 O
; O
Chen O
et O
al. O
, O
2015 O
; O
Krishna O
et O
al. O
, O
2017 O
; O
Sharma O
et O
al. O
, O
2018 O
; O
. O
Beyond O
English O
, O
there O
are O
a O
few O
datasets O
such O
as O
Multi30K O
with O
captions O
in O
German O
( O
Elliott O
et O
al. O
, O
2016 O
) O
, O
French O
( O
Elliott O
et O
al. O
, O
2017 O
) O
and O
Czech O
( O
Barrault O
et O
al. O
, O
2018 O
) O
, O
but O
they O
are O
limited O
to O
only O
a O
few O
languages O
that O
cover O
a O
small O
fraction O
of O
the O
world O
's O
population O
, O
while O
featuring O
images O
that O
severely O
under-represent O
the O
richness O
and O
diversity O
of O
cultures O
from O
across O
the O
globe. O
These O
aspects O
have O
hindered O
research O
on O
image O
captioning O
for O
a O
wide O
variety O
of O
languages O
, O
and O
directly O
hamper O
deploying O
accessibility O
solutions O
for O
a O
large O
potential O
audience O
around O
the O
world O
. O

Creating O
large O
training O
and O
evaluation O
datasets O
in O
multiple O
languages O
is O
a O
resource-intensive O
endeavor. O
Recent O
works O
( O
Thapliyal O
and O
Soricut O
, O
2020 O
) O
have O
shown O
that O
it O
is O
feasible O
to O
build O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models O
trained O
on O
machine-translated O
data O
( O
with O
English O
captions O
as O
the O
starting O
point O
) O
. O
This O
work O
also O
shows O
that O
the O
effectiveness O
of O
some O
of O
the O
most O
reliable O
automatic O
metrics O
for O
image O
captioning O
, O
such O
as O
CIDEr O
1 O
is O
severely O
diminished O
when O
applied O
to O
translated O
evaluation O
sets O
, O
resulting O
in O
poorer O
agreement O
with O
human O
evaluations O
compared O
to O
the O
English O
case. O
As O
such O
, O
the O
current O
situation O
is O
that O
trustworthy O
model O
evaluation O
can O
only O
be O
based O
on O
extensive O
and O
expensive O
human O
evaluations. O
However O
, O
such O
evaluations O
can O
not O
usually O
be O
replicated O
across O
different O
research O
efforts O
, O
and O
therefore O
do O
not O
offer O
a O
fast O
and O
robust O
mechanism O
for O
model O
hill-climbing O
and O
comparison O
of O
multiple O
lines O
of O
research O
. O

The O
proposed O
XM3600 B-DatasetName
image O
captioning O
evaluation O
dataset O
provides O
a O
robust O
benchmark O
for O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
, O
and O
can O
be O
reliably O
used O
to O
compare O
research O
contributions O
in O
this O
emerging O
field. O
Our O
contributions O
are O
as O
follows O
: O
( O
i O
) O
for O
human O
caption O
annotations O
, O
we O
have O
devised O
a O
protocol O
that O
allows O
annotators O
for O
a O
specific O
target O
language O
to O
produce O
image O
captions O
in O
a O
style O
that O
is O
consistent O
across O
languages O
; O
this O
protocol O
results O
in O
image-caption O
annotations O
that O
are O
free O
of O
direct O
translation O
artefacts O
, O
an O
issue O
that O
has O
plagued O
Machine O
Translation O
research O
for O
many O
years O
and O
is O
now O
well O
understood O
( O
Freitag O
et O
al. O
, O
2020 O
) O
; O
( O
ii O
) O
for O
image O
selection O
, O
we O
have O
devised O
an O
algorithmic O
approach O
to O
sample O
a O
set O
of O
3600 O
geographically-diverse O
images O
from O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O
al. O
, O
2020 O
) O
, O
aimed O
at O
creating O
a O
representative O
set O
of O
images O
from O
across O
the O
world O
; O
( O
iii O
) O
for O
the O
resulting O
XM3600 B-DatasetName
bench-Figure O
1 O
: O
Sample O
captions O
in O
three O
different O
languages O
( O
out O
of O
36 O
-see O
full O
list O
of O
captions O
in O
Appendix O
A O
) O
, O
showcasing O
the O
creation O
of O
annotations O
that O
are O
consistent O
in O
style O
across O
languages O
, O
while O
being O
free O
of O
directtranslation O
artefacts O
( O
e.g. O
the O
Spanish O
" O
number O
42 O
" O
or O
the O
Thai O
" O
convertibles O
" O
would O
not O
be O
possible O
when O
directly O
translating O
from O
the O
English O
versions O
) O
. O
mark O
, O
we O
empirically O
measure O
its O
ability O
to O
rank O
image O
captioning O
model O
variations O
, O
and O
show O
that O
it O
provides O
high O
levels O
of O
agreement O
with O
human O
judgements O
, O
therefore O
validating O
its O
usefulness O
as O
a O
benchmark O
and O
alleviating O
the O
need O
for O
human O
judgement O
in O
the O
future O
. O

Fig. O
1 O
shows O
a O
few O
sample O
captions O
for O
an O
image O
in O
XM3600 B-DatasetName
that O
exemplify O
point O
( O
i O
) O
above O
, O
and O
Fig. O
2 O
shows O
the O
variety O
of O
cultural O
aspects O
captured O
by O
the O
image O
sampling O
approach O
from O
point O
( O
ii O
) O
. O
We O
provide O
detailed O
explanations O
and O
results O
for O
each O
of O
the O
points O
above O
in O
the O
rest O
of O
the O
paper. O
We O
have O
released O
XM3600 B-DatasetName
under O
a O
CC-BY4.0 O
license O
at O
https O
: O
/ O
/ O
google.github.io O
/ O
crossmodal-3600 O
/ O
. O

The O
XM3600 B-DatasetName
Dataset O
In O
this O
section O
, O
we O
describe O
the O
heuristics O
used O
for O
language O
and O
image O
selection O
, O
the O
design O
of O
the O
caption O
annotation O
process O
, O
caption O
statistics O
including O
quality O
, O
and O
annotator O
details O
. O

Language O
Selection O
In O
this O
section O
, O
we O
describe O
the O
heuristic O
used O
for O
selecting O
the O
languages. O
As O
a O
first O
step O
, O
we O
take O
a O
quantitative O
stance O
and O
choose O
30 O
languages O
( O
L30 O
) O
roughly O
based O
on O
their O
percent O
of O
web O
content O
2 O
. O
As O
a O
second O
step O
, O
we O
consider O
an O
additional O
five O
languages O
( O
L5 O
) O
3 O
to O
cover O
low-resource O
languages O
with O
ish O
( O
da O
) O
, O
Dutch O
( O
nl O
) O
, O
Filipino O
( O
fil O
) O
, O
Finnish O
( O
fi O
) O
, O
French O
( O
fr O
) O
, O
German O
( O
de O
) O
, O
Greek O
( O
el O
) O
, O
Hebrew O
( O
he O
) O
, O
Hindi O
( O
hi O
) O
, O
Hungarian O
( O
hu O
) O
, O
Indonesian O
( O
id O
) O
, O
Italian O
( O
it O
) O
, O
Japanese O
( O
ja O
) O
, O
Korean O
( O
ko O
) O
, O
Norwegian O
( O
no O
) O
, O
Persian O
( O
fa O
) O
, O
Polish O
( O
pl O
) O
, O
Portuguese O
( O
pt O
) O
, O
Romanian O
( O
ro O
) O
, O
Russian O
( O
ru O
) O
, O
Spanish O
( O
es O
) O
, O
Swedish O
( O
sv O
) O
, O
Thai O
( O
th O
) O
, O
Turkish O
( O
tr O
) O
, O
Ukrainian O
( O
uk O
) O
, O
Vietnamese O
( O
vi O
) O
. O

3 O
Bengali O
( O
bn O
) O
, O
Cusco O
Quechua O
( O
quz O
) O
, O
Maori O
( O
mi O
) O
, O
Swahili O
( O
sw O
) O
, O
Telugu O
( O
te O
) O
. O

many O
native O
speakers O
, O
or O
major O
native O
languages O
from O
continents O
that O
would O
not O
be O
covered O
otherwise. O
The O
protocol O
for O
caption O
annotation O
( O
Sec. O
2.3 O
) O
has O
been O
applied O
to O
the O
resulting O
union O
of O
languages O
plus O
English O
, O
for O
a O
total O
of O
36 B-HyperparameterValue
languages B-HyperparameterName
. O

Image O
Selection O
In O
this O
section O
, O
we O
consider O
the O
heuristics O
used O
for O
selecting O
a O
geographically O
diverse O
set O
of O
images O
. O

For O
each O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
, O
we O
select O
100 O
images O
that O
, O
as O
far O
as O
it O
is O
possible O
for O
us O
to O
identify O
, O
are O
taken O
in O
an O
area O
where O
the O
given O
language O
is O
spoken. O
The O
images O
are O
selected O
among O
those O
in O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O
al. O
, O
2020 O
) O
that O
have O
GPS O
coordinates O
stored O
in O
their O
EXIF O
metadata. O
Since O
there O
are O
many O
regions O
where O
more O
than O
one O
language O
is O
spoken O
, O
and O
given O
that O
some O
areas O
are O
not O
well O
covered O
by O
Open B-DatasetName
Images I-DatasetName
, O
we O
design O
an O
algorithm O
that O
maximizes O
the O
percentage O
of O
selected O
images O
taken O
in O
an O
area O
in O
which O
the O
assigned O
language O
is O
spoken. O
This O
is O
a O
greedy O
algorithm O
that O
starts O
the O
selection O
of O
images O
by O
the O
languages O
for O
which O
we O
have O
the O
smallest O
pool O
( O
e.g. O
Persian O
) O
and O
processes O
them O
in O
increasing O
order O
of O
their O
candidate O
image O
pool O
size. O
Whenever O
there O
are O
not O
enough O
images O
in O
the O
area O
where O
a O
language O
is O
spoken O
, O
we O
have O
several O
back-off O
levels O
: O
( O
i O
) O
selecting O
from O
a O
country O
where O
the O
language O
is O
spoken O
; O
( O
ii O
) O
a O
continent O
where O
the O
language O
is O
spoken O
, O
and O
, O
as O
last O
resort O
, O
( O
iii O
) O
from O
anywhere O
in O
the O
world O
. O

This O
strategy O
succeeds O
in O
providing O
our O
target O
number O
of O
100 O
images O
from O
an O
appropriate O
region O
for O
most O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
except O
for O
Persian O
( O
where O
14 O
continent-level O
images O
are O
used O
) O
and O
Hindi O
( O
where O
all O
100 O
images O
are O
at O
the O
global O
level O
because O
the O
in-region O
images O
are O
assigned O
to O
Bengali O
and O
Telugu O
) O
. O
We O
keep O
the O
region O
each O
image O
is O
selected O
from O
as O
part O
of O
our O
data O
annotation O
, O
so O
that O
future O
evaluations O
can O
choose O
to O
either O
evaluate O
on O
images O
relevant O
to O
particular O
regions O
of O
interest O
or O
on O
the O
entire O
dataset O
. O

Caption B-TaskName
Annotation I-TaskName
In O
this O
section O
we O
detail O
the O
design O
of O
the O
caption B-TaskName
annotation I-TaskName
process. O
For O
a O
massively O
multilingual O
benchmark O
such O
as O
XM3600 B-DatasetName
, O
consistency O
in O
the O
style O
of O
the O
description O
language O
is O
critical O
, O
since O
language O
can O
serve O
multiple O
communication O
goals. O
For O
a O
more O
in-depth O
discussion O
on O
these O
issues O
as O
they O
relate O
to O
image O
captions O
, O
we O
refer O
the O
reader O
to O
( O
Alikhani O
et O
al. O
, O
2020 O
) O
. O
We O
borrow O
from O
their O
terminology O
, O
as O
it O
identifies O
coherence O
relations O
between O
image O
and O
captions O
such O
as O
VISIBLE O
, O
META O
, O
SUB-JECTIVE O
, O
and O
STORY. O
The O
goal O
for O
our O
caption B-TaskName
annotation I-TaskName
is O
to O
generate O
VISIBLE O
image O
captions O
, O
i.e. O
, O
use O
the O
target O
language O
to O
formulate O
a O
sentence O
that O
is O
intended O
to O
recognizably O
characterize O
what O
is O
visually O
depicted O
in O
the O
image O
. O

One O
possible O
approach O
to O
generating O
such O
captions O
is O
to O
generate O
them O
as O
such O
in O
English O
, O
and O
have O
them O
translated O
( O
automatically O
, O
semiautomatically O
, O
or O
manually O
) O
into O
all O
the O
other O
languages. O
However O
, O
this O
approach O
results O
in O
an O
English-language O
bias O
, O
as O
well O
as O
other O
problems O
that O
have O
been O
already O
identified O
in O
the O
literature. O
For O
instance O
, O
translations O
are O
often O
less O
fluent O
compared O
to O
natural O
target O
sentences O
, O
due O
to O
word O
order O
and O
lexical O
choices O
influenced O
by O
the O
source O
language. O
The O
impact O
of O
this O
phenomenon O
on O
metrics O
and O
modeling O
has O
recently O
received O
increased O
attention O
in O
the O
evaluation O
literature O
( O
Toral O
et O
al. O
, O
2018 O
; O
Zhang O
and O
Toral O
, O
2019 O
; O
Freitag O
et O
al. O
, O
2020 O
) O
, O
and O
references O
created O
in O
this O
style O
are O
thought O
to O
cause O
overlap-based O
metrics O
to O
favor O
model O
outputs O
that O
use O
such O
unnatural O
language O
. O

We O
have O
designed O
our O
caption B-TaskName
annotation I-TaskName
process O
to O
achieve O
two O
main O
goals O
: O
( O
i O
) O
produce O
caption O
annotations O
in O
a O
VISIBLE O
relation O
with O
respect O
to O
the O
image O
content O
, O
and O
, O
strongly O
, O
create O
consistency O
in O
the O
description O
style O
across O
languages O
; O
( O
ii O
) O
be O
free O
of O
translation O
artefacts. O
To O
achieve O
this O
, O
we O
use O
bi-lingual O
annotators O
with O
a O
requirement O
to O
be O
reading-proficient O
in O
English O
and O
fluent O
/ O
native O
in O
the O
target O
language. O
As O
a O
preliminary O
step O
, O
we O
train O
an O
image-captioning O
model O
on O
English-annotated O
data O
, O
which O
results O
in O
captions O
in O
the O
VISIBLE O
style O
of O
COCO-CAP O
( O
Chen O
et O
al. O
, O
2015 O
) O
. O

The O
annotation O
process O
proceeds O
as O
follows. O
Each O
annotation O
session O
is O
done O
over O
batches B-HyperparameterName
of O
N B-HyperparameterName
= O
15 B-HyperparameterValue
images O
, O
using O
the O
images O
selected O
as O
described O
in O
Sec. O
2.2. O
The O
first O
screen O
shows O
the O
N O
images O
with O
their O
captions O
in O
English O
as O
generated O
by O
the O
captioning O
model O
, O
and O
asks O
the O
annotators O
if O
the O
captions O
are O
EXCELLENT O
, O
GOOD O
, O
MEDIUM O
, O
BAD O
, O
or O
there O
is O
NOT-ENOUGH-INFO. O
We O
refer O
to O
this O
rating O
scale O
as O
the O
5-level O
quality O
scale O
in O
the O
subsequent O
text. O
We O
provide O
the O
annotators O
with O
clear O
guidelines O
about O
what O
constitutes O
an O
EXCEL-LENT O
caption O
, O
and O
how O
to O
evaluate O
degradations O
from O
that O
quality. O
This O
step O
forces O
the O
annotators O
to O
carefully O
assess O
caption O
quality O
and O
it O
primes O
them O
into O
internalizing O
the O
style O
of O
the O
captions O
without O
the O
need O
for O
complicated O
and O
lengthy O
annotation O
instructions O
. O

The O
second O
round O
shows O
the O
same O
N O
images O
again O
, O
but O
one O
image O
at O
a O
time O
without O
the O
English O
captions O
, O
and O
the O
annotators O
are O
asked O
to O
produce O
descriptive O
captions O
in O
the O
target O
language O
for O
each O
image. O
In O
the O
absence O
of O
the O
English O
captions O
, O
the O
annotators O
rely O
on O
the O
internalized O
caption O
style O
, O
and O
generate O
their O
annotations O
mostly O
based O
on O
the O
image O
content O
-with O
no O
support O
from O
the O
text O
modality O
, O
other O
than O
potentially O
from O
memory. O
Note O
, O
however O
, O
that O
we O
have O
designed O
the O
system O
to O
support O
N O
annotations O
simultaneously O
, O
and O
we O
have O
empirically O
selected O
the O
value O
of O
N O
as O
to O
be O
large O
enough O
to O
" O
overwrite O
" O
the O
memory O
of O
the O
annotators O
with O
respect O
to O
the O
exact O
textual O
formulation O
of O
the O
English O
captions. O
As O
a O
result O
, O
we O
observe O
that O
the O
produced O
annotations O
are O
free O
of O
translation O
artefacts O
: O
See O
the O
example O
in O
Fig. O
1 O
for O
Spanish O
mentioning O
" O
number O
42 O
" O
, O
and O
for O
Thai O
mentioning O
" O
convertibles O
" O
. O

We O
also O
provide O
the O
annotators O
with O
an O
annotation O
protocol O
to O
use O
when O
creating O
the O
captions O
, O
which O
provides O
useful O
guidance O
in O
achieving O
consistent O
annotations O
across O
all O
the O
targeted O
languages. O
We O
provide O
the O
annotation O
guidelines O
in O
Appendices O
B O
and O
C. O
For O
each O
language O
, O
we O
annotate O
all O
3600 O
images O
with O
captions O
using O
replication O
2 O
( O
two O
different O
annotators O
working O
independently O
) O
4 O
, O
except O
Bengali O
( O
bn O
) O
with O
replication O
1 O
and O
Maori O
( O
mi O
) O
with O
roughly O
1 O
for O
2 O
/ O
3 O
and O
2 O
for O
1 O
/ O
3 O
of O
the O
images O
, O
see O
Table O
1 O
. O

Caption O
Statistics O
In O
this O
section O
, O
we O
take O
a O
look O
at O
the O
the O
basic O
statistics O
of O
the O
captions O
in O
the O
dataset. O
captions O
per O
language O
. O

For O
languages O
with O
natural O
space O
tokenization O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
words I-HyperparameterName
per I-HyperparameterName
caption I-HyperparameterName
can O
be O
as O
low O
as O
5 B-HyperparameterValue
or O
6 B-HyperparameterValue
for O
some O
agglutinative O
languages O
like O
Cusco O
Quechua O
( O
quz O
) O
and O
Czech O
( O
cs O
) O
, O
and O
as O
high O
as O
18 B-HyperparameterValue
for O
an O
analytic O
language O
like O
Vietnamese O
( O
vi O
) O
. O
The O
number O
of O
characters O
per O
caption O
also O
varies O
drastically O
-from O
mid-20s O
for O
Korean O
( O
ko O
) O
to O
mid-90s O
for O
Indonesian O
( O
id O
) O
-depending O
on O
the O
alphabet O
and O
the O
script O
of O
the O
language O
. O

Caption O
Quality O
In O
this O
section O
, O
we O
describe O
the O
process O
for O
ensuring O
the O
creation O
of O
high O
quality O
annotations O
, O
and O
present O
quality O
statistics O
of O
the O
annotations O
produced O
. O

In O
order O
to O
ensure O
quality O
, O
the O
annotation O
process O
is O
initially O
started O
with O
pilot B-HyperparameterName
runs I-HyperparameterName
on O
150 B-HyperparameterValue
images. O
The O
caption O
ratings O
are O
spot O
checked O
by O
the O
authors O
to O
verify O
that O
the O
raters O
have O
a O
good O
understanding O
of O
the O
rating O
scale. O
Further O
, O
the O
generated O
captions O
go O
through O
a O
verification O
round O
where O
they O
are O
rated O
by O
the O
human O
annotators O
on O
the O
5-level O
quality O
scale O
described O
in O
Sec.2.3. O
If O
the O
annotations O
are O
below O
the O
desired O
quality O
, O
we O
clarify O
the O
guidelines O
and O
add O
more O
examples O
to O
provide O
feedback O
to O
the O
human O
annotators O
and O
then O
conduct O
another O
pilot. O
This O
process O
is O
repeated O
until O
very O
few O
low-quality O
captions O
are O
being O
produced O
5 O
. O
After O
this O
, O
for O
every O
language O
, O
we O
run O
the O
main O
annotation O
and O
finally O
a O
verification O
round O
where O
we O
select O
one O
caption O
for O
600 O
randomly O
selected O
images O
and O
have O
the O
annotator O
pool O
( O
per O
language O
) O
rate O
them O
on O
the O
5-level O
quality O
scale O
mentioned O
in O
Sec. O
2.3. O
The O
quality O
scores O
are O
presented O
in O
Table O
2 O
. O

Annotator O
Details O
We O
use O
an O
in-house O
annotation O
platform O
with O
professional O
( O
paid O
) O
annotators O
and O
quality O
assurance. O
Annotators O
are O
chosen O
to O
be O
native O
in O
the O
target O
language O
whenever O
possible O
, O
and O
fluent O
otherwise O
( O
for O
low-resource O
languages O
, O
they O
are O
usually O
linguists O
that O
have O
advanced-level O
knowledge O
of O
that O
language O
) O
. O
All O
annotators O
are O
required O
to O
be O
proficient O
in O
English O
since O
the O
instructions O
and O
guidelines O
are O
given O
in O
English O
. O

Model O
Comparison O
using O
XM3600 B-DatasetName
In O
this O
section O
, O
we O
detail O
our O
experiments O
for O
comparing O
several O
models O
using O
human O
evaluations O
, O
and O
also O
using O
XM3600 B-DatasetName
annotations O
as O
gold O
6 O
references O
for O
automated O
metrics O
. O

For O
model O
comparison O
, O
we O
train O
several O
multilingual O
image O
captioning O
models O
with O
different O
sizes O
over O
different O
datasets O
, O
and O
compare O
them O
on O
XM3600. B-DatasetName
As O
our O
main O
result O
, O
we O
show O
a O
high O
level O
of O
correlation O
between O
model O
rankings O
based O
on O
human-evaluation O
scores O
and O
the O
scores O
obtained O
using O
CIDEr B-MetricName
with O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
. O

Datasets O
We O
build O
two O
multilingual O
datasets O
for O
training O
, O
CC3M-35L B-DatasetName
and O
COCO-35L B-DatasetName
, O
by O
translating O
Conceptual O
Captions O
3M O
( O
Sharma O
et O
al. O
, O
2018 O
) O
and O
COCO O
Captions O
( O
Chen O
et O
al. O
, O
2015 O
) O
to O
the O
other O
34 O
languages O
using O
Google O
's O
machine O
translation O
API O
7 O
. O
The O
remaining O
language O
, O
Cusco O
Quechua O
( O
quz O
) O
, O
is O
not O
supported O
by O
the O
API O
8 O
. O
We O
use O
the O
standard O
train O
and O
validation O
splits O
for O
CC3M O
9 O
. O
For O
COCO O
, O
we O
use O
the O
Karpathy O
split O
( O
Karpathy O
and O
Fei-Fei O
, O
2014 O
) O

Models O
In O
this O
section O
we O
detail O
the O
model O
architecture O
we O
used O
for O
the O
experiments. O
Our O
Transformer-based O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
model O
architecture O
for O
image O
captioning O
is O
shown O
in O
Figure O
3. O
On O
the O
vision O
side O
, O
each O
input O
image O
is O
modeled O
by O
a O
Vision B-MethodName
Transformer I-MethodName
( O
ViT B-MethodName
) O
( O
Dosovitskiy O
et O
al. O
, O
2020 O
; O
Zhai O
et O
al. O
, O
2021 O
) O
. O
The O
visual O
features O
produced O
by O
ViT O
for O
every O
patch O
of O
the O
image O
are O
pooled O
into O
a O
single O
dense O
feature O
vector. O
On O
the O
text O
side O
, O
a O
Language O
Identifier O
( O
LangId O
) O
string O
is O
used O
to O
specify O
the O
language. O
The O
LangId O
string O
is O
tokenized O
and O
embedded O
into O
dense O
token O
embeddings O
, O
which O
are O
merged O
with O
the O
dense O
visual O
embeddings O
as O
the O
input O
to O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Encoder O
, O
followed O
by O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Decoder O
to O
generate O
the O
predicted O
captions O
. O

We O
take O
advantage O
of O
existing O
pretrained O
models O
to O
initialize O
different O
parts O
of O
our O
model O
: O
ViT O
( O
Zhai O
et O
al. O
, O
2021 O
) O
( O
green O
in O
Fig. O
3 O
) O
and O
mT5 O
( O
Xue O
et O
al. O
, O
2021 O
) O
( O
orange O
in O
Fig. O
3 O
) O
. O
We O
consider O
different O
model O
sizes O
: O
mT5-base O
, O
mT-large O
, O
ViT-B O
/ O
16 O
, O
and O
ViT-g O
/ O
14 O
, O
where O
16 O
and O
14 O
are O
the O
corresponding O
patch O
sizes. O
We O
choose O
three O
combinations O
resulting O
in O
three O
different O
model O
architectures O
: O
mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
, O
mT5-base B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
and O
mT5large B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
. O

We O
train O
these O
three O
models O
on O
COCO-35L. B-DatasetName
In O
addition O
, O
we O
consider O
a O
fourth O
model O
based O
on O
mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
and O
trained O
on O
CC3M-35L. B-DatasetName
The O
models O
are O
trained O
on O
a O
4x4x4 O
TPU-v4 O
architecture O
using O
an O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
optimizer O
with O
a O
constant B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
period I-HyperparameterName
between O
{ B-HyperparameterValue
1k I-HyperparameterValue
, I-HyperparameterValue
10k I-HyperparameterValue
} I-HyperparameterValue
steps I-HyperparameterValue
, O
followed O
by O
a O
reversed O
square-root O
decay O
with O
the O
number O
of O
steps. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
2048 B-HyperparameterValue
in O
all O
the O
experiments. O
The O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
is O
between O
{ B-HyperparameterValue
1e-4 I-HyperparameterValue
, I-HyperparameterValue
3e-4 I-HyperparameterValue
} I-HyperparameterValue
. O
We O
use O
the O
same O
vocabulary B-HyperparameterName
( O
size B-HyperparameterValue
250k I-HyperparameterValue
) O
as O
mT5 O
( O
Xue O
et O
al. O
, O
2021 O
) O
. O
The O
model O
trained O
with O
CC3M-35L B-DatasetName
is O
subsequently O
finetuned O
on O
COCO-35L B-DatasetName
with O
constant O
learning B-HyperparameterName
rate I-HyperparameterName
3e-5 B-HyperparameterValue
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
. O

Human O
Evaluation O
In O
this O
section O
, O
we O
detail O
the O
process O
used O
for O
human O
evaluations O
comparing O
the O
performance O
of O
two O
models O
. O

Our O
main O
goal O
in O
creating O
XM3600 B-DatasetName
is O
to O
automate O
the O
evaluation O
of O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
by O
eliminating O
expensive O
and O
timeconsuming O
human O
evaluations. O
Our O
results O
indicate O
that O
they O
can O
be O
substituted O
by O
using O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
for O
automated O
metrics O
such O
as O
CIDEr B-MetricName
. O
To O
quantify O
the O
correlation O
between O
the O
two O
methods O
, O
we O
train O
four O
different O
models O
( O
Tab. O
3 O
) O
and O
conduct O
side-by-side O
human O
evaluations O
using O
the O
outputs O
of O
these O
models O
in O
several O
languages. O
We O
observe O
strong O
correlations O
( O
Sec. O
3.4 O
) O
between O
the O
human O
evaluations O
and O
the O
CIDEr B-MetricName
scores O
using O
the O
XM3600 B-DatasetName
references O
. O

Specifically O
, O
we O
use O
a O
randomly O
selected O
subset O
of O
600 O
images O
from O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
, O
which O
we O
call O
XM600. O
Image O
captions O
generated O
by O
a O
given O
pairing O
of O
models O
( O
m O
1 O
vs O
m O
2 O
, O
where O
m O
1 O
is O
considered O
as O
the O
base O
condition O
and O
m O
2 O
as O
the O
test O
condition O
) O
are O
compared O
and O
rated O
side-by-side O
, O
using O
a O
similar O
pool O
of O
annotators O
as O
described O
in O
Sec. O
2.6. O
Each O
side-by-side O
pair O
( O
shown O
in O
a O
random O
per-example O
left-vs-right O
order O
) O
is O
rated O
using O
a O
7-point O
scale O
: O
MUCH-BETTER O
, O
BETTER O
, O
SLIGHTLY-BETTER O
, O
SIMILAR O
, O
SLIGHTLY-WORSE O
, O
WORSE O
, O
MUCH-WORSE O
, O
with O
a O
replication O
factor O
of O
3 O
( O
three O
annotators O
rate O
each O
pair O
) O
. O
We O
denote O
by O
WINS B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O
raters O
( O
i.e. O
2 O
out O
of O
3 O
) O
mark O
m O
2 O
's O
captions O
as O
better O
, O
and O
by O
LOSSES B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O
raters O
mark O
m O
2 O
's O
captions O
as O
worse. O
We O
then O
define O
the O
overall B-MetricName
side-by-side I-MetricName
gain I-MetricName
of O
m O
2 O
over O
m O
1 O
as O
∆S×S O
= O
WINS O
-LOSSES O
. O

Conducting O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
for O
each O
pair O
of O
models O
over O
the O
35 O
languages O
would O
require O
210 O
human O
evaluation O
sessions. O
This O
is O
prohibitively O
expensive O
and O
time O
consuming. O
Thus O
, O
we O
conduct O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
of O
the O
pairs O
of O
models O
, O
on O
a O
core O
set O
of O
four O
languages O
called O
LCORE O
11 O
. O
We O
call O
this O
set O
of O
24 O
evaluation O
sessions O
OCORE. O
Furthermore O
, O
we O
also O
conduct O
a O
sparser O
set O
of O
side-by-side O
evaluations O
over O
languages O
where O
the O
CIDEr B-MetricName
differences O
on O
XM3600 B-DatasetName
and O
on O
COCO-DEV O
12 O
indicate O
11 O
Chinese-Simplified O
( O
zh O
) O
, O
English O
( O
en O
) O
, O
Hindi O
( O
hi O
) O
, O
Spanish O
( O
es O
) O
12 O
COCO O
validation O
split O
with O
machine-translated O
references O
disagreement O
or O
ambiguity O
( O
e.g. O
, O
opposite O
sign O
of O
the O
CIDEr O
differences O
, O
and O
/ O
or O
small O
CIDEr O
differences O
) O
; O
this O
gives O
us O
a O
set O
of O
28 O
languages O
called O
LEXT O
13 O
. O
We O
call O
the O
resulting O
set O
of O
41 O
evaluation O
sessions O
OEXT. O
The O
set O
of O
all O
evaluations O
is O
called O
OALL O
=OCORE O
+ O
OEXT O
, O
which O
are O
conducted O
over O
the O
languages O
LALL O
= O
LCORE O
+ O
LEXT O
. O

The O
choice O
of O
which O
model O
is O
called O
m O
1 O
and O
which O
model O
is O
called O
m O
2 O
is O
arbitrary O
in O
the O
sideby-side O
evaluations O
, O
since O
we O
randomly O
flip O
left O
vs O
right O
before O
presenting O
the O
captions O
to O
the O
raters. O
Hence O
a O
single O
side-by-side O
evaluation O
gives O
two O
points O
for O
the O
correlation O
calculations O
: O
one O
with O
the O
m O
1 O
and O
m O
2 O
assigned O
as O
per O
the O
actual O
evaluation O
conducted O
, O
and O
one O
more O
with O
the O
m O
1 O
and O
m O
2 O
assignment O
flipped O
and O
the O
∆S×S O
sign O
flipped O
correspondingly O
. O

Results O
We O
present O
results O
that O
show O
that O
it O
is O
feasible O
to O
use O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
with O
automated O
metrics O
such O
as O
CIDEr B-MetricName
to O
compare O
models O
in O
lieu O
of O
human B-MetricName
evaluations I-MetricName
, O
and O
that O
this O
option O
is O
superior O
to O
using O
silver O
references O
created O
via O
automated O
translation. O
Table O
5 O
presents O
the O
results O
for O
the O
OCORE O
set O
of O
evaluations O
on O
XM600 B-DatasetName
on O
the O
LCORE O
languages O
, O
while O
Table O
7 O
in O
the O
appendix O
shows O
the O
results O
on O
the O
LEXT O
languages. O
The O
reference O
for O
the O
relative O
strength O
of O
each O
pairing O
is O
given O
by O
∆S×S O
, O
with O
positive O
numbers O
indicating O
the O
superiority O
of O
m O
2 O
, O
and O
negative O
numbers O
indicating O
a O
superiority O
of O
m O
1 O
. O
As O
can O
be O
seen O
from O
the O
table O
, O
the O
model O
comparisons O
span O
a O
range O
of O
model O
differences O
, O
from O
low O
∆S×S O
to O
high O
∆S×S. O
∆CIDEr O
XM600and B-DatasetName
∆CIDEr O
XM3600 B-DatasetName
capture O
similar O
information O
, O
except O
these O
numbers O
are O
based O
on O
CIDEr O
scores O
using O
as O
references O
XM600 B-DatasetName
and O
XM3600 B-DatasetName
, O
respectively O
, O
while O
∆CIDEr O
COCO-DEV O
is O
based O
on O
machine-translated O
references O
from O
the O
validation O
split O
of O
COCO O
. O

We O
use O
the O
results O
from O
Table O
5 O
( O
and O
Table O
7 O
) O
to O
compute O
the O
correlation O
between O
human O
judgements O
of O
the O
relative O
quality O
of O
the O
captioning O
models O
and O
the O
ability O
of O
the O
CIDEr B-DatasetName
14 O
metric O
-or O
, O
rather O
, O
of O
the O
underlying O
references O
used O
by O
the O
metricto O
perform O
an O
equivalent O
task. O
Table O
6 O
presents O
the O
correlation O
results O
using O
three O
correlation O
metrics O
: O
Pearson B-MetricName
, O
Spearman B-MetricName
, O
and O
Kendall. B-MetricName
The O
first O
section O
shows O
the O
correlations O
over O
all O
the O
side-by-side O
evaluations O
( O
i.e. O
OCORE O
and O
OEXT O
) O
; O
These O
cover O
the O
LCORE O
and O
the O
LEXT O
languages. O
The O
second O
section O
shows O
the O
correlations O
for O
the O
OEXT O
covering O
the O
LEXT O
languages. O
The O
third O
section O
shows O
the O
correlations O
for O
the O
OCORE O
evaluations O
covering O
the O
LCORE O
languages O
. O

We O
observe O
that O
∆CIDEr O
XM3600 B-DatasetName
is O
highly O
correlated O
with O
human O
judgement O
according O
to O
all O
the O
correlation O
metrics O
( O
Bonett O
and O
Wright O
, O
2000 O
) O
, O
over O
all O
the O
evaluations O
OALL O
, O
over O
the O
OCORE O
evaluations O
, O
and O
also O
the O
OEXT O
evaluations. O
Furthermore O
, O
for O
the O
OEXT O
evaluations O
, O
where O
most O
of O
the O
instances O
have O
opposite O
signs O
for O
∆CIDEr O
COCO-DEV O
and O
∆CIDEr O
XM3600 B-DatasetName
, O
we O
find O
that O
the O
former O
is O
strongly O
anti-correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results O
while O
the O
latter O
is O
highly O
correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results. O
Overall O
, O
these O
results O
indicate O
that O
: O
( O
i O
) O
we O
can O
reliably O
substitute O
∆CIDEr O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
on O
XM600 B-DatasetName
when O
comparing O
models O
similar O
to O
the O
ones O
we O
used O
; O
( O
ii O
) O
the O
gold O
XM3600 B-DatasetName
references O
are O
preferable O
over O
the O
silver O
references O
obtained O
from O
translating O
COCO O
captions O
, O
in O
terms O
of O
approximating O
the O
judgements O
of O
the O
human O
evaluators O
15 O
. O

Based O
on O
the O
results O
from O
Table O
6 O
, O
we O
recommend O
the O
use O
of O
the O
XM3600 B-DatasetName
references O
as O
a O
means O
to O
achieve O
high-quality O
automatic O
comparisons O
between O
multilingual O
image O
captioning O
models. O
We O
have O
provided O
the O
CIDEr B-MetricName
scores O
for O
XM3600 B-DatasetName
in O
35 O
languages O
for O
all O
the O
models O
, O
in O
Tables O
8-11 O
in O
the O
Appendix. O
These O
can O
be O
used O
as O
baselines O
in O
future O
work. O
15 O
However O
, O
it O
is O
unclear O
whether O
machine O
translated O
references O
for O
one O
particular O
language O
in O
XM3600 B-DatasetName
translated O
to O
all O
others O
, O
are O
worse O
than O
using O
the O
human O
generated O
references. O
In O
particular O
, O
we O
studied O
the O
correlations O
of O
CIDEr B-MetricName
computed O
using O
XM3600-en-MT B-DatasetName
( O
i.e. O
the O
XM3600 O
English O
references O
, O
machine O
translated O
to O
all O
the O
other O
languages O
) O
, O
with O
the O
human B-MetricName
evaluations. I-MetricName
We O
found O
that O
even O
though O
the O
translations O
have O
artifacts O
and O
disfluencies O
, O
CIDEr B-MetricName
differences O
calculated O
using O
them O
show O
comparable O
correlations O
with O
human O
judgement O
observations. O
We O
also O
studied O
such O
correlations O
for O
machine O
translated O
references O
from O
German O
, O
Greek O
, O
Hebrew O
, O
Hungarian O
and O
Swahili. O
We O
found O
that O
the O
correlations O
are O
similar O
and O
sometimes O
even O
a O
bit O
higher O
than O
using O
the O
human O
generated O
references. O
We O
believe O
this O
happens O
because O
the O
rater O
guidelines O
weigh O
informativeness O
over O
fluency O
and O
the O
CIDEr B-MetricName
metric O
is O
also O
not O
as O
sensitive O
to O
fluency. O
Further O
work O
is O
needed O
to O
understand O
the O
use O
of O
translated O
references O
as O
compared O
to O
human O
generated O
references. O
We O
believe O
that O
using O
the O
human O
generated O
references O
along O
with O
the O
set O
of O
machine O
translated O
references O
from O
all O
the O
other O
languages O
may O
provide O
even O
stronger O
correlations O
and O
show O
greater O
diversity O
in O
the O
coverage O
of O
the O
image O
constituents O
. O

Conclusions O
We O
introduce O
the O
XM3600 B-DatasetName
dataset O
as O
a O
benchmark O
for O
evaluating O
the O
performance O
of O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models. O
The O
images O
in O
the O
dataset O
are O
geographically O
diverse O
, O
covering O
all O
inhabited O
continents O
and O
a O
large O
fraction O
of O
the O
world O
population. O
We O
believe O
this O
benchmark O
has O
the O
potential O
to O
positively O
impact O
both O
the O
research O
and O
the O
applications O
of O
this O
technology O
, O
and O
enable O
( O
among O
other O
things O
) O
better O
accessibility O
for O
visually-impaired O
users O
across O
the O
world O
, O
including O
speakers O
of O
lowresource O
languages O
. O

The O
main O
appeal O
of O
this O
benchmark O
is O
that O
it O
alleviates O
the O
need O
for O
extensive O
human B-MetricName
evaluation I-MetricName
, O
which O
is O
difficult O
to O
achieve O
across O
multiple O
languages O
and O
hinders O
direct O
comparison O
between O
different O
research O
ideas O
and O
results. O
We O
show O
significant O
improvements O
in O
correlation O
with O
human O
judgements O
when O
using O
the O
XM3600 B-DatasetName
dataset O
as O
references O
for O
automatic O
metrics O
, O
and O
therefore O
hope O
that O
the O
adoption O
of O
this O
dataset O
as O
a O
standard O
benchmark O
will O
facilitate O
faster O
progress O
and O
better O
comparisons O
among O
competing O
ideas O
. O

Our O
empirical O
observations O
are O
primarily O
on O
the O
full O
set O
of O
side-by-side O
comparisons O
over O
English O
and O
three O
other O
languages O
( O
Spanish O
, O
Hindi O
, O
Chinese O
) O
. O
Due O
to O
the O
similarity O
in O
the O
data O
collection O
and O
the O
quality O
control O
process O
, O
we O
expect O
similar O
results O
to O
hold O
for O
all O
the O
other O
languages O
as O
well O
; O
we O
validated O
this O
expectation O
with O
additional O
empirical O
observations O
covering O
an O
additional O
28 O
languages O
. O


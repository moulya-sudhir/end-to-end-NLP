Learning B-TaskName
a I-TaskName
Grammar I-TaskName
Inducer I-TaskName
from O
Massive O
Uncurated O
Instructional O
Videos O

Video-aided B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
to O
leverage O
video O
information O
for O
finding O
more O
accurate O
syntactic O
grammars O
for O
accompanying O
text. O
While O
previous O
work O
focuses O
on O
building O
systems O
for O
inducing O
grammars O
on O
text O
that O
are O
well-aligned O
with O
video O
content, O
we O
investigate O
the O
scenario, O
in O
which O
text O
and O
video O
are O
only O
in O
loose O
correspondence. O
Such O
data O
can O
be O
found O
in O
abundance O
online, O
and O
the O
weak O
correspondence O
is O
similar O
to O
the O
indeterminacy O
problem O
studied O
in O
language O
acquisition. O
Furthermore, O
we O
build O
a O
new O
model O
that O
can O
better O
learn O
video-span O
correlation O
without O
manually O
designed O
features O
adopted O
by O
previous O
work. O
Experiments O
show O
that O
our O
model O
trained O
only O
on O
large-scale O
YouTube O
data O
with O
no O
textvideo O
alignment O
reports O
strong O
and O
robust O
performances O
across O
three O
unseen O
datasets, O
despite O
domain O
shift O
and O
noisy O
label O
issues. O
Furthermore O
our O
model O
yields O
higher O
F1 B-MetricName
scores O
than O
the O
previous O
state-of-the-art O
systems O
trained O
on O
in-domain O
data. O

Introduction O
Grammar B-TaskName
induction I-TaskName
is O
a O
fundamental O
and O
longlasting O
(Lari O
and O
Young, O
1990;Clark, O
2001;Klein O
and O
Manning, O
2002) O
problem O
in O
computational O
linguistics, O
which O
aims O
to O
find O
hierarchical O
syntactic O
structures O
from O
plain O
sentences. O
Unlike O
supervised O
methods O
(Charniak, O
2000;Collins, O
2003;Petrov O
and O
Klein, O
2007;Zhang O
and O
Clark, O
2011;Cross O
and O
Huang, O
2016;Kitaev O
and O
Klein, O
2018) O
that O
require O
human O
annotated O
treebanks, O
e.g., O
Penn B-MethodName
Treebank I-MethodName
(Marcus O
et O
al., O
1993), O
grammar O
inducers O
do O
not O
rely O
on O
any O
human O
annotations O
for O
training. O
Grammar B-TaskName
induction I-TaskName
is O
attractive O
since O
annotating O
syntactic O
trees O
by O
human O
language O
experts O
is O
expensive O
and O
time O
consuming, O
while O
the O
current O
treebanks O
are O
limited O
to O
several O
major O
languages O
and O
domains. O

Recently, O
deep O
learning O
models O
have O
achieved O
remarkable O
success O
across O
NLP O
tasks, O
and O
neural O
models O
have O
been O
designed O
(Shen O
et O
al., O
2018b,a;Kim O
et O
al., O
2019a,b;Jin O
et O
al., O
2018) O
for O
grammar B-TaskName
induction I-TaskName
, O
which O
greatly O
advanced O
model O
performance O
on O
induction O
with O
raw O
text. O
Recent O
efforts O
have O
started O
to O
consider O
other O
useful O
information O
from O
multiple O
modalities, O
such O
as O
images O
(Shi O
et O
al., O
2019;Jin O
and O
Schuler, O
2020) O
and O
videos O
(Zhang O
et O
al., O
2021). O
Specifically, O
Zhang O
et O
al. O
(2021) O
show O
that O
multi-modal O
information O
(e.g. O
motion, O
sound O
and O
objects) O
from O
videos O
can O
significantly O
improve O
the O
induction O
accuracy O
on O
verb O
and O
noun O
phrases. O
Such O
work O
uses O
curated O
multi-modal O
data O
publicly O
available O
on O
the O
web, O
which O
all O
assume O
that O
the O
meaning O
of O
a O
sentence O
needs O
to O
be O
identical O
(e.g., O
being O
a O
caption) O
to O
the O
corresponding O
video O
or O
image. O
This O
assumption O
limits O
usable O
data O
to O
several O
small-scale O
benchmarks O
(Lin O
et O
al., O
2014;Xu O
et O
al., O
2016;Hendricks O
et O
al., O
2017) O
with O
expensive O
human O
annotations O
on O
image/video O
captions. O

The O
noisy O
correspondence O
between O
form O
and O
meaning O
is O
one O
of O
the O
main O
research O
questions O
in O
language O
acquisition O
(Akhtar O
and O
Montague, O
1999;Gentner O
et O
al., O
2001;Dominey O
and O
Dodane, O
2004), O
where O
different O
proposals O
attempt O
to O
address O
this O
indeterminacy O
faced O
by O
children. O
There O
has O
been O
computational O
work O
incorporating O
such O
indeterminacy O
into O
their O
models O
(Yu O
and O
Siskind, O
2013;Huang O
et O
al., O
2021). O
For O
modeling O
empirical O
grammar O
learning O
with O
multi-modal O
inputs, O
two O
important O
questions O
still O
remain O
open: O
1) O
how O
can O
a O
grammar O
inducer O
benefit O
from O
large-scale O
multi-media O
data O
(e.g., O
YouTube O
videos) O
with O
noisy O
text-to-video O
correspondence? O
and O
2) O
how O
can O
a O
grammar O
inducer O
show O
robust O
performances O
across O
multiple O
domains O
and O
datasets? O
By O
using O
data O
with O
only O
weak O
cross-modal O
correspondence, O
such O
as O
YouTube O
videos O
and O
their O
automatically O
generated O
subtitles, O
we O
allow O
the O
computational O
models O
to O
face O
a O
similar O
indeterminacy O
problem, O
and O
exam-ine O
how O
indeterminacy O
interacts O
with O
data O
size O
to O
influence O
learning O
behavior O
and O
performance O
of O
the O
induction O
models. O

In O
this O
paper, O
we O
conduct O
the O
first O
investigation O
on O
both O
questions. O
Specifically, O
we O
collect O
2.4 O
million O
video O
clips O
and O
the O
corresponding O
subtitles O
from O
instructional O
YouTube O
videos O
( O
HowTo100M B-DatasetName
Miech O
et O
al. O
2019) O
to O
train O
multi-modal O
grammar O
inducers, O
instead O
of O
using O
the O
training O
data O
from O
a O
benchmark O
where O
text O
and O
video O
are O
in O
alignment. O
We O
then O
propose O
a O
novel O
model, O
named O
Pre-Trained B-MethodName
Compound I-MethodName
Probabilistic I-MethodName
Context-Free I-MethodName
Grammars I-MethodName
( O
PTC-PCFG B-MethodName
), O
that O
extends O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
incorporating O
a O
videospan O
matching O
loss O
term O
into O
the O
Compound B-MethodName
PCFG I-MethodName
(Kim O
et O
al., O
2019a) O
model. O
To O
better O
capture O
the O
video-span O
correlation, O
it O
leverages O
CLIP O
(Miech O
et O
al., O
2020), O
a O
state-of-the-art O
model O
pretrained O
on O
video O
subtitle O
retrieval, O
as O
the O
encoders O
for O
both O
video O
and O
text. O
Compared O
with O
previous O
work O
(Zhang O
et O
al., O
2021) O
that O
independently O
extracts O
features O
from O
each O
modality O
before O
merging O
them O
using O
a O
simple O
Transformer O
(Vaswani O
et O
al., O
2017) O
encoder, O
the O
encoders O
of O
our O
model O
have O
been O
pretrained O
to O
merge O
such O
multi-modal O
information, O
and O
no O
human O
efforts O
are O
needed O
to O
select O
useful O
modalities O
from O
the O
full O
set. O

Experiments O
on O
three O
benchmarks O
show O
that O
our O
model, O
which O
is O
trained O
on O
noisy O
YouTube O
video O
clips O
and O
no O
data O
from O
these O
benchmarks, O
produces O
substantial O
gains O
over O
the O
previous O
state-of-the-art O
system O
(Zhang O
et O
al., O
2021) O
trained O
on O
in-domain O
video O
clips O
with O
human O
annotated O
captions. O
Furthermore, O
our O
model O
demonstrates O
robust O
performances O
across O
all O
three O
datasets. O
We O
suggest O
the O
limitations O
of O
our O
model O
and O
future O
directions O
for O
improvements O
through O
analysis O
and O
discussions. O
Code O
will O
be O
released O
upon O
paper O
acceptance. O

In O
summary, O
the O
main O
contributions O
are: O

• O
We O
are O
the O
first O
to O
study O
training O
a O
grammar O
inducer O
with O
massive O
general-domain O
noisy O
video O
clips O
instead O
of O
benchmark O
data, O
introducing O
the O
indeterminacy O
problem O
to O
the O
induction O
model. O

• O
We O
propose O
PTC-PCFG B-MethodName
, O
a O
novel O
model O
for O
unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
It O
is O
simpler O
in O
design O
than O
previous O
models O
and O
can O
better O
capture O
the O
video-text O
matching O
information. O

• O
Trained O
only O
on O
noisy O
YouTube O
videos O
without O
finetuning O
on O
benchmark O
data, O
PTC-PCFG B-MethodName
reports O
stronger O
performances O
than O
previous O
mod-els O
trained O
on O
benchmark O
data O
across O
three O
benchmarks. O

Background O
and O
Motivation O

Compound B-MethodName
PCFGs I-MethodName
A O
PCFG B-MethodName
model O
in O
Chomsky O
Normal O
Form O
can O
be O
defined O
as O
a O
tuple O
of O
6 O
terms O
(S, O
N O
, O
P, O
Σ, O
R, O
Π), O
where O
they O
correspond O
to O
the O
start O
symbol, O
the O
sets O
of O
non-terminals, O
pre-terminals, O
terminals, O
production O
rules O
and O
their O
probabilities. O
Given O
pre-defined O
numbers O
of O
non-terminals O
and O
pre-terminals, O
a O
PCFG B-MethodName
induction O
model O
tries O
to O
estimate O
the O
probabilities O
for O
all O
production O
rules. O
The O
compound B-MethodName
PCFG I-MethodName
( O
C-PCFG B-MethodName
) O
model O
(Kim O
et O
al., O
2019a) O
adopts O
a O
mixture O
of O
PCFGs. O
Instead O
of O
a O
corpus-level O
prior O
used O
in O
previous O
work O
(Kurihara O
and O
Sato, O
2006;Johnson O
et O
al., O
2007;Wang O
and O
Blunsom, O
2013;Jin O
et O
al., O
2018), O
C-PCFG B-MethodName
imposes O
a O
sentence-specific O
prior O
on O
the O
distribution O
of O
possible O
PCFGs. O
Specifically O
in O
the O
generative O
story, O
the O
probability O
π O
r O
for O
production O
rule O
r O
is O
estimated O
by O
model O
g O
that O
assigns O
a O
latent O
variable O
z O
for O
each O
sentence O
σ, O
and O
z O
is O
drawn O
from O
a O
prior O
distribution: O

π O
r O
= O
g(r, O
z; O
θ), O
z O
∼ O
p(z). O

( O

)1 O

where O
θ O
represents O
the O
model O
parameters. O
The O
probabilities O
for O
all O
three O
types O
of O
CFG O
rules O
are O
defined O
as O
follows: O

π O
S→A O
= O
exp(u O
⊤ O
A O
f O
s O
([w O
S O
; O
z])) O
A O
′ O
∈N O
exp(u O
⊤ O
A O
′ O
f O
s O
([w O
S O
; O
z])) O
, O
π O
A→BC O
= O
exp(u O
⊤ O
BC O
[w O
A O
; O
z]) O
B O
′ O
,C O
′ O
∈N O
∪P O
exp(u O
⊤ O
B O
′ O
C O
′ O
[w O
A O
; O
z])) O
, O
π O
T O
→w O
= O
exp(u O
⊤ O
w O
f O
t O
([w O
T O
; O
z])) O
w O
′ O
∈Σ O
exp(u O
⊤ O
w O
′ O
f O
t O
([w O
T O
; O
z])) O
,(2) O

where O
A O
∈ O
N O
, O
B O
and O
C O
∈ O
N O
∪ O
P, O
T O
∈ O
P, O
w O
∈ O
Σ. O
Both O
w O
and O
u O
are O
dense O
vectors O
representing O
words O
and O
all O
types O
of O
non-terminals, O
and O
f O
s O
and O
f O
t O
are O
neural O
encoding O
functions. O

Optimizing O
the O
C-PCFG B-MethodName
model O
involves O
maximizing O
the O
marginal O
likelihood O
p(σ) O
of O
each O
training O
sentence O
σ O
for O
all O
possible O
z: O

log O
p O
θ O
(σ) O
= O
log O
z O
t∈T O
G O
(σ) O
p O
θ O
(t|z)p(z)dz O
(3) O

where O
T O
G O
(σ) O
indicates O
all O
possible O
parsing O
trees O
for O
sentence O
σ. O
Since O
computing O
the O
integral O
over O
z O
is O
intractable, O
this O
objective O
is O
optimized O
by O
maximizing O
its O
evidence O
lower O
bound O
ELBO(σ; O
ϕ, O
θ): O

ELBO(σ; O
ϕ, O
θ) O
= O
E O
q O
ϕ O
(z|σ) O
[log O
p O
θ O
(σ|z)] O
−KL[q O
ϕ O
(z|σ)||p(z)],(4) O

where O
q O
ϕ O
(z|σ) O
is O
the O
variational O
posterior O
calculated O
by O
another O
neural O
network O
with O
parameters O
ϕ. O
Given O
a O
sampled O
z, O
the O
log-likelihood O
term O
log O
p O
θ O
(σ|z) O
is O
calculated O
via O
the O
inside O
algorithm. O

The O
KL O
term O
can O
be O
computed O
analytically O
when O
both O
the O
prior O
p(z) O
and O
the O
variational O
posterior O
q O
ϕ O
(z|σ) O
are O
Gaussian O
(Kingma O
and O
Welling, O
2014). O

Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
MMC-PCFG B-MethodName
) O
(Zhang O
et O
al., O
2021) O
extends O
C-PCFG B-MethodName
with O
a O
model O
to O
match O
a O
video O
v O
with O
a O
span O
c O
in O
a O
parse O
tree O
t O
of O
a O
sentence O
σ. O
It O
extracts O
M O
visual O
and O
audio O
features O
from O
a O
video O
v O
and O
encodes O
them O
via O
a O
multi-modal O
transformer O
(Gabeur O
et O
al., O
2020), O
denoted O
as O
Ψ O
= O
{ψ O
i O
} O
M O
i=1 O
. O
The O
word O
representation O
h O
i O
of O
the O
ith O
word O
is O
computed O
by O
BiLSTM. O
Given O
a O
particular O
span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
, O
its O
representation O
c O
is O
the O
weighted O
sum O
of O
all O
label-specific O
span O
representations: O

c O
= O
|N O
| O
k=1 O
p(k|c, O
σ)f O
k O
1 O
j O
− O
i O
+ O
1 O
j O
l=i O
h O
l O
, O
(5) O

where O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|} O
are O
the O
phrasal O
label O
probabilities O
of O
span O
c. O
The O
representation O
of O
a O
span O
c O
is O
then O
correspondingly O
projected O
to O
M O
separate O
embeddings O
via O
gated O
embedding O
(Miech O
et O
al., O
2018), O
denoted O
as O
Ξ O
= O
{ξ O
i O
} O
M O
i=1 O
. O
Finally O
the O
video-text O
matching O
loss O
is O
defined O
as O
a O
sum O
over O
all O
video-span O
matching O
losses O
weighted O
by O
the O
marginal O
probability O
of O
a O
span O
from O
the O
parser: O

s O
mm O
(v, O
σ) O
= O
c∈σ O
p(c|σ)h O
mm O
(Ξ, O
Ψ), O
(6) O

where O
h O
mm O
(Ξ, O
Ψ) O
is O
a O
hinge O
loss O
measuring O
the O
distances O
from O
video O
v O
to O
the O
matched O
and O
unmatched O
(i.e. O
span O
from O
another O
sentence) O
span O
c O
and O
c O
′ O
and O
the O
distances O
from O
span O
c O
to O
the O
matched O
and O
unmatched O
(i.e. O
another O
video) O
video O
v O
and O
v O
′ O
: O

ω O
i O
(c) O
= O
exp(u O
⊤ O
i O
c) O
M O
j=1 O
exp(u O
⊤ O
j O
c) O
,(7) O

o(Ξ,Ψ) O
= O
M O
i=1 O
ω O
i O
(c)cos(ξ O
i O
, O
ψ O
i O
),(8) O

h O
mm O
(Ξ,Ψ) O
= O
E O
c O
′ O
[o(Ξ O
′ O
, O
Ψ) O
− O
o(Ξ, O
Ψ)) O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[o(Ξ, O
Ψ O
′ O
) O
− O
o(Ξ, O
Ψ) O
+ O
ϵ] O
+ O
, O
(9) O

where O
Ξ O
′ O
is O
a O
set O
of O
unmatched O
span O
expert O
embeddings O
of O
Ψ, O
Ψ O
′ O
is O
a O
set O
of O
unmatched O
video O
representations O
of O
Ξ, O
ϵ O
is O
a O
positive O
margin, O
[•] O
+ O
= O
max(0, O
•), O
{u O
i O
} O
M O
i=1 O
are O
learned O
weights, O
and O
the O
expectations O
are O
approximated O
with O
one O
sample O
drawn O
from O
the O
training O
data. O
During O
training, O
both O
ELBO O
and O
the O
video-text O
matching O
loss O
are O
jointly O
optimized. O

Limitation O
and O
Motivation O
Existing O
work O
on O
multi-modal B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
at O
leveraging O
strict O
correspondence O
between O
image/video O
and O
text O
for O
information O
about O
syntactic O
categories O
and O
structures O
of O
the O
words O
and O
spans O
in O
the O
text. O
However, O
such O
datasets O
are O
expensive O
to O
annotate. O
Besides, O
the O
ambiguous O
correspondence O
between O
language O
and O
real-world O
context, O
observed O
in O
language O
acquisition, O
is O
not O
really O
reflected O
in O
such O
training O
setups. O

As O
a O
result, O
we O
believe O
that O
the O
previous O
work O
fails O
to O
answer O
the O
following O
important O
questions: O
1) O
how O
well O
a O
grammar O
inducer O
would O
perform O
when O
it O
is O
trained O
only O
on O
noisy O
multi-media O
data; O
2) O
how O
the O
scale O
of O
training O
data O
would O
affect O
the O
performance O
and O
cross-domain O
robustness? O

Training O
a O
Grammar O
Inducer O
with O
Massive O
YouTube O
Videos O
We O
make O
the O
first O
investigation O
into O
the O
above O
questions O
by O
leveraging O
massive O
video O
clips O
from O
instructional O
YouTube O
videos O
to O
train O
our O
grammar O
inducer. O
Different O
from O
the O
benchmark O
data O
used O
by O
previous O
work, O
the O
YouTube O
video O
clips O
do O
not O
contain O
paired O
sentences. O
This O
section O
will O
first O
introduce O
the O
method O
for O
generating O
noisy O
training O
instances O
(video O
clip O
and O
sentence O
pairs) O
from O
YouTube O
videos O
( O
§3.1), O
before O
describing O
a O
novel O
grammar O
induction O
model O
( O
§3.2) O
with O
pre-trained O
text O
and O
video O
encoders. O

Harvesting O
Training O
Instances O
from O
YouTube O
Videos O
Given O
a O
YouTube O
video, O
we O
would O
like O
to O
generate O
a O
set O
of O
video O
clip O
and O
subtitle O
pairs O
Ω O
= O
{(v, O
σ)}, O
where O
each O
subtitle O
σ O
is O
a O
complete O
sentence O
and O
is O
aligned O
in O
time O
with O
its O
paired O
video O
clip O
v. O
To O
this O
end, O
the O
YouTube O
API O
is O
chosen O
to O
obtain O
all O
subtitles O
of O
the O
video. O
But, O
our O
observation O
finds O
that O
most O
obtained O
subtitles O
are O
not O
complete O
sentences, O
and O
in O
some O
cases, O
a O
complete O
sentence O
can O
last O
for O
several O
continuous O
video O
fragments. O
Meanwhile, O
they O
do O
not O
contain O
any O
punctuation, O
which O
is O
a O
key O
factor O
for O
sentence O
segmentation. O
As O
shown O
in O
the O
top O
part O
of O
Figure O
1, O
we O
design O
an O
algorithm O
that O
takes O
the O
following O
steps O
to O
find O
each O
complete O
sentence O
and O
its O
corresponding O
video O
clip. O
Sentence B-TaskName
segmentation I-TaskName
. O
In O
the O
first O
step, O
we O
try O
to O
find O
complete O
sentences O
from O
the O
subtitles. O
We O
first O
concatenate O
all O
subtitles O
from O
the O
same O
video O
are O
concatenated O
into O
a O
very O
long O
sequence O
of O
tokens. O
Next, O
a O
punctuation O
restoration O
model O
1 O
(Tilk O
and O
Alumäe, O
2016) O
is O
adopted O
to O
insert O
punctuation O
into O
the O
sequence. O
Lastly, O
sentences O
are O
segmented O
based O
on O
certain O
punctuation O
(e.g., O
".", O
"?", O
"!"). O

Video B-TaskName
clip I-TaskName
extraction I-TaskName
. O
In O
the O
second O
step, O
we O
trim O
the O
corresponding O
video O
clips. O
Each O
raw O
subtitle O
contains O
its O
start O
and O
an O
end O
times. O
We O
assume O
each O
word O
within O
the O
raw O
subtitle O
occupies O
equal O
time O
and O
record O
the O
start O
and O
end O
times O
for O
1 O
We O
manually O
punctuate O
subtitles O
from O
10 O
videos O
randomly O
selected O
from O
HowTo100M B-DatasetName
, O
which O
contains O
461 O
sentences O
after O
annotation. O
The O
punctuation B-MethodName
restoration I-MethodName
model I-MethodName
has O
an O
overall O
F1 B-MetricName
score O
of O
74.1% B-MetricValue
with O
the O
manual O
labels. O
each O
word. O
After O
that, O
given O
a O
complete O
sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
..., O
w O
N O
, O
we O
use O
the O
start O
time O
of O
its O
first O
word O
w O
1 O
and O
the O
end O
time O
of O
its O
last O
word O
w O
N O
as O
the O
start O
and O
end O
times O
of O
σ. O
Lastly, O
we O
segment O
a O
complete O
sentence O
σ's O
corresponding O
video O
clip O
v O
based O
on O
its O
start O
and O
end O
times. O

Model: O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
After O
harvesting O
large-scale O
sentence O
and O
video O
pairs, O
the O
next O
step O
is O
to O
build O
a O
strong O
grammar O
induction O
model O
that O
can O
benefit O
from O
them. O
In O
this O
section, O
we O
introduce O
our O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
PTC-PCFG B-MethodName
) O
model O
for O
unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
As O
shown O
in O
the O
lower O
part O
of O
Figure O
1, O
the O
PTC-PCFG B-MethodName
model O
composes O
of O
a O
video O
encoder, O
a O
span O
encoder O
and O
a O
parsing O
model. O
Both O
the O
video O
encoder O
and O
the O
span O
encoder O
are O
initialized O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020), O
a O
pre-trained O
video-text O
matching O
model O
that O
takes O
a O
simple O
design O
and O
has O
shown O
superior O
zero-shot O
results O
on O
many O
video O
understanding O
tasks, O
such O
as O
video O
retrieval, O
video O
question O
answering, O
etc. O
We O
first O
introduce O
the O
pre-trained O
video O
and O
span O
encoders, O
before O
covering O
the O
training O
and O
inference O
details O
of O
PTC-PCFG B-MethodName
. O
Video O
encoding. O
The O
first O
step O
is O
to O
encode O
a O
video O
v O
to O
its O
representation O
v. O
To O
do O
this, O
we O
first O
segment O
v O
into O
small O
video O
clips, O
where O
each O
video O
clip O
v O
i O
consists O
of O
T O
frames. O
Following O
Zhang O
et O
al. O
(2021), O
we O
sample O
L O
video O
clips O
with O
equal O
interval O
for O
efficiency. O
We O
use O
the O
video O
encoder O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020) O
as O
our O
video O
encoder O
and O
only O
fine-tune O
its O
last O
fully O
connected O
layer O
f O
v O
for O
efficiency. O
In O
more O
detail, O
for O
each O
sampled O
video O
clip, O
we O
pre-compute O
the O
input O
of O
f O
v O
as O
its O
representation, O
denoted O
as O
{h O
v O
i O
} O
L O
i=1 O
. O
Then O
we O
feed O
them O
into O
f O
v O
and O
average O
the O
output O
as O
its O
representation O
v, O
denoted O
as, O

v O
= O
AvgPool({f O
v O
(h O
v O
i O
)} O
L O
i=1 O
),(10) O

where O
AvgPool O
indicates O
average O
pooling. O
Span O
encoding. O
The O
next O
step O
is O
to O
compute O
a O
span O
representation O
c O
for O
each O
particular O
span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
(1 O
≤ O
i O
< O
j O
≤ O
N O
) O
in O
sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
N O
. O
The O
pre-trained O
text O
encoder O
of O
MIL-NCE B-MethodName
consists O
of O
a O
word O
embedding O
layer O
and O
two O
stacked O
fully O
connected O
layers, O
f O
c O
0 O
and O
f O
c O
1 O
. O
Motivated O
by O
Zhao O
and O
Titov O
(2020); O
Zhang O
et O
al. O
(2021), O
we O
expect O
to O
learn O

|N O
| O
different O
span O
representations, O
each O
is O
specified O
for O
one O
non-terminal O
node. O
However, O
directly O
applying O
the O
pre-trained O
text O
encoder O
is O
not O
feasible, O
since O
it O
has O
only O
one O
output O
layer O
f O
c O
1 O
. O
Therefore, O
we O
duplicate O
f O
c O
1 O
for O
|N O
| O
times, O
denoted O
as O
{f O
c O
k O
} O
|N O
| O

k=1 O
, O
and O
compose O
|N O
| O
label-specific O
output O
layers. O
In O
more O
detail, O
we O
first O
encode O
each O
word O
w O
i O
with O
the O
word O
embedding O
layer, O
denoted O
as O
h O
c O
i O
. O
Then O
we O
feed O
the O
word O
embeddings O
to O
f O
c O
0 O
, O
ReLU, O
maximum O
pooling O
and O
each O
label-specific O
output O
layer O
sequentially. O
we O
also O
compute O
the O
probabilities O
of O
its O
phrasal O
labels O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|}, O
as O
illustrated O
in O
Section O
2.1. O
Lastly, O
the O
span O
representation O
c O
is O
the O
sum O
of O
all O
label-specific O
span O
representations O
weighted O
by O
the O
probabilities O
we O
predicted, O
denoted O
as: O

τ O
= O
MaxPool(ReLU(f O
c O
0 O
(h O
c O
i O
))) O
c O
= O
|N O
| O
k=1 O
p(k|c, O
σ)f O
c O
k O
(τ O
),(11) O

where O
MaxPool O
is O
a O
maximum O
pooling O
operation O
and O
ReLU O
is O
a O
ReLU O
activation O
function. O

Training. O
As O
shown O
in O
lower O
left O
of O
Figure O
1, O
we O
optimize O
both O
the O
video-text O
matching O
loss O
and O
evidence O
lower O
bound O
during O
training. O
We O
first O
compute O
the O
similarity O
between O
a O
video O
clip O
v O
and O
a O
particular O
span O
c O
via O
dot O
product O
and O
then O
compute O
a O
triplet B-MetricName
hinge I-MetricName
loss I-MetricName
as O
following, O

h(v, O
c) O
= O
E O
c O
′ O
[c O
′ O
• O
v O
− O
c O
• O
v O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[c O
• O
v O
′ O
− O
c O
• O
v O
+ O
ϵ] O
+ O
, O
(12 O
) O

where O
ϵ O
is O
a O
positive O
margin, O

[•] O
+ O
= O
max(0, O
•), O
v O
′ O

is O
a O
clip O
from O
a O
different O
video O
and O
c O
′ O
is O
a O
span O
from O
a O
different O
sentence. O
The O
video-text B-MetricName
matching I-MetricName
loss I-MetricName
is O
correspondingly O
defined O
as, O

s(v, O
σ) O
= O
Σ O
c∈σ O
p(c|σ)h(v, O
c),(13) O

where O
p(c|σ) O
is O
the O
probability O
of O
a O
particular O
span O
c O
being O
a O
syntactic O
phrase. O
Finally, O
the O
overall O
loss O
function O
is O
composed O
by O
the O
ELBO O
and O
the O
videotext B-MetricName
matching I-MetricName
loss I-MetricName
: O

L(ϕ, O
θ) O
= O
(v,σ)∈Ω O
−ELBO(σ; O
ϕ, O
θ) O
+ O
αs(v, O
σ),(14) O

where O
α O
is O
a O
constant O
balancing O
these O
two O
terms. O
Inference. O
During O
inference, O
given O
a O
sentence O
σ, O
we O
predict O
the O
most O
likely O
tree O

t O
* O
without O
accessing O
videos, O
as O
shown O
in O
the O
lower O
right O
of O
Figure O
1. O
Since O
computing O
the O
integral O
over O
z O
is O
intractable, O
we O
estimate O
t O
* O
with O
the O
following O
approximation, O
t O
* O
= O
arg O
max O
t O
z O
p O
θ O
(t|z)p O
θ O
(z|σ)dz O
≈ O
arg O
max O
t O
p O
θ O
(t|σ, O
µ O
ϕ O
(σ)),(15) O

where O
µ O
ϕ O
(σ) O
is O
the O
mean O
vector O
of O
the O
variational O
posterior O
q O
ϕ O
(z|σ), O
and O
t O
* O
is O
obtained O
by O
the O
CYK O
algo. O
(Cocke, O
1969;Younger, O
1967;Kasami, O
1966). O

Evaluation O
We O
discard O
punctuation, O
lowercase O
all O
words, O
replace O
numbers O
with O
a O
special O
token O
and O
ignore O
trivial O
single-word O
and O
sentence-level O
spans O
during O
testing O
following O
Kim O
et O
al. O
(2019a). O
Besides, O
we O
follow O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
using O
a O
state-of-the-art O
constituency O
parser O
(Benepar O
Kitaev O
et O
al. O
2019) O
to O
obtain O
the O
reference O
trees O
for O
evaluation O
2 O
. O
Following O
Shi O
et O
al. O
(2020); O
Zhang O
et O
al. O
(2021), O
all O
models O
are O
run O
5 O
times O
for O
1 O
epoch O
with O
different O
random O
seeds. O
For O
each O
model, O
we O
report O
the O
averaged B-MetricName
sentence-level I-MetricName
F1 I-MetricName
( O
S-F1 B-MetricName
) O
and O
corpus-level B-MetricName
F1 I-MetricName
( O
C-F1 B-MetricName
) O
of O
its O
runs O
on O
each O
testing O
set. O

Implementation O
Details O
We O
use O
Spacy O
3 O
for O
tokenization O
and O
keep O
sentences O
with O
fewer O
than O
40 O
words O
for O
training O
due O
to O
the O
limited O
computational O
resources. O
Each O
video O
is O
decoded O
at O
16 B-HyperparameterValue
fps B-HyperparameterName
and O
L B-HyperparameterName
= O
8 B-HyperparameterValue
video B-HyperparameterName
clips I-HyperparameterName
are O
sampled O
in O
total, O
where O
each O
clip O
contains O
T B-HyperparameterName
= O
16 B-HyperparameterValue
frames B-HyperparameterName
. O
We O
train O
baseline O
models, O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
with O
the O
same O
hyper-parameters O
suggested O
by O
Kim O
et O
al. O
(2019a) O
and O
Zhang O
et O
al. O
(2021). O
The O
parsing O
model O
of O
PTC-PCFG B-MethodName
has O
the O
same O
hyperparameter O
setting O
as O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
(Please O
refer O
their O
papers O
for O
details). O
The O
constant O
α B-HyperparameterName
is O
set O
to O
1 B-HyperparameterValue
. O
We O
select O
the O
top O
20 O
000 O
most O
common O
words O
in O
HowTo100M B-DatasetName
as O
vocabulary O
for O
all O
datasets. O
All O
baseline O
methods O
and O
ours O
are O
optimized O
by O
Adam O
(Kingma O
and O
Ba, O
2015) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
, O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.75 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O
All O
parameters O
(except O
the O
video-text O
matching O
model O
in O
PTC-PCFG B-MethodName
) O
are O
initialized O
with O
Xavier O
uniform O
initializer O
(Glorot O
and O
Bengio, O
2010). O
All O
our O
models O
in O
experiments O
are O
trained O
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
without O
finetuning O
on O
the O
2 O
For O
each O
dataset, O
we O
randomly O
select O
50 O
sentences O
and O
manually O
label O
their O
constituency O
parse O
trees. O
Benepar O
has O
S-F1 B-MetricName
scores O
of O
98.1% B-MetricValue
( O
DiDeMo B-DatasetName
), O
97.2% B-MetricValue
( O
YouCook2 B-DatasetName
) O
and O
98.1% B-MetricValue
( O
MSRVTT B-DatasetName
) O
with O
manual O
labels. O

3 O
https://spacy.io/ O
target O
dataset. O

Main O
Results O
Figure O

Cross-dataset O
Evaluation O
We O
evaluate O
the O
robustness O
of O
models O
across O
different O
datasets, O
as O
shown O
in O
Table O
1. O
Comparing O
MMC-PCFG B-MethodName
trained O
on O
in-domain O
datasets O
(Row O
1-3), O
we O
can O
observe O
that O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
achieves O
the O
best O
overall O
performance, O
while O
MMC-PCFG B-MethodName
trained O
on O
YouCook2 B-DatasetName
is O
the O
worst. O
We O
believe O
this O
is O
due O
to O
the O
different O
number O
of O
training O
instances O
5 O
and O
the O
domain O
gap O
between O
different O
datasets. O
Comparing O
Rows O
1-4, O
we O
can O
observe O
that O
the O
MMC-PCFG B-MethodName
model O
trained O
on O
HT(592k) B-DatasetName
(Row O
4) O
is O
the O
best O
or O
the O
second O
place O
regarding O
C-F1 B-MetricName
and O
S-F1 B-MetricName
compared O
with O
its O
variants O
trained O
on O
in-domain O
datasets O
(Rows O
1-3). O
This O
demonstrates O
that O
the O
our O
processed O
videotext O
training O
instances O
are O
abundant, O
rich O
in O
content O
and O
can O
serve O
for O
general O
purpose. O
Comparing O
Rows O
4 O
and O
5, O
PTC-PCFG B-MethodName
outperforms O
MMC-PCFG B-MethodName
in O
both O
C-F1 B-MetricName
and O
S-F1 B-MetricName
in O
all O
three O
datasets O
and O
has O
smaller O
variance. O
This O
demonstrate O
that O
our O
model O
can O
leverage O
pre-trained O
video-text O
matching O
knowledge O
and O
learn O
consistent O
grammar O
induction. O

Effectiveness O
of O
Pre-Training O
In O
this O
section, O
we O
explore O
how O
different O
pretrained O
video O
and O
text O
encoders O
can O
affect O
the O
parsing O
performance, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
In O
particular, O
we O
study O
different O
(Zhang O
et O
al., O
2021;Zhao O
and O
Titov, O
2020), O
a O
pre-trained O
TinyBERT O
(Jiao O
et O
al., O
2020) O
model, O
the O
text O
encoder O
from O
MIL-NCE B-MethodName
(Miech O
et O
al., O
2020), O
and O
the O
text O
encoder O
from O
CLIP O
(Radford O
et O
al., O
2021). O

Comparing O
Rows O
1 O
with O
2, O
we O
can O
observe O
that O
MM O
is O
better O
than O
the O
video O
encoder O
of O
MIL-NCE B-MethodName
regarding O
C-F1 B-MetricName
and O
S-F1 B-MetricName
on O
all O
three O
datasets, O
as O
MM O
provides O
more O
comprehensive O
video O
features. O
By O
comparing O
row O
1 O
with O
3, O
we O
can O
also O
observe O
that O
TinyBERT B-MethodName
, O
which O
is O
distilled O
from O
BERT O
(Devlin O
et O
al., O
2019), O
outperforms O
the O
randomly O
initialized O
LSTM O
encoder. O
However, O
both O
MM B-MethodName
and O
TinyBERT B-MethodName
are O
independently O
trained O
only O
on O
vision O
or O
language O
tasks, O
where O
the O
vision-language O
correspondences O
are O
not O
considered O
during O
pretraining. O
Therefore, O
we O
further O
investigate O
the O
encoders O
jointly O
pre-trained O
on O
large O
scale O
multimedia O
datasets, O
including O
the O
video-text O
matching O
model O
MIL-NCE B-MethodName
(Row O
4) O
and O
the O
image-text O
matching O
model O
CLIP O
(Row O
5). O
We O
can O
observe O
that O
by O
leveraging O
both O
video O
and O
text O
encoders O
in O
MIL-NCE B-MethodName
can O
improve O
the O
parsing O
performance O
by O
a O
large O
margin O
on O
all O
three O
datasets. O
On O
the O
other O
hand, O
CLIP O
does O
not O
perform O
well, O
since O
it O
is O
designed O
for O
static O
images O
and O
other O
multi-modal O
information O
(e.g., O
motion) O
is O
ignored. O

Qualitative O
Analysis O
In O
figure O
5, O
we O
visualize O
a O
parser O
tree O
predicted O
by O
the O
best O
run O
of O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
and O
PTC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
, O
as O
well O
as O
its O
reference O
tree. O
We O
can O
observe O
that O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
fails O
at O
noun O
phrase O
"a O
lady", O
while O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
succeeds. O
MMC-PCFG B-MethodName
can O
be O
further O
improved O
by O
training O
on O
HT(296k) B-DatasetName
, O
however, O
fails O
at O
noun O
phrase O
"the O
groceries O
she O
had O
kept O
in O
her O
refrigerator". O
Our O
PTC-PCFG B-MethodName
can O
leverage O
the O
pretrained O
matching O
knowledge O
and O
make O
the O
correct O
prediction. O
Multilingual B-TaskName
Relation I-TaskName
Classification I-TaskName
via O
Efficient O
and O
Effective O
Prompting O
Prompting O
pre-trained O
language O
models O
has O
achieved O
impressive O
performance O
on O
various O
NLP O
tasks, O
especially O
in O
low O
data O
regimes. O
Despite O
the O
success O
of O
prompting O
in O
monolingual O
settings, O
applying O
prompt-based O
methods O
in O
multilingual O
scenarios O
has O
been O
limited O
to O
a O
narrow O
set O
of O
tasks, O
due O
to O

the O
high O
cost O
of O
handcrafting O
multilingual O
prompts. O
In O
this O
paper, O
we O
present O
the O
first O
work O
on O
prompt-based O
multilingual B-TaskName
relation I-TaskName
classification I-TaskName
( O
RC B-TaskName
), O
by O
introducing O
an O
efficient O
and O
effective O
method O
that O
constructs O
prompts O
from O
relation O
triples O
and O
involves O
only O
minimal O
translation O
for O
the O
class O
labels. O
We O
evaluate O
its O
performance O
in O

fully O
supervised, O
few-shot O
and O
zero-shot O
scenarios, O
and O
analyze O
its O
effectiveness O
across O
14 O
languages, O
prompt O
variants, O
and O
English-task O
training O
in O
cross-lingual O
settings. O
We O
find O
that O
in O
both O
fully O
supervised O
and O
few-shot O
scenarios, O
our O
prompt O
method O
beats O
competitive O
baselines: O
fine-tuning O
XLM-R B-MethodName
EM I-MethodName
and O
null O
prompts. O
It O
also O
outperforms O
the O
random O
baseline O
by O

a O
large O
margin O
in O
zero-shot O
experiments. O
Our O
method O
requires O
little O
in-language O
knowledge O
and O
can O
be O
used O
as O
a O
strong O
baseline O
for O
similar O
multilingual O
classification O
tasks. O
Introduction O
Relation B-TaskName
classification I-TaskName
( O
RC B-TaskName
) O
is O
a O
crucial O
task O
in O
information O
extraction O
(IE), O
aiming O
to O
identify O
the O
relation O
between O
entities O
in O
a O
text O
(Alt O

et O
al., O
2019). O
Extending O
RC B-TaskName
to O
multilingual O
settings O
has O
recently O
received O
increased O
interest O
(Zou O
et O
al., O
2018;Kolluru O
et O
al., O
2022), O
but O
the O
majority O
of O
prior O
work O
still O
focuses O
on O
English O
(Baldini O
Soares O
et O
al., O
2019;Lyu O
and O
Chen, O
2021). O
A O
main O
bottleneck O
for O
multilingual B-TaskName
RC I-TaskName
is O
the O
lack O
of O
supervised O
resources, O

comparable O
in O
size O
to O
large O
English O
datasets O
(Riedel O
et O
al., O
2010;Zhang O
et O
al., O
2017). O
The O
SMiLER B-DatasetName
dataset O
(Seganti O
et O
al., O
2021) O
provides O
a O
starting O
point O
to O
test O
fully O
supervised O
and O
more O
efficient O
approaches O
due O
to O
different O
resource O
availability O
for O
different O
languages. O
Previous O
studies O
have O
shown O
the O
promising O
performance O
of O
prompting O

PLMs O
compared O
to O
the O
datahungry O
fine-tuning, O
especially O
in O
low-resource O
scenarios O
(Gao O
et O
al., O
2021;Le O
Scao O
and O
Rush, O
2021;. O
Multilingual O
pre-trained O
language O
models O
(Conneau O
et O
al., O
2020;Xue O
et O
al., O
2021) O
further O
enable O
multiple O
languages O
to O
be O
represented O
in O
a O
shared O
semantic O
space, O
thus O
making O
prompting O
in O
multilingual O
scenarios O
feasible. O
However, O
the O

study O
of O
prompting O
for O
multilingual O
tasks O
so O
far O
remains O
limited O
to O
a O
small O
range O
of O
tasks O
such O
as O
text O
classification O
(Winata O
et O
al., O
2021) O
and O
natural O
language O
inference O
(Lin O
et O
al., O
2022). O
To O
our O
knowledge, O
the O
effectiveness O
of O
prompt-based O
methods O
for O
multilingual B-TaskName
RC I-TaskName
is O
still O
unexplored. O
To O
analyse O
this O
gap, O

we O
pose O
two O
research O
questions O
for O
multilingual B-TaskName
RC I-TaskName
with O
prompts: O
RQ1. O
What O
is O
the O
most O
effective O
way O
to O
prompt? O
We O
investigate O
whether O
prompting O
should O
be O
done O
in O
English O
or O
the O
target O
language O
and O
whether O
to O
use O
soft O
prompt O
tokens. O
RQ2. O
How O
well O
do O
prompts O
perform O
in O
different O
data O
regimes O
and O

languages? O
We O
investigate O
the O
effectiveness O
of O
our O
prompting O
approach O
in O
three O
scenarios: O
fully O
supervised, O
few-shot O
and O
zero-shot. O
We O
explore O
to O
what O
extent O
the O
results O
are O
related O
to O
the O
available O
language O
resources. O
We O
present O
an O
efficient O
and O
effective O
prompt O
method O
for O
multilingual B-TaskName
RC I-TaskName
(see O
Figure O
1) O
that O
derives O
prompts O
from O
relation O

triplets O
(see O
Section O
3.1). O
The O
derived O
prompts O
include O
the O
original O
sentence O
and O
entities O
and O
are O
supposed O
to O
be O
filled O
with O
the O
relation O
label. O
We O
evaluate O
the O
prompts O
with O
three O
variants, O
two O
of O
which O
require O
no O
translation, O
and O
one O
of O
which O
requires O
minimal O
translation, O
i.e., O
of O
the O
relation O
labels O
only. O
We O

find O
that O
our O
method O
outperforms O
fine-tuning O
and O
a O
strong O
taskagnostic O
prompt O
baseline O
in O
fully O
supervised O
and O
few-shot O
scenarios, O
especially O
for O
relatively O
lowresource O
languages. O
Our O
method O
also O
improves O
over O
the O
random O
baseline O
in O
zero-shot O
settings, O
and O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
_______ O
Cameron O

. O
Goethe O
schrieb O
die O
Tragödie O
Faust O
. O
Relation B-TaskName
Classification I-TaskName
Task O
Definition O
Relation B-TaskName
classification I-TaskName
is O
the O
task O
of O
classifying O
the O
relationship O
such O
as O
date_of_birth, O
founded_by O
or O
parents O
between O
pairs O
of O
entities O
in O
a O
given O
context. O
Formally, O
given O
a O
relation O
set O
R O
and O
a O
text O
x O
= O
[x O
1 O
, O
x O
2 O

, O
. O
. O
. O
, O
x O
n O
] O
(where O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
are O
tokens) O
with O
two O
disjoint O
spans O
e O
h O
and O
e O
t O
denoting O
the O
head O
and O
tail O
entity, O
RC B-TaskName
aims O
to O
predict O
the O
relation O
r O
∈ O
R O
between O
e O
h O
and O
e O
t O

, O
or O
give O
a O
no_relation O
prediction O
if O
no O
relation O
in O
R O
holds. O
RC B-TaskName
is O
a O
multilingual O
task O
if O
the O
token O
sequences O
come O
from O
different O
languages. O
Fine-tuning O
for O
Relation B-TaskName
Classification I-TaskName
In O
fine-tuning, O
a O
task-specific O
linear O
classifier O
is O
added O
on O
top O
of O
the O
PLM. O
Fine-tuning O
hence O
introduces O
a O
different O
scenario O
from O
pre-training, O

since O
language O
model O
(LM) O
pre-training O
is O
usually O
formalized O
as O
a O
cloze-style O
task O
to O
predict O
target O
tokens O
at O
[MASK] O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019) O
or O
a O
corrupted O
span O
(Raffel O
et O
al., O
2020;Lewis O
et O
al., O
2020). O
For O
the O
RC B-TaskName
task, O
the O
classifier O
aims O
to O
predict O
the O
target O
class O
r O
at O

[CLS] O
or O
at O
the O
entity O
spans O
denoted O
by O
MARKER O
(Baldini O
Soares O
et O
al., O
2019). O
Prompting O
for O
Relation B-TaskName
Classification I-TaskName
Prompting O
is O
proposed O
to O
bridge O
the O
gap O
between O
pre-training O
and O
fine-tuning O
. O
The O
essence O
of O
prompting O
is, O
by O
appending O
extra O
text O
to O
the O
original O
text O
according O
to O
a O
task-specific O
template O
T O
(•), O

to O
reformulate O
the O
downstream O
task O
to O
an O
LM O
pre-training O
task O
such O
as O
masked O
language O
modeling O
(MLM), O
and O
apply O
the O
same O
training O
objective O
during O
the O
task-specific O
training. O
For O
the O
RC B-TaskName
task, O
to O
identify O
the O
relation O
between O
"Angela O
Merkel" O
and O
"Joachim O
Sauer" O
in O
the O
text O
"Angela O
Merkel's O
current O
husband O
is O
quantum O
chemist O

Joachim O
Sauer," O
an O
intuitive O
template O
for O
prompting O
can O
be O
"The O
relation O
between O
Angela O
Merkel O
and O
Joachim O
Sauer O
is O
[MASK]," O
and O
the O
LM O
is O
supposed O
to O
assign O
a O
higher O
likelihood O
to O
the O
term O
couple O
than O
to O
e.g. O
friends O
or O
colleagues O
at O
[MASK]. O
This O
"fill-in O
the O
blank" O
paradigm O
is O
well O
aligned O
with O

the O
pre-training O
scenario, O
and O
enables O
prompting O
to O
better O
coax O
the O
PLMs O
for O
pre-trained O
knowledge O
(Petroni O
et O
al., O
2019). O
Methods O
We O
now O
present O
our O
method, O
as O
shown O
in O
Figure O
1. O
We O
introduce O
its O
template O
and O
verbalizer, O
and O
propose O
several O
variants O
of O
the O
prompt. O
Lastly, O
we O
explain O
the O
training O
and O
inference O
process. O

Template O
For O
prompting O
, O
a O
prompt O
often O
consists O
of O
a O
template O
T O
(•) O
and O
a O
verbalizer O
V. O
Given O
a O
plain O
text O
x, O
the O
template O
T O
adds O
taskrelated O
instruction O
to O
x O
to O
yield O
the O
prompt O
input O
x O
prompt O
= O
T O
(x).(1) O
Following O
and O
Han O
et O
al. O
(2021), O
we O
treat O
relations O
as O

predicates O
and O
use O
the O
cloze O
"e O
h O
{relation} O
e O
t O
" O
for O
the O
LM O
to O
fill O
in. O
Our O
template O
is O
formulated O
as O
T O
(x) O
:= O
"x. O
e O
h O
____ O
e O
t O
". O
(2) O
In O
the O
template O
T O
(x), O
x O
is O
the O
original O
text O
and O
the O
two O
entities O
e O
h O
and O

e O
t O
come O
from O
x. O
Therefore, O
our O
template O
does O
not O
introduce O
extra O
tokens, O
thus O
involves O
no O
translation O
at O
all. O
Verbalizer O
After O
being O
prompted O
by O
x O
prompt O
, O
the O
PLM O
M O
predicts O
the O
masked O
text O
y O
at O
the O
blank. O
To O
complete O
an O
NLP O
classification O
task, O
a O
verbalizer O
ϕ O
is O
required O
to O

bridge O
the O
set O
of O
labels O
Y O
and O
the O
set O
of O
predicted O
texts O
(verbalizations O
V). O
For O
the O
simplicity O
of O
our O
prompt, O
we O
use O
the O
one-to-one O
verbalizer: O
ϕ O
: O
Y O
→ O
V, O
r O
→ O
ϕ(r),(3) O
where O
r O
is O
a O
relation, O
and O
ϕ(r) O
is O
the O
simple O
verbalization O
of O
r. O
ϕ(•) O
normally O
only O
involves O

splitting O
r O
by O
"-" O
or O
"_" O
and O
replacing O
abbreviations O
such O
as O
org O
with O
organization. O
E.g., O
the O
relation O
org-has-member O
corresponds O
to O
the O
verbalization O
"organization O
has O
member". O
Then O
the O
prediction O
is O
formalized O
as O
p(r|x) O
∝ O
p(y O
= O
ϕ(r)|x O
prompt O
; O
θ O
M O
), O
(4 O
) O
where O
θ O
M O
denotes O
the O
parameters O
of O

model O
M. O
p(r|x) O
is O
normalized O
by O
the O
likelihood O
sum O
over O
all O
relations. O
Variants O
To O
find O
the O
optimal O
way O
to O
prompt, O
we O
investigate O
three O
variants O
as O
follows. O
Hard O
prompt O
vs O
soft O
prompt O
(SP) O
Hard O
prompts O
(a.k.a. O
discrete O
prompts) O
are O
entirely O
formulated O
in O
natural O
language. O
Soft O
prompts O
(a.k.a. O
continuous O
prompts) O
consist O
of O

learnable O
tokens O
(Lester O
et O
al., O
2021) O
that O
are O
not O
contained O
in O
the O
PLM O
vocabulary. O
Following O
Han O
et O
al. O
(2021), O
we O
insert O
soft O
tokens O
before O
entities O
and O
blanks O
as O
shown O
for O
SP O
in O
Table O
1. O
Code-switch O
(CS) O
vs O
in-language O
(IL) O
Relation O
labels O
are O
in O
English O
across O
almost O
all O
RC O
datasets. O
Given O

a O
text O
from O
a O
non-English O
input O
L O
with O
a O
blank, O
the O
recovered O
text O
is O
code-mixed O
after O
being O
completed O
with O
an O
English O
verbalization, O
corresponding O
to O
code-switch O
prompting. O
It O
is O
probably O
more O
reasonable O
for O
the O
PLM O
to O
fill O
in O
the O
blank O
in O
language O
L. O
Inspired O
by O
Lin O
et O
al. O
(2022) O
(Hendrickx O
et O

al., O
2010) O
10 O
cause O
effect, O
entity O
origin, O
product O
producer, O
... O
2.50 O
0.81 O
NYT O
(Riedel O
et O
al., O
2010) O
24 O
ethnicity, O
major O
shareholder O
of, O
religion, O
... O
show O
that O
the O
label O
space O
of O
the O
RC B-TaskName
task O
is O
more O
complex O
than O
most O
few-class O
classification O
tasks. O
The O
verbalizations O
of O
RC O
datasets O
are O
listed O
in O
Appendix O

B. O
For O
SemEval B-DatasetName
, O
the O
two O
possible O
directions O
of O
a O
relation O
are O
combined. O
For O
NYT B-DatasetName
, O
we O
use O
the O
version O
from O
Zeng O
et O
al. O
(2018). O
For O
SMiLER B-DatasetName
, O
"EN" O
is O
the O
English O
split; O
"ALL" O
contains O
all O
data O
from O
14 O
languages. O
Table O
1 O
visualizes O
both O
code-switch O
(CS) O
and O
inlanguage O
(IL) O
prompting. O

For O
English, O
CS-and O
ILprompting O
are O
equivalent, O
since O
L O
is O
English O
itself. O
Word O
order O
of O
prompting O
For O
the O
RC O
task, O
head-relation-tail O
triples O
involve O
three O
elements. O
Therefore, O
deriving O
natural O
language O
prompts O
from O
them O
requires O
handling O
where O
to O
put O
the O
predicate O
(relation). O
In O
the O
case O
of O
SOV O
languages, O
filling O
in O
a O
relation O
that O

occurs O
between O
e O
h O
and O
e O
t O
seems O
less O
intuitive. O
Therefore, O
to O
investigate O
if O
the O
word O
order O
of O
prompting O
affects O
prediction O
accuracy, O
we O
swap O
the O
entities O
and O
the O
blank O
in O
the O
SVOtemplate O
"x. O
e O
h O
____ O
e O
t O
" O
and O
get O
"x. O
e O
h O
e O
t O
____" O
as O
the O
SOV-template. O

Training O
and O
Inference O
The O
training O
and O
inference O
setups O
depend O
on O
the O
employed O
model. O
Prompting O
autoencoding O
language O
models O
requires O
the O
verbalizations O
to O
be O
of O
fixed O
length, O
since O
the O
length O
of O
masks, O
which O
is O
identical O
with O
verbalization O
length, O
is O
unknown O
during O
inference. O
Encoder-decoders O
can O
handle O
verbalizations O
of O
varying O
length O
by O
nature O
. O

Han O
et O
al. O
(2021) O
adjust O
all O
the O
verbalizations O
in O
TACRED B-DatasetName
to O
a O
length O
of O
3, O
to O
enable O
prompting O
with O
RoBERTa B-MethodName
for O
RC B-TaskName
. O
We O
argue O
that O
for O
multilingual B-TaskName
RC I-TaskName
, O
this O
fix O
is O
largely O
infeasible, O
because: O
(1) O
in O
case O
of O
in-language O
prompting O
on O
SMiLER B-DatasetName
, O
the O
variance B-MetricName
of O
the O
length O

of O
the O
verbalizations O
increases O
from O
0.68 B-HyperparameterValue
to O
1.44 B-HyperparameterValue
after O
translation O
(see O
Table O
2), O
and O
surpasses O
most O
of O
listed O
monolingual O
RC O
datasets O
( O
SemEval B-DatasetName
, O
NYT B-DatasetName
and O
SCIERC B-DatasetName
), O
making O
it O
harder O
to O
unify O
the O
length; O
(2) O
manually O
adjusting O
the O
translated O
prompts O
requires O
manual O
effort O
per O
target O
language, O
making O
it O
much O

more O
expensive O
than O
adjusting O
only O
English O
verbalizations. O
Therefore, O
we O
use O
an O
encoder-decoder O
PLM O
for O
prompting O
Song O
et O
al., O
2022). O
Training O
objective O
For O
an O
encoder-decoder O
PLM O
M, O
given O
the O
prompt O
input O
T O
(x) O
and O
the O
target O
sequence O
ϕ(r) O
(i.e. O
label O
verbalization), O
we O
denote O
the O
output O
sequence O
as O
y. O
The O
probability O
of O

an O
exact-match O
decoding O
is O
calculated O
as O
follows: O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
,(5) O
where O
y O
t O
, O
ϕ O
t O
(r) O
denote O
the O
t-th O
token O
of O
y O
and O
ϕ(r), O
respectively. O
y O
<t O
denotes O
the O
decoded O
sequence O
on O
the O
left. O
θ O
represents O
the O

set O
of O
all O
the O
learnable O
parameters, O
including O
those O
of O
the O
PLM O
θ O
M O
, O
and O
those O
of O
the O
soft O
tokens O
θ O
sp O
in O
case O
of O
variant O
"soft O
prompt". O
Hence, O
the O
final O
objective O
over O
the O
training O
set O
X O
is O
to O
minimize O
the O
negative O
loglikelihood: O
argmin O
θ O
− O
1 O
|X O
| O
x∈X O

|ϕ(r)| O
t=1 O
log O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
.(6) O
Inference O
We O
collect O
the O
output O
logits O
of O
the O
decoder, O
L O
∈ O
R O
|V O
|×L O
, O
where O
|V O
| O
is O
the O
vocabulary O
size O
of O
M, O
and O
L O
is O
the O
maximum O
decode O
length. O
For O
each O
relation O

r O
∈ O
R, O
its O
score O
is O
given O
by O
: O
where O
we O
compute O
P O
by O
looking O
up O
in O
the O
t-th O
column O
of O
L O
and O
applying O
softmax O
at O
each O
time O
step O
t. O
We O
aggregate O
P O
by O
addition O
to O
encourage O
partial O
matches O
as O
well, O
instead O
of O
enforcing O
exact O
matches. O
The O
score O
is O
normalized O

by O
the O
length O
of O
verbalization O
in O
order O
to O
avoid O
predictions O
favoring O
longer O
relations. O
Finally, O
we O
select O
the O
relation O
with O
the O
highest O
score O
as O
prediction. O
score O
θ O
(r) O
:= O
1 O
|ϕ(r)| O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)), O
(7) O
Experiments O
We O
implement O
our O
experiments O
using O
the O
Hugging O
Face O

Transformers O
library O
(Wolf O
et O
al., O
2020), O
Hydra O
(Yadan, O
2019) O
and O
PyTorch O
(Paszke O
et O
al., O
2019). O
2 O
We O
use O
micro-F1 B-MetricName
as O
the O
evaluation O
metric, O
as O
the O
SMiLER B-DatasetName
paper O
(Seganti O
et O
al., O
2021) O
suggests. O
To O
measure O
the O
overall O
performance O
over O
multiple O
languages, O
we O
report O
the O
macro B-MetricName
average I-MetricName
across O
languages, O
following O
Zhao O
and O

Schütze O
(2021) O
and O
Lin O
et O
al. O
(2022). O
We O
also O
group O
the O
languages O
by O
their O
available O
resources O
in O
both O
pretraining O
and O
fine-tuning O
datasets O
for O
additional O
aggregate O
results. O
Details O
of O
the O
dataset, O
the O
models, O
and O
the O
experimental O
setups O
are O
as O
follows. O
Further O
experimental O
details O
are O
listed O
in O
Appendix O
A. O
Dataset O
We O
conduct O

an O
experimental O
evaluation O
of O
our O
multilingual O
prompt O
methods O
on O
the O
SMiLER B-DatasetName
(Seganti O
2 O
We O
make O
our O
code O
publicly O
available O
at O
https://github. O
com/DFKI-NLP/meffi-prompt O
for O
better O
reproducibility. O
Grouping O
of O
the O
languages O
We O
visualize O
the O
languages O
in O
Figure O
2 O
based O
on O
the O
sizes O
of O
RC B-TaskName
training O
data, O
but O
include O
the O
pre-training O
data O
as O

well, O
to O
give O
a O
more O
comprehensive O
overview O
of O
the O
availability O
of O
resources O
for O
each O
language. O
We O
divide O
the O
14 O
languages O
into O
4 O
groups, O
according O
to O
the O
detectable O
clusters O
in O
Figure O
2 O
and O
language O
origins. O
Model O
For O
prompting, O
we O
use O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021), O
an O
encoder-decoder O
PLM O
that O
supports O

101 O
languages, O
including O
all O
languages O
in O
SMiLER B-DatasetName
. O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021) O
has O
220M B-HyperparameterValue
parameters B-HyperparameterName
. O
XLM-R B-MethodName
EM I-MethodName
To O
provide O
a O
fine-tuning O
baseline, O
we O
re-implement O
BERT B-MethodName
EM I-MethodName
(Baldini O
Soares O
et O
al., O
2019) O
with O
the O
ENTITY O
START O
variant. O
4 O
In O
this O
method, O
the O
top-layer O
representations O
at O
the O
starts O
of O

the O
two O
entities O
are O
concatenated O
for O
linear O
classification. O
To O
adapt O
BERT B-MethodName
EM I-MethodName
to O
multilingual O
tasks, O
we O
change O
the O
PLM O
from O
BERT O
to O
a O
multilingual O
autoencoder, O
XLM-R B-MethodName
BASE I-MethodName
(Conneau O
et O
al., O
2020), O
and O
refer O
to O
this O
model O
as O
XLM-R B-MethodName
EM I-MethodName
. O
XLM-R B-MethodName
BASE I-MethodName
has O
125M B-HyperparameterValue
parameters B-HyperparameterName
. O
Null O
prompts O
(Logan O
IV O

et O
al., O
2022) O
To O
better O
verify O
the O
effectiveness O
of O
our O
method, O
we O
implement O
null O
prompts O
as O
a O
strong O
task-agnostic O
prompt O
baseline. O
Null O
prompts O
involve O
minimal O
prompt O
engineering O
by O
directly O
asking O
the O
LM O
about O
the O
relation, O
without O
giving O
any O
task O
instruction O
(see O
Table O
1). O
Logan O
IV O
et O
al. O
(2022) O
show O
that O

null O
prompts O
surprisingly O
achieve O
on-par O
performance O
with O
handcrafted O
prompts O
on O
many O
tasks. O
For O
best O
comparability, O
we O
use O
the O
same O
PLM O
mT5 B-MethodName
BASE I-MethodName
. O
Fully O
Supervised O
Setup O
We O
evaluate O
the O
performance O
of O
XLM-R B-MethodName
EM I-MethodName
, O
null O
prompts, O
and O
our O
method O
on O
each O
of O
the O
14 O
languages, O
after O
training O
on O
the O
full O

train O
split O
from O
that O
language. O
The O
prompt O
input O
and O
target O
of O
null O
prompts O
and O
our O
prompts O
are O
listed O
in O
Table O
1. O
We O
employ O
the O
randomly O
generated O
seed B-HyperparameterName
319 B-HyperparameterValue
for O
all O
the O
evaluated O
methods. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
follow O
Baldini O
Soares O
et O
al. O
(2019) O
and O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O

be O
64 B-HyperparameterValue
, O
the O
optimizer O
to O
be O
Adam O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
number O
of O
epochs B-HyperparameterName
to O
be O
5 B-HyperparameterValue
. O
For O
null O
prompts O
and O
ours, O
we O
use O
AdamW O
as O
the O
optimizer O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
as O
suggest O
for O
most O
of O

the O
sequence-to-sequence O
tasks, O
the O
number O
of O
epochs B-HyperparameterName
to O
5 B-HyperparameterValue
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
. O
The O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
for O
all O
methods. O
Few-shot O
Setup O
Few-shot O
learning O
is O
normally O
cast O
as O
a O
K-shot O
problem, O
where O
K O
labelled O
examples O
per O
class O
are O
available. O
We O
follow O
and O
Han O
et O
al. O

(2021), O
and O
evaluate O
on O
8, O
16 O
and O
32 O
shots. O
The O
few-shot O
training O
set O
D O
train O
is O
generated O
by O
randomly O
sampling O
K O
instances O
per O
relation O
from O
the O
training O
split. O
The O
test O
set O
D O
test O
is O
the O
original O
test O
split O
from O
that O
language. O
We O
follow O
Gao O
et O
al. O
(2021) O
and O
sample O
another O

K-shot O
set O
from O
the O
English O
train O
split O
as O
validation O
set O
D O
val O
. O
We O
tune O
hyperparameters O
on O
D O
val O
for O
the O
English O
task, O
and O
apply O
these O
to O
all O
languages. O
We O
evaluate O
the O
same O
methods O
as O
in O
the O
fully O
supervised O
scenarios, O
but O
repeat O
5 O
runs O
as O
suggested O
in O
Gao O
et O
al. O

(2021), O
and O
report O
the O
mean O
and O
standard O
deviation O
of O
micro-F1 B-MetricName
. O
We O
use O
a O
fixed O
set O
of O
random O
seeds B-HyperparameterName
{13, B-HyperparameterValue
36, I-HyperparameterValue
121, I-HyperparameterValue
223, I-HyperparameterValue
319} I-HyperparameterValue
for O
data O
generation O
and O
training O
across O
the O
5 O
runs. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
use O
the O
same O
hyperparameters O
as O
Baldini O
Soares O
et O
al. O
( O
2019), O
a O

batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
, O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
. O
For O
null O
prompts O
and O
our O
prompts, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
and O
the O
number O
of O
epochs B-HyperparameterName
to O
20 B-HyperparameterValue
. O
Zero-shot O
Setup O
We O
consider O
two O

scenarios O
for O
zero-shot O
multilingual O
relation O
classification. O
Zero-shot O
in-context O
learning O
Following O
Kojima O
et O
al. O
( O
2022), O
we O
investigate O
whether O
PLMs O
are O
also O
decent O
zero-shot O
reasoners O
for O
RC B-TaskName
. O
This O
scenario O
does O
not O
require O
any O
samples O
or O
training. O
We O
test O
the O
out-of-the-box O
performance O
of O
the O
PLM O
by O
directly O
prompting O
it O
with O
x O

prompt O
. O
Zero-shot O
in-context O
learning O
does O
not O
specify O
further O
hyperparameters O
since O
it O
is O
training-free. O
Zero-shot O
cross-lingual O
transfer O
In O
this O
scenario, O
following O
Krishnan O
et O
al. O
(2021), O
we O
finetune O
the O
model O
with O
in-language O
prompting O
on O
the O
English O
train O
split, O
and O
then O
conduct O
zero-shot O
incontext O
tests O
with O
this O
fine-tuned O
model O
on O
other O
languages O

using O
code-switch O
prompting. O
Through O
this O
setting, O
we O
want O
to O
verify O
if O
task-specific O
pretraining O
in O
a O
high-resource O
language O
such O
as O
English O
helps O
in O
other O
languages. O
In O
zero-shot O
crosslingual O
transfer, O
we O
use O
the O
same O
hyperparameters O
and O
random O
seed O
to O
fine-tune O
on O
the O
English O
task. O
Fully O
Supervised O
Results O
the O
three O
variants O
of O
our O

method O
beat O
the O
finetuning O
baseline O
XLM-R B-MethodName
EM I-MethodName
and O
the O
prompting O
baseline O
null O
prompts, O
according O
to O
the O
macroaveraged O
performance O
across O
14 O
languages. O
Inlanguage O
prompting O
delivers O
the O
most O
promising O
result, O
achieving O
an O
average O
F B-MetricName
1 I-MetricName
of O
85.0 B-MetricValue
, O
which O
is O
higher O
than O
XLM-R B-MethodName
EM I-MethodName
( O
68.2 B-MetricValue
) O
and O
null B-MethodName
prompts I-MethodName
( O
66.2 B-MetricValue

). O
The O
other O
two O
variants, O
code-switch B-MethodName
prompting I-MethodName
with O
and O
w/o B-MethodName
soft I-MethodName
tokens I-MethodName
, O
achieve O
F B-MetricName
1 I-MetricName
scores O
of O
84.1 B-MetricValue
and O
82.7 B-MetricValue
, O
respectively, O
only O
0.9 O
and O
2.3 O
lower O
than O
in-language. O
All O
three O
prompt O
variants O
are O
hence O
effective O
in O
fully O
supervised O
scenarios. O
On O
a O
per-group O
basis, O
we O
find O
that O
the O
lowerresourced O

a O
language O
is, O
the O
greater O
an O
advantage O
prompting O
enjoys O
against O
fine-tuning. O
In O
particular, O
in-language O
prompts O
shows O
better O
robustness O
compared O
to O
XLM-R B-MethodName
EM I-MethodName
in O
low-resource O
languages. O
They O
both O
yield O
95.9 B-MetricValue
- O
96.0 B-MetricValue
F B-MetricName
1 I-MetricName
scores O
for O
English, O
but O
XLM-R B-MethodName
EM I-MethodName
decreases O
to O
54.3 B-MetricValue
and O
3.7 B-MetricValue
F B-MetricName
1 I-MetricName
in O
Group-M O
and O
-L, O

while O
in-language O
prompting O
still O
delivers O
83.5 B-MetricValue
and O
65.2 B-MetricValue
F B-MetricValue
1 I-MetricValue
. O
Few-shot O
Results O
Table O
5 O
presents O
the O
per-group O
results O
in O
few-shot O
experiments. O
All O
the O
methods O
benefit O
from O
larger O
K. O
Similarly, O
in-language O
prompting O
still O
turns O
out O
to O
be O
the O
best O
contender, O
performing O
1st O
in O
8-and O
32-shot, O
and O
the O
2nd O
in O
16-shot. O

We O
see O
that O
inlanguage O
outperforms O
XLM-R B-MethodName
EM I-MethodName
in O
all O
K-shots, O
while O
code-switch B-MethodName
achieves O
comparable O
or O
even O
lower O
F B-MetricName
1 I-MetricName
to O
XLM-R B-MethodName
EM I-MethodName
for O
K O
= O
8, O
suggesting O
that O
the O
choice O
of O
prompt O
affects O
the O
few-shot O
performance O
greatly, O
thus O
needs O
careful O
consideration. O
On O
a O
per-group O
basis, O
we O
find O
that O
in-language O
prompting O

outperforms O
other O
methods O
for O
middleand O
low-resourced O
languages. O
Similar O
observations O
can O
also O
be O
drawn O
from O
fully O
supervised O
results. O
We O
conclude O
that, O
with O
sufficient O
supervision, O
inlanguage O
is O
the O
optimal O
variant O
to O
prompt O
rather O
Table O
5: O
Few-shot O
results O
by O
group O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
(Seganti O
et O
al., O
2021) O
dataset O
averaged O
over O

five O
runs. O
We O
macro-average B-MetricName
results O
for O
each O
language O
group O
(see O
Figure O
2) O
and O
over O
all O
languages O
(X). O
Inlanguage O
prompting O
performs O
best O
in O
most O
settings O
and O
language O
groups. O
Our O
variants O
are O
especially O
strong O
for O
medium-and O
lower-resource O
language O
groups. O
See O
Table O
7 O
in O
Appendix O
C O
for O
detailed O
results O
with O
mean O
and O
std. O

for O
each O
language. O
than O
code-switch B-MethodName
. O
We O
hypothesize O
it O
is O
due O
to O
the O
pre-training O
scenario, O
where O
the O
PLM O
rarely O
sees O
code-mixed O
text O
(Santy O
et O
al., O
2021). O
Zero-shot O
Results O
Table O
6 O
presents O
the O
per-language O
results O
in O
zeroshot O
scenarios. O
We O
consider O
the O
random O
baseline O
for O
comparison O
(Zhao O
and O
Schütze, O
2021;Winata O
et O
al., O

2021). O
We O
notice O
that O
performance O
of O
the O
random O
baseline O
varies O
a O
lot O
across O
languages, O
since O
the O
languages O
have O
different O
number O
of O
classes O
in O
the O
dataset O
(cf. O
Table O
6: O
Zero-shot O
results O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
dataset. O
"SVO" O
and O
"SOV": O
word O
order O
of O
prompting. O
Overall, O
Code-switch B-MethodName
prompting I-MethodName
performs O
the O
best O

in O
the O
zero-shot O
in-context O
scenario. O
In O
cross-lingual O
transfer O
experiments, O
English-task O
training O
greatly O
improves O
the O
performance O
on O
all O
the O
other O
13 O
languages. O
margin, O
in O
both O
word O
orders, O
while O
in-language B-MethodName
prompting I-MethodName
performs O
worse O
than O
the O
random O
baseline O
in O
6 O
languages. O
Code-switch B-MethodName
prompting I-MethodName
outperforms O
in-language O
prompting O
across O
all O
the O
13 O
non-English O
languages, O
using O

SVO-template. O
We O
assume O
that, O
without O
in-language O
training, O
the O
PLM O
understands O
the O
task O
best O
when O
prompted O
in O
English. O
The O
impressive O
performance O
of O
code-switch B-MethodName
shows O
the O
PLM O
is O
able O
to O
transfer O
its O
pre-trained O
knowledge O
in O
English O
to O
other O
languages. O
We O
also O
find O
that O
the O
performance O
is O
also O
highly O
indicated O
by O
the O
number O

of O
classes, O
with O
worst O
F B-MetricName
1 I-MetricName
scores O
achieved O
in O
EN, O
KO O
and O
PT O
(36, O
28 O
and O
22 O
classes), O
and O
best O
scores O
in O
AR, O
RU O
and O
UK O
(9, O
8 O
and O
7 O
classes). O
In O
addition, O
we O
observe O
that O
word O
order O
does O
not O
play O
a O
significant O
role O
for O
most O
languages, O
except O
for O
FA, O

which O
is O
an O
SOV-language O
and O
has O
54.5 B-MetricValue
F B-MetricName
1 I-MetricName
gain O
from O
in-language O
prompting O
with O
an O
SOV-template. O
For O
zero-shot O
cross-lingual O
transfer, O
we O
see O
that O
non-English O
tasks O
benefit O
from O
English O
in-domain B-MethodName
prompt-based I-MethodName
fine-tuning I-MethodName
, O
and O
the O
F B-MetricName
1 I-MetricName
gain O
improves O
with O
the O
English O
data O
size. O
For O
5 O
languages O
(ES, O
FA, O
NL, O
SV, O

and O
UK), O
zero-shot O
transfer O
after O
training O
on O
268k O
English O
examples O
delivers O
even O
better O
results O
than O
in-language O
fully O
supervised O
training O
(cf. O
Table O
4). O
Sanh O
et O
al. O
(2022) O
show O
that O
including O
RC B-TaskName
-specific O
prompt O
input O
in O
English O
during O
pre-training O
can O
help O
in O
other O
languages. O
Discussion O
Based O
on O
the O
results O
above, O
we O
answer O

the O
research O
questions O
from O
Section O
1. O
RQ1. O
Which O
is O
the O
most O
effective O
way O
to O
prompt? O
In O
the O
fully-supervised O
and O
few-shot O
scenario, O
in-language B-MethodName
prompting I-MethodName
displays O
the O
best O
re-sults. O
This O
appears O
to O
stem O
from O
a O
solid O
performance O
across O
all O
languages O
in O
both O
settings. O
Its O
worst O
performance O
is O
31.8 O
F O
1 O
for O
Polish O

8-shot O
(see O
Table O
7 O
in O
Appendix O
C). O
All O
other O
methods O
have O
results O
lower O
than O
15.0 B-MetricValue
F B-MetricName
1 I-MetricName
for O
some O
language. O
This O
indicates O
that O
with O
little O
supervision O
mT5 B-MethodName
is O
able O
to O
perform O
the O
task O
when O
prompted O
in O
the O
language O
of O
the O
original O
text. O
However, O
zero-shot O
results O
strongly O
prefer O
code-switch B-MethodName
prompting I-MethodName
. O

It O
could O
follow O
that, O
without O
fine-tuning, O
the O
model's O
understanding O
of O
this O
task O
is O
much O
better O
in O
English. O
RQ2. O
How O
well O
does O
our O
method O
perform O
in O
different O
data O
regimes O
and O
languages? O
Averaged O
over O
all O
languages, O
all O
our O
variants O
outperform O
the O
baselines, O
except O
for O
8-shot. O
For O
some O
high-resource O
languages, O
XLM-R B-MethodName
EM I-MethodName
is O

able O
to O
outperform O
our O
method. O
On O
the O
other O
hand, O
for O
low-resource O
languages O
null O
prompts O
are O
a O
better O
baseline O
which O
we O
consistently O
outperform. O
This O
could O
indicate O
that O
prompting O
the O
underlying O
mT5 B-MethodName
model O
is O
better O
suited O
for O
multilingual B-TaskName
RC I-TaskName
on O
SMiLER B-DatasetName
. O
Overall, O
the O
results O
suggest O
that O
minimal O
translation O
can O
be O
very O

helpful O
for O
multilingual O
relation O
classification. O
B O
Verbalizers O
for O
SMiLER B-DatasetName
• O
EN O
"birth-place": O
"birth O
place", O
"eats": O
"eats", O
"event-year": O
"event O
year", O
"firstproduct": O
"first O
product", O
"from-country": O
"from O
country", O
"has-author": O
"has O
author", O
"has-child": O
"has O
child", O
"has-edu": O
"has O
education", O
"has-genre": O
"has O
genre", O
"has-height": O
"has O
height", O
"has-highestmountain": O
"has O
highest O
mountain", O
"haslength": O
"has O
length", O
"has-lifespan": O
"has O
lifespan", O

"has-nationality": O
"has O
nationality", O
"has-occupation": O
"has O
occupation", O
"has-parent": O
"has O
parent", O
"has-population": O
"has O
population", O
"has-sibling": O
"has O
sibling", O
"has-spouse": O
"has O
spouse", O
"hastourist-attraction": O
"has O
tourist O
attraction", O
"has-type": O
"has O
type", O
"has-weight": O
"has O
weight", O
"headquarters": O
"headquarters", O
"invented-by": O
"invented O
by", O
"inventedwhen": O
"invented O
when", O
"is-member-of": O
"is O
member O
of", O
"is-where": O
"located O
in", O
"loc-leader": O
"location O
leader", O
"movie-hasdirector": O
"movie O
has O
director", O

"no_relation": O
"no O
relation", O
"org-has-founder": O
"organization O
has O
founder", O
"org-has-member": O
"organization O
has O
member", O
"org-leader": O
"organization O
leader", O
"post-code": O
"post O
code", O
"starring": O
"starring", O
"won-award": O
"won O
award"; O
1 O
Introduction O
010 O
• O
AR O
"event-year": O
" O
", O
"hasedu": O
" O
", O
"has-genre": O
" O
"has-population": O
" O
", O
"has-type": O
" O
", O
"is-member-of": O
" O
", O
• O
DE O
"birth-place": O
"Geburtsort", O
"eventyear": O
"Veranstaltungsjahr", O

"from-country": O
"vom O
Land", O
"has-author": O
"hat O
Autor", O
"haschild": O
"hat O
Kind", O
"has-edu": O
"hat O
Bildung", O
"has-genre": O
"hat O
Genre", O
"has-occupation": O
"hat O
Beruf", O
"has-parent": O
"hat O
Elternteil", O
"has-population": O
"hat O
Bevölkerung", O
"has-spouse": O
"hat O
Ehepartner", O
"has-type": O
"hat O
Typ", O
"headquarters": O
"Hauptsitz", O
"ismember-of": O
"ist O
Mitglied O
von", O
"is-where": O
"gelegen O
in", O
"loc-leader": O
"Standortleiter", O
"movie-has-director": O
"Film O
hat O
Regisseur", O
"no_relation": O
"keine O
Beziehung", O
"org-hasfounder": O
"Organisation O

hat O
Gründer", O
"orghas-member": O
"Organisation O
hat O
Mitglied", O
"org-leader": O
"Organisationsleiter", O
"wonaward": O
"gewann O
eine O
Auszeichnung"; O
• O
ES O
"birth-place": O
"lugar O
de O
nacimiento", O
"event-year": O
"año O
del O
evento", O
"fromcountry": O
"del O
país", O
"has-author": O
"tiene O
autor", O
"has-child": O
"tiene O
hijo", O
"hasedu": O
"tiene O
educación", O
"has-genre": O
"tiene O
género", O
"has-occupation": O
"tiene O
ocupación", O
"has-parent": O
"tiene O
padre", O
"has-population": O
"tiene O
población", O
"has-spouse": O
"tiene O
cónyuge", O
"has-type": O

"tiene O
tipo", O
"headquarters": O
"sede O
central", O
"is-member-of": O
"es O
miembro O
de", O
"is-where": O
"situado O
en", O
"loc-leader": O
"líder O
de O
ubicación", O
"moviehas-director": O
"película O
cuenta O
con O
el O
director", O
"no_relation": O
"sin O
relación", O
"orghas-founder": O
"organización O
cuenta O
con O
el O
fundador", O
"org-has-member": O
"organización O
tiene O
miembro", O
"won-award": O
"ganó O
el O
premio"; O
• O
AR O
"event-year": O
" O
", O
"has-011 O
edu": O
" O
", O
"has-genre": O
" O

A.1 O
Hyperparameter O
Search O
We O
investigated O
the O
following O
possible O
hyperparameters O
for O
few-shot O
settings. O
For O
fully-supervised, O
we O
take O
hyperparameters O
from O
literature O
(see O
Section O
4.4). O
Number O
of O
epochs B-HyperparameterName
: O
[10,20] B-HyperparameterValue
; O
Learning B-HyperparameterName
rate I-HyperparameterName
: O
[1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
1 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue

] I-HyperparameterValue
. O
Batch B-HyperparameterName
size I-HyperparameterName
: O
[16,64,256] B-HyperparameterValue
, O
not O
tuned O
but O
selected O
based O
on O
available O
GPU O
VRAM. O
We O
manually O
tune O
these O
hyperparameters, O
based O
on O
the O
micro-F B-MetricName
1 I-MetricName
score O
on O
the O
validation O
set. O
A.2 O
Computing O
Infrastructure O
Fully O
supervised O
experiments O
are O
conducted O
on O
a O
single O
A100-80GB O
GPU. O
Few-shot O
and O
zero-shot O
experiments O
are O
conducted O

on O
a O
single O
A100 O
GPU. O
A.3 O
Average O
Running O
Time O
Fully O
supervised O
It O
takes O
5 O
hours O
to O
train O
for O
1 O
run O
with O
mT5 B-MethodName
BASE I-MethodName
and O
a O
prompt O
method O
(null O
prompts, O
CS, O
SP O
and O
IL) O
on O
either O
English, O
or O
all O
other O
languages O
in O
total. O
With O
XLM-R B-MethodName
EM I-MethodName
the O
running O
time O
is O
3 O

hours. O
Few O

Inducer-tuning B-MethodName
: O
Connecting O
Prefix-tuning B-TaskName
and O
Adapter-tuning B-TaskName
Prefix-tuning B-TaskName
, O
or O
more O
generally O
continuous O
prompt O
tuning, O
has O
become O
an O
essential O
paradigm O
of O
parameter-efficient O
transfer O
learning. O
Using O
a O
large O
pre-trained O
language O
model O
(PLM), O
prefix-tuning B-TaskName
can O
obtain O
strong O
performance O
by O
training O
only O
a O
small O
portion O
of O
parameters. O
In O
this O
paper, O
we O
propose O
to O
understand O

and O
further O
develop O
prefix-tuning B-TaskName
through O
the O
kernel O
lens. O
Specifically, O
we O
make O
an O
analogy O
between O
prefixes O
and O
inducing O
variables O
in O
kernel O
methods O
and O
hypothesize O
that O
prefixes O
serving O
as O
inducing O
variables O
would O
improve O
their O
overall O
mechanism. O
From O
the O
kernel O
estimator O
perspective, O
we O
suggest O
a O
new O
variant O
of O
prefix-tuning-inducer-tuning B-MethodName
, O
which O
shares O
the O

exact O
mechanism O
as O
prefix-tuning B-TaskName
while O
leveraging O
the O
residual O
form O
found O
in O
adaptertuning B-TaskName
. O
This O
mitigates O
the O
initialization O
issue O
in O
prefix-tuning B-TaskName
. O
Through O
comprehensive O
empirical O
experiments O
on O
natural O
language O
understanding O
and O
generation O
tasks, O
we O
demonstrate O
that O
inducer-tuning B-MethodName
can O
close O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
fine-tuning. O
* O
Equal O
contribution. O
This O
work O

was O
performed O
while O
the O
first O
author O
was O
interning O
at O
Amazon O
Alexa O
AI. O
Introduction O
Transfer O
learning O
from O
large O
pre-trained O
language O
models O
(PLMs) O
has O
been O
the O
de-facto O
method O
to O
tackle O
downstream O
natural O
language O
processing O
(NLP) O
tasks O
with O
proven O
performance O
and O
scalability O
(Peters O
et O
al., O
2018). O
Among O
all O
the O
adaption O
techniques, O
fine-tuning O
(Howard O

and O
Ruder, O
2018;Kale O
and O
Rastogi, O
2020) O
is O
predominant O
for O
PLMs O
and O
maintains O
the O
models' O
architecture O
while O
updating O
all O
the O
parameters O
within. O
Though O
powerful, O
fine-tuning O
is O
considered O
parameter-inefficient O
since O
it O
results O
in O
separate O
copies O
of O
model O
parameters O
for O
each O
task/client O
after O
training. O
With O
the O
sizes O
of O
PLMs O
increasing O
to O
hundreds O
of O

millions O
(Brown O
et O
al., O
2020) O
or O
even O
up O
to O
tril-lion O
(Fedus O
et O
al., O
2021) O
parameters, O
the O
trend O
motivates O
a O
range O
of O
parameter-efficient O
adaptation O
techniques, O
including O
adapter-tuning B-TaskName
and O
prompting, O
as O
promising O
lightweight O
alternatives O
to O
finetuning O
to O
reduce O
computational O
consumption O
and O
storage O
space. O
Adapter-tuning B-TaskName
inserts O
bottlenecked O
Multi-layer O
Perception O
(MLP) O
modules O
between O
the O

pre-trained O
layers O
of O
PLMs O
and O
tunes O
only O
these O
new O
parameters O
for O
task O
adaptation O
(Houlsby O
et O
al., O
2019;Pfeiffer O
et O
al., O
2020a). O
Prompting, O
instead, O
aims O
to O
adapt O
the O
general-purpose O
PLMs O
through O
prompts, O
whose O
effectiveness O
has O
been O
shown O
on O
a O
frozen O
GPT-3 O
model O
(Brown O
et O
al., O
2020). O
An O
implicit O
drawback O
of O
the O
prompt-based O

adaptation O
is O
the O
difficulty O
of O
searching O
for O
the O
proper O
prompt. O
To O
avoid O
manually O
designing O
the O
prompts, O
Shin O
et O
al. O
(2020) O
propose O
a O
search O
algorithm O
to O
find O
the O
effective O
prompt O
over O
discrete O
space O
of O
vocabularies; O
prefix-tuning O
(Li O
and O
Liang, O
2021) O
and O
other O
concurrent O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b,a) O

further O
extend O
the O
discrete O
search O
to O
continuous O
prompts, O
attaining O
performance O
close O
to O
fine-tuning O
in O
some O
tasks. O
Despite O
the O
effort, O
there O
is O
still O
a O
performance O
gap O
between O
" O
prefixtuning B-TaskName
" O
and O
"fine-tuning" O
in O
many O
tasks, O
especially O
when O
the O
model O
size O
is O
small O
(Lester O
et O
al., O
2021;He O
et O
al., O
2021a). O
In O
addition, O

the O
mechanism O
of O
prefix-tuning O
is O
still O
poorly O
understood O
and O
underexplored. O
Prefix-tuning B-TaskName
is O
also O
similar O
to O
adaptertuning B-TaskName
, O
since O
they O
both O
insert O
additional O
modules O
into O
each O
transformer O
layer O
(classical O
prompt-based O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b) O
only O
add O
prompts O
to O
the O
embedding O
layer). O
Scrutinizing O
the O
evolution O
of O
prompt-based O
methods, O

we O
can O
observe O
they O
have O
gradually O
deviated O
from O
the O
concept O
of O
"prompts". O
Compared O
to O
the O
manually O
designed O
prompts, O
the O
discrete O
search O
usually O
results O
in O
counter-intuitive O
prompt O
tokens, O
which O
vaguely O
match O
the O
topic O
but O
are O
not O
as O
sensible O
as O
the O
manual O
one; O
for O
continuous O
prompt O
tuning, O
it O
even O
breaks O
the O
limit O

of O
the O
existing O
vo-cabulary. O
All O
these O
pieces O
imply O
that O
the O
mechanism O
behind O
prompt-based O
tuning O
might O
be O
more O
complicated O
than O
guiding O
the O
output O
through O
hint O
prompts. O
To O
open O
the O
black O
box O
of O
"prompts", O
in O
this O
work, O
we O
propose O
to O
consider O
the O
prompts O
(either O
hard O
or O
soft) O
as O
"inducing O
variables" O
in O
kernel O

methods O
(Titsias, O
2009). O
This O
analogy O
is O
justified O
due O
to O
the O
close O
connection O
between O
attention O
modules O
in O
PLMs O
and O
kernel O
estimators O
(Choromanski O
et O
al., O
2020;Tsai O
et O
al., O
2019). O
This O
kernel O
perspective O
explains O
the O
potential O
mechanism O
of O
prefix-tuning B-TaskName
and O
motivates O
a O
new O
method, O
inducer-tuning B-MethodName
. O
Specifically, O
inducertuning B-MethodName
freezes O
all O
the O
original O
parameters O

in O
the O
PLMs O
as O
other O
prompt-based O
methods; O
when O
computing O
the O
attention O
output O
for O
a O
certain O
input O
token O
in O
each O
layer, O
inducer-tuning O
utilizes O
a O
point O
close O
to O
the O
query O
vector O
as O
the O
"inducer". O
This O
unique O
"soft O
prompt" O
eases O
the O
search O
for O
appropriate O
prompts O
and O
builds O
a O
new O
connection O
between O
"prompting" O
and O

" O
adapter-tuning B-TaskName
". O
In O
summary, O
the O
contribution O
of O
this O
work O
is O
three-fold: O
1 O
We O
explain O
the O
underlying O
mechanism O
of O
prefix-tuning B-TaskName
as O
the O
inducing O
variables O
in O
kernel O
learning. O
2 O
We O
propose O
a O
new O
parameterefficient O
adaptation O
technique, O
inducer-tuning B-MethodName
, O
to O
further O
improve O
prefix-tuning B-TaskName
. O
3 O
Through O
comprehensive O
empirical O
studies, O
we O
verify O
our O

proposed O
method O
can O
close O
the O
gap O
between O
" O
prefix-tuning B-TaskName
" O
and O
"fine-tuning" O
on O
relatively O
small O
PLMs, O
and O
provide O
a O
tighter O
lower O
bound O
on O
the O
potential O
of O
continuous O
prompt O
tuning. O
Related O
Work O
In O
this O
section, O
we O
briefly O
introduce O
the O
classical O
form O
of O
adapter-tuning B-TaskName
and O
mainly O
focus O
on O
the O
different O
variants O
of O

prompting. O
Adapter-tuning B-TaskName
. O
Compared O
to O
fine-tuning O
all O
the O
parameters O
in O
the O
PLMs, O
Houlsby O
et O
al. O
(2019), O
Pfeiffer O
et O
al. O
(2020a) O
propose O
to O
modulate O
the O
output O
of O
a O
transformer O
layer O
through O
inserting O
additional O
small-bottleneck O
MLP O
layers O
(adapters) O
(Houlsby O
et O
al., O
2019) O
1 O
: O
Adapter(h) O
= O
h O
+ O
ReLU(hW O
1 O
)W O
2 O

, O
(1) O
where O
h O
is O
the O
dimension-d O
hidden O
state O
in O
the O
transformer O
and O
W O
1 O
, O
W O
2 O
are O
d-by-r O
and O
r-by-d O
projection O
matrices. O
Adapters O
have O
a O
residual O
form O
similar O
to O
skip O
connection, O
while O
only O
W O
1 O
, O
W O
2 O
will O
be O
trained, O
greatly O
decreasing O
the O
size O
of O
tunable O
parameters. O

Up O
to O
now, O
the O
adapter-based O
method O
has O
been O
widely O
used O
for O
multiple O
NLP O
tasks O
(Stickland O
and O
Murray, O
2019;Pfeiffer O
et O
al., O
2020a;Wang O
et O
al., O
2020;Pfeiffer O
et O
al., O
2020b;Üstün O
et O
al., O
2020;Vidoni O
et O
al., O
2020;Pfeiffer O
et O
al., O
2021;He O
et O
al., O
2021b;Xu O
et O
al., O
2021;Rücklé O
et O
al., O
2020;Karimi O
Mahabadi O
et O
al., O
2021), O
and O

adapters O
are O
also O
intrinsically O
connected O
to O
many O
other O
parameter-efficient O
adaptation O
techniques, O
as O
detailed O
in O
He O
et O
al. O
(2021a). O
Prompting. O
Prompting O
prepends O
task-specific O
instructions O
to O
the O
task O
input O
and O
was O
originally O
demonstrated O
in O
Brown O
et O
al. O
(2020). O
As O
manual O
prompts O
rely O
on O
trial O
and O
error, O
, O
Shin O
et O
al. O
(2020) O
suggests O

search O
algorithms O
to O
specify O
the O
prompts O
among O
all O
the O
tokens O
in O
the O
vocabulary. O
Prompt-tuning B-TaskName
(Lester O
et O
al., O
2021) O
and O
P-tuning O
(Liu O
et O
al., O
2021b) O
remove O
the O
vocabulary O
restriction O
on O
prompts O
by O
using O
trainable O
"soft O
prompts". O
The O
prompts O
in O
the O
aforementioned O
methods O
are O
only O
inserted O
into O
the O
bottom O
embedding O
layer O
of O

PLMs, O
while O
Prefix-tuning B-TaskName
(Li O
and O
Liang, O
2021;Liu O
et O
al., O
2021a) O
adds O
soft O
prompts O
to O
all O
the O
transformer O
layers O
to O
further O
increase O
the O
capacity O
of O
prompting. O
Though O
effective, O
proper O
initialization O
of O
the O
soft O
prompts O
remains O
challenging. O
To O
mitigate O
the O
issue, O
Li O
and O
Liang O
(2021) O
used O
an O
extra O
MLP O
to O
reparameterize O
the O

prompts O
in O
each O
layer, O
thus O
adding O
more O
parameters O
that O
need O
training; O
SPoT O
(Vu O
et O
al., O
2021) O
suggests O
performing O
pre-training O
for O
soft O
prompts O
using O
a O
wide O
range O
of O
NLP O
tasks, O
which O
requires O
additional O
computational O
resources. O
In O
contrast, O
though O
adapters O
have O
a O
similar O
expression O
form O
to O
prefix-tuning B-TaskName
(He O
et O
al., O
2021a), O
adaptertuning B-TaskName

only O
requires O
regular O
initialization. O
We O
speculate O
that O
the O
residual O
form O
of O
adapters O
mitigates O
the O
initialization O
issue O
since O
the O
output O
of O
each O
layer O
in O
the O
new O
model O
would O
be O
centered O
around O
the O
output O
in O
the O
frozen O
PLMs, O
and O
the O
residual O
form O
contributes O
to O
gradient O
back-propagation O
as O
in O
skip O
connection. O
We O
rely O

on O
this O
intuition O
and O
utilize O
the O
above-mentioned O
advantages O
of O
adapters O
to O
guide O
the O
design O
of O
our O
proposed O
inducer-tuning B-MethodName
. O
Preliminaries: O
Transformer O
Layers O
Before O
discussing O
the O
mechanism O
of O
prompt-tuning, O
we O
introduce O
the O
structure O
of O
transformer O
layers O
and O
necessary O
notations O
in O
this O
section. O
A O
general O
transformer-based O
PLM O
is O
mainly O
composed O
of O
L O

stacked O
layers. O
Each O
layer O
contains O
a O
multi-headed O
self-attention O
and O
a O
fully O
connected O
feed-forward O
network O
(FFN) O
sub-layer, O
both O
followed O
by O
an O
"Add O
& O
Norm" O
module O
(Vaswani O
et O
al., O
2017). O
2 O
Hereon, O
we O
shall O
focus O
on O
the O
structure O
of O
the O
attention O
sub-layer O
since O
prefix-tuning B-TaskName
directly O
works O
on O
this O
sub-layer. O
Passing O
a O
length-n O

input O
sequence O
X O
∈ O
R O
n×N O
h O
p O
to O
an O
attention O
sub-layer O
(assuming O
N O
h O
heads O
and O
dimension O
size O
p O
for O
each O
head), O
we O
first O
perform O
linear O
transforms O
to O
the O
input O
X O
and O
obtain O
the O
query O
matrix O
(Q), O
the O
key O
matrix O
(K), O
and O
the O
value O
matrix O
(V O
) O
as: O
Q/K/V O

= O
XW O
[q/k/v] O
+ O
1b O
T O
[q/k/v] O
, O
(2) O
where O
Q, O
K, O
V O
∈ O
R O
n×N O
h O
p O
are O
the O
query/ O
key/ O
value O
matrix; O
W O
[q/k/v] O
∈ O
R O
N O
h O
p×N O
h O
p O
are O
the O
weight O
matrices, O
and O
b O
[q/k/v] O
∈ O
R O
N O
h O
p O
are O
the O
bias O
terms O
in O

the O
corresponding O
transformations. O
3 O
To O
increase O
the O
model O
capacity, O
the O
three O
components O
Q, O
K, O
V O
are O
respectively O
divided O
into O
N O
h O
blocks, O
contributing O
to O
the O
attention O
output O
in O
each O
head O
of O
the O
multi-headed O
self-attention O
module. O
For O
instance, O
we O
represent O
Q O
as O
Q O
= O
Q O
(1) O
, O
• O
• O
• O
, O

Q O
(N O
h O
) O
, O
where O
each O
block O
Q O
(h) O
= O
XW O
(h) O
q O
+ O
1(b O
(h) O
q O
) O
T O
is O
an O
n-by-p O
matrix, O
and O
W O
(h) O
q O
, O
b O
(h) O
q O
are O
the O
corresponding O
parts O
in O
W O
q O
, O
b O
q O
. O
The O
attention O
output O
for O
the O
h O
th O

head O
is: O
L O
(h) O
V O
(h) O
:= O
softmax(Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p)V O
(h) O
= O
(D O
(h) O
) O
−1 O
M O
(h) O
V O
(h) O
,(3) O
where O
M O
(h) O
:= O
exp O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
and O
D O
(h) O
is O
a O
diagonal O
matrix O
in O
which O

D O
(h) O
ii O
is O
the O
sum O
of O
the O
i-th O
row O
in O
M O
(h) O
, O
serving O
as O
the O
normalization O
procedure O
in O
softmax. O
The O
attention O
outputs O
in O
each O
head O
are O
then O
concatenated O
as O
L O
:= O
(L O
(1) O
V O
(1) O
, O
. O
. O
. O
, O
L O
(N O
h O
) O
V O
(N O
h O
) O

). O
After O
concatenating O
the O
heads, O
there O
is O
a O
linear O
transform O
following O
the O
output O
LW O
o O
+ O
1b O
T O
o O
,(4) O
where O
W O
o O
and O
b O
o O
are O
similarly O
sized O
as O
the O
other O
matrices O
in O
Equation O
(2). O
This O
is O
the O
overall O
output O
of O
the O
attention O
sub-layer, O
which O
we O
shall O
revisit O
in O

§ O
4.4. O
Attention O
as O
Kernel O
Estimators O
Traditionally, O
attention O
operation O
(Equation O
( O
3)) O
is O
viewed O
as O
a O
transformation O
g(•) O
of O
the O
input O
sequence O
X. O
However, O
in O
prefix-tuning B-TaskName
, O
parameters O
within O
PLMs O
are O
frozen, O
which O
implies O
that O
given O
the O
input O
X, O
the O
represenattions O
Q, O
K, O
and O
V O
are O
invariant. O
4 O
This O
observation O

allows O
us O
to O
reinterpret O
attention O
as O
a O
kernel O
estimator O
f O
(•) O
with O
Q O
as O
its O
input. O
Specifically, O
we O
denote O
the O
i-th O
input O
vector O
X O
i O
's O
attention O
operation O
as O
f O
(Q O
i O
) O
:= O
g(X O
i O
). O
This O
attention O
representation O
can O
be O
seen O
as O
modifying O
the O
input O
query O
vector O
Q O

i O
to O
f O
(Q O
i O
) O
via O
supporting O
points O
{K O
j O
} O
n O
j=1 O
(Choromanski O
et O
al., O
2020;Peng O
et O
al., O
2020;, O
which O
can O
be O
considered O
as O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39): O
row-normalize O
(κ O
(Q, O
K)) O
V O
, O
where O
κ(•, O
•) O
is O
a O
kernel O
function. O
(Refer O
to O
Appendix O

C O
for O
more O
details O
on O
this O
claim.) O
Prefix-Tuning B-TaskName
and O
Inducing O
Variables O
Prefix-tuning O
(Li O
and O
Liang, O
2021) O
alters O
the O
attention O
output O
in O
each O
layer. O
Concretely, O
it O
prepends O
length-l O
prefix O
vectors O
P O
k O
, O
P O
v O
∈ O
R O
l×p O
to O
K O
and O
V O
, O
respectively; O
for O
a O
certain O
query O
token O
Q O
i O

(the O
i-th O
row O
of O
the O
query O
matrix O
Q), O
its O
attention O
output O
f O
(Q O
i O
) O
:= O
Attn(Q O
i O
, O
K, O
V O
) O
is O
updated O
as O
a O
weighted O
sum O
of O
f O
(Q O
i O
) O
and O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
) O
(He O
et O
al., O
2021a, O
Equation O
(7)). O
Remark. O

From O
the O
kernel O
estimator O
perspective, O
the O
two O
categories O
of O
virtual O
tokens O
play O
different O
roles. O
The O
virtual O
key O
vectors O
P O
k O
apply O
to O
the O
empirical O
kernel O
matrix O
part O
and O
can O
alter O
the O
attention O
scores O
(and O
thus O
the O
weights O
for O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
)); O
whereas O
P O
v O

takes O
effect O
in O
the O
value O
part. O
It O
might O
not O
be O
optimal O
for O
prefix-tuning B-TaskName
to O
model O
the O
two O
categories O
of O
virtual O
tokens O
similarly. O
In O
§ O
4.3 O
we O
will O
show O
how O
inducer-tuning O
addresses O
the O
two O
parts O
through O
different O
residual O
forms. O
We O
suggest O
that O
the O
mechanism O
of O
prefix-tuning B-TaskName
can O
be O
further O
understood O
through O

the O
concept O
of O
inducing O
variables O
in O
kernel O
learning O
literature O
(Titsias, O
2009). O
Many O
computational O
methods O
in O
kernel O
learning O
utilize O
a O
small O
set O
of O
support O
points O
(inducing O
variables) O
to O
improve O
the O
inference O
performance O
(Musco O
and O
Musco, O
2017;. O
Snelson O
and O
Ghahramani O
(2005) O
specifically O
consider O
the O
inducing O
variables O
as O
auxiliary O
pseudo-inputs O
and O
infer O
them O

using O
continuous O
optimization, O
which O
is O
similar O
to O
prefix-tuning B-TaskName
. O
We O
emphasize O
that O
from O
the O
first O
sight O
the O
main O
character O
of O
inducing-point O
methods O
is O
representing O
a O
vast O
amount O
of O
training O
examples O
through O
a O
small O
number O
of O
points, O
so O
as O
to O
reduce O
the O
computational O
cost; O
however, O
here O
we O
instead O
aim O
to O
leverage O

the O
mechanism O
of O
inducing O
variables O
to O
well-steer O
the O
estimation: O
the O
goal O
we O
try O
to O
attain O
is O
to O
strengthen O
prefix-tuning B-TaskName
by O
making O
the O
prefixes O
better O
modulate O
the O
attention O
output. O
We O
introduce O
and O
analyze O
the O
mechanism O
as O
follows. O
Mechanism O
for O
well-steering O
inference O
outputs O
in O
inducing-point O
methods. O
Conceptually, O
inducing O
variables O
help O
the O
inference O

because O
they O
can O
represent O
the O
distribution O
of O
the O
query O
inputs O
and O
steer O
the O
kernel O
methods O
without O
changing O
the O
kernel O
in O
use. O
In O
particular, O
we O
consider O
the O
distribution O
pattern O
of O
unconstrained O
inducing O
points O
X O
M O
(Snelson O
and O
Ghahramani, O
2005, O
Figure O
1). O
We O
observe O
that O
most O
of O
them O
are O
close O
to O
the O

testing O
examples O
X O
* O
, O
and O
in O
the O
new O
estimation O
(Snelson O
and O
Ghahramani, O
2005, O
Equation O
( O
8)) O
the O
inducers O
X O
M O
will O
receive O
great O
weights O
through O
the O
weights O
assignment O
mechanism O
in O
kernel O
methods O
(we O
recall O
kernel O
methods O
can O
assign O
the O
weights O
of O
samples O
as O
attention O
(Choromanski O
et O
al., O
2020;Tsai O
et O

al., O
2019); O
for O
inducing O
variables O
close O
to O
the O
query, O
they O
would O
automatically O
receive O
more O
attention), O
and O
thus O
effectively O
modulate O
the O
output. O
From O
this O
mechanism, O
we O
draw O
an O
inductive O
bias O
"the O
prefix O
should O
be O
close O
to O
the O
query" O
(which O
is O
not O
enforced O
in O
the O
method O
of O
prefix-tuning B-TaskName
) O
and O
accordingly O
propose O

inducer-tuning B-MethodName
. O
We O
remark O
since O
we O
are O
not O
pursuing O
the O
original O
goal, O
reducing O
computational O
cost, O
of O
inducing O
variables, O
it O
is O
ordinary O
that O
the O
concrete O
design O
in O
the O
next O
subsection O
is O
different O
from O
the O
usual O
form O
of O
inducing O
points, O
a O
small O
number O
of O
samples. O
We O
speculate O
prefix-tuning B-TaskName
partially O
benefits O
from O
the O

above O
mechanism O
as O
well. O
Furthermore, O
some O
indirect O
evidence O
is O
stated O
as O
follows. O
As O
discussed O
in O
previous O
studies, O
to O
make O
the O
full O
potential O
of O
prompting, O
the O
manually O
designed O
prompts O
are O
expected O
to O
be O
related O
to O
the O
topic O
of O
the O
input O
sequence O
(Brown O
et O
al., O
2020) O
(close O
to O
the O
query); O
even O
for O

the O
soft O
prompts O
they O
are O
recommended O
to O
be O
initialized O
with O
the O
token O
relevant O
to O
the O
specific O
tasks O
(Li O
and O
Liang, O
2021), O
which O
also O
requires O
the O
prompts O
to O
be O
close O
to O
the O
query O
to O
provide O
effective O
adaptation. O
With O
this O
belief, O
we O
propose O
inducer-tuning B-MethodName
to O
exploit O
further O
the O
mechanism O
of O
inducing O
variables O

and O
improve O
upon O
prefix-tuning B-TaskName
. O
Method O
Inducer-tuning B-MethodName
follows O
the O
same O
design O
principle O
as O
prefix-tuning B-TaskName
, O
which O
modulates O
the O
attention O
output O
through O
inserting O
virtual O
tokens O
(vectors). O
However, O
unlike O
prefix-tuning B-TaskName
, O
our O
virtual O
tokens O
are O
not O
shared O
among O
the O
input O
sequences. O
Inducertuning O
also O
incorporates O
the O
benefits O
of O
residual O
forms O
to O
ease O
the O

initialization O
and O
remove O
the O
reparametrization O
trick O
in O
prefix-tuning B-TaskName
. O
Specifically, O
we O
suggest O
the O
following O
modifications: O
1 O
The O
"inducers" O
are O
adaptive O
to O
and O
customized O
for O
each O
input O
token O
to O
strengthen O
the O
expressiveness O
of O
the O
new O
attention O
output. O
2 O
We O
propose O
to O
model O
the O
virtual O
vectors O
in O
a O
residual O
form O
as O
an O

adapter, O
which O
makes O
the O
final O
attention O
output O
be O
in O
a O
residual O
form O
as O
well. O
We O
now O
dive O
into O
discussing O
the O
intuitions O
behind O
the O
modifications O
in O
detail. O
Adaptive B-TaskName
inducers I-TaskName
. O
There O
is O
an O
important O
difference O
between O
language O
models O
and O
kernel O
methods, O
making O
fixed O
prefixes O
less O
effective O
than O
inducing O
variables O
in O
kernel O

methods. O
In O
language O
models, O
the O
distribution O
of O
the O
input O
queries O
keeps O
changing, O
and O
for O
some O
inputs, O
the O
fixed O
prefixes O
fail O
to O
be O
qualified O
as O
"inducing O
variables". O
Even O
worse, O
for O
a O
long O
input, O
there O
probably O
exists O
some O
query O
vectors O
away O
(regarding O
ℓ O
2 O
distance) O
from O
all O
the O
virtual O
vectors O
in O
the O

fixed O
prefixes, O
which O
are O
thus O
unable O
to O
modulate O
the O
attention O
output O
well. O
The O
phenomenon O
that O
prefix-tuning B-TaskName
has O
a O
relatively O
poorer O
performance O
on O
tasks O
with O
longer O
inputs O
can O
be O
observed O
in O
our O
experiments O
( O
§ O
6). O
To O
alleviate O
the O
above O
issue, O
we O
propose O
adaptive O
modeling O
of O
the O
virtual O
key O
vectors. O
For O

a O
query O
Q O
i O
, O
we O
suggest O
taking O
a O
vector O
close O
to O
Q O
i O
itself O
as O
the O
corresponding O
virtual O
key O
vector O
(the O
length O
of O
the O
new O
prefix O
is O
thus O
1), O
in O
the O
hope O
of O
leading O
to O
better O
inference. O
As O
for O
the O
virtual O
value O
vectors, O
we O
relate O
them O
to O
the O
corresponding O

virtual O
key O
vectors. O
The O
motivation O
comes O
from O
traditional O
(non-self-)attention, O
whose O
mechanism O
coincides O
with O
a O
kernel O
estimator: O
the O
value O
V O
is O
independent O
of O
the O
query O
sequence O
Q O
and O
related O
to O
the O
supporting O
points O
K. O
Specifically, O
considering O
our O
design O
above O
that O
the O
virtual O
key O
vectors O
are O
close O
to O
Q O
i O
(we O
take O

the O
virtual O
key O
vectors O
as O
transforms O
of O
the O
input O
query O
vectors O
Q O
i O
's), O
we O
propose O
to O
accordingly O
model O
the O
virtual O
value O
vectors O
as O
a O
map O
of O
Q O
i O
as O
well, O
which O
implies O
the O
virtual O
value O
vectors O
are O
also O
adaptive O
to O
the O
input O
query O
vectors. O
Adapter O
Structures. O
To O
stabilize O
the O

training O
procedure, O
we O
propose O
incorporating O
the O
adapter O
structures O
into O
modeling O
the O
virtual O
key/value O
vectors. O
Specifically, O
for O
the O
i-th O
token O
Q O
i O
(in O
a O
certain O
head), O
we O
represent O
the O
corresponding O
virtual O
key/value O
vectors O
respectively O
as O
P O
k,i O
= O
Q O
i O
+ O
MLP O
k O
(Q O
i O
)(5) O
P O
v,i O
= O
f O
(Q O

i O
) O
+ O
MLP O
v O
(Q O
i O
),(6) O
where O
MLP O
k/v O
will O
both O
return O
a O
vector O
of O
the O
same O
dimension O
as O
the O
input O
Q O
i O
. O
5 O
It O
is O
natural O
to O
model O
P O
k,i O
in O
a O
residual O
form O
as O
in O
Equation O
( O
1), O
considering O
P O
k,i O
is O
expected O
to O
center O

around O
Q O
i O
; O
as O
for O
P O
v,i O
, O
we O
claim O
the O
specific O
form O
in O
Equation O
( O
6) O
allows O
the O
complete O
expression O
of O
inducer-tuning B-MethodName
to O
be O
adapter-like, O
and O
the O
justification O
is O
stated O
as O
the O
following O
derivation. O
To O
derive O
the O
expression O
for O
inducer-tuning B-MethodName
, O
we O
denote O
the O
new O
key O
matrix O
and O

value O
matrix O
(specific O
to O
the O
input O
query O
vector O
Q O
i O
) O
as O
K O
(i) O
= O
P O
T O
k,i O
K O
, O
V O
(i) O
= O
P O
T O
v,i O
V O
T O
. O
The O
new O
attention O
outputf O
(Q O
i O
) O
for O
the O
query O
Q O
i O
is O
thus O
(omitting O
the O
factor O
1/ O
√ O
p O
for O

clarity) O
Attn(Q O
i O
, O
K O
(i) O
, O
V O
(i) O
) O
= O
exp(⟨Q O
i O
, O
P O
k,i O
⟩)P O
v,i O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩)V O
j O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
=λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O

)f O
(Q O
i O
)(7) O
where O
we O
define O
the O
weight O
λ O
i O
as, O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
. O
Combining O
the O
pieces, O
we O
state O
the O
complete O
equation O
for O
the O
new O
attention O
outputf O
(Q O
i O
) O
as, O

λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=λ O
i O
(f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
)) O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
(8 O
) O
We O
observe O

inducer-tuning B-MethodName
now O
perturbs O
the O
output O
f O
(Q O
i O
) O
in O
a O
residual O
form, O
which O
therefore O
connects O
prefix-tuning B-TaskName
and O
adapter-tuning B-TaskName
. O
The O
procedure O
of O
inducer-tuning B-MethodName
is O
summarized O
in O
Figure O
1, O
and O
§ O
6.2 O
shows O
the O
residual O
form O
greatly O
impacts O
the O
model O
performance. O
Extending O
the O
Scope O
of O
Value O
Besides O
the O
representation O
of O

the O
virtual O
vectors, O
we O
propose O
another O
improvement O
via O
the O
self-attention O
decomposition O
proposed O
by O
Hou O
et O
al. O
(2020). O
Considering O
the O
linear O
transform O
right O
after O
the O
attention O
module, O
we O
can O
accordingly O
rewrite O
the O
attention O
sub-layer O
as O
(ignoring O
the O
bias O
term O
in O
the O
linear O
transform) O
N O
h O
h=1 O
softmax(Q O
(h) O
(K O
(h) O
) O

T O
/ O
√ O
p)V O
(h) O
W O
(h) O
o O
, O
where O
W O
(h) O
o O
is O
the O
h-th O
row O
block O
in O
W O
o O
. O
No- O
tably, O
W O
(h) O
o O
is O
attached O
to O
the O
value O
matrix O
V O
(h) O
, O
suggesting O
that O
W O
(h) O
o O
's O
should O
be O
counted O
into O
the O
complete O
kernel O
structure O

of O
a O
head. O
We O
therefore O
define O
the O
complete O
attention O
outputf O
(Q O
(h) O
) O
as O
softmax O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
V O
(h) O
W O
(h) O
o O
, O
(9) O
and O
align O
the O
prefix O
vectors O
P O
v,i O
's O
with O
the O
rows O
in O
(h) O
as O
in O
prefixtuning. O
The O
detailed O
implementation O

of O
the O
extended O
P O
v,i O
is O
provided O
in O
Appendix O
B.3. O
We O
can O
verify O
the O
improvement O
by O
this O
extension O
through O
the O
ablation O
studies O
in O
§ O
6.2. O
V O
(h) O
W O
(h) O
o O
, O
instead O
of O
solely O
V O
A O
Potential O
Limitation O
of O
Prompting O
A O
potential O
limitation O
of O
prompt-based O
methods O
comes O
from O
the O
frozen O

weight O
matrices O
W O
q O
and O
W O
k O
. O
For O
all O
the O
n(n O
+ O
l) O
pairs O
of O
query O
/ O
key O
vectors O
in O
a O
head, O
most O
of O
the O
pairs O
(corresponding O
to O
the O
elements O
within O
QK O
T O
) O
have O
invariant O
. O
. O
MLPk O
+ O
MLPv O
+ O
P O
k O
K O
V O
P O
V O
Q O

i O
Q O
i O
P O
k,i O
K O
P O
v,i O
V O
f(Q O
i O
) O
Prefix-Tuning B-TaskName
Inducer-Tuning B-MethodName
Attention O
Scores O
Attention O
Scores O
Figure O
1: O
The O
mechanisms O
of O
prefix-tuning B-TaskName
(left) O
and O
inducer-tuning B-MethodName
(right) O
in O
inference O
(the O
MLP O
module O
for O
reparameterization O
in O
prefix-tuning O
is O
dropped). O
For O
prefix-tuning B-TaskName
, O
the O
virtual O
tokens O
(P O
k O
, O
P O
v O

) O
are O
shared O
among O
all O
the O
query O
vectors; O
inducer-tuning O
instead O
prepends O
customized O
inducers O
(P O
k,i O
, O
P O
v,i O
) O
for O
a O
certain O
vector O
Q O
i O
. O
pairwise O
positional O
interactions O
due O
to O
the O
frozen O
weight O
matrices O
W O
q O
and O
W O
k O
. O
However, O
on O
downstream O
tasks, O
there O
can O
be O
a O
mismatch O

between O
W O
q O
and O
W O
k O
maintained O
from O
pre-training: O
the O
distribution O
of O
Q, O
K O
will O
substantially O
change O
due O
to O
the O
distinct O
task-specific O
datasets O
as O
well O
as O
the O
virtual O
tokens O
added O
in O
the O
previous O
layers. O
There O
is O
no O
adaptation O
to O
ensure O
the O
positional O
interactions O
between O
Q, O
K O
still O
contribute O
to O
the O

proper O
representation O
f O
(Q O
i O
). O
To O
resolve O
the O
potential O
issue O
of O
prefixtuning B-TaskName
, O
we O
suggest O
applying O
low-rank O
adaptation O
(LoRA) O
(Hu O
et O
al., O
2021) O
to O
W O
q O
as O
a O
complement O
to O
prompt-based O
methods, O
including O
inducer-tuning B-MethodName
. O
Specifically, O
before O
we O
compute O
the O
attention O
output O
in O
each O
layer, O
W O
q O
will O
be O

updated O
as O
W O
q O
← O
W O
q O
+ O
BA,(10) O
where O
W O
q O
is O
kept O
frozen O
and O
B O
∈ O
R O
N O
h O
p×r O
, O
A O
∈ O
R O
r×N O
h O
p O
will O
be O
tunable O
in O
training. O
We O
report O
in O
§ O
6 O
that O
combining O
both O
inducer-tuning O
and O
LoRA O
outperforms O
their O
individual O
counterparts. O
Final O

Model. O
Our O
final O
proposed O
model O
does O
the O
inferencex O
as O
follows: O
1 O
in O
each O
layer, O
we O
first O
apply O
Equation O
(10) O
to O
update O
W O
q O
before O
obtaining O
Q, O
K, O
V O
; O
2 O
construct O
the O
inducer O
matrices O
P O
k O
= O
Q O
+ O
MLP O
k O
(Q), O
and O
compute O
the O
vector O
a O
with O
the O
i-th O

component O
a O
i O
= O
⟨Q O
i O
, O
P O
k,i O
⟩; O
3 O
compute O
the O
matrix O
product O
[a; O
QK O
T O
]/ O
√ O
p O
and O
then O
perform O
softmax O
over O
the O
product-the O
first O
column O
(denoted O
as O
p) O
is O
the O
weights O
λ O
i O
's O
in O
Equation O
(7); O
4 O
obtainf O
(Q) O
as O
in O
Equation O
( O
9), O

and O
returnf O
(Q) O
+ O
diag(p)MLP O
v O
(Q) O
(corresponding O
to O
Equation O
( O
8)) O
as O
the O
complete O
attention O
output. O
Experiments O
While O
prefix-tuning B-TaskName
has O
been O
shown O
comparable O
to O
fine-tuning O
on O
some O
natural O
language O
understanding O
(NLU) O
tasks O
(Liu O
et O
al., O
2021a), O
there O
is O
still O
a O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
finetuning O
on O
natural O
language O

generation O
(NLG) O
tasks, O
especially O
for O
those O
tasks O
with O
long O
input O
sequences. O
Complete O
settings O
of O
the O
experiments O
below O
can O
be O
found O
in O
Appendix O
A O
and O
Appendix O
B. O
The O
code O
for O
our O
algorithms O
is O
publicly O
available O
at O
https://github.com/ychen-stat-ml/kerneladapters. O
Sketch O
of O
the O
Tasks O
We O
test O
the O
performance O
of O
our O
methods O
on O
both O
NLU B-TaskName

and O
NLG B-TaskName
tasks. O
For O
NLU B-TaskName
tasks, O
we O
follow O
(He O
et O
al., O
2021a) O
to O
use O
RoBERTa B-MethodName
BASE I-MethodName
(Liu O
et O
al., O
2019) O
on O
MNLI B-DatasetName
(Williams O
et O
al., O
2018) O
and O
SST2 B-DatasetName
(Socher O
et O
al., O
2013) O
from O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019); O
in O
SST2, O
the O
models O
predict O
the O
two-way O
sentiment O
(positive/negative) O
of O

a O
given O
sentence, O
and O
the O
MNLI B-DatasetName
task O
is O
to O
decide, O
given O
a O
premise O
and O
a O
hypothesis, O
whether O
there O
is O
entailment, O
contradiction, O
or O
neither. O
We O
use O
GPT-2 B-MethodName
SMALL I-MethodName
(Radford O
et O
al., O
2019) O
for O
NLG B-TaskName
tasks: O
WebNLG-challenge B-DatasetName
(Gardent O
et O
al., O
2017) O
focuses O
on O
table-to-text O
tasks, O
in O
which O
the O
language O
models O
generate O
some O

relatively O
long O
and O
sensible O
sentences O
based O
on O
the O
triples O
with O
solely O
a O
few O
words; O
in O
contrast, O
CoQA B-DatasetName
(Reddy O
et O
al., O
2019) O
provides O
the O
data O
for O
conversational O
question O
answering O
6 O
, O
which O
requires O
the O
language O
model O
to O
return O
short O
answers O
to O
questions O
based O
on O
long O
conversational O
materials. O
More O
details O
about O
the O

datasets O
(including O
the O
average O
sequence O
length) O
and O
the O
evaluation O
metrics O
used O
are O
provided O
in O
Appendix O
A. O
Baselines O
We O
compare O
our O
method O
with O
other O
representative O
methods: O
Fine-Tuning O
(Howard O
and O
Ruder, O
2018) O
We O
differentiate O
the O
number O
of O
parameters O
to O
store O
and O
tune, O
as O
for O
prefix-tuning B-TaskName
, O
the O
two O
numbers O
are O
inconsistent O
due O

to O
a O
re-parametrization O
trick O
(Li O
and O
Liang, O
2021) O
to O
mitigate O
the O
initialization O
issue. O
Instead O
of O
directly O
setting O
up O
an O
embedding O
matrix O
for O
virtual O
tokens, O
an O
additional O
MLP O
module O
in O
each O
layer O
is O
used O
in O
prefix-tuning O
to O
model O
the O
representation O
for O
those O
virtual O
tokens; O
after O
the O
fine-tuning O
stage, O
the O
additional O
MLP O

modules O
are O
dropped O
and O
only O
the O
output O
embedding O
for O
virtual O
tokens O
needs O
storing, O
which O
leads O
to O
a O
regular O
number O
of O
parameters O
to O
store. O
For O
the O
proposed O
inducer-tuning B-MethodName
, O
we O
adopt O
the O
residual O
form O
to O
address O
the O
initialization O
issue O
and O
avoid O
the O
usage O
of O
the O
extra O
MLP, O
which O
makes O
inducer-tuning B-MethodName
have O

the O
same O
number O
of O
parameters O
to O
store O
as O
to O
train O
and O
behave O
more O
like O
a O
regular O
adapter. O
To O
make O
a O
fair O
comparison, O
we O
intentionally O
choose O
the O
number O
of O
parameters O
to O
store O
in O
prefixtuning B-TaskName
roughly O
the O
same O
as O
its O
adapter O
counterpart O
by O
adjusting O
the O
prefix O
length. O
Detailed O
settings O
are O
available O
in O

Appendix O
B.4. O
Main O
Results O
We O
conclude O
our O
experimental O
results O
in O
Tables O
1 O
and O
2, O
comparing O
the O
proposed O
inducertuning B-MethodName
( O
§ O
4.3), O
or O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
( O
§ O
4.5), O
against O
other O
baselines. O
The O
benefit O
of O
using O
Mix-And-Match O
(MAM) O
techniques O
(He O
et O
al., O
2021a) O
The O
MAM O
technique O
benefits O
inducer-tuning B-MethodName
. O
As O
remarked O

by O
He O
et O
al. O
(2021a), O
the O
"Mix-And-Match" O
of O
adapters O
in O
both O
self-attention O
and O
FFN O
sub-layers O
can O
better O
exploit O
parameter-efficient O
transfer O
learning O
than O
only O
modulating O
a O
single O
sublayer. O
We O
obtain O
a O
similar O
conclusion O
by O
replacing O
prefix-tuning B-TaskName
with O
inducer-tuning B-MethodName
(+ I-MethodName
LoRA) I-MethodName
in O
self-attention O
sub-layers. O
The O
combination O
( O
MAM B-MethodName
inducer-tuning I-MethodName
) O
performs O
well O

on O
most O
of O
the O
tasks; O
especially O
on O
the O
tasks O
with O
relatively O
longer O
sequences, O
MNLI B-DatasetName
and O
CoQA B-DatasetName
, O
MAM O
inducer-tuning O
attains O
respectively O
0.6% B-MetricValue
and O
1.2% B-MetricValue
performance O
improvement O
over O
vanilla O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
. O
Long O
inputs O
deteriorate O
prefix-tuning. O
Notably, O
the O
performance O
of O
prefix-tuning B-TaskName
is O
sensitive O
to O
the O
input O
length O
(c.f. O
§ O
4.3). O

For O
WebNLG B-DatasetName
with O
short O
inputs, O
prefix-tuning B-TaskName
attains O
comparable O
performance O
with O
fine-tuning O
and O
other O
parameter-efficient O
methods. O
On O
CoQA B-DatasetName
, O
however, O
prefix-tuning B-TaskName
has O
a O
substantially O
lower O
exact-match O
/ O
F1 B-MetricName
score O
than O
others O
(e.g., O
over O
7% B-MetricValue
decrease O
in O
F1 B-MetricName
score O
compared O
with O
fine-tuning). O
The O
similar O
pattern O
can O
be O
observed O
on O
the O
two O
NLU B-TaskName

tasks O
as O
well: O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
other O
candidate O
methods O
is O
much O
smaller O
on O
SST2 B-DatasetName
, O
whose O
mean O
sequence O
length O
is O
shorter O
than O
MNLI B-DatasetName
. O
We O
remark O
our O
proposed O
adaptive O
inducers O
somewhat O
resolve O
the O
issue: O
both O
variants O
of O
inducer-tuning B-DatasetName
in O
Table O
1 O
obtain O
a O
5%+ B-MetricValue
improvement O
on O
CoQA B-DatasetName

. O
Enhance O
inducer-tuning B-MethodName
through O
adapting O
pairwise O
positional O
interactions. O
In O
§ O
4.5, O
we O
speculate O
the O
prompt-based O
methods O
can O
benefit O
from O
adapting O
pairwise O
positional O
interactions, O
and O
we O
investigate O
it O
on O
both O
NLU B-TaskName
and O
NLG B-TaskName
tasks. O
With O
the O
same O
parameter O
budgets, O
the O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
outperforms O
the O
pure O
inducer-tuning B-MethodName
on O
all O
tasks. O
The O

improvement O
is O
more O
evident O
in O
CoQA B-DatasetName
, O
the O
more O
challenging O
generation O
task O
with O
longer O
input O
sequences. O
We O
remark O
that O
inducer-tuning O
more O
effectively O
exploits O
the O
tunable O
parameters O
than O
LoRA-54 O
for O
the O
value O
part, O
as O
the O
combination O
variant O
also O
performs O
better O
than O
pure O
LoRA. O
Ablation O
Studies O
We O
perform O
ablation O
studies O
on O
generation O

tasks O
to O
analyze O
the O
efficacy O
of O
the O
different O
components O
in O
our O
proposed O
method. O
We O
recall O
there O
are O
four O
different O
features O
in O
inducer-tuning B-MethodName
compared O
to O
prefix-tuning B-TaskName
, O
including O
the O
usage O
of O
adaptive O
inducers, O
the O
extension O
of O
virtual O
value O
vectors, O
the O
residual O
form O
of O
P O
k O
, O
and O
the O
design O
for O
P O

v,i O
to O
concentrate O
around O
attention O
output. O
Accordingly, O
we O
implement O
three O
other O
variants O
of O
inducer-tuning O
to O
help O
ablate O
the O
effects O
of O
the O
above-mentioned O
components. O
Among O
them, O
Adaptive O
directly O
takes O
Q O
i O
as O
P O
k,i O
but O
still O
models O
P O
v,i O
as O
f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
upon O

Adaptive, O
Extension O
changes O
P O
v,i O
tof O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
compared O
to O
Extension, O
Inducer-tuning B-MethodName
just O
mod-ifies O
P O
k,i O
to O
Q O
i O
+ O
MLP O
k O
(Q O
i O
); O
to O
justify O
the O
design O
that O
P O
v,i O
centers O
around O
the O
attention O
output, O
Gating O
models O
P O
v,i O
simply O
as O

MLP O
v O
(Q O
i O
), O
and O
the O
new O
complete O
attention O
output O
thus O
becomes O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
The O
concrete O
setting O
of O
each O
variant O
is O
deferred O
to O
Appendix O
B.4 O
due O
to O
limited O
space. O
The O
usage O
of O
adaptive B-TaskName
inducers I-TaskName
. O

To O
demonstrate O
the O
benefits O
of O
adaptive B-TaskName
inducers I-TaskName
, O
we O
compare O
Prefix-tuning-108 O
with O
the O
basic O
counterpart-Adaptive. O
Table O
3 O
shows O
Adaptive O
attains O
close O
performance O
to O
Prefix-tuning B-TaskName
-108 O
on O
WebNLG B-DatasetName
while O
obtaining O
a O
substantial O
improvement O
on O
CoQA B-DatasetName
, O
which O
has O
longer O
inputs. O
The O
extension O
of O
virtual O
value O
vectors. O
We O
observe O
an O
obvious O
improvement O

attributed O
to O
extending O
the O
scope O
of O
virtual O
value O
vectors O
by O
comparing O
the O
performance O
of O
Adaptive O
and O
Extension. O
For O
almost O
all O
the O
metrics, O
Extension O
obtains O
better O
performance O
than O
Adaptive, O
with O
the O
same O
number O
of O
tunable O
parameters. O
The O
residual O
form O
of O
P O
k O
. O
A O
natural O
design O
for O
P O
k O
is O
to O

directly O
model O
it O
as O
Q, O
which O
would O
automatically O
be O
the O
closest O
vectors O
to O
the O
ones O
in O
Q. O
To O
ablate O
the O
usage O
of O
MLP O
k O
, O
we O
compare O
Inducertuning B-MethodName
against O
Extension, O
which O
follows O
the O
natural O
design O
to O
model O
P O
k O
. O
Through O
the O
empirical O
results, O
we O
find O
assigning O
parameters O
to O
MLP O

k O
can O
still O
slightly O
help O
the O
performance O
of O
inducer-tuning B-MethodName
. O
P O
v,i O
centers O
aroundf O
(Q O
i O
). O
Lastly, O
to O
show O
the O
benefits O
of O
modeling O
P O
v,i O
as O
centering O
around O
f O
(Q O
i O
), O
we O
compare O
the O
variant O
Gating B-MethodName
against O
Inducer-tuning B-MethodName
. O
While O
Gating B-MethodName
has O
a O
weighted O
sum O
form O
similar O
to O

prefix-tuning B-TaskName
, O
it O
suffers O
from O
a O
great O
performance O
drop O
on O
both O
tasks, O
which O
justifies O
the O
effectiveness O
of O
our O
design O
for O
P O
v,i O
's. O
Ethics O
Statement O
As O
an O
efficient O
method O
for O
NLP, O
we O
consider O
our O
work O
to O
have O
a O
low O
ethical O
risk O
since O
the O
outcomes O
of O
the O
algorithm O
mainly O
depend O
on O

the O
downstream O
applications. O
The O
usage O
of O
the O
method O
would O
be O
the O
same O
as O
some O
previous O
methods, O
i.e., O
the O
practical O
deployment O
for O
some O
applications. O
Also, O
our O
method O
doesn't O
assume O
any O
specific O
structure O
of O
the O
input O
and O
thus O
doesn't O
leverage O
biases O
in O
the O
data. O
We O
conclude O
that O
our O
work O
will O
not O
likely O

have O
a O
negative O
ethical O
impact. O
A O
Dataset O
Details O
• O
The O
Multi-Genre B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
Corpus O
(Williams O
et O
al., O
2018, O
MNLI B-DatasetName
) O
involves O
433k O
sentence O
pairs O
of O
premises O
and O
hypotheses, O
labeled O
with O
textual O
entailment O
annotations. O
The O
premise O
sentences O
include O
ten O
distinct O
genres, O
and O
the O
classification O
can O
be O
performed O
on O
both O
the O

matched O
(in-domain) O
and O
mismatched O
(cross-domain) O
sections. O
Concatenating O
premises O
and O
hypothesis O
as O
the O
inputs, O
we O
obtain O
the O
sequence O
lengths O
are O
on O
average O
39.9 O
and O
max O
444. O
For O
the O
results O
reported O
in O
Table O
2, O
we O
follow O
Hu O
et O
al. O
( O
2021) O
and O
take O
mismatched O
accuracy O
as O
the O
metric. O
• O
The O
Stanford B-DatasetName
Sentiment I-DatasetName

Treebank I-DatasetName
(Socher O
et O
al., O
2013, O
SST2) O
is O
a O
corpus O
of O
movie O
reviews O
and O
human O
annotations O
of O
their O
sentiment. O
This O
task O
is O
incorporated O
into O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019), O
and O
the O
dataset O
split O
assigns O
67k O
sentences O
to O
the O
training O
set O
and O
0.9k O
to O
the O
dev O
set. O
In O
SST2 B-DatasetName
, O

the O
sequence O
lengths O
are O
on O
average O
13.3 O
and O
max O
66, O
much O
shorter O
than O
in O
MNLI B-DatasetName
. O
As O
specified O
in O
the O
GLUE B-DatasetName
benchmark, O
we O
test O
the O
accuracy O
metric O
on O
whether O
the O
sentiment O
of O
a O
review O
sentence O
is O
positive O
or O
negative. O
• O
The O
instances O
in O
WebNLG B-DatasetName
dataset O
are O
the O
mapping O
set O
of O

RDF O
triples O
to O
text. O
They O
are O
Data/Text O
pairs, O
where O
the O
"Data" O
is O
in O
a O
format O
of O
(subject, O
property, O
object) O
triples. O
For O
the O
train O
and O
the O
validation O
set, O
they O
involve O
nine O
categories O
which O
are O
extracted O
from O
DBpedia B-DatasetName
; O
while O
in O
the O
test O
set, O
there O
are O
five O
extra O
unseen O
categories, O
which O
can O

partially O
reflect O
the O
generalization O
of O
the O
adaptation O
methods. O
The O
input O
sequences O
in O
the O
training O
set O
consist O
of O
1 O
to O
7 O
triples, O
and O
the O
lengths O
of O
most O
sequences O
are O
bounded O
by O
50 O
(as O
each O
triple O
only O
includes O
three O
short O
phrases). O
The O
official O
evaluation O
script O
is O
used O
in O
our O
experiments, O
and O
we O

report O
BLEU B-MetricName
(Papineni O
et O
al., O
2002), O
METEOR B-MetricName
, O
(Lavie O
and O
Agarwal, O
2007) O
and O
TER B-MetricName
(Snover O
et O
al., O
2006) O
as O
the O
metrics. O
• O
CoQA B-DatasetName
is O
a O
large-scale O
dataset, O
mainly O
for O
conversational O
question O
answering. O
It O
collects O
more O
than O
8K O
conversations O
over O
text O
passages, O
involving O
over O
127K O
questions O
with O
answers O
in O
5 O
domains. O

The O
average O
conversation O
length O
is O
15 O
turns O
(each O
turn O
consists O
of O
a O
question O
and O
an O
answer). O
The O
task O
requires O
the O
language O
model O
to O
generate O
answers O
to O
the O
given O
questions O
based O
on O
related O
conversation O
histories O
and O
documents O
in O
the O
dataset. O
The O
average O
passage O
length O
in O
CoQA B-DatasetName
is O
271 O
(Reddy O
et O
al., O

2019, O
Table O
3). O
We O
simply O
follow O
the O
evaluation O
script O
provided O
on O
the O
official O
website, O
reporting O
both O
the O
macro-average B-MetricName
F1 I-MetricName
score O
of O
word O
overlap O
and O
the O
exact-match O
metric O
(Reddy O
et O
al., O
2019). O
B O
Training O
Details O
We O
mainly O
implement O
our O
methods O
based O
on O
the O
GitHub O
repositories O
provided O
by O
Lin O
et O
al. O
(2020) O

and O
He O
et O
al. O
(2021a). O
Our O
code O
will O
be O
made O
public O
after O
the O
review O
procedure. O
B.1 O
General O
Training O
Settings O
For O
the O
NLU B-TaskName
tasks, O
we O
exactly O
follow O
the O
experimental O
setup O
used O
by O
He O
et O
al. O
(2021a), O
and O
more O
details O
can O
be O
found O
in O
Appendix O
B.2. O
For O
the O
two O
NLG B-TaskName
tasks, O
we O

mainly O
follow O
the O
experimental O
setting O
adopted O
by O
Lin O
et O
al. O
(2020), O
and O
specifically, O
keep O
using O
"task O
embeddings" O
in O
our O
experiments, O
as O
they O
are O
also O
applied O
in O
the O
original O
GPT-2 B-MethodName
model. O
These O
task O
embeddings O
are O
specialized O
segment O
embeddings O
used O
to O
indicate O
the O
different O
components O
of O
the O
text O
input O
(e.g., O
the O
three O

components O
of O
a O
triple O
in O
WebNLG B-DatasetName
, O
questions, O
and O
answers O
in O
CoQA B-DatasetName
, O
etc.). O
8 O
We O
list O
the O
task O
embedding O
used O
in O
each O
NLG B-TaskName
task: O
for O
CoQA B-DatasetName
, O
we O
follow O
the O
task O
embedding O
suggested O
by O
Lin O
et O
al. O
(2020); O
for O
WebNLG B-DatasetName
, O
we O
simply O
use O
the O
special O
tokens O
to O
indicate O

the O
different O
components O
in O
the O
triples. O
The O
details O
of O
the O
special O
tokens O
in O
each O
task O
are O
summarized O
in O
Table O
4. O
Notably, O
the O
parameter O
budget O
for O
task O
embedding O
is O
much O
smaller O
than O
the O
number O
of O
tunable O
parameters O
in O
the O
aforementioned O
parameter-efficient O
adaptation O
methods O
(around O
2M). O
B.2 O
Hyper-parameters O
for O
Training O
For O
NLU B-TaskName

tasks, O
we O
train O
the O
models O
with O
Adam O
(Kingma O
and O
Ba, O
2015) O
optimizer O
and O
use O
a O
polynomial O
learning O
rate O
scheduler O
to O
make O
the O
learning O
rate O
linearly O
decay; O
specifically, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
linearly O
warmed O
up O
from O
0 O
for O
the O
first O
6For O
NLG B-TaskName
tasks, O
an O
AdamW O
(Loshchilov O
and O
Hutter, O
2018) O
optimizer O
is O

applied O
to O
train O
the O
models, O
and O
a O
linear O
learning O
rate O
scheduler O
with O
a O
500-step B-HyperparameterValue
warmup B-HyperparameterName
duration I-HyperparameterName
is O
used. O
For O
the O
evaluation O
of O
NLG B-TaskName
tasks, O
we O
follow O
the O
script O
provided O
by O
Lin O
et O
al. O
(2020) O
to O
generate O
the O
texts O
through O
a O
greedy O
search O
for O
both O
WebNLG B-DatasetName
and O
CoQA B-DatasetName
. O
As O
for O

the O
number O
of O
epochs O
and O
the O
argument O
for O
weight O
decay, O
we O
mainly O
follow O
the O
setting O
used O
by O
Lin O
et O
al. O
(2020); O
Hu O
et O
al. O
( O
2021): O
for O
WebNLG B-DatasetName
, O
we O
train O
the O
model O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
; O
for O
CoQA B-DatasetName
, O
we O
train O
the O
model O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
the O

model-specific O
hyper-parameters, O
namely O
batch O
size O
(gradient O
accumulation O
is O
used O
if O
necessary) O
and O
learning O
rate, O
we O
decide O
them O
for O
different O
methods O
based O
on O
the O
loss O
on O
the O
validation O
set. O
For O
the O
proposed O
method O
inducer-tuning B-MethodName
with/without O
LoRA O
and O
MAM B-MethodName
inducer-tuning I-MethodName
in O
Table O
1, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.00125 B-HyperparameterValue
, O
and O

the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
for O
WebNLG B-DatasetName
; O
for O
CoQA B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
is O
0.001 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
. O
On O
MNLI B-DatasetName
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.0002 B-HyperparameterValue
for O
both O
two O
methods O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
32 B-HyperparameterValue
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName

and O
16 B-HyperparameterValue
for O
MAM B-MethodName
inducer-tuning I-MethodName
; O
on O
SST2 B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
similarly O
set O
as O
0.0002 B-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
is O
16 B-HyperparameterValue
, O
and O
for O
MAM B-MethodName
inducer-tuning I-MethodName
64 B-HyperparameterValue
. O
To O
reduce O
the O
random O
variability O
in O
the O
results, O
all O
the O
methods O
reported O
are O
trained O
for O
multiple O

independent O
runs. O
In O
particular, O
for O
WebNLG B-DatasetName
, O
we O
train O
models O
over O
5 B-HyperparameterValue
runs B-HyperparameterName
, O
and O
for O
CoQA B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SST2 B-DatasetName
3 B-HyperparameterValue
runs B-HyperparameterName
. O
The O
reported O
numbers O
in O
the O
cells O
in O
Tables O
1 O
and O
2 O
Rücklé O
et O
al. O
(2020). O
For O
the O
implementation O
of O
MLP O
v O
, O
we O
provide O
the O

exact O
expression O
for O
MLP O
(h) O
v O
(Q O
(h) O
) O
in O
head O
h O
as O
follows: O
σ O
Q O
(h) O
W O
(h) O
1 O
+ O
1(b O
(h) O
1 O
) O
T O
W O
(h) O
2 O
+ O
1b O
T O
2 O
, O
(11 O
) O
where O
σ O
is O
the O
activation O
function. O
As O
the O
superscript O
suggests, O
W O
h) O
1 O
∈ O

R O
p×r O
, O
b O
(h) O
1 O
∈ O
R O
r O
,( O
and O
W O
(h) O
2 O
∈ O
R O
r×N O
h O
p O
are O
specific O
to O
the O
head O
h, O
while O
b O
2 O
∈ O
R O
N O
h O
p O
are O
shared O
among O
all O
the O
heads, O
which O
is O
the O
same O
case O
as O
in O
Equation O
( O
4) O
(in O

the O
original O
attention O
sub-layer, O
the O
bias O
term O
b O
o O
applies O
to O
all O
the O
heads O
as O
well). O
B.4 O
Specific O
settings O
for O
baseline O
methods O
In O
this O
subsection, O
we O
provide O
the O
detailed O
setting O
for O
the O
methods O
in O
Tables O
1, O
2, O
and O
3 O
that O
need O
further O
specification. O
In O
Table O
1, O
the O
settings O
for O
Adapter-108 O

and O
Prefix-tuning-108 O
are O
clear, O
as O
the O
only O
arguments O
are O
the O
bottleneck O
size O
/ O
prefix O
length; O
for O
LoRA-54, O
we O
apply O
rank-54 O
updates O
for O
both O
W O
q O
and O
W O
v O
, O
as O
suggested O
by O
Hu O
et O
al. O
(2021); O
for O
MAM O
adapter, O
we O
mimic O
the O
parameter O
assignment O
scheme O
( O
bottleneck B-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
for O

FFN O
and O
prefix B-HyperparameterName
length I-HyperparameterName
30 B-HyperparameterValue
) O
by O
He O
et O
al. O
(2021a), O
and O
use O
the O
ratio O
102 O
: O
6 O
to O
implement O
MAM O
adapters O
with O
1.61% O
tunable O
parameters. O
For O
the O
variants O
of O
inducer O
tuning, O
their O
settings O
are O
summarized O
in O
Table O
5. O
In O
this O
table, O
the O
numbers O
in O
column O
MLP O
k O
and O
MLP O

v O
are O
the O
bottleneck O
sizes O
used O
for O
computing O
P O
k O
and O
P O
v O
; O
notice O
for O
Adaptive, O
the O
scope O
of O
virtual O
value O
tokens O
is O
not O
extended O
and O
thus O
has O
a O
larger O
bottleneck O
size O
than O
others. O
(Recall O
for O
MLP O
v O
, O
the O
size O
of O
W O
(h) O
2 O
in O
Equation O
( O
11) O

is O
larger O
than O
the O
counterparts O
in O
MLP O
v O
. O
For O
the O
numbers O
in O
column O
LoRA, O
they O
are O
the O
rank O
of O
the O
update O
used O
in O
the O
LoRA O
component O
to O
adjust O
W O
q O
; O
only O
for O
our O
proposed O
method O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
, O
the O
number O
will O
be O
nonzero. O
C O
Attention O
as O
Kernels O

To O
justify O
the O
claim O
that O
attention O
is O
a O
kernel O
operation, O
we O
construct O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39) O
of O
a O
query O
vector O
Q O
i O
(taking O
{K O
j O
} O
n O
j=1 O
as O
the O
supporting O
points) O
as O
follows: O
f O
(Q O
i O
) O
= O
n O
j=1 O
ℓ O
j O
(Q O
i O
)C O

j O
,(12) O
where O
ℓ O
j O
(Q O
i O
) O
:= O
κ(Q O
i O
, O
K O
j O
) O
n O
k=1 O
κ(Q O
i O
, O
K O
j O
) O
. O
κ(•, O
•) O
is O
a O
kernel O
function, O
and O
C O
j O
's O
are O
the O
coefficients O
corresponding O
to O
the O
rows O
V O
j O
's O
in O
the O
value O
matrix O
V O
. O

Take O
kernel O
function O
κ(x, O
y) O
= O
exp O
⟨x, O
y⟩ O
/ O
√ O
p O
. O
We O
slightly O
abuse O
the O
notation O
κ(Q, O
K) O
to O
represent O
the O
n-by-n O
empirical O
kernel O
matrix O
M O
, O
in O
which O
the O
i-th O
row O
and O
the O
j-th O
column O
is O
κ(Q O
i O
, O
K O
j O
), O
∀i O
∈ O
[n], O
j O
∈ O

[N O
]. O
With O
these O
notations, O
the O
output O
of O
the O
kernel O
estimator O
will O
be, O
D O
−1 O
M O
C, O
(13 O
) O
where O
D O
is O
a O
diagonal O
matrix O
serving O
as O
the O
row O
normalization O
in O
Equation O
( O
12), O
and O
C O
is O
an O
nby-p O
matrix O
with O
C O
j O
as O
its O
j-th O
row. O
We O
observe O
an O

obvious O
correspondence O
between O
Equation O
( O
13) O
and O
the O
standard O
attention O
in O
Equation O
(3). O
The O
correspondence O
implies O
a O
finer O
division O
of O
the O
attention O
module: O
the O
empirical O
kernel O
matrix O
M O
(D O
is O
decided O
by O
κ(Q, O
K)) O
and O
the O
value O
part O
C. O
(In O
Section O
4.4, O
we O
show O
that O
C O
includes O
but O
is O
not O

limited O
to O
the O
value O
matrix O
in O
attention.) O
D O
Example O
We O
provide O
an O
example O
answer O
generated O
by O
finetuning O
and O
our O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
on O
CoQA B-DatasetName
in O
Table O
6. O
Acknowledgements O
We O
appreciate O
all O
the O
valuable O
feedback O
from O
the O
anonymous O
reviewers. O

Unsupervised O
Boundary-Aware O
Language O
Model O
Pretraining O
for O
Chinese B-TaskName
Sequence I-TaskName
Labeling I-TaskName
Chinese O
language O
processing O
tasks, O
such O
as O
word O
segmentation, O
part-of-speech O
tagging, O
and O
named O
entity O
recognition. O
Previous O
studies O
usually O
resorted O
to O
the O
use O
of O
a O
high-quality O
external O
lexicon, O
where O
lexicon O
items O
can O
offer O
explicit O
boundary O
information. O
However, O
to O
ensure O
the O
quality O
of O
the O

lexicon, O
great O
human O
effort O
is O
always O
necessary, O
which O
has O
been O
generally O
ignored. O
In O
this O
work, O
we O
suggest O
unsupervised O
statistical O
boundary O
information O
instead, O
and O
propose O
an O
architecture O
to O
encode O
the O
information O
directly O
into O
pre-trained O
language O
models, O
resulting O
in O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
). O
We O
apply O
BABERT B-MethodName
for O
feature O
induction O
of O
Chinese B-TaskName

sequence I-TaskName
labeling I-TaskName
tasks. O
Experimental O
results O
on O
ten O
benchmarks O
of O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
demonstrate O
that O
BABERT B-MethodName
can O
provide O
consistent O
improvements O
on O
all O
datasets. O
In O
addition, O
our O
method O
can O
complement O
previous O
supervised O
lexicon O
exploration, O
where O
further O
improvements O
can O
be O
achieved O
when O
integrated O
with O
external O
lexicon O
information. O
Introduction O
The O
representative O
sequence O
labeling O
tasks O

for O
the O
Chinese O
language, O
such O
as O
word B-TaskName
segmentation I-TaskName
, O
partof-speech B-TaskName
(POS) I-TaskName
tagging I-TaskName
and O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
(Emerson, O
2005;Jin O
and O
Chen, O
2008), O
have O
been O
inclined O
to O
be O
performed O
at O
the O
characterlevel O
in O
an O
end-to-end O
manner O
(Shen O
et O
al., O
2016). O
The O
paradigm, O
naturally, O
is O
standard O
to O
Chinese B-TaskName
word I-TaskName
segmentation I-TaskName

( O
CWS B-TaskName
), O
while O
for O
Chinese B-TaskName
POS I-TaskName
tagging I-TaskName
and O
NER B-TaskName
, O
it O
can O
better O
help O
reduce O
the O
error O
propagation O
(Sun O
and O
Uszkoreit, O
2012;Yang O
et O
al., O
2016;Liu O
et O
al., O
2019a) O
compared O
with O
word-based O
counterparts O
by O
straightforward O
modeling. O
Recently, O
all O
the O
above O
tasks O
have O
reached O
stateof-the-art O
performances O
with O
the O
help O
of O
BERTlike O

pre-trained O
language O
models O
(Yan O
et O
al., O
2019;Meng O
et O
al., O
2019). O
The O
BERT O
variants, O
such O
as O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021), O
ERNIE B-MethodName
, O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
NEZHA B-MethodName
, O
etc., O
further O
improve O
the O
vanilla O
BERT O
by O
either O
using O
external O
knowledge O
or O
larger-scale O
training O
corpus. O
The O
improvements O
can O
also O
benefit O
character-level B-TaskName

Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Notably, O
since O
the O
output O
tags O
of O
all O
these O
character-level B-TaskName
Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
involve O
identifying O
Chinese O
words O
or O
entities O
(Zhang O
and O
Yang, O
2018;Yang O
et O
al., O
2019), O
prior O
boundary O
knowledge O
could O
be O
highly O
helpful O
for O
them. O
A O
number O
of O
studies O
propose O
the O
integration O
of O
an O
external O
lexicon O

to O
enhance O
their O
baseline O
models O
by O
feature O
representation O
learning O
(Jia O
et O
al., O
2020;Tian O
et O
al., O
2020a;. O
Moreover, O
some O
works O
suggest O
injecting O
similar O
resources O
into O
the O
pre-trained O
BERT O
weights. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
the O
representatives, O
which O
leverage O
an O
external O
lexicon O
for O
masked O
word O
prediction O
in O
Chinese O
BERT. O

The O
lexicon-based O
methods O
have O
indeed O
achieved O
great O
success O
for O
boundary O
integration. O
However, O
there O
are O
two O
major O
drawbacks. O
First, O
the O
lexicon O
resources O
are O
always O
constructed O
manually O
(Zhang O
and O
Yang, O
2018;Diao O
et O
al., O
2020;Jia O
et O
al., O
2020;, O
which O
is O
expensive O
and O
time-consuming. O
The O
quality O
of O
the O
lexicon O
is O
critical O
to O
our O
tasks. O

Second, O
different O
tasks O
as O
well O
as O
different O
domains O
require O
different O
lexicons O
(Jia O
et O
al., O
2020;. O
A O
well-studied O
lexicon O
for O
word O
segmentation O
might O
be O
inappropriate O
for O
NER, O
and O
a O
lexicon O
for O
news O
NER O
might O
also O
be O
problematic O
for O
finance O
NER. O
The O
two O
drawbacks O
can O
be O
due O
to O
the O
supervised O
characteristic O
of O

these O
lexicon-based O
enhancements. O
Thus, O
it O
is O
more O
desirable O
to O
offer O
boundary O
information O
in O
an O
unsupervised O
manner. O
In O
this O
paper, O
we O
propose O
an O
unsupervised O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
), O
which O
is O
achieved O
by O
fully O
exploring O
the O
potential O
of O
statisti-cal O
features O
mined O
from O
a O
large-scale O
raw O
corpus. O
We O
extract O
a O
set O
of O

N-grams O
(a O
predefined O
fixed O
N) O
no O
matter O
they O
are O
valid O
words O
or O
entities, O
and O
then O
calculate O
their O
corresponding O
unsupervised O
statistical O
features, O
which O
are O
mostly O
related O
to O
boundary O
information. O
We O
inject O
the O
boundary O
information O
into O
the O
internal O
layer O
of O
a O
pre-trained O
BERT, O
so O
that O
our O
final O
BABERT B-MethodName
model O
can O
approximate O
the O

boundary O
knowledge O
softly O
by O
using O
inside O
representations. O
The O
BABERT B-MethodName
model O
has O
no O
difference O
from O
the O
original O
BERT, O
so O
that O
we O
can O
use O
it O
in O
the O
same O
way O
as O
the O
standard O
BERT O
exploration. O
We O
conduct O
experiments O
on O
three O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
to O
demonstrate O
the O
effectiveness O
of O
our O
proposed O
method. O
Experimental O

results O
show O
that O
our O
approach O
can O
significantly O
outperform O
other O
Chinese O
pre-trained O
language O
models. O
In O
addition, O
compared O
with O
supervised O
lexicon-based O
methods, O
BABERT B-MethodName
obtains O
competitive O
results O
on O
all O
tasks O
and O
achieves O
further O
improvements O
when O
integrated O
with O
external O
lexicon O
knowledge. O
We O
also O
conduct O
extensive O
analyses O
to O
understand O
our O
method O
comprehensively. O
The O
pre-trained O
model O

and O
code O
are O
publicly O
available O
at O
http://github.com/modelscope/ O
adaseq/examples/babert. O
Our O
contributions O
in O
this O
paper O
include O
the O
following: O
1) O
We O
design O
a O
method O
to O
encode O
unsupervised O
statistical O
boundary O
information O
into O
boundary-aware O
representation, O
2) O
propose O
a O
new O
pre-trained O
language O
model O
called O
BABERT B-MethodName
as O
a O
boundary-aware O
extension O
for O
BERT, O
3) O
verify O
BABERT B-MethodName
on O
ten O

benchmark O
datasets O
of O
three O
Chinese O
sequence O
labeling O
tasks. O
Related O
Work O
In O
the O
past O
decades, O
machine O
learning O
has O
achieved O
good O
performance O
on O
sequence O
labeling O
tasks O
with O
statistical O
information O
(Bellegarda, O
2004;Low O
et O
al., O
2005;Bouma, O
2009). O
Recently, O
neural O
models O
have O
led O
to O
state-of-the-art O
results O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Lample O
et O
al., O
2016;Ma O
and O

Hovy, O
2016;Chiu O
and O
Nichols, O
2016). O
In O
addition, O
the O
presence O
of O
language O
representation O
models O
such O
as O
BERT O
(Devlin O
et O
al., O
2019) O
has O
led O
to O
impressive O
improvements. O
In O
particular, O
many O
variants O
of O
BERT O
are O
devoted O
to O
integrating O
boundary O
information O
into O
BERT O
to O
improve O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Diao O
et O
al., O
2020;Jia O
et O
al., O

2020;. O
Statistical O
Machine O
Learning O
Statistical O
information O
is O
critical O
for O
sequence O
labeling. O
Previous O
works O
attempt O
to O
count O
such O
information O
from O
large O
corpora O
in O
order O
to O
combine O
it O
with O
machine O
learning O
methods O
for O
sequence O
labeling O
(Bellegarda, O
2004;Liang, O
2005;Bouma, O
2009). O
Peng O
et O
al. O
(2004) O
attempts O
to O
conduct O
sequence O
labeling O
by O
CRF O
and O
a O

statistical-based O
new O
word O
discovery O
method. O
Low O
et O
al. O
(2005) O
introduce O
a O
maximum O
entropy O
approach O
for O
sequence O
labeling. O
Liang O
(2005) O
utilizes O
unsupervised O
statistical O
information O
in O
Markov O
models, O
and O
gets O
a O
boost O
on O
Chinese O
NER O
and O
CWS. O
Pre-trained O
Language O
Model O
Pre-trained O
language O
model O
is O
a O
hot O
topic O
in O
natural O
language O
processing O
(NLP) O

communities O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019b;Clark O
et O
al., O
2020;Diao O
et O
al., O
2020; O
and O
has O
been O
extensively O
studied O
for O
Chinese O
sequence O
labeling. O
For O
instance, O
TENER O
(Yan O
et O
al., O
2019) O
adopts O
Transformer O
encoder O
to O
model O
characterlevel O
features O
for O
Chinese O
NER. O
Glyce O
(Meng O
et O
al., O
2019) O
uses O
BERT O
to O
capture O
the O

contextual O
representation O
combined O
with O
glyph O
embeddings O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Lexicon-based O
Methods O
In O
recent O
studies, O
lexicon O
knowledge O
has O
been O
applied O
to O
improve O
model O
performance. O
There O
are O
two O
mainstream O
categories O
to O
the O
work O
of O
lexicon O
enhancement. O
The O
first O
aims O
to O
enhance O
the O
original O
BERT O
with O
implicit O
boundary O
information O
by O
using O

the O
multi-granularity O
word O
masking O
mechanism. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
representatives O
of O
this O
category, O
which O
propose O
to O
mask O
tokens, O
entities, O
and O
phrases O
as O
the O
mask O
units O
in O
the O
masked O
language O
modeling O
(MLM) O
task O
to O
learn O
the O
coarse-grained O
lexicon O
information O
during O
pre-training. O
ERNIE-Gram B-MethodName
, O
an O
extension O
of O
ERNIE, O

utilizes O
statistical O
boundary O
information O
for O
unsupervised O
word O
extraction O
to O
support O
masked O
word O
prediction, O
The O
second O
category, O
which O
includes O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
EEBERT B-MethodName
(Jia O
et O
al., O
2020), O
and O
LEBERT B-MethodName
, O
exploits O
the O
potential O
of O
directly O
injecting O
lexicon O
information O
into O
BERT O
via O
extra O
modules, O
leading O
to O
better O
performance O
but O
is O

limited O
in O
predefined O
external O
knowledge. O
Our O
work O
follows O
the O
first O
line O
of O
work, O
most O
similar O
to O
ERNIE-Gram B-MethodName
. O
However, O
different O
from O
ERNIE-Gram B-MethodName
, O
we O
do O
not O
discretize O
the O
real-valued O
statistical O
information O
ex- O
tracted O
from O
corpus, O
but O
adopt O
a O
regression O
manner O
to O
leverage O
the O
information O
fully. O
PMI O
LRE O
1 O
2 O
… O

… O
−1 O
MLM O
Loss O
ℒ O
MLM O
MSE O
Loss O
ℒ O
BA O
(c). O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Input O
Sentence O
Raw O
Corpus O
N-gram O
Statistical O
Dictionary O
Contextual O
N-gram O
Sets O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
N-gram O
Set O
of O
N-gram O
Set O
of O
+1 O
… O
+ O
−1 O
• O
• O
• O
• O

• O
• O
−1 O
− O
/2 O
… O
+ O
/2 O
• O
• O
• O
• O
• O
• O
+1 O
+2 O
− O
+1 O
… O
−1 O
+1 O
N-gram O
Set O
1 O
of O
1 O
Method O
Figure O
1 O
shows O
the O
overall O
architecture O
of O
our O
unsupervised O
boundary-aware O
pre-trained O
language O
model, O
which O
mainly O
consists O
of O
three O
components: O
1) O
boundary O
information O
extractor O

for O
unsupervised O
statistical O
boundary O
information O
mining, O
2) O
boundary-aware O
representation O
to O
integrate O
statistical O
information O
at O
the O
character-level, O
and O
3) O
boundary-aware B-MethodName
BERT I-MethodName
learning O
which O
injects O
boundary O
knowledge O
into O
the O
internal O
layer O
of O
BERT. O
In O
this O
section, O
we O
first O
focus O
on O
the O
details O
of O
the O
above O
components, O
and O
then O
introduce O
the O
fine-tuning O
method O

for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Boundary O
Information O
Extractor O
Statistical O
boundary O
information O
has O
been O
shown O
with O
a O
positive O
influence O
on O
a O
variety O
of O
Chinese O
NLP O
tasks O
(Song O
and O
Xia, O
2012;Higashiyama O
et O
al., O
2019;Ding O
et O
al., O
2020;. O
We O
follow O
this O
line O
of O
work, O
designing O
a O
boundary O
information O
extractor O
to O
mine O
statistical O
information O

from O
a O
large O
raw O
corpus O
in O
an O
unsupervised O
way. O
The O
overall O
flow O
of O
the O
extractor O
includes O
two O
steps: O
I) O
First, O
we O
collect O
all O
N-grams O
from O
the O
raw O
corpus O
to O
build O
a O
dictionary O
N O
, O
in O
which O
we O
count O
the O
frequencies O
of O
each O
N-gram O
and O
filter O
out O
the O
low O
frequencies O
items; O

II) O
second, O
considering O
that O
word O
frequency O
is O
insufficient O
for O
representing O
the O
flexible O
boundary O
relation O
in O
the O
Chinese O
context, O
we O
further O
compute O
two O
unsupervised O
indicators O
which O
can O
capture O
most O
of O
the O
boundary O
information O
in O
the O
corpus. O
In O
the O
following, O
we O
will O
describe O
these O
two O
indicators O
in O
detail. O
Pointwise O
Mutual O
Information O
(PMI) O

Given O
an O
N-gram, O
we O
split O
it O
into O
two O
sub-strings O
and O
compute O
the O
mutual O
information O
(MI) O
between O
them O
as O
a O
candidate. O
Then, O
we O
enumerate O
all O
sub-string O
pairs O
and O
choose O
the O
minimum O
MI O
as O
the O
overall O
PMI O
to O
estimate O
the O
tightness O
of O
the O
N-gram. O
Let O
g O
= O
{c O
1 O
...c O
m O
} O

be O
an O
N-gram O
that O
consists O
of O
m O
characters, O
we O
calculate O
PMI O
using O
this O
formula: O
PMI(g) O
= O
min O
i∈[1:m−1] O
{ O
p(g) O
p(c1...ci) O
• O
p(ci+1...cm) O
}, O
(1) O
where O
p(•) O
denotes O
the O
probability O
over O
the O
corpus. O
Note O
that, O
when O
m O
= O
1, O
the O
corresponding O
PMI O
is O
constantly O
equal O
to O
1. O
The O
higher O
PMI O

indicates O
that O
the O
N-gram O
(e.g., O
"贝克汉姆 O
(Beckham)") O
has O
a O
similar O
occurrence O
probability O
to O
the O
sub-string O
pair O
(e.g., O
"贝克 O
(Beck)" O
and O
"汉姆 O
(Ham)"), O
leading O
to O
a O
higher O
association O
between O
internal O
sub-string O
pairs, O
which O
makes O
the O
N-gram O
more O
likely O
to O
be O
a O
word/entity. O
In O
contrast, O
a O
lower O
PMI O
means O
the O
Ngram O
(e.g., O

"克汉(Kehan)") O
is O
possibly O
an O
invalid O
word/entity. O
Left O
and O
Right O
Entropy O
(LRE) O
Given O
an O
Ngram O
g, O
we O
first O
collect O
a O
left-adjacent O
character O
set O
S O
l O
m O
= O
{c O
l O
1 O
, O
..., O
c O
l O
n O
l O
} O
with O
n O
l O
characters. O
Then, O
we O
utilize O
the O
conditional O
probability O
between O
g O
and O
its O

left O
adjacent O
characters O
in O
S O
l O
m O
to O
compute O
the O
left O
entropy O
(LE), O
which O
measures O
sufficient O
boundary O
information. O
LE O
can O
be O
defined O
as: O
LE(g) O
= O
− O
n O
l O
i O
p(c O
l O
i O
g|g) O
log O
p(c O
l O
i O
g|g).(2) O
Similar O
to O
LE, O
we O
further O
collect O
a O
right O
adjacent O
set O
S O
r O

m O
= O
{c O
r O
1 O
, O
..., O
c O
r O
nr O
} O
with O
n O
r O
characters O
to O
calculate O
the O
right O
entropy O
(RE) O
for O
the O
N-gram O
g: O
RE(g) O
= O
− O
nr O
i O
p(gc O
r O
i O
|g) O
log O
p(gc O
r O
i O
|g).(3) O
Intuitively, O
LRE O
represents O
the O
abundance O
of O
neighboring O
characters O
for O
the O
N-gram. O

With O
a O
lower O
LRE, O
the O
N-gram O
(e.g., O
"汉姆 O
") O
has O
a O
more O
fixed O
context, O
indicating O
it O
is O
more O
likely O
to O
be O
a O
part O
of O
a O
phrase O
or O
entity. O
Conversely, O
the O
N-gram O
with O
a O
higher O
LRE O
(e.g., O
"贝克汉姆") O
will O
interact O
more O
with O
context, O
which O
prefers O
to O
be O
an O
independent O
word O
or O

phrase. O
Finally, O
we O
utilize O
PMI O
and O
LRE O
to O
measure O
the O
flexible O
boundary O
relations O
in O
the O
Chinese O
context, O
and O
then O
update O
each O
N-gram O
in O
N O
with O
the O
unsupervised O
statistical O
indicators O
above. O
Boundary-Aware O
Representation O
By O
using O
the O
boundary O
information O
extractor, O
we O
can O
obtain O
an O
N-gram O
dictionary O
N O
with O
unsupervised O
statistical O
boundary O
information. O

Unfortunately, O
since O
the O
context O
independence O
and O
the O
high O
relevance O
to O
N-gram, O
previous O
works O
(Ding O
et O
al., O
2020; O
use O
such O
statistical O
features O
for O
word O
extraction O
only, O
which O
ignore O
the O
potential O
of O
statistical O
boundary O
information O
in O
representation O
learning. O
To O
alleviate O
this O
problem, O
we O
propose O
boundary-aware O
representation, O
a O
highly O
extensible O
method, O
to O
fully O

benefit O
from O
the O
statistical O
boundary O
information O
for O
representation O
learning. O
To O
achieve O
boundary-aware O
representation, O
we O
first O
build O
contextual O
N-gram O
sets O
from O
the O
sentence. O
As O
shown O
in O
Figure O
1 O
(b), O
given O
a O
sentence O
x O
= O
{c O
1 O
, O
c O
2 O
, O
..., O
c O
n O
} O
with O
n O
characters O
and O
the O
maximum O
N-gram O

length O
N O
, O
we O
extract O
all O
N-grams O
that O
include O
c O
i O
as O
the O
contextual O
N-gram O
set O
S O
c O
i O
= O
{c O
i O
, O
c O
i O
c O
i+1 O
, O
• O
• O
• O
, O
c O
i−N O
+1 O
...c O
i O
} O
for O
char- O
acter O
c O
i O
. O
Then, O
we O
design O
a O
composition O
method O

to O
integrate O
the O
statistical O
features O
of O
N-grams O
in O
S O
c O
i O
by O
using O
specific O
conditions O
and O
rules, O
aiming O
to O
avoid O
the O
sparsity O
and O
contextual O
independence O
limitations O
of O
statistical O
information. O
Concretely, O
we O
divide O
the O
information O
composition O
method O
into O
PMI O
and O
entropy O
representation. O
First, O
we O
concatenate O
the O
PMI O
of O
all O
N-grams O
in O

S O
c O
i O
to O
generate O
PMI O
representation: O
e O
p O
i O
=PMI( O
ci O
) O
⊕PMI( O
ci O
ci+1) O
⊕ O
PMI(ci−1 O
ci O
) O
⊕PMI( O
ci O
ci+1ci+2) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−2ci−1 O
ci O
) O
• O
• O
• O
• O
• O
• O
⊕PMI( O
ci O
...ci+N−1) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−N+1... O
ci O
),(4) O
where O
e O

p O
i O
∈ O
R O
a O
, O
and O
a O
= O
1+2+• O
• O
•+N O
is O
the O
number O
of O
the O
N-grams O
that O
contain O
c O
i O
. O
Note O
that O
the O
position O
of O
each O
N-gram O
is O
fixed O
in O
PMI O
representation. O
We O
strictly O
follow O
the O
order O
of O
N-gram O
length O
and O
the O
position O
of O
c O
i O
in O

N-gram O
to O
concatenate O
their O
corresponding O
PMI, O
ensuring O
that O
the O
position O
and O
context O
information O
can O
be O
encoded O
into O
e O
p O
i O
. O
Entropy O
representation O
focuses O
on O
the O
contextual O
interactions O
of O
each O
character. O
When O
c O
i O
is O
the O
border O
of O
N-grams O
in O
S O
c O
i O
, O
we O
separately O
aggregate O
the O
LE O
and O

RE O
as O
left O
and O
right O
entropy O
representation: O
e O
le O
i O
=LE( O
ci O
) O
⊕ O
LE( O
ci O
ci+1) O
⊕ O
• O
• O
• O
⊕ O
LE( O
ci O
...ci+N−1), O
e O
re O
i O
=RE( O
ci O
) O
⊕ O
RE(ci−1 O
ci O
) O
⊕ O
• O
• O
• O
⊕ O
RE(ci−N+1... O
ci O
),(5) O
where O
e O
le O
i O
∈ O
R O
b O

, O
e O
re O
i O
∈ O
R O
b O
, O
and O
b O
= O
N O
1 O
is O
the O
number O
of O
integrated O
N-grams. O
Similar O
to O
PMI O
representation, O
the O
position O
of O
each O
N-gram O
in O
e O
le O
i O
and O
e O
le O
i O
is O
fixed O
and O
symmetric. O
Therefore, O
the O
boundary-aware O
representation O
e O
i O
of O
c O
i O
can O

be O
formalized O
as: O
e O
i O
= O
e O
le O
i O
⊕ O
e O
p O
i O
⊕ O
e O
re O
i O
,(6) O
where O
e O
i O
∈ O
R O
a+2b O
. O
Finally, O
by O
composing O
multigranularity O
statistical O
boundary O
information O
in O
a O
specific O
order, O
we O
are O
able O
to O
obtain O
the O
boundaryaware O
representation, O
which O
explicitly O
contains O
the O
boundary O
and O

context O
information. O
Figure O
2 O
shows O
an O
example O
of O
the O
boundaryaware O
representation. O
Given O
a O
sentence O
"南 O
京 O
市 O
长 O
江 O
大 O
桥 O
(Nanjing O
Yangtze O
River O
Bridge)" O
and O
a O
maximum O
N-gram O
length O
N O
= O
3, O
we O
first O
build O
a O
contextual O
N-gram O
set O
for O
the O
character O
"长 O
(Long)". O
Then, O
we O
integrate O
the O
PMI O

of O
all O
N-grams O
in O
a O
specific O
order O
(from O
N-gram O
"长" O
to O
"京市长 O
(Mayor O
of O
Jing)") O
to O
compute O
PMI O
representation. O
Furthermore, O
left O
and O
right O
entropy O
representations O
are O
also O
calculated O
in O
a O
particular O
order O
(from O
Ngram O
"长" O
to O
"长江大 O
(Yangtze O
River O
Big)" O
and O
"京市长", O
respectively). O
Finally, O
we O
concatenate O
the O
above O
features O
to O

produce O
the O
overall O
boundaryaware O
representation O
of O
the O
character O
"长". O
Contextual O
N-grams O
Set O
of O
"长" O
长 O
Long O
长江 O
Yangtze O
River O
+1 O
市长 O
Mayor O
−1 O
京市长 O
Mayor O
of O
Jing O
−2 O
−1 O
长江大 O
Yangtze O
River O
Big O
+1 O
+2 O
PMI O
Representation O
市长江 O
Mayor O
Jiang O
−1 O
+1 O
RE O
Representation O
LE O
Representation O
• O
• O
• O
• O

• O
• O
• O
• O
• O
• O
• O
• O
⊕ O
Boundary-Aware O
Representation O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Boundary-aware B-MethodName
BERT I-MethodName
is O
a O
variant O
of O
BERT, O
enhanced O
with O
boundary O
information O
simply O
and O
effectively. O
In O
this O
subsection, O
we O
describe O
how O
the O
boundary O
information O
can O
be O
integrated O
into O
BERT O
during O
pre-training O
by O
boundary-aware O
learning. O
Boundary-Aware O
Objective O
As O

mentioned O
in O
Section O
3.2, O
given O
a O
sentence O
x O
with O
characterlength O
n, O
we O
can O
compute O
the O
corresponding O
boundary-aware O
representation O
E O
= O
{e O
1 O
, O
..., O
e O
n O
}. O
Then, O
we O
transfer O
the O
BERT O
feature O
into O
the O
boundary O
information O
space O
and O
approximate O
it O
to O
E O
for O
boundary-aware O
learning. O
Moreover, O
shows O
that O
encoding O

basic O
lexical O
knowledge O
in O
the O
shallow O
BERT O
layers O
is O
a O
more O
effective O
approach. O
Hence, O
we O
use O
the O
hidden O
features O
H O
l O
= O
{h O
l O
1 O
, O
..., O
h O
l O
n O
} O
of O
the O
l-th O
shallow O
layer O
to O
achieve O
the O
boundary-aware O
objective: O
L O
BA O
= O
n O
i O
MSE(W O
B O
h O
l O

i O
, O
e O
i O
),(7) O
where O
MSE(•) O
denotes O
the O
mean O
square O
error O
loss. O
W O
B O
is O
a O
trainable O
matrix O
used O
to O
project O
BERT O
representation O
into O
boundary O
information O
space. O
Previous O
classification-based O
word-level O
masking O
methods O
use O
statistical O
information O
as O
thresholds O
to O
filter O
valid O
words O
for O
masked O
word O
prediction. O
Unlike O
the O
above O
works, O

we O
softly O
utilize O
such O
information O
in O
a O
regression O
manner, O
avoiding O
possible O
errors O
in O
empirically O
filtering O
valid O
tags, O
thereby O
fully O
exploring O
the O
potential O
of O
this O
information. O
Jia O
et O
al. O
(2020) O
and O
Gao O
and O
Callan O
(2021), O
we O
opt O
to O
initialize O
our O
model O
with O
a O
pre-trained O
BERT O
model O
released O
by O
Google O
2 O
and O

randomly O
initialize O
the O
other O
parameters, O
alleviating O
the O
enormous O
cost O
of O
train-ing O
BABERT B-MethodName
from O
scratch. O
In O
particular, O
we O
discard O
the O
next O
sentence O
prediction O
task O
during O
pretraining, O
which O
is O
confirmed O
to O
be O
not O
essential O
for O
the O
pre-trained O
language O
models O
(Lan O
et O
al., O
2020;Liu O
et O
al., O
2019b). O
The O
total O
pre-training O
loss O
of O
BABERT B-MethodName

can O
be O
formalized O
as: O
Pre-training O
Following O
L O
pre O
= O
L O
MLM O
+ O
L O
BA O
,(8) O
where O
L O
MLM O
is O
the O
standard O
objective O
of O
MLM O
task. O
Fine-tuning O
for O
Sequence O
Labeling O
Straightforward O
Fine-tuning O
As O
shown O
in O
Figure O
1 O
(c), O
because O
BABERT B-MethodName
has O
the O
same O
architecture O
as O
BERT, O
we O
can O
adopt O
the O
identical O

procedure O
that O
BERT O
uses O
for O
fine-tuning, O
where O
the O
output O
of O
BABERT O
can O
be O
used O
as O
the O
contextual O
character O
representation O
for O
sequence O
labeling. O
Concretely, O
given O
a O
sequence O
labeling O
dataset O
D O
= O
{(x O
j O
, O
y O
j O
)} O
N O
j=1 O
, O
where O
y O
j O
is O
the O
label O
sequence O
of O
x O
j O
, O

we O
utilize O
the O
output O
of O
BABERT B-MethodName
and O
a O
CRF O
layer O
to O
calculate O
the O
sentence-level O
output O
probability O
p(y O
j O
|x O
j O
), O
which O
is O
exactly O
the O
same O
as O
. O
The O
negative O
log-likelihood O
loss O
for O
training O
can O
be O
defined O
as: O
L O
sq O
= O
− O
N O
j O
log O
p(y O
j O
|x O
j O
),(9) O

At O
the O
inference O
stage, O
we O
use O
the O
Viterbi O
algorithm O
(Viterbi, O
1967) O
to O
generate O
the O
final O
label O
sequence. O
Combining O
with O
Supervised O
Lexicon O
Features O
We O
can O
naturally O
combine O
BABERT B-MethodName
with O
other O
supervised O
lexicon-based O
methods O
because O
of O
the O
unsupervised O
setting O
of O
BABERT B-MethodName
. O
To O
this O
end, O
we O
propose O
a O
lexicon-enhanced B-MethodName
BABERT I-MethodName
( O
BABERT-LE B-MethodName

) O
for O
the O
fine-tuning O
stage, O
which O
utilizes O
the O
lexicon O
adapter O
proposed O
by O
to O
incorporate O
external O
lexicon O
knowledge O
into O
BABERT B-MethodName
feature: O
h O
i O
= O
LA(h O
i O
, O
S O
lex O
i O
),(10) O
where O
LA(•) O
is O
the O
lexicon O
adapter, O
S O
lex O
i O
is O
a O
set O
of O
related O
N-gram O
embeddings O
of O
character O
c O
i O

, O
andĥ O
i O
is O
the O
lexicon-enhanced O
version O
of O
original O
BERT O
feature O
h O
i O
. O
We O
apply O
the O
lexicon O
adapter O
after O
the O
l-th O
layer O
to O
be O
consistent O
with O
boundary-aware O
learning. O
Finally, O
BABERT-LE B-MethodName
performs O
a O
similar O
fine-tuning O
procedure O
as O
BABERT B-MethodName
for O
training: O
L O
lex O
= O
− O
N O
j O
log O
p(yj|xj, O
[S O
lex O

1 O
, O
..., O
S O
lex O
n O
j O
]).(11) O
4 O
Experiments O
Datasets O
Following O
previous O
works O
(Devlin O
et O
al., O
2019;, O
we O
draw O
the O
mixed O
corpus O
of O
Chinese B-DatasetName
Wikipedia I-DatasetName
3 O
and O
Baidu B-DatasetName
Baike I-DatasetName
4 O
as O
our O
pretraining O
corpus, O
which O
contains O
3B O
tokens O
and O
62M O
sentences. O
To O
further O
confirm O
the O
effectiveness O
of O
our O
proposed O

method O
for O
Chinese O
sequence O
labeling, O
we O
evaluate O
BABERT B-MethodName
on O
ten O
benchmark O
datasets O
of O
three O
representative O
tasks: O
Chinese O
Word O
Segmentation O
We O
use O
three O
CWS O
benchmarks O
to O
evaluate O
our O
BABERT. O
Penn B-DatasetName
Chinese I-DatasetName
TreeBank I-DatasetName
version I-DatasetName
6.0 I-DatasetName
CTB6 B-DatasetName
) O
is O
from O
Xue O
et O
al. O
(2005), O
and O
MSRA B-DatasetName
and O
PKU B-DatasetName
are O
from O
SIGHAN O
2005 O

Bakeoff O
(Emerson, O
2005). O
Part-Of-Speech O
Tagging O
For O
Chinese O
POS O
tagging, O
we O
conduct O
experiments O
on O
CTB6 B-DatasetName
(Xue O
et O
al., O
2005) O
and O
the O
Chinese O
part O
of O
Universal B-DatasetName
Dependencies I-DatasetName
( O
UD B-DatasetName
) O
(Nivre O
et O
al., O
2016). O
The O
UD B-DatasetName
dataset O
uses O
two O
different O
POS O
tagsets, O
which O
are O
universal O
and O
language-specific O
tagsets. O
We O
follow O
Shao O
et O

al. O
(2017), O
referring O
to O
the O
corpus O
with O
the O
two O
tagsets O
as O
UD1 O
and O
UD2, O
respectively. O
Named O
Entity O
Recognition O
For O
the O
Chinese O
NER O
task, O
we O
conduct O
experiments O
on O
OntoNotes B-DatasetName
4.0 I-DatasetName
( O
Onto4 B-DatasetName
) O
(Weischedel O
et O
al., O
2011) O
and O
News B-DatasetName
datasets O
(Jia O
et O
al., O
2020), O
both O
of O
which O
are O
from O
the O
standard O

newswire O
domain. O
Moreover, O
we O
evaluate O
BABERT B-MethodName
in O
the O
internet O
novel O
(Book) O
and O
financial O
report O
(Finance) O
domains O
(Jia O
et O
al., O
2020) O
to O
further O
verify O
the O
robustness O
of O
our O
method. O
The O
statistics O
of O
the O
benchmark O
datasets O
are O
shown O
in O
Table O
1. O
For O
a O
fair O
comparison, O
we O
split O
these O
datasets O
into O
training, O
development, O

and O
test O
sections O
following O
previous O
works O
(Jia O
et O
al., O
2020;. O
Note O
that O
MSRA B-DatasetName
, O
PKU B-DatasetName
, O
and O
Finance B-DatasetName
do O
not O
have O
development O
sections. O
Therefore, O
we O
randomly O
select O
10% O
instances O
from O
the O
training O
set O
as O
the O
development O
set O
for O
these O
datasets. O
Experimental O
Settings O
Hyperparameters O
During O
pre-training, O
we O
use O
the O
hyperparameters O
of O

BERT B-MethodName
BASE I-MethodName
to O
initialize O
BABERT B-MethodName
and O
Adam O
(Kingma O
and O
Ba, O
2014) O
for O
optimizing. O
The O
number O
of O
BERT O
layers B-HyperparameterName
L B-HyperparameterName
is O
12 B-HyperparameterValue
, O
with O
12 B-HyperparameterValue
self-attention B-HyperparameterName
heads I-HyperparameterName
, O
768 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
hidden B-HyperparameterName
states I-HyperparameterName
, O
and O
64 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
each B-HyperparameterName
head I-HyperparameterName
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
, O
the O
learning B-HyperparameterName

rate I-HyperparameterName
is O
1e-4 B-HyperparameterValue
with O
a O
warmup B-HyperparameterName
ratio I-HyperparameterName
of O
0.1 B-HyperparameterValue
, O
and O
the O
max B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
input I-HyperparameterName
sequence I-HyperparameterName
is O
512 B-HyperparameterValue
. O
To O
extract O
unsupervised O
boundary O
information, O
we O
set O
the O
maximum B-HyperparameterName
N-gram I-HyperparameterName
length I-HyperparameterName
N B-HyperparameterName
to O
4 B-HyperparameterValue
5 I-HyperparameterValue
and O
the O
frequency B-HyperparameterName
filtering I-HyperparameterName
threshold I-HyperparameterName
to O
50 B-HyperparameterValue
. O
Then O
we O
use O
the O
3-th O
BERT O

layer O
to O
compute O
boundary-aware O
objective. O
BABERT B-MethodName
has O
no O
extra O
modules, O
which O
is O
why O
the O
parameter O
size O
and O
model O
architecture O
are O
the O
same O
as O
those O
of O
BERT B-MethodName
BASE I-MethodName
. O
Finally, O
we O
train O
the O
BABERT B-MethodName
on O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
with O
32GB O
memory. O
For O
Chinese O
sequence O
labeling, O
we O
empirically O
set O
hyperparameters O

based O
on O
previous O
studies O
(Jia O
et O
al., O
2020; O
and O
preliminary O
experiments. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
32 B-HyperparameterValue
, O
the O
max B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
fixed O
to O
2e-5 B-HyperparameterValue
. O
Baselines O
To O
verify O
the O
effectiveness O
of O
our O
proposed O
BABERT B-MethodName
, O
we O
build O
systems O
on O
the O
following O
methods O

to O
conduct O
fair O
comparisons: O
• O
BERT O
is O
the O
Chinese O
version O
BERT B-MethodName
BASE I-MethodName
model O
released O
by O
Google. O
• O
BERT-wwm B-MethodName
performs O
segmentation O
on O
the O
corpus O
and O
further O
conduct O
word-level O
masking O
in O
pre-training O
(Cui O
et O
al., O
2021). O
• O
ERNIE B-MethodName
is O
an O
extension O
of O
BERT, O
which O
leverages O
external O
lexicons O
for O
word-level O
masking O
. O
• O

ERNIE-Gram B-MethodName
is O
an O
extension O
of O
ERNIE B-MethodName
, O
which O
alleviates O
the O
limitations O
of O
external O
lex- O
icons O
by O
using O
statistical O
information O
for O
entity O
and O
phrase O
extraction O
. O
• O
ZEN B-MethodName
uses O
an O
extra O
N-gram O
encoder O
to O
integrate O
external O
lexicon O
knowledge O
into O
BERT O
during O
pre-training O
(Diao O
et O
al., O
2020). O
• O
NEZHA B-MethodName
leverages O
functional O
relative O

positional O
encoding, O
supervised O
word-level O
masking O
strategy, O
and O
enormous O
training O
data O
6 O
to O
enhance O
vanilla O
BERT O
. O
• O
BERT-LE B-MethodName
is O
a O
lexicon-enhanced O
BERT O
, O
which O
introduces O
a O
lexicon O
adapter O
between O
BERT O
layers O
to O
incorporate O
external O
lexicon O
embeddings. O
We O
strictly O
follow. O
to O
reimplement O
it O
with O
open-source O
word O
embeddings O
7 O
Main O
Results O
The O

overall O
Chinese O
sequence O
labeling O
results O
are O
shown O
in O
Table O
2. O
We O
report O
the O
F1 B-MetricName
-score O
of O
the O
test O
datasets O
on O
CWS B-TaskName
, O
POS B-TaskName
, O
and O
NER B-TaskName
tasks. O
Here, O
we O
first O
compare O
our O
BABERT B-MethodName
with O
various O
Chinese O
pre-trained O
language O
models O
to O
evaluate O
its O
effectiveness. O
Then, O
we O
compare O
BABERT-LE B-MethodName
with O
other O
supervised O

lexicon-based O
methods O
to O
show O
the O
potential O
of O
BABERT B-MethodName
in O
combining O
with O
external O
lexicon O
knowledge. O
6 O
NEZHA B-MethodName
uses O
three O
large O
corpora, O
including O
Chinese B-DatasetName
Wikipedia I-DatasetName
, O
Baidu B-DatasetName
Baike I-DatasetName
, O
and O
Chinese B-DatasetName
News I-DatasetName
, O
which O
contain O
11B O
tokens O
and O
are O
four O
times O
more O
than O
us. O
7 O
https://ai.tencent.com/ailab/nlp/en/embedding.html O
First, O
we O
examine O
the O
F1 B-MetricName
values O

of O
the O
BERT B-MethodName
baseline. O
As O
shown, O
BERT B-MethodName
obtains O
comparable O
results O
on O
all O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks, O
which O
is O
similar O
to O
that O
of O
Diao O
et O
al. O
(2020), O
Tian O
et O
al. O
(2020a) O
and O
. O
BABERT B-MethodName
significantly O
outperforms O
BERT B-MethodName
, O
resulting O
in O
an O
increase O
of O
90.47 O
− O
89.80 O
= O
0.67 B-MetricValue
on O
average. O
This O

observation O
clearly O
indicates O
the O
advantage O
of O
introducing O
boundary O
information O
into O
BERT O
pre-training. O
Compared O
with O
various O
BERT O
extensions, O
our O
BABERT B-MethodName
can O
achieve O
competitive O
performances O
as O
a O
whole. O
First, O
in O
comparison O
with O
BERT-wwm B-MethodName
, O
ERNIE B-MethodName
, O
ERNIE-gram B-MethodName
, O
and O
ZEN B-MethodName
, O
which O
leverage O
external O
lexicons O
that O
include O
high-frequency O
words O
for O
pre-training, O
BABERT B-MethodName

outperforms O
all O
of O
them O
by O
averaging O
0.54+0.41+0.40+0.57 O
4 O
= O
0.48 B-MetricValue
point, O
and O
achieves O
top O
scores O
on O
eight O
of O
the O
ten O
benchmarks. O
This O
result O
is O
consistent O
with O
our O
intuition O
that O
directly O
exploiting O
a O
supervised O
lexicon O
can O
only O
achieve O
good O
performance O
in O
specific O
tasks, O
indicating O
the O
limitation O
of O
these O
methods O
when O
the O

chosen O
lexicon O
is O
incompatible O
with O
the O
target O
tasks. O
Second, O
we O
find O
that O
BABERT B-MethodName
surpasses O
NEZHA B-MethodName
in O
the O
average O
F1 B-MetricName
values, O
indicating O
that O
the O
boundary O
information O
is O
more O
critical O
than O
the O
data O
scale O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Then, O
we O
compare O
our O
method O
with O
supervised O
lexicon-based O
methods. O
Lattice-based O
methods O
(Zhang O
and O

Yang, O
2018;Yang O
et O
al., O
2019) O
2020a,b) O
designs O
an O
external O
memory O
network O
after O
the O
BERT O
encoder O
to O
incorporate O
lexicon O
knowledge. O
EEBERT B-MethodName
(Jia O
et O
al., O
2020) O
builds O
entity O
embeddings O
from O
the O
corpus O
and O
further O
utilizes O
them O
in O
the O
multi-head O
attention O
mechanism. O
The O
results O
are O
shown O
in O
Table O
2 O
(II). O
All O
the O
above O

methods O
lead O
to O
significant O
improvements O
over O
the O
base O
BERT O
model, O
which O
shows O
the O
effectiveness O
of O
external O
lexicon O
knowledge. O
Moreover, O
BABERT B-MethodName
can O
achieve O
comparable O
performance O
with O
the O
above O
methods, O
which O
further O
demonstrates O
the O
potential O
of O
our O
unsupervised O
manner. O
BABERT B-MethodName
learns O
boundary O
information O
from O
unsupervised O
statistical O
features O
with O
vanilla O
BERT, O
which O
means O

it O
has O
excellent O
scalability O
to O
fuse O
with O
other O
BERT-based O
supervised O
lexicon O
models. O
As O
shown, O
we O
can O
see O
that O
our O
BABERT-LE B-MethodName
achieves O
further O
improvements O
and O
state-of-the-art O
performances O
on O
all O
tasks, O
showing O
the O
advantages O
of O
our O
unsupervised O
setting O
and O
boundary-aware O
learning. O
Interestingly, O
compared O
with O
MEM-ZEN B-MethodName
, O
BABERT-LE B-MethodName
has O
larger O
improvements O
over O
their O

corresponding O
baselines. O
One O
reason O
might O
be O
that O
both O
ZEN B-MethodName
and O
the O
memory O
network O
module O
exploits O
supervised O
lexicons, O
which O
leads O
to O
a O
duplication O
of O
introduced O
knowledge. O
Analysis O
In O
this O
subsection, O
we O
conduct O
detailed O
experimental O
analyses O
for O
an O
in-depth O
comprehensive O
understanding O
of O
our O
method. O
Few-Shot O
Setting O
To O
further O
verify O
the O
effectiveness O
of O

BABERT B-MethodName
, O
we O
conduct O
experiments O
under O
the O
few-shot O
setting, O
where O
we O
randomly O
sample O
10, O
50, O
and O
100 O
instances O
of O
the O
original O
training O
data O
from O
PKU B-DatasetName
( O
CWS B-TaskName
) O
and O
Onto4 B-DatasetName
( O
NER B-TaskName
). O
For O
fair O
comparisons, O
we O
compare O
BABERT B-MethodName
with O
the O
pretrained O
language O
models O
without O
external O
supervised O
knowledge. O
The O
results O

are O
presented O
in O
Table O
3. O
As O
the O
size O
of O
training O
data O
is O
reduced, O
the O
adding O
T-test O
does O
not O
bring O
further O
improvements. O
One O
possible O
reason O
is O
that O
the O
T-test O
is O
essentially O
similar O
to O
the O
entropy O
measure O
of O
2-grams, O
which O
has O
already O
been O
injected O
into O
our O
BABERT B-MethodName
model. O
Boundary O
Information O
Encoding O
Layer O

Previous O
works O
(Jawahar O
et O
al., O
2019; O
exploit O
the O
fact O
that O
different O
BERT O
layers O
would O
generate O
different O
concept O
representations. O
The O
shallow O
BERT O
layers O
are O
more O
likely O
to O
capture O
basic O
lexicon O
information, O
while O
the O
top O
layers O
focus O
on O
the O
semantic O
representation. O
We O
empirically O
set O
l O
in O
{1, O
3, O
6, O
12} O
to O
explore O

the O
effect O
of O
computing O
boundary-aware O
loss O
by O
the O
hidden O
features O
H O
l O
of O
different O
BERT O
layers O
on O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Table O
5 O
shows O
the O
results. O
We O
can O
see O
that O
the O
best O
F1 B-MetricName
-score O
can O
be O
achieved O
when O
l O
= O
3 O
on O
all O
datasets, O
which O
indicates O
that O
the O
BABERT B-MethodName
still O

needs O
sufficient O
parameters O
to O
learn O
the O
basic O
boundary O
information. O
Interestingly, O
the O
BABERT B-MethodName
performs O
poorly O
when O
l O
= O
12, O
which O
might O
be O
due O
to O
a O
conflict O
between O
the O
MLM O
loss O
and O
our O
boundary-aware O
regression O
loss O
during O
pretraining. O
Qualitative O
Analysis O
To O
explore O
how O
BABERT B-MethodName
improves O
the O
performance O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
, O

we O
conduct O
qualitative O
analysis O
on O
the O
News B-DatasetName
test O
dataset, O
which O
consists O
of O
four O
different O
subdomains, O
namely O
game O
(GAM), O
entertainment O
(ENT), O
lottery O
(LOT) O
and O
finance O
(FIN). O
The O
results O
are O
shown O
in O
Table O
6. O
We O
can O
see O
that O
compared O
with O
other O
pre-trained O
language O
models, O
BABERT B-MethodName
can O
obtain O
consistent O
improvement O
in O
all O
domains O

with O
unsupervised O
statistical O
boundary O
information, O
while O
the O
other O
models O
only O
improve O
performance O
on O
specific O
domains. O
Moreover, O
as O
shown O
in O
Table O
7, O
we O
also O
give O
an O
example O
from O
the O
game O
domain O
to O
further O
demonstrate O
the O
effectiveness O
of O
our O
method. O
BABERT B-MethodName
is O
the O
only O
model O
that O
correctly O
recognizes O
all O
entities. O
In O
particular, O

the O
prediction O
of O
BABERT B-MethodName
for O
the O
entity O
"WCG2011 O
org O
" O
indicates O
the O
potential O
of O
boundary O
information. O

Co-guiding B-MethodName
Net I-MethodName
: O
Achieving O
Mutual O
Guidances O
between O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
and O
Slot B-TaskName
Filling I-TaskName
via O
Heterogeneous O
Semantics-Label O
Graphs O
Recent O
graph-based O
models O
for O
joint B-TaskName
multiple I-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
have O
obtained O
promising O
results O
through O
modeling O
the O
guidance O
from O
the O
prediction O
of O
intents O
to O
the O
decoding O
of O
slot B-TaskName
filling I-TaskName
. O
However, O
existing O

methods O
(1) O
only O
model O
the O
unidirectional O
guidance O
from O
intent O
to O
slot; O
(2) O
adopt O
homogeneous O
graphs O
to O
model O
the O
interactions O
between O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes, O
which O
limit O
the O
performance. O
In O
this O
paper, O
we O
propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
achieving O

the O
mutual O
guidances O
between O
the O
two O
tasks. O
In O
the O
first O
stage, O
the O
initial O
estimated O
labels O
of O
both O
tasks O
are O
produced, O
and O
then O
they O
are O
leveraged O
in O
the O
second O
stage O
to O
model O
the O
mutual O
guidances. O
Specifically, O
we O
propose O
two O
heterogeneous O
graph O
attention O
networks O
working O
on O
the O
proposed O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName

, O
which O
effectively O
represent O
the O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes. O
Experiment O
results O
show O
that O
our O
model O
outperforms O
existing O
models O
by O
a O
large O
margin, O
obtaining O
a O
relative O
improvement O
of O
19.3% B-MetricValue
over O
the O
previous O
best O
model O
on O
Mix-ATIS O
dataset O
in O
overall O
accuracy B-MetricName
. O
Introduction O
Spoken B-TaskName
language I-TaskName
understanding I-TaskName
( O
SLU B-TaskName

) O
(Young O
et O
al., O
2013) O
is O
a O
fundamental O
task O
in O
dialog O
systems. O
Its O
objective O
is O
to O
capture O
the O
comprehensive O
semantics O
of O
user O
utterances, O
and O
it O
typically O
includes O
two O
subtasks: O
intent B-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
(Tur O
and O
De O
Mori, O
2011). O
Intent B-TaskName
detection I-TaskName
aims O
to O
predict O
the O
intention O
of O
the O
user O
utterance O

and O
slot O
filling O
aims O
to O
extract O
additional O
information O
or O
constraints O
expressed O
in O
the O
utterance. O
Recently, O
researchers O
discovered O
that O
these O
two O
tasks O
are O
closely O
tied, O
and O
a O
bunch O
of O
models O
(Goo O
et O
al., O
2018;Liu O
et O
al., O
2019a;E O
et O
al., O
2019;Qin O
et O
al., O
2019) O
are O
proposed O
to O
combine O
the O
single-intent O
detection O
and O

slot B-TaskName
filling I-TaskName
in O
multi-task O
frameworks O
to O
leverage O
their O
correlations. O
However, O
in O
real-world O
scenarios, O
a O
user O
usually O
expresses O
multiple O
intents O
in O
a O
single O
utterance. O
To O
this O
end, O
(Kim O
et O
al., O
2017) O
begin O
to O
tackle O
the O
multi-intent B-TaskName
detection I-TaskName
task O
and O
(Gangadharaiah O
and O
Narayanaswamy, O
2019) O
make O
the O
first O
attempt O
to O
jointly O
model O
the O

multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
in O
a O
multi-task O
framework. O
(Qin O
et O
al., O
2020) O
propose O
an O
AGIF B-MethodName
model O
to O
adaptively O
integrate O
the O
fine-grained O
multi-intent O
prediction O
information O
into O
the O
autoregressive O
decoding O
process O
of O
slot B-TaskName
filling I-TaskName
via O
graph O
attention O
network O
(GAT) O
(Velickovic O
et O
al., O
2018). O
And O
(Qin O
et O
al., O
2021b) O
further O
propose O

a O
non-autoregressive B-MethodName
GAT-based I-MethodName
model I-MethodName
which O
enhances O
the O
interactions O
between O
the O
predicted O
multiple O
intents O
and O
the O
slot O
hidden O
states, O
obtaining O
state-of-the-art O
results O
and O
significant O
speedup. O
Despite O
the O
promising O
progress O
that O
existing O
multi-intent O
SLU O
joint O
models O
have O
achieved, O
we O
discover O
that O
they O
suffer O
from O
two O
main O
issues: O
(1) O
Ignoring O
the O
guidance O
from O

slot O
to O
intent. O
Since O
previous O
researchers O
realized O
that O
"slot O
labels O
could O
depend O
on O
the O
intent" O
(Gangadharaiah O
and O
Narayanaswamy, O
2019), O
existing O
models O
leverage O
the O
information O
of O
the O
predicted O
intents O
to O
guide O
slot B-TaskName
filling I-TaskName
, O
as O
shown O
in O
Fig. O
1(a). O
However, O
they O
ig- O
However, O
in O
previous O
works, O
the O
only O
guidance O
that O
the O

multiple B-TaskName
intent I-TaskName
detection I-TaskName
task O
can O
get O
from O
the O
joint O
model O
is O
sharing O
the O
basic O
semantics O
with O
the O
slot B-TaskName
filling I-TaskName
task. O
As O
a O
result, O
the O
lack O
of O
guidance O
from O
slot O
to O
intent O
limits O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
so O
the O
joint O
task. O
(2) O
Node O
and O
edge O
ambiguity O
in O
the O
semanticslabel O
graph. O

(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b O
apply O
GATs B-MethodName
over O
the O
constructed O
graphs O
to O
model O
the O
interactions O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
However, O
their O
graphs O
are O
homogeneous, O
in O
which O
all O
nodes O
and O
edges O
are O
treated O
as O
the O
same O
type. O
For O
a O
slot O
semantics O
node, O
the O

information O
from O
intent O
label O
nodes O
and O
other O
slot O
semantics O
nodes O
play O
different O
roles, O
while O
the O
homogeneous O
graph O
cannot O
discriminate O
their O
specific O
contributions, O
causing O
ambiguity. O
Therefore, O
the O
heterogeneous O
graphs O
should O
be O
designed O
to O
represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes O
to O
facilitate O
better O
interactions. O
In O
this O
paper, O
we O

propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
to O
tackle O
the O
above O
two O
issues. O
For O
the O
first O
issue, O
Co-guiding B-MethodName
Net I-MethodName
implements O
a O
two-stage O
framework O
as O
shown O
in O
Fig. O
1 O
(b). O
The O
first O
stage O
produces O
the O
initial O
estimated O
labels O
for O
the O
two O
tasks O
and O
the O
second O
stage O
leverages O
the O
estimated O
labels O
as O

prior O
label O
information O
to O
allow O
the O
two O
tasks O
mutually O
guide O
each O
other. O
For O
the O
second O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
( O
HSLGs B-MethodName
): O
(1) O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
intent O
semantics O
nodes O
and O
slot O
label O
nodes; O
(2) O
an O
intent-to-slot B-MethodName

semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
Moreover, O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
( O
HGATs B-MethodName
) O
are O
proposed O
to O
work O
on O
the O
two O
proposed O
graphs O
for O
modeling O
the O
guidances O
from O
slot O
to O
intent O
and O
intent O
to O
slot, O
respectively. O

Experiment O
results O
show O
that O
our O
Co-guiding B-MethodName
Net I-MethodName
significantly O
outperforms O
previous O
models, O
and O
model O
analysis O
further O
verifies O
the O
advantages O
of O
our O
model. O
The O
contributions O
of O
our O
work O
are O
three-fold: O
(1) O
We O
propose O
Co-guiding B-MethodName
Net I-MethodName
1 I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
allowing O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
mutually O
guide O
each O

other. O
We O
make O
the O
first O
attempt O
to O
achieve O
the O
mutual O
guidances O
between O
the O
two O
tasks. O
(2) O
We O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
as O
appropriate O
platforms O
for O
interactions O
between O
semantics O
nodes O
and O
label O
nodes. O
And O
we O
propose O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
to O
model O
the O
mutual O
guidances O
between O
the O
two O
tasks. O

(3) O
Experiment O
results O
demonstrate O
that O
our O
model O
achieves O
new O
state-of-the-art O
performance. O
Co-guiding O
Problem O
Definition O
Given O
a O
input O
utterance O
denoted O
as O
U O
= O
{u O
i O
} O
n O
1 O
, O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
can O
be O
formulated O
as O
a O
multi-label O
classification O
task O
that O
outputs O
multiple O
intent O
labels O
corresponding O
to O
the O
input O
utterance. O
And O

slot B-TaskName
filling I-TaskName
is O
a O
sequence O
labeling O
task O
that O
maps O
each O
u O
i O
into O
a O
slot O
label. O
Next, O
before O
diving O
into O
the O
details O
of O
Coguiding B-MethodName
Net I-MethodName
's O
architecture, O
we O
first O
introduce O
the O
construction O
of O
the O
two O
heterogeneous B-MethodName
graphs I-MethodName
. O
Slot-to-Intent B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
provide O
an O
appropriate O
platform O
for O
modeling O
the O
guidance O

from O
the O
estimated O
slot O
labels O
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
design O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
), O
which O
represents O
the O
relations O
among O
the O
semantics O
of O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
the O
estimated O
slot O
labels. O
S2I-SLG B-MethodName
is O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
3 O
(a). O
It O

contains O
two O
types O
of O
nodes: O
intent O
semantics O
nodes O
(e.g., O
I O
1 O
, O
..., O
I O
5 O
) O
and O
slot O
label O
(SL) O
nodes O
(e.g., O
SL O
1 O
, O
..., O
SL O
5 O
). O
And O
there O
are O
four O
types O
of O
edges O
in O
S2I-SLG B-MethodName
, O
as O
shown O
in O
Fig. O
3 O
(b). O
Each O
edge O
type O
corresponds O
to O

an O
individual O
kind O
of O
information O
aggregation O
on O
the O
graph. O
Mathematically, O
the O
S2I-SLG B-MethodName
can O
be O
denoted O
as O
G O
s2i O
= O
(V O
s2i O
, O
E O
s2i O
, O
A O
s2i O
, O
R O
s2i O
), O
in O
which O
V O
s2i O
is O
the O
set O
of O
all O
nodes, O
E O
s2i O
is O
the O
set O
of O
all O
edges, O
A O

s2i O
is O
the O
set O
of O
two O
node O
types O
and O
R O
s2i O
is O
the O
set O
of O
four O
edge O
types. O
Each O
node O
v O
s2i O
and O
each O
edge O
e O
s2i O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
s2i O
) O
: O
V O
s2i O
→ O
A O
s2i O
and O
ϕ(e O
s2i O
) O
: O
E O

s2i O
→ O
R O
s2i O
. O
For O
instance, O
in O
Fig. O
3, O
the O
SL O
2 O
node O
belongs O
to O
V O
s2i O
, O
while O
its O
node O
type O
SL O
belongs O
to O
A O
s2i O
; O
the O
edge O
from O
SL O
2 O
to O
I O
3 O
belongs O
to O
E O
s2i O
, O
while O
its O
edge O
type O
slot_to_intent_guidance O
belongs O
to O
R O

s2i O
. O
Besides, O
edges O
in O
S2I-SLG B-MethodName
are O
based O
on O
local O
connections. O
For O
example, O
node O
I O
i O
is O
connected O
to O
{I O
i−w O
, O
..., O
I O
i+w O
} O
and O
{SL O
i−w O
, O
..., O
SL O
i+w O
}, O
where O
w O
is O
a O
hyper-parameter O
of O
the O
local O
window O
size. O
Intent-to-Slot B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
present O
a O

platform O
for O
accommodating O
the O
guidance O
from O
the O
estimated O
intent O
labels O
to O
slot O
filling, O
we O
design O
an O
intent-to-slot B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
the O
intent O
label O
nodes. O
I2S-SLG B-MethodName
is O
also O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
4 O

(a). O
It O
contains O
two O
types O
of O
nodes: O
slot O
semantics O
nodes O
(e.g., O
S O
1 O
, O
..., O
S O
5 O
) O
and O
intent O
label O
(IL) O
nodes O
(e.g., O
IL O
1 O
, O
..., O
IL O
5 O
). O
And O
Fig. O
4 O
(b) O
shows O
the O
four O
edge O
types. O
Each O
edge O
type O
corresponds O
to O
an O
individual O
kind O
of O
information O

aggregation O
on O
the O
graph. O
Mathematically, O
the O
I2S-SLG B-MethodName
can O
be O
denoted O
as O
G O
i2s O
= O
(V O
i2s O
, O
E O
i2s O
, O
A O
i2s O
, O
R O
i2s O
). O
Each O
node O
v O
i2s O
and O
each O
edge O
e O
i2s O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
i2s O
) O
and O
ϕ(e O
i2s O
). O

The O
connections O
in O
I2S-SLG B-MethodName
are O
a O
little O
different O
from O
S2I-SLG B-MethodName
. O
Since O
intents O
are O
sentence-level, O
each O
IL O
node O
is O
globally O
connected O
with O
all O
nodes. O
For O
S O
i O
node, O
it O
is O
connected O
to O
{S O
i−w O
, O
..., O
S O
i+w O
} O
and O
{IL O
1 O
, O
..., O
IL O
m O
}, O
where O
w O
is O

the O
local O
window O
size O
and O
m O
is O
the O
number O
of O
estimated O
intents. O
Shared O
Self-Attentive O
Encoder O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
adopt O
a O
shared O
self-attentive O
encoder O
to O
produce O
the O
initial O
hidden O
states O
containing O
the O
basic O
semantics. O
It O
includes O
a O
BiLSTM O
and O
a O
self-attention O
module. O
BiLSTM O
captures O

the O
temporal O
dependencies: O
hi O
= O
BiLSTM O
xi, O
hi−1, O
hi+1(1) O
where O
x O
i O
is O
the O
word O
vector O
of O
u O
i O
. O
Now O
we O
obtain O
the O
context-sensitive O
hidden O
statesĤ O
= O
{ĥ O
i O
} O
n O
1 O
. O
Self-attention O
captures O
the O
global O
dependencies: O
H O
′ O
= O
softmax O
QK O
⊤ O
√ O
d O
k O
V O
(2) O

where O
H O
′ O
is O
the O
global O
contextual O
hidden O
states O
output O
by O
self-attention; O
Q, O
K O
and O
V O
are O
matrices O
obtained O
by O
applying O
different O
linear O
projections O
on O
the O
input O
utterance O
word O
vector O
matrix. O
Then O
we O
concatenate O
the O
output O
of O
BiLSTM O
and O
self-attention O
to O
form O
the O
output O
of O
the O
shared O
selfattentive O
encoder: O
H O

=Ĥ∥H O
′ O
, O
where O
H O
= O
{h O
i O
} O
n O
1 O
and O
∥ O
denotes O
concatenation O
operation. O
Initial O
Estimation O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
To O
obtain O
the O
taskspecific O
features O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
apply O
a O
BiLSTM O
layer O
over O
H: O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
conduct O
token-level O

multi-intent B-TaskName
detection I-TaskName
. O
Each O
h O
[I,0] O
i O
is O
fed O
into O
the O
intent O
decoder. O
Specifically, O
the O
intent O
label O
distributions O
of O
the O
i-th O
word O
are O
obtained O
by: O
h O
[I,0] O
i O
= O
BiLSTM O
I O
hi, O
h O
[I,0] O
i−1 O
, O
h O
[I,0] O
i+1 O
(3) O
Shared O
Self- O
y O
[I,0] O
i O
= O
sigmoid O
W O
1 O
I O

σ(W O
2 O
I O
h O
[I,0] O
i O
+b O
2 O
I O
) O
+b O
1 O
I O
(4) O
where O
σ O
denotes O
the O
non-linear O
activation O
function; O
W O
* O
and O
b O
* O
are O
model O
parameters. O
Then O
the O
estimated O
sentence-level O
intent O
labels O
{IL O
1 O
, O
..., O
IL O
m O
} O
are O
obtained O
by O
the O
token-level O
intent O
voting O
(Qin O

et O
al., O
2021b). O
Slot B-TaskName
Filling I-TaskName
(Qin O
et O
al., O
2021b) O
propose O
a O
nonautoregressive O
paradigm O
for O
slot B-TaskName
filling I-TaskName
decoding, O
which O
achieves O
significant O
speedup. O
In O
this O
paper, O
we O
also O
conduct O
parallel O
slot O
filling O
decoding. O
We O
first O
apply O
a O
BiLSTM O
over O
H O
to O
obtain O
the O
task-specific O
features O
for O
slot O
filling: O
h O
[S,0] O
i O
= O

BiLSTM O
S O
(hi, O
h O
[S,0] O
i−1 O
, O
h O
[S,0] O
i+1 O
)(5) O
Then O
use O
a O
softmax O
classifier O
to O
generate O
the O
slot O
label O
distribution O
for O
each O
word: O
y O
[S,0] O
i O
= O
softmax O
W O
1 O
S O
σ(W O
2 O
S O
h O
[S,0] O
i O
+b O
2 O
S O
) O
+b O
1 O
S O
(6) O
And O
the O
estimated O

slot O
label O
for O
each O
word O
is O
obtained O
by O
SL O
i O
= O
arg O
max(y O
[S,0] O
i O
). O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
State-of-the-art O
models O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b) O
use O
a O
homogeneous O
graph O
to O
connect O
the O
semantic O
nodes O
of O
slot O
filling O
and O
the O
intent O
label O
nodes. O
And O
GAT B-MethodName
(Velickovic O

et O
al., O
2018) O
is O
adopted O
to O
achieve O
information O
aggregation. O
In O
Sec. O
1, O
we O
propose O
that O
this O
manner O
cannot O
effectively O
learn O
the O
interactions O
between O
one O
task's O
semantics O
and O
the O
estimated O
labels O
of O
the O
other O
task. O
To O
tackle O
this O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
graphs I-MethodName
( O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
) O
to O
effectively O

represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes. O
To O
model O
the O
interactions O
between O
semantics O
and O
labels O
on O
the O
proposed O
graphs, O
we O
propose O
a O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
( O
HGAT B-MethodName
). O
When O
aggregating O
the O
information O
into O
a O
node, O
HGAT B-MethodName
can O
discriminate O
the O
specific O
information O
from O
different O
types O
of O
nodes O

along O
different O
relations. O
And O
two O
HGATs B-MethodName
( O
S2I-HGAT B-MethodName
and O
I2S-HGAT B-MethodName
) O
are O
applied O
on O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
, O
respectively. O
Specifically, O
S2I-HGAT B-MethodName
can O
be O
formulated O
as O
follows: O
h O
l+1 O
i O
= O
K O
∥ O
k=1 O
σ O
 O
 O
j∈N O
i O
s2i O
W O
[r,k,1] O
s2i O
α O
[r,k] O
ij O
h O
l O
j O
 O
 O

, O
r O
= O
ϕ O
e O
[j,i] O
s2i O
α O
[r,k] O
ij O
= O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
j O
T O
/ O
√ O
d O
u∈N O
r,i O
s2i O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
u O
T O
/ O
√ O
d O
(7) O
where O

K O
denotes O
the O
total O
head O
number; O
N O
i O
s2i O
denotes O
the O
set O
of O
incoming O
neighbors O
of O
node O
i O
on O
S2I-SLG B-MethodName
; O
W O
[r,k, O
* O
] O
s2i O
are O
weight O
matrices O
of O
edge O
type O
r O
on O
the O
k-th O
head; O
e O
[j,i] O
s2i O
denotes O
the O
edge O
from O
node O
j O
to O
node O
i O
on O

S2I-SLG B-MethodName
; O
N O
r,i O
s2i O
denotes O
the O
nodes O
connected O
to O
node O
i O
with O
r-type O
edges O
on O
S2I-SLG B-MethodName
; O
d O
is O
the O
dimension O
of O
node O
hidden O
state. O
I2S-HGAT B-MethodName
can O
be O
derived O
like O
Eq. O
7. O
Intent O
Decoding O
with O
Slot O
Guidance O
In O
the O
first O
stage, O
we O
obtain O
the O
initial O
intent O
features O
H O
[I,0] O

= O
{h O
I,0 O
i O
} O
n O
i O
and O
the O
initial O
estimated O
slot O
labels O
sequence O
{SL O
1 O
, O
..., O
SL O
n O
}. O
Now O
we O
project O
the O
slot O
labels O
into O
vector O
form O
using O
the O
slot O
label O
embedding O
matrix, O
obtaining O
E O
sl O
= O
{e O
1 O
sl O
, O
..., O
e O
n O
sl O
}. O
Then O

we O
feed O
H O
[I,0] O
and O
E O
sl O
into O
S2I-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
slot O
label O
information O
to O
guide O
the O
intent O
decoding: O
where O
[H O
[I,0] O
, O
E O
sl O
] O
denotes O
the O
input O
node O
representation; O
θ O
I O
denotes O
S2I-HGAT B-MethodName
's O
parameters. O
L O
denotes O
the O
total O
layer O
number. O
Finally, O
H O

[I,L] O
is O
fed O
to O
intent O
decoder, O
producing O
the O
intent O
label O
distributions O
for O
the O
utterance O
words: O
Y O
[I,1] O
= O
{y O
[I,1] O
i O
, O
..., O
y O
[I,1] O
n O
}. O
And O
the O
final O
output O
sentence-level O
intents O
are O
obtained O
via O
applying O
token-level O
intent O
voting O
over O
Y O
[I,1] O
. O
Slot O
Decoding O
with O
Intent O
Guidance O
Intent-aware O

BiLSTM O
Since O
the O
B-I-O O
tags O
of O
slot O
labels O
have O
temporal O
dependencies, O
we O
use O
an O
intent-aware O
BiLSTM O
to O
model O
the O
temporal O
dependencies O
among O
slot O
hidden O
states O
with O
the O
guidance O
of O
estimated O
intents: O
h O
[S,0] O
i O
= O
BiLSTM(y O
[I,0] O
i O
∥h O
[S,0] O
i O
,h O
[S,0] O
i−1 O
,h O
[S,0] O
i+1 O
)(9) O
I2S-HGAT B-MethodName
We O

first O
project O
the O
estimated O
intent O
labels O
{IL O
j O
} O
m O
1 O
into O
vectors O
using O
the O
intent O
label O
embedding O
matrix, O
obtaining O
E O
il O
= O
{e O
1 O
il O
, O
..., O
e O
m O
il O
}. O
Then O
we O
feedH O
S O
and O
E O
il O
into O
I2S-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
intent O
label O

information O
to O
guide O
the O
slot O
decoding: O
H O
[S,L] O
= O
I2S-HGAT B-MethodName
[H O
S O
, O
E O
il O
], O
Gi2s, O
θS(10) O
where O
[H O
[S] O
, O
E O
il O
] O
denotes O
the O
input O
node O
representation; O
θ O
S O
denotes O
I2S-HGAT B-MethodName
's O
parameters. O
Finally, O
H O
[S,L] O
is O
fed O
to O
slot O
decoder, O
producing O
the O
slot O
label O
distributions O
for O

each O
word: O
Y O
[S,1] O
= O
{y O
[S,1] O
i O
, O
..., O
y O
[S,1] O
n O
}. O
And O
the O
final O
output O
slot O
labels O
are O
obtained O
by O
applying O
arg O
max O
over O
Y O
[S,1] O
. O
Training O
Objective O
Loss O
Function O
The O
loss O
function O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
is: O
CE(ŷ, O
y) O
=ŷ O
log(y) O
+ O
(1 O
−ŷ) O
log(1 O

− O
y) O
LI O
= O
1 O
t=0 O
n O
i=1 O
N O
I O
j=1 O
CE O
ŷ O
I O
i O
[j], O
y O
[I,t] O
i O
[j](11) O
And O
the O
loss O
function O
for O
slot B-TaskName
filling I-TaskName
is: O
LS O
= O
1 O
t=0 O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
log O
y O
[S,t] O
i O
[j](12) O
where O
N O
I O
and O
N O

S O
denote O
the O
total O
numbers O
of O
intent O
labels O
and O
slot O
labels;ŷ O
I O
i O
andŷ O
S O
i O
denote O
the O
ground-truth O
intent O
labels O
and O
slot O
labels. O
Margin O
Penalty O
The O
core O
of O
our O
model O
is O
to O
let O
the O
two O
tasks O
mutually O
guide O
each O
other. O
Intuitively, O
the O
predictions O
in O
the O
second O
stage O
should O
be O

better O
than O
those O
in O
the O
first O
stage. O
To O
force O
our O
model O
obey O
this O
rule, O
we O
design O
a O
margin O
penalty O
(L O
mp O
) O
for O
each O
task, O
whose O
aim O
is O
to O
improve O
the O
probabilities O
of O
the O
correct O
labels. O
Specifically, O
the O
formulations O
of O
L O
mp O
I O
and O
L O
mp O
S O
are: O
L O
mp O

I O
= O
n O
i=1 O
N O
I O
j=1ŷ O
I O
i O
[j] O
max O
0, O
y O
[I,0] O
i O
[j] O
− O
y O
[I,1] O
i O
[j] O
L O
mp O
S O
= O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
max O
0, O
y O
[S,0] O
i O
[j] O
− O
y O
[S,1] O
i O
[j](13) O
Model O
Training O
The O
training O
objective O
L O

is O
the O
weighted O
sum O
of O
loss O
functions O
and O
margin O
regularizations O
of O
the O
two O
tasks: O
L O
= O
γ O
(LI O
+ O
βI O
L O
mp O
I O
) O
+ O
(1 O
− O
γ) O
(LS O
+ O
βSL O
mp O
S O
) O
(14) O
where O
γ O
is O
the O
coefficient O
balancing O
the O
two O
tasks; O
β O
I O
and O
β O
S O
are O

the O
coefficients O
of O
the O
margin O
regularization O
for O
the O
two O
tasks. O
3 O
Experiments O
Datasets O
and O
Metrics O
Following O
previous O
works, O
MixATIS B-DatasetName
and O
MixS-NIPS B-DatasetName
(Hemphill O
et O
al., O
1990;Coucke O
et O
al., O
2018;Qin O
et O
al., O
2020) O
As O
for O
evaluation O
metrics, O
following O
previous O
works, O
we O
adopt O
accuracy B-MetricName
( O
Acc B-MetricName
) O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
F1 B-MetricName

score O
for O
slot B-TaskName
filling I-TaskName
, O
and O
overall B-MetricName
accuracy I-MetricName
for O
the O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
. O
Overall O
accuracy O
denotes O
the O
ratio O
of O
sentences O
whose O
intents O
and O
slots O
are O
all O
correctly O
predicted. O
Implementation O
Details O
Following O
previous O
works, O
the O
word O
and O
label O
embeddings O
are O
trained O
from O
scratch O
2 O
. O
The O
dimensions O
of O
word B-HyperparameterName

embedding I-HyperparameterName
, O
label B-HyperparameterName
embedding I-HyperparameterName
, O
and O
hidden B-HyperparameterName
state I-HyperparameterName
are O
256 B-HyperparameterValue
on O
MixATIS B-DatasetName
, O
while O
on O
MixS-NIPS B-DatasetName
they O
are O
256 B-HyperparameterValue
, O
128 B-HyperparameterValue
, O
and O
256 B-HyperparameterValue
. O
The O
layer B-HyperparameterName
number I-HyperparameterName
of O
all O
GNNs O
is O
2 B-HyperparameterValue
. O
Adam O
(Kingma O
and O
Ba, O
2015) O
is O
used O
to O
train O
our O
model O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O

1e B-HyperparameterValue
−3 I-HyperparameterValue
and O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e B-HyperparameterValue
−6 I-HyperparameterValue
. O
As O
for O
the O
coefficients O
Eq.14, O
γ B-HyperparameterName
is O
0.9 B-HyperparameterValue
on O
MixATIS B-DatasetName
and O
0.8 B-HyperparameterValue
on O
MixSNIPS B-DatasetName
; O
on O
both O
datasets, O
β B-HyperparameterName
I I-HyperparameterName
is O
1e B-HyperparameterValue
−6 I-HyperparameterValue
and O
β B-HyperparameterName
S I-HyperparameterName
is O
1e B-HyperparameterValue
0 I-HyperparameterValue
. O
The O
model O
performing O
best O
on O
the O
dev O
set O
is O
selected O

then O
we O
report O
its O
results O
on O
the O
test O
set. O
All O
experiments O
are O
conducted O
on O
RTX O
6000. O
Our O
source O
code O
will O
be O
released. O
Main O
Results O
The O
performance O
comparison O
of O
Co-guiding B-MethodName
Net I-MethodName
and O
baselines O
are O
shown O
in O
Table O
1, O
from O
which O
we O
have O
the O
following O
observations: O
(1) O
Co-guiding O
Net O
gains O
significant O
and O

consistent O
improvements O
on O
all O
tasks O
and O
datasets. O
Specifically, O
on O
MixATIS B-DatasetName
dataset, O
it O
overpasses O
the O
previous O
state-of-the-art O
model O
GL-GIN B-MethodName
by O
19.3% B-MetricValue
, O
1.8% B-MetricValue
, O
and O
3.7% B-MetricValue
on O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
, O
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively; O
on O
MixSNIPS B-DatasetName
dataset, O
it O
overpasses O
GL-GIN B-MethodName
by O
5.2% B-MetricValue
, O
1.2% B-MetricValue

and O
2.1% B-MetricValue
on O
sentencelevel B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively. O
This O
is O
because O
our O
model O
achieves O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
allowing O
the O
two O
tasks O
to O
provide O
crucial O
clues O
for O
each O
other. O
Besides, O
our O
designed O
HSLGs B-MethodName
and O
HGATs B-MethodName

can O
effectively O
model O
the O
interactions O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
extracting O
the O
indicative O
clues O
from O
initial O
predictions. O
(2) O
Co-guiding B-MethodName
Net I-MethodName
achieves O
a O
larger O
improvement O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
than O
slot B-TaskName
filling I-TaskName
. O
The O
reason O
is O
that O
except O
for O
the O
guidance O
from O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
to O
slot B-TaskName
filling I-TaskName
, O

our O
model O
also O
achieves O
the O
guidance O
from O
slot B-TaskName
filling I-TaskName
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
while O
previous O
models O
all O
ignore O
this. O
Besides, O
previous O
methods O
model O
the O
semantics-label O
interactions O
by O
homogeneous B-MethodName
graph I-MethodName
and O
GAT B-MethodName
, O
limiting O
the O
performance. O
Differently, O
our O
model O
uses O
the O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
to O
represent O
different O
relations O
among O
the O

semantic O
nodes O
and O
the O
label O
nodes, O
then O
applies O
the O
proposed O
HGATs B-MethodName
over O
the O
graphs O
to O
achieve O
the O
interactions. O
Consequently, O
their O
performances O
(especially O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
) O
are O
significantly O
inferior O
to O
our O
model. O
(3) O
The O
improvements O
in O
overall B-MetricName
accuracy I-MetricName
are O
much O
sharper. O
We O
suppose O
the O
reason O
is O
that O
the O
achieved O

mutual O
guidances O
make O
the O
two O
tasks O
deeply O
coupled O
and O
allow O
them O
to O
stimulate O
each O
other O
using O
their O
initial O
predictions. O
For O
each O
task, O
its O
final O
outputs O
are O
guided O
by O
its O
and O
another O
task's O
initial O
predictions. O
By O
this O
means, O
the O
correct O
predictions O
of O
the O
two O
tasks O
can O
be O
better O
aligned. O
As O
a O

result, O
more O
test O
samples O
get O
correct O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
results, O
and O
then O
overall B-MetricName
accuracy I-MetricName
is O
boosted. O
Model O
Analysis O
We O
conduct O
a O
set O
of O
ablation O
experiments O
to O
verify O
the O
advantages O
of O
our O
work O
from O
different O
perspectives, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
Effect O
of O
Slot-to-Intent O
Guidance O
One O
of O

the O
core O
contributions O
of O
our O
work O
is O
achieving O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
while O
previous O
works O
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot. O
Therefore, O
compared O
with O
previous O
works, O
one O
of O
the O
advantages O
of O
our O
work O
is O
modeling O
the O
slot-tointent O
guidance. O
To O
verify O

this, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
S2I-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O
can O
observe O
that O
Intent B-MetricName
Acc I-MetricName
drops O
by O
2.0% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.8% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
Moreover, O
Overall B-MetricName
Acc I-MetricName
drops O
more O
significantly: O
3.6% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.9% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
This O
proves O
that O
the O

guidance O
from O
slot O
to O
intent O
can O
effectively O
benefit O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
achieving O
the O
mutual O
guidances O
between O
the O
two O
tasks O
can O
significantly O
improve O
Overall B-MetricName
Acc I-MetricName
. O
Besides, O
although O
both O
of O
w/o B-MetricName
S2I-guidance I-MetricName
and O
GL-GIN B-MethodName
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot, O
w/o B-MetricName
S2I-guidance I-MetricName
outperforms O
GL-GIN B-MethodName
by O
large O

margins. O
We O
attribute O
this O
to O
our O
proposed O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
and O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
, O
whose O
advantages O
are O
verified O
in O
Sec. O
3.4.3. O
Effect O
of O
Intent-to-Slot O
Guidance O
To O
verify O
the O
effectiveness O
of O
intent-to-slot O
guidance, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
I2S-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O

can O
find O
that O
the O
intent-to-slot O
guidance O
has O
a O
significant O
impact O
on O
performance. O
Specifically, O
w/o B-MetricName
I2S-guidance I-MetricName
cause O
nearly O
the O
same O
extent O
of O
performance O
drop O
on O
Overall B-MetricName
Acc I-MetricName
, O
proving O
that O
both O
of O
the O
intent-toslot O
guidance O
and O
slot-to-intent O
guidance O
are O
indispensable O
and O
achieving O
the O
mutual O
guidances O
can O
significantly O
boost O
the O
performance. O
Effect O

of O
HSLGs B-MethodName
and O
HGATs B-MethodName
In O
this O
paper, O
we O
design O
two O
HSLGs B-MethodName
: O
(i.e., O
S2I-SLG B-MethodName
, O
I2S-SLG B-MethodName
) O
and O
two O
HGATs B-MethodName
(i.e., O
S2I-HGAT B-MethodName
, O
I2S-HGAT B-MethodName
). O
To O
verify O
their O
effectiveness, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
relations I-MetricName
by O
removing O
the O
relations O
on O
the O
two O
HSLGs B-MethodName
. O
In O
this O
case, O
S2I-SLG B-MethodName
/ O

I2S-SLG B-MethodName
collapses O
to O
a O
homogeneous B-MethodName
graph I-MethodName
, O
and O
S2I-HGAT B-MethodName
/ O
I2S-HGAT B-MethodName
collapses O
to O
a O
general O
GAT B-MethodName
based O
on O
multi-head O
attentions. O
From O
Table O
2, O
we O
can O
observe O
that O
w/o B-MetricName
relations I-MetricName
obtains O
dramatic O
drops O
on O
all O
metrics O
on O
both O
datasets. O
The O
apparent O
performance O
gap O
between O
w/o B-MetricName
relations I-MetricName
and O
Co-guiding B-MethodName
Net I-MethodName
verifies O
that O

(1) O
our O
proposed O
HSLGs B-MethodName
can O
effectively O
represent O
the O
different O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
providing O
appropriate O
platforms O
for O
modeling O
the O
mutual O
guidances O
between O
the O
two O
tasks; O
(2) O
our O
proposed O
HGATs B-MethodName
can O
sufficiently O
and O
effectively O
model O
interactions O
between O
the O
semantics O
and O
indicative O
label O
information O
via O
achieving O
the O
relation-specific O

attentive O
information O
aggregation O
on O
the O
HSLGs B-MethodName
. O
Besides, O
although O
w/o B-MetricName
relations I-MetricName
obviously O
un-derperforms O
Co-guiding B-MethodName
Ne I-MethodName
t, O
it O
still O
significantly O
outperforms O
all O
baselines. O
We O
attribute O
this O
to O
the O
fact O
that O
our O
model O
achieves O
the O
mutual O
guidances O
between O
the O
two O
tasks, O
which O
allows O
them O
to O
promote O
each O
other O
via O
cross-task O
correlations. O
et O

al. O
(2021b) O
propose O
a O
Local O
Slot-aware O
GAT O
module O
to O
alleviate O
the O
uncoordinated O
slot O
problem O
(e.g., O
B-singer O
followed O
by O
I-song) O
(Wu O
et O
al., O
2020) O
caused O
by O
the O
non-autoregressive O
fashion O
of O
slot O
filling. O
And O
the O
ablation O
study O
in O
(Qin O
et O
al., O
2021b) O
proves O
that O
this O
module O
effectively O
improves O
the O
slot O
filling O
performance O

by O
modeling O
the O
local O
dependencies O
among O
slot O
hidden O
states. O
Effect O
of O
I2S-HGAT B-MethodName
for O
Capturing O
Local O
Slot O
Dependencies O
Qin O
In O
their O
model O
( O
GL-GIN B-MethodName
), O
the O
local O
dependencies O
are O
modeled O
in O
both O
of O
the O
local B-MethodName
slot-aware I-MethodName
GAT I-MethodName
and O
subsequent O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
. O
We O
suppose O
the O
reason O
why O
GL-GIN B-MethodName
needs O
the O

local B-MethodName
Slotaware I-MethodName
GAT I-MethodName
is O
that O
the O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
in O
GL-GIN B-MethodName
cannot O
effectively O
capture O
the O
local O
slot O
dependencies. O
GL-GIN's B-MethodName
global B-MethodName
slot-intent I-MethodName
graph I-MethodName
is O
homogeneous, O
and O
the O
GAT B-MethodName
working O
on O
it O
treats O
the O
slot O
semantics O
nods O
and O
the O
intent O
label O
nodes O
equally O
without O
discrimination. O
Therefore, O
each O
slot O
hidden O
state O
receives O
indiscriminate O

information O
from O
both O
of O
its O
local O
slot O
hidden O
states O
and O
all O
intent O
labels, O
making O
it O
confusing O
to O
capture O
the O
local O
slot O
dependencies. O
In O
contrast, O
we O
believe O
our O
I2S-HLG B-MethodName
and O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
slot O
local O
dependencies O
along O
the O
specific O
slot_semantics_dependencies O
relation, O
which O
is O
modeled O
together O
with O
other O
relations. O
Therefore, O

our O
Co-guiding B-MethodName
Net I-MethodName
does O
not O
include O
another O
module O
to O
capture O
the O
slot O
local O
dependencies. O
To O
verify O
this, O
we O
design O
a O
variant O
termed O
+Local B-MetricName
Slot-aware I-MetricName
GAT I-MetricName
, O
which O
is O
implemented O
by O
augmenting O
Co-guiding B-MethodName
Net I-MethodName
with O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
(Qin O
et O
al., O
2021b) O
that O
not O
only O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
does O

not O
bring O
improvement, O
it O
even O
causes O
performance O
drops. O
This O
proves O
that O
our O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
local O
slot O
dependencies. O
A.1 O
Settings O
To O
evaluate O
Co-guiding B-MethodName
Net's I-MethodName
performance O
based O
on O
the O
pre-trained O
language O
model, O
we O
use O
the O
pretrained O
RoBERTa O
(Liu O
et O
al., O
2019b) O
encoder O
to O
replace O
the O
original O
self-attentive O
encoder. O
We O

adopt O
the O
pre-trained O
RoBERTa-base O
version O
provided O
by O
Transformers O
(Wolf O
et O
al., O
2020). O
For O
each O
word, O
its O
first O
subwords' O
hidden O
state O
generated O
by O
RoBERTa O
is O
taken O
as O
the O
word O
representation. O
AdamW O
(Loshchilov O
and O
Hutter, O
2019) O
optimizer O
is O
used O
for O
model O
training O
with O
the O
default O
setting, O
and O
RoBERTa O
is O
fine-tuned O
with O
model O

training. O
Other O
model O
components O
are O
identical O
to O
the O
Coguiding B-MethodName
Net I-MethodName
based O
on O
LSTM, O
and O
we O
use O
the O
same O
hyper-parameters O
of O
the O
model O
rather O
than O
search O
for O
the O
optimal O
ones O
for O
RoBERTa+Co-guiding O
Net O
due O
to O
our O
limited O
computation O
resource. O
A.2 O
Results O
Table O
3 O
shows O
the O
result O
comparison O
of O
Coguiding B-MethodName
Net I-MethodName
, O

RoBERTa+Co-guiding B-MethodName
Net I-MethodName
, O
and O
their O
state-of-the-art O
counterparts: O
AGIF B-MethodName
, O
GL-GIN B-MethodName
, O
RoBERTa+AGIF B-MethodName
, O
and O
RoBERTa+GL-GIN B-MethodName
. O
We O
can O
find O
that O
although O
RoBERTa O
boosts O
the O
models' O
performance, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
 I-MethodName
still O
significantly O
outperforms O
RoBERTa+AGIF B-MethodName
and O
RoBERTa+GL-GIN B-MethodName
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
although O
the O
pre-trained O
language O
model O
(PTLM) O

can O
enhance O
the O
word O
representations, O
it O
cannot O
achieve O
the O
guidance O
between O
the O
two O
tasks O
or O
the O
interactions O
between O
the O
semantics O
and O
label O
information, O
which O
are O
exactly O
the O
advantages O
of O
our O
Co-guiding B-MethodName
Net I-MethodName
. O
Therefore, O
collaborating O
with O
PTLM O
that O
has O
strong O
ability O
of O
language O
modeling, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
gets O
its O
performance O
further O

boosted, O
achieving O
new O
state-of-the-art. O

Learning B-TaskName
a I-TaskName
Grammar I-TaskName
Inducer I-TaskName
from O
Massive O
Uncurated O
Instructional O
Videos O
Video-aided B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
to O
leverage O
video O
information O
for O
finding O
more O
accurate O
syntactic O
grammars O
for O
accompanying O
text. O
While O
previous O
work O
focuses O
on O
building O
systems O
for O
inducing O
grammars O
on O
text O
that O
are O
well-aligned O
with O
video O
content, O
we O
investigate O
the O
scenario, O
in O
which O

text O
and O
video O
are O
only O
in O
loose O
correspondence. O
Such O
data O
can O
be O
found O
in O
abundance O
online, O
and O
the O
weak O
correspondence O
is O
similar O
to O
the O
indeterminacy O
problem O
studied O
in O
language O
acquisition. O
Furthermore, O
we O
build O
a O
new O
model O
that O
can O
better O
learn O
video-span O
correlation O
without O
manually O
designed O
features O
adopted O
by O
previous O
work. O

Experiments O
show O
that O
our O
model O
trained O
only O
on O
large-scale O
YouTube O
data O
with O
no O
textvideo O
alignment O
reports O
strong O
and O
robust O
performances O
across O
three O
unseen O
datasets, O
despite O
domain O
shift O
and O
noisy O
label O
issues. O
Furthermore O
our O
model O
yields O
higher O
F1 B-MetricName
scores O
than O
the O
previous O
state-of-the-art O
systems O
trained O
on O
in-domain O
data. O
Introduction O
Grammar B-TaskName
induction I-TaskName

is O
a O
fundamental O
and O
longlasting O
(Lari O
and O
Young, O
1990;Clark, O
2001;Klein O
and O
Manning, O
2002) O
problem O
in O
computational O
linguistics, O
which O
aims O
to O
find O
hierarchical O
syntactic O
structures O
from O
plain O
sentences. O
Unlike O
supervised O
methods O
(Charniak, O
2000;Collins, O
2003;Petrov O
and O
Klein, O
2007;Zhang O
and O
Clark, O
2011;Cross O
and O
Huang, O
2016;Kitaev O
and O
Klein, O
2018) O
that O
require O
human O
annotated O
treebanks, O

e.g., O
Penn B-MethodName
Treebank I-MethodName
(Marcus O
et O
al., O
1993), O
grammar O
inducers O
do O
not O
rely O
on O
any O
human O
annotations O
for O
training. O
Grammar B-TaskName
induction I-TaskName
is O
attractive O
since O
annotating O
syntactic O
trees O
by O
human O
language O
experts O
is O
expensive O
and O
time O
consuming, O
while O
the O
current O
treebanks O
are O
limited O
to O
several O
major O
languages O
and O
domains. O
Recently, O
deep O
learning O

models O
have O
achieved O
remarkable O
success O
across O
NLP O
tasks, O
and O
neural O
models O
have O
been O
designed O
(Shen O
et O
al., O
2018b,a;Kim O
et O
al., O
2019a,b;Jin O
et O
al., O
2018) O
for O
grammar B-TaskName
induction I-TaskName
, O
which O
greatly O
advanced O
model O
performance O
on O
induction O
with O
raw O
text. O
Recent O
efforts O
have O
started O
to O
consider O
other O
useful O
information O
from O
multiple O
modalities, O

such O
as O
images O
(Shi O
et O
al., O
2019;Jin O
and O
Schuler, O
2020) O
and O
videos O
(Zhang O
et O
al., O
2021). O
Specifically, O
Zhang O
et O
al. O
(2021) O
show O
that O
multi-modal O
information O
(e.g. O
motion, O
sound O
and O
objects) O
from O
videos O
can O
significantly O
improve O
the O
induction O
accuracy O
on O
verb O
and O
noun O
phrases. O
Such O
work O
uses O
curated O
multi-modal O
data O
publicly O

available O
on O
the O
web, O
which O
all O
assume O
that O
the O
meaning O
of O
a O
sentence O
needs O
to O
be O
identical O
(e.g., O
being O
a O
caption) O
to O
the O
corresponding O
video O
or O
image. O
This O
assumption O
limits O
usable O
data O
to O
several O
small-scale O
benchmarks O
(Lin O
et O
al., O
2014;Xu O
et O
al., O
2016;Hendricks O
et O
al., O
2017) O
with O
expensive O
human O
annotations O

on O
image/video O
captions. O
The O
noisy O
correspondence O
between O
form O
and O
meaning O
is O
one O
of O
the O
main O
research O
questions O
in O
language O
acquisition O
(Akhtar O
and O
Montague, O
1999;Gentner O
et O
al., O
2001;Dominey O
and O
Dodane, O
2004), O
where O
different O
proposals O
attempt O
to O
address O
this O
indeterminacy O
faced O
by O
children. O
There O
has O
been O
computational O
work O
incorporating O
such O
indeterminacy O
into O

their O
models O
(Yu O
and O
Siskind, O
2013;Huang O
et O
al., O
2021). O
For O
modeling O
empirical O
grammar O
learning O
with O
multi-modal O
inputs, O
two O
important O
questions O
still O
remain O
open: O
1) O
how O
can O
a O
grammar O
inducer O
benefit O
from O
large-scale O
multi-media O
data O
(e.g., O
YouTube O
videos) O
with O
noisy O
text-to-video O
correspondence? O
and O
2) O
how O
can O
a O
grammar O
inducer O
show O
robust O

performances O
across O
multiple O
domains O
and O
datasets? O
By O
using O
data O
with O
only O
weak O
cross-modal O
correspondence, O
such O
as O
YouTube O
videos O
and O
their O
automatically O
generated O
subtitles, O
we O
allow O
the O
computational O
models O
to O
face O
a O
similar O
indeterminacy O
problem, O
and O
exam-ine O
how O
indeterminacy O
interacts O
with O
data O
size O
to O
influence O
learning O
behavior O
and O
performance O
of O
the O

induction O
models. O
In O
this O
paper, O
we O
conduct O
the O
first O
investigation O
on O
both O
questions. O
Specifically, O
we O
collect O
2.4 O
million O
video O
clips O
and O
the O
corresponding O
subtitles O
from O
instructional O
YouTube O
videos O
( O
HowTo100M B-DatasetName
Miech O
et O
al. O
2019) O
to O
train O
multi-modal O
grammar O
inducers, O
instead O
of O
using O
the O
training O
data O
from O
a O
benchmark O
where O
text O

and O
video O
are O
in O
alignment. O
We O
then O
propose O
a O
novel O
model, O
named O
Pre-Trained B-MethodName
Compound I-MethodName
Probabilistic I-MethodName
Context-Free I-MethodName
Grammars I-MethodName
( O
PTC-PCFG B-MethodName
), O
that O
extends O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
incorporating O
a O
videospan O
matching O
loss O
term O
into O
the O
Compound B-MethodName
PCFG I-MethodName
(Kim O
et O
al., O
2019a) O
model. O
To O
better O
capture O

the O
video-span O
correlation, O
it O
leverages O
CLIP O
(Miech O
et O
al., O
2020), O
a O
state-of-the-art O
model O
pretrained O
on O
video O
subtitle O
retrieval, O
as O
the O
encoders O
for O
both O
video O
and O
text. O
Compared O
with O
previous O
work O
(Zhang O
et O
al., O
2021) O
that O
independently O
extracts O
features O
from O
each O
modality O
before O
merging O
them O
using O
a O
simple O
Transformer O
(Vaswani O
et O

al., O
2017) O
encoder, O
the O
encoders O
of O
our O
model O
have O
been O
pretrained O
to O
merge O
such O
multi-modal O
information, O
and O
no O
human O
efforts O
are O
needed O
to O
select O
useful O
modalities O
from O
the O
full O
set. O
Experiments O
on O
three O
benchmarks O
show O
that O
our O
model, O
which O
is O
trained O
on O
noisy O
YouTube O
video O
clips O
and O
no O
data O
from O

these O
benchmarks, O
produces O
substantial O
gains O
over O
the O
previous O
state-of-the-art O
system O
(Zhang O
et O
al., O
2021) O
trained O
on O
in-domain O
video O
clips O
with O
human O
annotated O
captions. O
Furthermore, O
our O
model O
demonstrates O
robust O
performances O
across O
all O
three O
datasets. O
We O
suggest O
the O
limitations O
of O
our O
model O
and O
future O
directions O
for O
improvements O
through O
analysis O
and O
discussions. O
Code O

will O
be O
released O
upon O
paper O
acceptance. O
In O
summary, O
the O
main O
contributions O
are: O
• O
We O
are O
the O
first O
to O
study O
training O
a O
grammar O
inducer O
with O
massive O
general-domain O
noisy O
video O
clips O
instead O
of O
benchmark O
data, O
introducing O
the O
indeterminacy O
problem O
to O
the O
induction O
model. O
• O
We O
propose O
PTC-PCFG B-MethodName
, O
a O
novel O
model O
for O

unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
It O
is O
simpler O
in O
design O
than O
previous O
models O
and O
can O
better O
capture O
the O
video-text O
matching O
information. O
• O
Trained O
only O
on O
noisy O
YouTube O
videos O
without O
finetuning O
on O
benchmark O
data, O
PTC-PCFG B-MethodName
reports O
stronger O
performances O
than O
previous O
mod-els O
trained O
on O
benchmark O
data O
across O
three O
benchmarks. O
Background O
and O
Motivation O
Compound B-MethodName

PCFGs I-MethodName
A O
PCFG B-MethodName
model O
in O
Chomsky O
Normal O
Form O
can O
be O
defined O
as O
a O
tuple O
of O
6 O
terms O
(S, O
N O
, O
P, O
Σ, O
R, O
Π), O
where O
they O
correspond O
to O
the O
start O
symbol, O
the O
sets O
of O
non-terminals, O
pre-terminals, O
terminals, O
production O
rules O
and O
their O
probabilities. O
Given O
pre-defined O
numbers O
of O
non-terminals O
and O
pre-terminals, O
a O

PCFG B-MethodName
induction O
model O
tries O
to O
estimate O
the O
probabilities O
for O
all O
production O
rules. O
The O
compound B-MethodName
PCFG I-MethodName
( O
C-PCFG B-MethodName
) O
model O
(Kim O
et O
al., O
2019a) O
adopts O
a O
mixture O
of O
PCFGs. O
Instead O
of O
a O
corpus-level O
prior O
used O
in O
previous O
work O
(Kurihara O
and O
Sato, O
2006;Johnson O
et O
al., O
2007;Wang O
and O
Blunsom, O
2013;Jin O
et O
al., O
2018), O

C-PCFG B-MethodName
imposes O
a O
sentence-specific O
prior O
on O
the O
distribution O
of O
possible O
PCFGs. O
Specifically O
in O
the O
generative O
story, O
the O
probability O
π O
r O
for O
production O
rule O
r O
is O
estimated O
by O
model O
g O
that O
assigns O
a O
latent O
variable O
z O
for O
each O
sentence O
σ, O
and O
z O
is O
drawn O
from O
a O
prior O
distribution: O
π O
r O
= O

g(r, O
z; O
θ), O
z O
∼ O
p(z). O
( O
)1 O
where O
θ O
represents O
the O
model O
parameters. O
The O
probabilities O
for O
all O
three O
types O
of O
CFG O
rules O
are O
defined O
as O
follows: O
π O
S→A O
= O
exp(u O
⊤ O
A O
f O
s O
([w O
S O
; O
z])) O
A O
′ O
∈N O
exp(u O
⊤ O
A O
′ O
f O
s O
([w O
S O

; O
z])) O
, O
π O
A→BC O
= O
exp(u O
⊤ O
BC O
[w O
A O
; O
z]) O
B O
′ O
,C O
′ O
∈N O
∪P O
exp(u O
⊤ O
B O
′ O
C O
′ O
[w O
A O
; O
z])) O
, O
π O
T O
→w O
= O
exp(u O
⊤ O
w O
f O
t O
([w O
T O
; O
z])) O
w O
′ O
∈Σ O
exp(u O
⊤ O
w O
′ O

f O
t O
([w O
T O
; O
z])) O
,(2) O
where O
A O
∈ O
N O
, O
B O
and O
C O
∈ O
N O
∪ O
P, O
T O
∈ O
P, O
w O
∈ O
Σ. O
Both O
w O
and O
u O
are O
dense O
vectors O
representing O
words O
and O
all O
types O
of O
non-terminals, O
and O
f O
s O
and O
f O
t O
are O
neural O
encoding O
functions. O
Optimizing O

the O
C-PCFG B-MethodName
model O
involves O
maximizing O
the O
marginal O
likelihood O
p(σ) O
of O
each O
training O
sentence O
σ O
for O
all O
possible O
z: O
log O
p O
θ O
(σ) O
= O
log O
z O
t∈T O
G O
(σ) O
p O
θ O
(t|z)p(z)dz O
(3) O
where O
T O
G O
(σ) O
indicates O
all O
possible O
parsing O
trees O
for O
sentence O
σ. O
Since O
computing O
the O
integral O
over O
z O

is O
intractable, O
this O
objective O
is O
optimized O
by O
maximizing O
its O
evidence O
lower O
bound O
ELBO(σ; O
ϕ, O
θ): O
ELBO(σ; O
ϕ, O
θ) O
= O
E O
q O
ϕ O
(z|σ) O
[log O
p O
θ O
(σ|z)] O
−KL[q O
ϕ O
(z|σ)||p(z)],(4) O
where O
q O
ϕ O
(z|σ) O
is O
the O
variational O
posterior O
calculated O
by O
another O
neural O
network O
with O
parameters O
ϕ. O
Given O
a O
sampled O
z, O

the O
log-likelihood O
term O
log O
p O
θ O
(σ|z) O
is O
calculated O
via O
the O
inside O
algorithm. O
The O
KL O
term O
can O
be O
computed O
analytically O
when O
both O
the O
prior O
p(z) O
and O
the O
variational O
posterior O
q O
ϕ O
(z|σ) O
are O
Gaussian O
(Kingma O
and O
Welling, O
2014). O
Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
MMC-PCFG B-MethodName
) O
(Zhang O
et O
al., O

2021) O
extends O
C-PCFG B-MethodName
with O
a O
model O
to O
match O
a O
video O
v O
with O
a O
span O
c O
in O
a O
parse O
tree O
t O
of O
a O
sentence O
σ. O
It O
extracts O
M O
visual O
and O
audio O
features O
from O
a O
video O
v O
and O
encodes O
them O
via O
a O
multi-modal O
transformer O
(Gabeur O
et O
al., O
2020), O
denoted O
as O
Ψ O
= O

{ψ O
i O
} O
M O
i=1 O
. O
The O
word O
representation O
h O
i O
of O
the O
ith O
word O
is O
computed O
by O
BiLSTM. O
Given O
a O
particular O
span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
, O
its O
representation O
c O
is O
the O
weighted O
sum O
of O
all O
label-specific O
span O
representations: O
c O
= O
|N O

| O
k=1 O
p(k|c, O
σ)f O
k O
1 O
j O
− O
i O
+ O
1 O
j O
l=i O
h O
l O
, O
(5) O
where O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|} O
are O
the O
phrasal O
label O
probabilities O
of O
span O
c. O
The O
representation O
of O
a O
span O
c O
is O
then O
correspondingly O
projected O
to O
M O
separate O
embeddings O
via O
gated O
embedding O

(Miech O
et O
al., O
2018), O
denoted O
as O
Ξ O
= O
{ξ O
i O
} O
M O
i=1 O
. O
Finally O
the O
video-text O
matching O
loss O
is O
defined O
as O
a O
sum O
over O
all O
video-span O
matching O
losses O
weighted O
by O
the O
marginal O
probability O
of O
a O
span O
from O
the O
parser: O
s O
mm O
(v, O
σ) O
= O
c∈σ O
p(c|σ)h O
mm O
(Ξ, O
Ψ), O

(6) O
where O
h O
mm O
(Ξ, O
Ψ) O
is O
a O
hinge O
loss O
measuring O
the O
distances O
from O
video O
v O
to O
the O
matched O
and O
unmatched O
(i.e. O
span O
from O
another O
sentence) O
span O
c O
and O
c O
′ O
and O
the O
distances O
from O
span O
c O
to O
the O
matched O
and O
unmatched O
(i.e. O
another O
video) O
video O
v O
and O
v O
′ O

: O
ω O
i O
(c) O
= O
exp(u O
⊤ O
i O
c) O
M O
j=1 O
exp(u O
⊤ O
j O
c) O
,(7) O
o(Ξ,Ψ) O
= O
M O
i=1 O
ω O
i O
(c)cos(ξ O
i O
, O
ψ O
i O
),(8) O
h O
mm O
(Ξ,Ψ) O
= O
E O
c O
′ O
[o(Ξ O
′ O
, O
Ψ) O
− O
o(Ξ, O
Ψ)) O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[o(Ξ, O

Ψ O
′ O
) O
− O
o(Ξ, O
Ψ) O
+ O
ϵ] O
+ O
, O
(9) O
where O
Ξ O
′ O
is O
a O
set O
of O
unmatched O
span O
expert O
embeddings O
of O
Ψ, O
Ψ O
′ O
is O
a O
set O
of O
unmatched O
video O
representations O
of O
Ξ, O
ϵ O
is O
a O
positive O
margin, O
[•] O
+ O
= O
max(0, O
•), O
{u O
i O
} O
M O
i=1 O

are O
learned O
weights, O
and O
the O
expectations O
are O
approximated O
with O
one O
sample O
drawn O
from O
the O
training O
data. O
During O
training, O
both O
ELBO O
and O
the O
video-text O
matching O
loss O
are O
jointly O
optimized. O
Limitation O
and O
Motivation O
Existing O
work O
on O
multi-modal B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
at O
leveraging O
strict O
correspondence O
between O
image/video O
and O
text O
for O
information O
about O
syntactic O

categories O
and O
structures O
of O
the O
words O
and O
spans O
in O
the O
text. O
However, O
such O
datasets O
are O
expensive O
to O
annotate. O
Besides, O
the O
ambiguous O
correspondence O
between O
language O
and O
real-world O
context, O
observed O
in O
language O
acquisition, O
is O
not O
really O
reflected O
in O
such O
training O
setups. O
As O
a O
result, O
we O
believe O
that O
the O
previous O
work O
fails O
to O

answer O
the O
following O
important O
questions: O
1) O
how O
well O
a O
grammar O
inducer O
would O
perform O
when O
it O
is O
trained O
only O
on O
noisy O
multi-media O
data; O
2) O
how O
the O
scale O
of O
training O
data O
would O
affect O
the O
performance O
and O
cross-domain O
robustness? O
Training O
a O
Grammar O
Inducer O
with O
Massive O
YouTube O
Videos O
We O
make O
the O
first O
investigation O
into O

the O
above O
questions O
by O
leveraging O
massive O
video O
clips O
from O
instructional O
YouTube O
videos O
to O
train O
our O
grammar O
inducer. O
Different O
from O
the O
benchmark O
data O
used O
by O
previous O
work, O
the O
YouTube O
video O
clips O
do O
not O
contain O
paired O
sentences. O
This O
section O
will O
first O
introduce O
the O
method O
for O
generating O
noisy O
training O
instances O
(video O
clip O
and O

sentence O
pairs) O
from O
YouTube O
videos O
( O
§3.1), O
before O
describing O
a O
novel O
grammar O
induction O
model O
( O
§3.2) O
with O
pre-trained O
text O
and O
video O
encoders. O
Harvesting O
Training O
Instances O
from O
YouTube O
Videos O
Given O
a O
YouTube O
video, O
we O
would O
like O
to O
generate O
a O
set O
of O
video O
clip O
and O
subtitle O
pairs O
Ω O
= O
{(v, O
σ)}, O
where O

each O
subtitle O
σ O
is O
a O
complete O
sentence O
and O
is O
aligned O
in O
time O
with O
its O
paired O
video O
clip O
v. O
To O
this O
end, O
the O
YouTube O
API O
is O
chosen O
to O
obtain O
all O
subtitles O
of O
the O
video. O
But, O
our O
observation O
finds O
that O
most O
obtained O
subtitles O
are O
not O
complete O
sentences, O
and O
in O
some O
cases, O
a O

complete O
sentence O
can O
last O
for O
several O
continuous O
video O
fragments. O
Meanwhile, O
they O
do O
not O
contain O
any O
punctuation, O
which O
is O
a O
key O
factor O
for O
sentence O
segmentation. O
As O
shown O
in O
the O
top O
part O
of O
Figure O
1, O
we O
design O
an O
algorithm O
that O
takes O
the O
following O
steps O
to O
find O
each O
complete O
sentence O
and O
its O
corresponding O

video O
clip. O
Sentence B-TaskName
segmentation I-TaskName
. O
In O
the O
first O
step, O
we O
try O
to O
find O
complete O
sentences O
from O
the O
subtitles. O
We O
first O
concatenate O
all O
subtitles O
from O
the O
same O
video O
are O
concatenated O
into O
a O
very O
long O
sequence O
of O
tokens. O
Next, O
a O
punctuation O
restoration O
model O
1 O
(Tilk O
and O
Alumäe, O
2016) O
is O
adopted O
to O
insert O

punctuation O
into O
the O
sequence. O
Lastly, O
sentences O
are O
segmented O
based O
on O
certain O
punctuation O
(e.g., O
".", O
"?", O
"!"). O
Video B-TaskName
clip I-TaskName
extraction I-TaskName
. O
In O
the O
second O
step, O
we O
trim O
the O
corresponding O
video O
clips. O
Each O
raw O
subtitle O
contains O
its O
start O
and O
an O
end O
times. O
We O
assume O
each O
word O
within O
the O
raw O
subtitle O
occupies O
equal O

time O
and O
record O
the O
start O
and O
end O
times O
for O
1 O
We O
manually O
punctuate O
subtitles O
from O
10 O
videos O
randomly O
selected O
from O
HowTo100M B-DatasetName
, O
which O
contains O
461 O
sentences O
after O
annotation. O
The O
punctuation B-MethodName
restoration I-MethodName
model I-MethodName
has O
an O
overall O
F1 B-MetricName
score O
of O
74.1% B-MetricValue
with O
the O
manual O
labels. O
each O
word. O
After O
that, O
given O
a O
complete O

sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
..., O
w O
N O
, O
we O
use O
the O
start O
time O
of O
its O
first O
word O
w O
1 O
and O
the O
end O
time O
of O
its O
last O
word O
w O
N O
as O
the O
start O
and O
end O
times O
of O
σ. O
Lastly, O
we O
segment O
a O
complete O
sentence O
σ's O
corresponding O

video O
clip O
v O
based O
on O
its O
start O
and O
end O
times. O
Model: O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
After O
harvesting O
large-scale O
sentence O
and O
video O
pairs, O
the O
next O
step O
is O
to O
build O
a O
strong O
grammar O
induction O
model O
that O
can O
benefit O
from O
them. O
In O
this O
section, O
we O
introduce O
our O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
PTC-PCFG B-MethodName
) O
model O

for O
unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
As O
shown O
in O
the O
lower O
part O
of O
Figure O
1, O
the O
PTC-PCFG B-MethodName
model O
composes O
of O
a O
video O
encoder, O
a O
span O
encoder O
and O
a O
parsing O
model. O
Both O
the O
video O
encoder O
and O
the O
span O
encoder O
are O
initialized O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020), O
a O
pre-trained O
video-text O

matching O
model O
that O
takes O
a O
simple O
design O
and O
has O
shown O
superior O
zero-shot O
results O
on O
many O
video O
understanding O
tasks, O
such O
as O
video O
retrieval, O
video O
question O
answering, O
etc. O
We O
first O
introduce O
the O
pre-trained O
video O
and O
span O
encoders, O
before O
covering O
the O
training O
and O
inference O
details O
of O
PTC-PCFG B-MethodName
. O
Video O
encoding. O
The O
first O
step O

is O
to O
encode O
a O
video O
v O
to O
its O
representation O
v. O
To O
do O
this, O
we O
first O
segment O
v O
into O
small O
video O
clips, O
where O
each O
video O
clip O
v O
i O
consists O
of O
T O
frames. O
Following O
Zhang O
et O
al. O
(2021), O
we O
sample O
L O
video O
clips O
with O
equal O
interval O
for O
efficiency. O
We O
use O
the O
video O

encoder O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020) O
as O
our O
video O
encoder O
and O
only O
fine-tune O
its O
last O
fully O
connected O
layer O
f O
v O
for O
efficiency. O
In O
more O
detail, O
for O
each O
sampled O
video O
clip, O
we O
pre-compute O
the O
input O
of O
f O
v O
as O
its O
representation, O
denoted O
as O
{h O
v O
i O
} O
L O

i=1 O
. O
Then O
we O
feed O
them O
into O
f O
v O
and O
average O
the O
output O
as O
its O
representation O
v, O
denoted O
as, O
v O
= O
AvgPool({f O
v O
(h O
v O
i O
)} O
L O
i=1 O
),(10) O
where O
AvgPool O
indicates O
average O
pooling. O
Span O
encoding. O
The O
next O
step O
is O
to O
compute O
a O
span O
representation O
c O
for O
each O
particular O

span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
(1 O
≤ O
i O
< O
j O
≤ O
N O
) O
in O
sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
N O
. O
The O
pre-trained O
text O
encoder O
of O
MIL-NCE B-MethodName
consists O
of O
a O
word O
embedding O
layer O
and O

two O
stacked O
fully O
connected O
layers, O
f O
c O
0 O
and O
f O
c O
1 O
. O
Motivated O
by O
Zhao O
and O
Titov O
(2020); O
Zhang O
et O
al. O
(2021), O
we O
expect O
to O
learn O
|N O
| O
different O
span O
representations, O
each O
is O
specified O
for O
one O
non-terminal O
node. O
However, O
directly O
applying O
the O
pre-trained O
text O
encoder O
is O
not O
feasible, O
since O

it O
has O
only O
one O
output O
layer O
f O
c O
1 O
. O
Therefore, O
we O
duplicate O
f O
c O
1 O
for O
|N O
| O
times, O
denoted O
as O
{f O
c O
k O
} O
|N O
| O
k=1 O
, O
and O
compose O
|N O
| O
label-specific O
output O
layers. O
In O
more O
detail, O
we O
first O
encode O
each O
word O
w O
i O
with O
the O
word O

embedding O
layer, O
denoted O
as O
h O
c O
i O
. O
Then O
we O
feed O
the O
word O
embeddings O
to O
f O
c O
0 O
, O
ReLU, O
maximum O
pooling O
and O
each O
label-specific O
output O
layer O
sequentially. O
we O
also O
compute O
the O
probabilities O
of O
its O
phrasal O
labels O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|}, O
as O
illustrated O
in O
Section O
2.1. O
Lastly, O

the O
span O
representation O
c O
is O
the O
sum O
of O
all O
label-specific O
span O
representations O
weighted O
by O
the O
probabilities O
we O
predicted, O
denoted O
as: O
τ O
= O
MaxPool(ReLU(f O
c O
0 O
(h O
c O
i O
))) O
c O
= O
|N O
| O
k=1 O
p(k|c, O
σ)f O
c O
k O
(τ O
),(11) O
where O
MaxPool O
is O
a O
maximum O
pooling O
operation O
and O
ReLU O
is O

a O
ReLU O
activation O
function. O
Training. O
As O
shown O
in O
lower O
left O
of O
Figure O
1, O
we O
optimize O
both O
the O
video-text O
matching O
loss O
and O
evidence O
lower O
bound O
during O
training. O
We O
first O
compute O
the O
similarity O
between O
a O
video O
clip O
v O
and O
a O
particular O
span O
c O
via O
dot O
product O
and O
then O
compute O
a O
triplet B-MetricName
hinge I-MetricName

loss I-MetricName
as O
following, O
h(v, O
c) O
= O
E O
c O
′ O
[c O
′ O
• O
v O
− O
c O
• O
v O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[c O
• O
v O
′ O
− O
c O
• O
v O
+ O
ϵ] O
+ O
, O
(12 O
) O
where O
ϵ O
is O
a O
positive O
margin, O
[•] O
+ O
= O
max(0, O
•), O
v O

′ O
is O
a O
clip O
from O
a O
different O
video O
and O
c O
′ O
is O
a O
span O
from O
a O
different O
sentence. O
The O
video-text B-MetricName
matching I-MetricName
loss I-MetricName
is O
correspondingly O
defined O
as, O
s(v, O
σ) O
= O
Σ O
c∈σ O
p(c|σ)h(v, O
c),(13) O
where O
p(c|σ) O
is O
the O
probability O
of O
a O
particular O
span O
c O
being O
a O
syntactic O
phrase. O
Finally, O
the O
overall O

loss O
function O
is O
composed O
by O
the O
ELBO O
and O
the O
videotext B-MetricName
matching I-MetricName
loss I-MetricName
: O
L(ϕ, O
θ) O
= O
(v,σ)∈Ω O
−ELBO(σ; O
ϕ, O
θ) O
+ O
αs(v, O
σ),(14) O
where O
α O
is O
a O
constant O
balancing O
these O
two O
terms. O
Inference. O
During O
inference, O
given O
a O
sentence O
σ, O
we O
predict O
the O
most O
likely O
tree O
t O
* O
without O
accessing O
videos, O

as O
shown O
in O
the O
lower O
right O
of O
Figure O
1. O
Since O
computing O
the O
integral O
over O
z O
is O
intractable, O
we O
estimate O
t O
* O
with O
the O
following O
approximation, O
t O
* O
= O
arg O
max O
t O
z O
p O
θ O
(t|z)p O
θ O
(z|σ)dz O
≈ O
arg O
max O
t O
p O
θ O
(t|σ, O
µ O
ϕ O
(σ)),(15) O
where O
µ O
ϕ O

(σ) O
is O
the O
mean O
vector O
of O
the O
variational O
posterior O
q O
ϕ O
(z|σ), O
and O
t O
* O
is O
obtained O
by O
the O
CYK O
algo. O
(Cocke, O
1969;Younger, O
1967;Kasami, O
1966). O
Evaluation O
We O
discard O
punctuation, O
lowercase O
all O
words, O
replace O
numbers O
with O
a O
special O
token O
and O
ignore O
trivial O
single-word O
and O
sentence-level O
spans O
during O
testing O
following O
Kim O
et O

al. O
(2019a). O
Besides, O
we O
follow O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
using O
a O
state-of-the-art O
constituency O
parser O
(Benepar O
Kitaev O
et O
al. O
2019) O
to O
obtain O
the O
reference O
trees O
for O
evaluation O
2 O
. O
Following O
Shi O
et O
al. O
(2020); O
Zhang O
et O
al. O
(2021), O
all O
models O
are O
run O
5 O
times O
for O

1 O
epoch O
with O
different O
random O
seeds. O
For O
each O
model, O
we O
report O
the O
averaged B-MetricName
sentence-level I-MetricName
F1 I-MetricName
( O
S-F1 B-MetricName
) O
and O
corpus-level B-MetricName
F1 I-MetricName
( O
C-F1 B-MetricName
) O
of O
its O
runs O
on O
each O
testing O
set. O
Implementation O
Details O
We O
use O
Spacy O
3 O
for O
tokenization O
and O
keep O
sentences O
with O
fewer O
than O
40 O
words O
for O
training O
due O

to O
the O
limited O
computational O
resources. O
Each O
video O
is O
decoded O
at O
16 B-HyperparameterValue
fps B-HyperparameterName
and O
L B-HyperparameterName
= O
8 B-HyperparameterValue
video B-HyperparameterName
clips I-HyperparameterName
are O
sampled O
in O
total, O
where O
each O
clip O
contains O
T B-HyperparameterName
= O
16 B-HyperparameterValue
frames B-HyperparameterName
. O
We O
train O
baseline O
models, O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
with O
the O
same O
hyper-parameters O
suggested O
by O
Kim O
et O
al. O
(2019a) O
and O
Zhang O

et O
al. O
(2021). O
The O
parsing O
model O
of O
PTC-PCFG B-MethodName
has O
the O
same O
hyperparameter O
setting O
as O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
(Please O
refer O
their O
papers O
for O
details). O
The O
constant O
α B-HyperparameterName
is O
set O
to O
1 B-HyperparameterValue
. O
We O
select O
the O
top O
20 O
000 O
most O
common O
words O
in O
HowTo100M B-DatasetName
as O
vocabulary O
for O
all O
datasets. O
All O
baseline O
methods O

and O
ours O
are O
optimized O
by O
Adam O
(Kingma O
and O
Ba, O
2015) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
, O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.75 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O
All O
parameters O
(except O
the O
video-text O
matching O
model O
in O
PTC-PCFG B-MethodName
) O
are O
initialized O
with O
Xavier O
uniform O
initializer O
(Glorot O
and O
Bengio, O
2010). O
All O
our O
models O

in O
experiments O
are O
trained O
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
without O
finetuning O
on O
the O
2 O
For O
each O
dataset, O
we O
randomly O
select O
50 O
sentences O
and O
manually O
label O
their O
constituency O
parse O
trees. O
Benepar O
has O
S-F1 B-MetricName
scores O
of O
98.1% B-MetricValue
( O
DiDeMo B-DatasetName
), O
97.2% B-MetricValue
( O
YouCook2 B-DatasetName
) O
and O
98.1% B-MetricValue
( O
MSRVTT B-DatasetName

) O
with O
manual O
labels. O
3 O
https://spacy.io/ O
target O
dataset. O
Main O
Results O
Figure O
Cross-dataset O
Evaluation O
We O
evaluate O
the O
robustness O
of O
models O
across O
different O
datasets, O
as O
shown O
in O
Table O
1. O
Comparing O
MMC-PCFG B-MethodName
trained O
on O
in-domain O
datasets O
(Row O
1-3), O
we O
can O
observe O
that O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
achieves O
the O
best O
overall O
performance, O
while O
MMC-PCFG B-MethodName

trained O
on O
YouCook2 B-DatasetName
is O
the O
worst. O
We O
believe O
this O
is O
due O
to O
the O
different O
number O
of O
training O
instances O
5 O
and O
the O
domain O
gap O
between O
different O
datasets. O
Comparing O
Rows O
1-4, O
we O
can O
observe O
that O
the O
MMC-PCFG B-MethodName
model O
trained O
on O
HT(592k) B-DatasetName
(Row O
4) O
is O
the O
best O
or O
the O
second O
place O
regarding O
C-F1 B-MetricName

and O
S-F1 B-MetricName
compared O
with O
its O
variants O
trained O
on O
in-domain O
datasets O
(Rows O
1-3). O
This O
demonstrates O
that O
the O
our O
processed O
videotext O
training O
instances O
are O
abundant, O
rich O
in O
content O
and O
can O
serve O
for O
general O
purpose. O
Comparing O
Rows O
4 O
and O
5, O
PTC-PCFG B-MethodName
outperforms O
MMC-PCFG B-MethodName
in O
both O
C-F1 B-MetricName
and O
S-F1 B-MetricName
in O
all O
three O
datasets O
and O

has O
smaller O
variance. O
This O
demonstrate O
that O
our O
model O
can O
leverage O
pre-trained O
video-text O
matching O
knowledge O
and O
learn O
consistent O
grammar O
induction. O
Effectiveness O
of O
Pre-Training O
In O
this O
section, O
we O
explore O
how O
different O
pretrained O
video O
and O
text O
encoders O
can O
affect O
the O
parsing O
performance, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
In O
particular, O
we O

study O
different O
(Zhang O
et O
al., O
2021;Zhao O
and O
Titov, O
2020), O
a O
pre-trained O
TinyBERT O
(Jiao O
et O
al., O
2020) O
model, O
the O
text O
encoder O
from O
MIL-NCE B-MethodName
(Miech O
et O
al., O
2020), O
and O
the O
text O
encoder O
from O
CLIP O
(Radford O
et O
al., O
2021). O
Comparing O
Rows O
1 O
with O
2, O
we O
can O
observe O
that O
MM O
is O
better O
than O
the O

video O
encoder O
of O
MIL-NCE B-MethodName
regarding O
C-F1 B-MetricName
and O
S-F1 B-MetricName
on O
all O
three O
datasets, O
as O
MM O
provides O
more O
comprehensive O
video O
features. O
By O
comparing O
row O
1 O
with O
3, O
we O
can O
also O
observe O
that O
TinyBERT B-MethodName
, O
which O
is O
distilled O
from O
BERT O
(Devlin O
et O
al., O
2019), O
outperforms O
the O
randomly O
initialized O
LSTM O
encoder. O
However, O
both O
MM B-MethodName

and O
TinyBERT B-MethodName
are O
independently O
trained O
only O
on O
vision O
or O
language O
tasks, O
where O
the O
vision-language O
correspondences O
are O
not O
considered O
during O
pretraining. O
Therefore, O
we O
further O
investigate O
the O
encoders O
jointly O
pre-trained O
on O
large O
scale O
multimedia O
datasets, O
including O
the O
video-text O
matching O
model O
MIL-NCE B-MethodName
(Row O
4) O
and O
the O
image-text O
matching O
model O
CLIP O
(Row O
5). O
We O

can O
observe O
that O
by O
leveraging O
both O
video O
and O
text O
encoders O
in O
MIL-NCE B-MethodName
can O
improve O
the O
parsing O
performance O
by O
a O
large O
margin O
on O
all O
three O
datasets. O
On O
the O
other O
hand, O
CLIP O
does O
not O
perform O
well, O
since O
it O
is O
designed O
for O
static O
images O
and O
other O
multi-modal O
information O
(e.g., O
motion) O
is O
ignored. O
Qualitative O

Analysis O
In O
figure O
5, O
we O
visualize O
a O
parser O
tree O
predicted O
by O
the O
best O
run O
of O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
and O
PTC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
, O
as O
well O
as O
its O
reference O
tree. O
We O
can O
observe O
that O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
fails O

at O
noun O
phrase O
"a O
lady", O
while O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
succeeds. O
MMC-PCFG B-MethodName
can O
be O
further O
improved O
by O
training O
on O
HT(296k) B-DatasetName
, O
however, O
fails O
at O
noun O
phrase O
"the O
groceries O
she O
had O
kept O
in O
her O
refrigerator". O
Our O
PTC-PCFG B-MethodName
can O
leverage O
the O
pretrained O
matching O
knowledge O
and O
make O
the O
correct O
prediction. O

Learning O
to O
Adapt O
to O
Low-Resource B-TaskName
Paraphrase I-TaskName
Generation I-TaskName
Paraphrase B-TaskName
generation I-TaskName
is O
a O
longstanding O
NLP O
task O
and O
achieves O
great O
success O
with O
the O
aid O
of O
large O
corpora. O
However O
, O
transferring O
a O
paraphrasing O
model O
to O
another O
domain O
encounters O
the O
problem O
of O
domain O
shifting O
especially O
when O
the O
data O
is O
sparse. O
At O
the O
same O
time O

, O
widely O
using O
large O
pre-trained O
language O
models O
( O
PLMs O
) O
faces O
the O
overfitting O
problem O
when O
training O
on O
scarce O
labeled O
data. O
To O
mitigate O
these O
two O
issues O
, O
we O
propose O
, O
LAPA B-MethodName
, O
an O
effective O
adapter O
for O
PLMs O
optimized O
by O
meta-learning. O
LAPA B-MethodName
has O
three-stage O
training O
on O
three O
types O
of O
related O
resources O
to O

solve O
this O
problem O
: O
1. O
pre-training O
PLMs O
on O
unsupervised O
corpora O
, O
2. O
inserting O
an O
adapter O
layer O
and O
meta-training O
on O
source O
domain O
labeled O
data O
, O
and O
3. O
fine-tuning O
adapters O
on O
a O
small O
amount O
of O
target O
domain O
labeled O
data. O
This O
method O
enables O
paraphrase O
generation O
models O
to O
learn O
basic O
language O
knowledge O
first O
, O

then O
learn O
the O
paraphrasing O
task O
itself O
later O
, O
and O
finally O
adapt O
to O
the O
target O
task. O
Our O
experimental O
results O
demonstrate O
that O
LAPA B-MethodName
achieves O
state-of-the-art O
in O
supervised O
, O
unsupervised O
, O
and O
low-resource O
settings O
on O
three O
benchmark O
datasets. O
With O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
labeled O
data O
of O
the O
target O

task O
, O
our O
approach O
can O
achieve O
a O
competitive O
performance O
with O
previous O
work O
. O
Introduction O
Paraphrase B-TaskName
generation I-TaskName
can O
comprehend O
a O
sentence O
and O
generate O
another O
with O
the O
same O
semantics O
but O
with O
variations O
in O
lexicon O
or O
syntax O
, O
which O
has O
various O
applications O
on O
downstream O
tasks O
including O
query O
rewriting O
( O
Dong O
et O
al. O
, O

2017 O
) O
, O
data O
augmentation O
( O
Iyyer O
et O
al. O
, O
2018 O
) O
and O
language O
model O
pre-training O
( O
Lewis O
et O
al. O
, O
2020a O
) O
. O
Conventional O
approaches O
( O
Prakash O
et O
al. O
, O
2016 O
; O
Chowdhury O
et O
al. O
, O
2022 O
) O
model O
the O
paraphrase B-TaskName
generation I-TaskName
as O
a O
supervised O
encoding-decoding O
problem O
, O
inspired O

by O
machine O
translation O
systems. O
However O
, O
the O
success O
of O
these O
methods O
often O
relies O
on O
a O
large O
number O
of O
parallel O
paraphrases O
, O
whose O
collection O
is O
timeconsuming O
and O
requires O
a O
lot O
of O
domain O
knowledge. O
Therefore O
, O
in O
real O
scenarios O
with O
a O
small O
amount O
of O
parallel O
data O
, O
the O
model O
suffers O
from O
performance O

drops O
facing O
domain O
gaps. O
This O
phenomenon O
, O
known O
as O
domain O
shift O
problem O
( O
Pan O
and O
Yang O
, O
2009 O
) O
, O
comes O
from O
the O
representation O
gap O
between O
training O
and O
testing O
domains O
with O
different O
writing O
styles O
or O
forms O
. O
To O
tackle O
this O
problem O
, O
unsupervised O
methods O
such O
as O
editing-based O
approaches O
( O
Bowman O

et O
al. O
, O
2016 O
; O
Miao O
et O
al. O
, O
2019 O
) O
or O
reinforcement O
learning O
( O
Li O
et O
al. O
, O
2018 O
; O
Siddique O
et O
al. O
, O
2020 O
) O
, O
and O
weakly-supervised O
methods O
such O
as O
retrievalenhanced O
( O
Ding O
et O
al. O
, O
2021 O
; O
Yin O
et O
al. O
, O
2022 O
) O
or O
prompt-based O
do O

not O
introduce O
or O
only O
introduce O
a O
small O
number O
of O
supervised O
signals O
, O
which O
limits O
their O
performance O
such O
that O
underperforms O
supervised O
methods. O
In O
fact O
, O
largescale O
unlabeled O
corpus O
data O
( O
UCD O
) O
and O
labeled O
source O
domain O
data O
( O
LSDD O
) O
, O
as O
well O
as O
a O
few O
labeled O
target O
domain O
data O
( O

LTDD O
) O
, O
can O
be O
easily O
achieved. O
Therefore O
, O
we O
propose O
a O
new O
three-stage O
learning O
paradigm O
: O
pre-training O
, O
meta-learning O
, O
and O
fine-tuning O
, O
aiming O
to O
leverage O
the O
pre-trained O
knowledge O
on O
UCD O
, O
source O
domain O
knowledge O
on O
LSDD O
, O
and O
adapt O
to O
target O
domain O
on O
LSDD O
to O
improve O
the O
performance O

of O
low-resource O
paraphrase O
generation. O
In O
order O
to O
successfully O
implement O
this O
learning O
paradigm O
, O
we O
propose O
a O
simple O
yet O
effective O
model O
which O
combined O
pre-trained O
language O
model O
( O
PLM O
) O
and O
MAML O
( O
Finn O
et O
al. O
, O
2017 O
) O
, O
named O
Learning O
to O
Adapt O
to O
low-resource B-MethodName
PAraphrase I-MethodName
generation I-MethodName
( O
LAPA B-MethodName
) O
. O

Specifically O
, O
before O
meta-learning O
, O
we O
insert O
an O
adapter O
layer O
into O
each O
transformer O
layer O
of O
PLM. O
An O
adapter O
layer O
is O
composed O
of O
a O
few O
parameters O
of O
feedforward O
layer O
and O
residual O
connection. O
During O
meta-training O
and O
fine-tuning O
, O
only O
the O
adapter O
layer O
and O
normalization O
layer O
are O
trainable. O
Parameter O
freezing O
and O
residual O
connection O

can O
retain O
the O
prior O
knowledge O
of O
PLM O
to O
avoid O
negative O
transfer O
effects. O
Smaller-scale O
parameter O
updating O
can O
prevent O
MAML O
from O
gradient O
explosion O
or O
diminishing O
problems O
when O
the O
number O
of O
MAML O
inner O
loop O
iterations O
and O
model O
depth O
increase O
( O
Antoniou O
et O
al. O
, O
2019 O
) O
or O
training O
data O
is O
extremely O
scarce O
. O

Overall O
, O
we O
hold O
the O
idea O
that O
paraphrasing O
is O
a O
fundamental O
ability O
of O
human O
beings. O
The O
paraphrase O
model O
should O
not O
rely O
on O
domain O
and O
seen O
data. O
Therefore O
, O
we O
are O
committed O
to O
characterizing O
the O
basic O
ability O
of O
the O
paraphrase O
model O
, O
obtaining O
gains O
from O
each O
domain O
, O
and O
applying O
it O

to O
a O
specific O
domain. O
Our O
contributions O
are O
summarized O
as O
follows O
: O
• O
We O
define O
a O
novel O
three O
stages O
learning O
paradigm O
for O
low-resource B-TaskName
paraphrase I-TaskName
generation I-TaskName
in O
data O
scarcity O
scenarios O
. O
• O
We O
propose O
that O
LAPA B-MethodName
implement O
this O
learning O
paradigm O
, O
which O
transferred O
the O
PLM O
knowledge O
and O
source O
domain O
knowledge O
to O
complete O

the O
low-resource O
learning O
in O
the O
target O
domain O
quickly O
and O
with O
high O
quality O
. O
• O
The O
supervised O
, O
unsupervised O
and O
weakly O
supervised O
experimental O
results O
of O
LAPA B-MethodName
on O
three O
benchmark O
datasets O
achieve O
state-of-theart O
( O
SOTA O
) O
. O
LAPA B-MethodName
with O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
target O
task O
labeled O
data O

can O
achieve O
a O
competitive O
performance O
with O
previous O
works O
. O
Related O
Work O
While O
the O
paraphrase O
generation O
performance O
is O
greatly O
improved O
with O
various O
supervised O
techniques O
( O
Zhao O
et O
al. O
, O
2008 O
; O
Prakash O
et O
al. O
, O
2016 O
; O
Egonmwan O
and O
Chali O
, O
2019 O
; O
Cao O
and O
Wan O
, O
2020 O
; O
Hosking O
and O

Lapata O
, O
2021 O
; O
Chowdhury O
et O
al. O
, O
2022 O
) O
, O
there O
are O
few O
studies O
regarding O
the O
lowresource O
setting. O
West O
et O
al. O
( O
2021 O
) O
and O
Meng O
et O
al. O
( O
2021 O
) O
proposed O
novel O
unsupervised O
paraphrasing O
strategies O
by O
data O
augmentation O
based O
on O
reflective O
decoding O
or O
diverse O
decoding. O
Ding O
et O
al. O

( O
2021 O
) O
and O
Yin O
et O
al. O
( O
2022 O
) O
achieved O
improvements O
on O
various O
low-resource O
datasets O
with O
retrieved O
data O
and O
meta O
reinforcement O
learning. O
However O
, O
these O
studies O
only O
use O
a O
single O
large O
corpus O
for O
training O
the O
full O
PLM O
, O
which O
suffers O
from O
domainshifting O
problems O
( O
Wang O
et O
al. O
, O
2019 O

) O
. O
Besides O
, O
under O
the O
extreme O
low-resource O
setting O
, O
directly O
fine-tuning O
the O
full O
PLM O
will O
cause O
an O
over-fitting O
problem O
( O
Antoniou O
et O
al. O
, O
2019 O
) O
. O
Meta-learning O
helps O
improve O
low-resource O
performance O
in O
various O
recent O
studies O
, O
such O
as O
image O
classification O
( O
Soh O
et O
al. O
, O
2020 O
) O
, O

vehicle O
tracking O
and O
natural O
language O
processing O
( O
Park O
et O
al. O
, O
2021 O
; O
Chen O
and O
Shuai O
, O
2021 O
; O
Hong O
and O
Jang O
, O
2022 O
) O
. O
Finn O
et O
al. O
( O
2017 O
) O
proposed O
a O
meta O
learner O
named O
MAML B-MethodName
, O
which O
uses O
other O
example O
tasks O
to O
learn O
how O
to O
effectively O
initialize O

a O
basic O
learner O
, O
which O
can O
be O
quickly O
generalized O
to O
new O
tasks. O
Adapter O
modules O
have O
been O
mainly O
used O
for O
parameter-efficient O
and O
quick O
fine-tuning O
of O
a O
basic O
PLMs O
to O
new O
tasks O
( O
Houlsby O
et O
al. O
, O
2019 O
; O
Bapna O
and O
Firat O
, O
2019 O
; O
Pfeiffer O
et O
al. O
, O
2020Pfeiffer O
et O
al. O

, O
, O
2021. O
Our O
paper O
proposes O
to O
incorporate O
meta-learning O
approaches O
to O
realize O
multi-domain O
migration O
and O
task O
adapter O
to O
realize O
parameter O
effective O
transfer O
learning O
( O
i.e. O
, O
limited O
trainable O
parameters O
) O
to O
mitigate O
the O
above O
problems O
of O
paraphrase O
generation O
. O
3 O
The O
Approach O
Learning O
Paradigm O
As O
shown O
in O
Figure O
1 O
, O

the O
workflow O
of O
our O
learning O
paradigm O
including O
three O
stages O
: O
1. O
Backbone O
model O
pre-training O
on O
large O
unlabeled O
corpora O
2. O
Adapter O
model O
meta-training O
on O
large O
source O
corpora O
using O
the O
meta-learning O
and O
3. O
Adapter O
model O
fine-tuning O
on O
target O
corpora O
and O
evaluate O
model O
performance. O
The O
prior O
knowledge O
K O
pri O
comes O
from O
first O
two O

stages O
: O
pre-training O
and O
meta-learning. O
We O
denote O
our O
backbone O
model O
by O
f O
( O
θ O
) O
with O
parameters O
θ. O
The O
first O
stage O
is O
pretraining O
on O
unlabeled O
corpora O
D O
pre O
, O
and O
we O
get O
f O
( O
θ O
pre O
) O
. O
The O
second O
stage O
is O
meta-training O
on O
adapter O
model O
f O
[ O
θ O
pre O

, O
Φ O
] O
with O
additional O
parameters O
Φ O
and O
frozen O
θ O
pre O
on O
related O
source O
corpora O
D O
src O
, O
and O
we O
got O
f O
[ O
θ O
pre O
, O
Φ O
src O
] O
. O
Finally O
, O
we O
initialize O
the O
adapter O
model O
with O
[ O
θ O
pre O
, O
Φ O
src O
] O
and O
finetune O
Φ O
src O
on O

the O
target O
corpus O
D O
tgt O
to O
obtain O
a O
target O
model O
f O
[ O
θ O
pre O
, O
Φ O
tgt O
] O
which O
are O
model O
parameters O
after O
target O
adapter O
, O
i.e. O
, O
the O
posterior O
knowledge O
K O
por O
. O
Backbone O
Model O
Because O
PLM O
is O
equipped O
with O
prior O
knowledge O
K O
pri O
and O
exhibits O
strong O
capabilities O
in O

a O
range O
of O
different O
generative O
tasks O
, O
we O
choose O
the O
pretrained B-MethodName
BART I-MethodName
( O
Lewis O
et O
al. O
, O
2020b O
) O
as O
the O
backbone O
model O
for O
paraphrase O
generation. O
Specifically O
, O
given O
a O
labeled O
paraphrase O
pair O
i O
= O
( O
x O
, O
ŷ O
) O
, O
where O
x O
= O
[ O
x O
1 O
, O
. O
. O

. O
, O
x O
N O
] O
, O
ŷ O
= O
[ O
ŷ O
1 O
, O
. O
. O
. O
, O
ŷ O
M O
] O
, O
and O
inputting O
x O
, O
the O
model O
has O
produced O
a O
predicted O
segment O
sequence O
y O
< O
t O
= O
[ O
y O
1 O
, O
. O
. O
. O
, O
y O
t−1 O
] O
before O
time O
t O

, O
then O
the O
probability O
that O
the O
token O
generated O
at O
time O
t O
is O
y O
t O
is O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O
The O
model O
is O
optimized O
by O
minimizing O
the O
negative O
log-likelihood O
: O
L O
i O
( O
f O
( O
θ O
) O
) O
= O
− O
M O
t=1 O

log O
p O
( O
ŷ O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O
Adapter B-MethodName
Model I-MethodName
The O
adapter O
model O
is O
obtained O
by O
inserting O
the O
adapter O
layer O
into O
each O
transformer O
layer O
of O
the O
backbone O
model. O
An O
adapter O
layer O
is O
a O
bottlenecked O
feed-forward O
network O
consisting O
of O
a O
downproject O
layer O
, O
a O
nonlinearity O

function O
and O
an O
upproject O
layer. O
In O
addition O
, O
a O
skip O
connection O
layer O
from O
input O
to O
output O
prevents O
the O
noised O
initialization O
from O
interference O
with O
the O
training O
initially. O
For O
the O
adapter O
in O
layer O
l O
, O
the O
function O
can O
be O
formulated O
as O
: O
Adapter O
( O
z O
l O
) O
= O
W O
l O
u O
ReLU O

( O
W O
l O
d O
z O
l O
) O
+ O
z O
l O
where O
z O
l O
represents O
the O
inputs O
of O
the O
adapter O
in O
layer O
l. O
Besides O
, O
the O
normalization O
layers O
are O
trainable O
and O
initialized O
from O
the O
previous O
training O
stage O
. O
Meta-Learning O
The O
second O
stage O
is O
adapter O
model O
meta O
traning O
based O
on O
MAML B-MethodName
( O

Finn O
et O
al. O
, O
2017 O
) O
. O
The O
learning O
process O
is O
shown O
in O
Algorithm O
1. O
First O
, O
we O
freeze O
the O
backbone O
model O
parameters O
θ O
pre O
that O
have O
been O
pre-trained O
in O
the O
pre-training O
stage O
, O
then O
, O
add O
new O
adapters O
with O
parameters O
Φ O
to O
get O
adapter O
model O
f O
[ O
θ O
pre O

, O
Φ O
] O
. O
Based O
on O
Algorithm O
1 O
, O
we O
first O
complete O
the O
meta-learning O
of O
the O
adapter O
model O
on O
the O
source O
corpus O
D O
src O
to O
help O
the O
adapters O
Φ O
find O
the O
initialization O
parameters O
Φ O
src O
suitable O
for O
paraphrase O
generation O
to O
adapt O
faster O
target O
task. O
At O
this O
Compute O
adapted O
parameters O
with O

gradient O
descent O
: O
[ O
θ O
, O
Φ O
] O
= O
[ O
θ O
, O
Φ O
] O
− O
α∇ O
Φ O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O
) O
8 O
: O
end O
for O
9 O
: O
Update O
[ O
θ O
, O
Φ O
] O
← O
[ O
θ O
, O
Φ O
] O
− O
β∇ O
Φ O
T O
i O

∼p O
( O
T O
) O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O
Datasets O
We O
conducted O
experiments O
on O
Quora B-DatasetName
1 I-DatasetName
, O
Twitter B-DatasetName
( O
Lan O
et O
al. O
, O
2017 O
) O
and O
MSCOCO B-DatasetName
( O
Lin O
et O
al. O
, O
2014 O
) O
benchmark O
datasets O
, O
and O
followed O
the O
same O
setting O
in O
previous O
works O
( O

Lin O
et O
al. O
, O
2014 O
; O
Liu O
et O
al. O
, O
2020 O
; O
Ding O
et O
al. O
, O
2021 O
) O
. O
For O
meta-learning O
, O
we O
choose O
a O
different O
source O
task O
's O
labeld O
train-set O
from O
the O
target O
task O
to O
randomly O
construct O
meta O
tasks. O
Appendix O
Table O
4 O
describes O
more O
details O
. O
Baselines O
Supervised O
methods O

are O
trained O
with O
all O
parallel O
sentences O
of O
target O
task. O
Unsupervised O
baselines O
( O
Lewis O
et O
al. O
, O
2020b O
) O
. O
Like O
our O
work O
, O
they O
all O
used O
BART B-MethodName
as O
PLM. O
To O
compare O
the O
performance O
of O
our O
method O
against O
the O
previous O
works O
, O
we O
use O
BLEU B-MetricName
( O
Papineni O
et O
al. O
, O
2002 O

) O
, O
iBLEU B-MetricName
( O
Sun O
and O
Zhou O
, O
2012 O
) O
and O
ROUGE B-MetricName
( O
Hovy O
et O
al. O
, O
2006 O
) O
metrics. O
All O
metrics O
are O
computed O
between O
the O
generated O
and O
the O
reference O
paraphrases O
in O
the O
test O
set O
( O
Kumar O
et O
al. O
, O
2020 O
) O
. O
We O
also O
separately O
analyze O
the O
impact O
of O

target O
task O
Example O
Input O
Can O
we O
ever O
store O
energy O
produced O
in O
lightning O
? O
Experimental O
Results O
How O
does O
a O
pencil O
and O
a O
liquid O
eyeliner O
differ O
? O
How O
come O
there O
's O
no O
physical O
evidence O
for O
sea O
dragons O
existing O
if O
they O
're O
the O
largestanimal O
in O
the O
sea. O
Table O
2 O
: O
Examples O
of O
the O

generated O
paraphrases O
on O
Quora B-DatasetName
dataset. O
We O
highlight O
the O
key O
phrases O
in O
the O
paraphrases O
generated O
and O
use O
wavy O
underline O
to O
show O
the O
matched O
parts O
between O
LAPA B-MethodName
and O
reference O
. O
labeled O
data O
scale O
under O
low-resource O
setting. O
Figure O
2 O
shows O
the O
experimental O
results O
on O
the O
Quora B-DatasetName
dataset. O
It O
can O
be O
conclused O
that O
LAPA B-MethodName

has O
a O
significant O
effect O
compared O
with O
BART B-MethodName
under O
the O
same O
small O
data O
size. O
LAPA B-MethodName
can O
achieve O
the O
effect O
of O
89 B-MetricValue
% I-MetricValue
to O
93 B-MetricValue
% I-MetricValue
of O
the O
full O
amount O
of O
data O
when O
not O
using O
any O
target O
task O
labeled O
data O
; O
when O
using O
a O
very O
small O
amount O
of O
data O
such O
as O
0.5k O

( O
i.e O
0.5 O
% O
of O
the O
full O
data O
) O
, O
it O
can O
be O
improved O
to O
94 B-MetricValue
% I-MetricValue
to O
96 B-MetricValue
% I-MetricValue
; O
when O
the O
amount O
of O
data O
increases O
to O
10k O
( O
i.e O
10 O
% O
of O
the O
full O
data O
) O
, O
the O
performance O
is O
almost O
the O
same O
as O
the O
full O
amount O
of O

data O
100k. O
It O
should O
be O
pointed O
out O
that O
which O
dataset O
is O
selected O
as O
the O
source O
data O
can O
not O
have O
a O
substantial O
impact O
on O
the O
migration O
results O
, O
as O
shown O
in O
Figure O
3. O
The O
results O
independent O
of O
the O
source O
dataset O
prove O
that O
LAPA B-MethodName
can O
learn O
the O
paraphrasing O
task O
itself O
on O
any O

dataset O
, O
so O
it O
has O
strong O
adaptability O
to O
the O
target O
task. O
We O
conduct O
an O
ablation O
study O
with O
three O
variants O
under O
the O
low-resource O
setting O
of O
the O
Quora B-DatasetName
dataset O
to O
investigate O
the O
contribution O
of O
each O
component O
in O
the O
proposed O
method. O
The O
experimental O
results O
are O
shown O
in O
Table O
3. O
We O
can O
get O
: O

first O
, O
using O
pre-trained B-MethodName
BART I-MethodName
can O
get O
good O
results O
; O
second O
, O
by O
adding O
the O
source O
task O
dataset O
for O
pre-trained B-MethodName
BART I-MethodName
, O
the O
knowledge O
of O
the O
source O
domain O
can O
be O
effectively O
learned O
, O
thereby O
improving O
the O
performance O
of O
the O
model O
in O
the O
target O
domain O
; O
third O
, O
adding O
our O
proposed O

meta-learning O
framework O
can O
again O
effectively O
improve O
the O
speed O
and O
quality O
of O
learning O
the O
source O
domain O
( O
LAPA B-MethodName
only O
has O
2.8 O
% O
training O
parameters O
compared O
with O
BART B-MethodName
) O
and O
achieve O
the O
best O
performance O
. O
Case O
Study O
Table O
2 O
lists O
some O
paraphrases O
generated O
by O
LAPA B-MethodName
and O
BART B-MethodName
with O
different O
experimental O
settings. O
We O

can O
observe O
that O
paraphrases O
produced O
by O
LAPA B-MethodName
are O
not O
only O
grammatically O
correct O
but O
preserve O
the O
semantics O
of O
Input O
more O
completely O
, O
and O
the O
expression O
is O
closer O
to O
Reference O
than O
the O
other O
methods. O
This O
benefits O
from O
the O
fact O
that O
our O
LAPA B-MethodName
approach O
can O
make O
full O
use O
of O
source O
domain O
data O
and O

task O
features O
, O
and O
better O
preserve O
the O
prior O
knowledge O
of O
PLM O
, O
so O
as O
to O
adapt O
to O
new O
target O
tasks O
quickly O
and O
efficiently O
. O
Conclusion O
In O
this O
work O
, O
we O
investigate O
the O
problem O
of O
paraphrase B-TaskName
generation I-TaskName
under I-TaskName
the I-TaskName
low-resource I-TaskName
setting I-TaskName
and O
propose O
a O
simple O
yet O
effective O
approach O
LAPA. B-MethodName
We O
effectively O

combine O
transfer O
learning O
and O
meta-learning O
by O
using O
adapter O
modules O
as O
the O
bridge. O
Whether O
in O
supervised O
, O
unsupervised O
or O
low-resource O
setting O
, O
the O
results O
that O
our O
approach O
achieves O
the O
SOTA O
results O
on O
benchmark O
datasets. O
In O
the O
future O
, O
we O
plan O
to O
explore O
how O
to O
choose O
a O
smaller O
but O
suitable O
high-quality O
source O

corpus O
for O
learning O
in O
the O
source O
domain O
to O
improve O
the O
effect O
of O
transferring O
to O
the O
target O
domain O
, O
because O
not O
all O
source O
domain O
data O
has O
a O
positive O
effect. O
Second O
, O
we O
plan O
to O
extend O
this O
framework O
to O
other O
AI O
fields O
to O
solve O
low-resource O
problems O
in O
other O
scenarios O
and O
enable O
more O

industrial O
applications O
. O
Limitations O
The O
major O
limitation O
of O
present O
study O
is O
the O
need O
for O
source O
domain O
annotated O
data O
that O
can O
adapt O
to O
the O
target O
domain. O
Because O
this O
is O
the O
source O
of O
data O
for O
the O
knowledge O
of O
the O
learning O
task O
itself O
, O
it O
can O
not O
be O
avoided. O
In O
the O
real O
world O

, O
we O
can O
find O
it O
from O
public O
free O
datasets O
, O
exchange O
it O
commercially O
with O
other O
institutions O
, O
or O
annotate O
a O
batch O
of O
raw O
data O
ourselves O
as O
a O
cold O
start O
to O
solve O
this O
problem. O
Secondly O
, O
this O
study O
also O
has O
insufficient O
research O
on O
related O
variables. O
Due O
to O
the O
limitation O
of O
time O

and O
article O
length O
, O
we O
have O
not O
been O
able O
to O
study. O
These O
findings O
provide O
the O
following O
insights O
for O
future O
research O
: O
What O
is O
the O
lower O
bound O
of O
the O
amount O
of O
source O
domain O
data O
that O
can O
be O
well O
adapted O
to O
the O
target O
task O
? O
Whether O
we O
can O
apply O
weak O
supervision O
, O

data O
augmentation O
and O
other O
methods O
to O
create O
source O
domain O
data O
? O
How O
to O
select O
high-quality O
source O
domain O
data O
to O
get O
a O
better O
adapter O
model O
? O
We O
leave O
these O
questions O
to O
future O
research. O
Twitter O
The O
twitter O
URL O
paraphrasing O
corpus O
is O
built O
by O
Lan O
et O
al. O
( O
2017 O
) O
for O
paraphrase O
identification. O

We O
follow O
the O
setting O
in O
Li O
et O
al. O
( O
2018 O
) O
, O
Kazemnejad O
et O
al. O
( O
2020 O
) O
and O
Siddique O
et O
al. O
( O
2020 O
) O
. O
The O
detailed O
dataset O
statistics O
are O
summarized O
in O
Table O
4 O
. O
A.2 O
Evaluation O
Details O
To O
make O
a O
fair O
and O
comprehensive O
assessment O
, O
we O
follow O
the O

same O
experiment O
setting O
of O
each O
comparison O
work O
( O
Li O
et O
al. O
, O
2018 O
; O
Liu O
et O
al. O
, O
2020 O
; O
Ding O
et O
al. O
, O
2021 O
) O
and O
conduct O
the O
comparison O
respectively. O
For O
data O
preprocessing O
, O
all O
the O
sentences O
are O
lower O
cased O
, O
and O
truncate O
all O
sentences O
to O
up O
to O
20 O

words. O
< O
s O
> O
and O
< O
/ O
s O
> O
are O
spliced O
to O
the O
front O
and O
back O
end O
of O
the O
sentence O
as O
start O
and O
end O
markers O
. O
For O
evaluation O
metrics O
, O
we O
use O
BLEU B-MetricName
, O
i-BLEU B-MetricName
and O
ROUGE B-MetricName
that O
have O
been O
widely O
used O
in O
the O
previous O
work O
to O
measure O
the O
quality O

of O
the O
paraphrases. O
The O
i-BLUE B-MetricName
aims O
to O
measure O
the O
diversity O
of O
expression O
in O
the O
generated O
paraphrases O
by O
penalizing O
copying O
words O
from O
input O
sentences. O
Specifically O
, O
we O
follow O
the O
unsupervised O
paraphrase O
generation O
baselines O
and O
set O
the O
balancing B-HyperparameterName
parameter I-HyperparameterName
α B-HyperparameterName
= O
0.9 B-HyperparameterName
. O
A.3 O
Implementation O
Our O
experiments O
were O
conducted O
with O
PyToch O
on O

NVIDIA O
Tesla O
V100 O
16GB O
GPU. O
Following O
the O
comparison O
methods O
, O
we O
used O
BART-large B-MethodName
as O
the O
pre-trained O
language O
model O
and O
use O
its O
pre-trained O
parameters. O
For O
adapter O
modules O
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
is O
128. B-HyperparameterValue
For O
meta-training O
, O
unless O
otherwise O
specified O
, O
a O
meta O
batch O
includes O
3 B-HyperparameterValue
tasks B-HyperparameterName
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName

of O
each O
task O
is O
10. B-HyperparameterValue
Both O
basic O
learners O
and O
meta O
learners O
use O
the O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
optimizer O
for O
optimization O
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
by O
grid O
search O
in O
1e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
1e-6 B-HyperparameterValue
and O
5e-6. B-HyperparameterValue
The O
internal B-HyperparameterName
gradient I-HyperparameterName
step I-HyperparameterName
size I-HyperparameterName
is O
4 B-HyperparameterValue
, O

and O
the O
whole O
model O
has O
enough O
step O
size O
for O
training. O
For O
meta O
verification O
, O
we O
use O
a O
corpus O
excluded O
from O
the O
source O
task O
and O
the O
target O
task. O
For O
fine-tuning O
, O
we O
use O
validation O
set O
to O
select O
the O
best O
model O
for O
metrics O
calculation O
. O

Crossmodal-3600 B-DatasetName
: O
A O
Massively O
Multilingual O
Multimodal O
Evaluation O
Dataset O
Research O
in O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
has O
been O
severely O
hampered O
by O
a O
lack O
of O
high-quality O
evaluation O
datasets. O
In O
this O
paper O
we O
present O
the O
Crossmodal-3600 B-DatasetName
dataset I-DatasetName
( O
XM3600 B-DatasetName
in O
short O
) O
, O
a O
geographically-diverse O
set O
of O
3600 O
images O
annotated O
with O
humangenerated O
reference O

captions O
in O
36 O
languages. O
The O
images O
were O
selected O
from O
across O
the O
world O
, O
covering O
regions O
where O
the O
36 O
languages O
are O
spoken O
, O
and O
annotated O
with O
captions O
that O
achieve O
consistency O
in O
terms O
of O
style O
across O
all O
languages O
, O
while O
avoiding O
annotation O
artifacts O
due O
to O
direct O
translation. O
We O
apply O
this O
benchmark O
to O

model O
selection O
for O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
and O
show O
strong O
correlation O
results O
with O
human O
evaluations O
when O
using O
XM3600 B-DatasetName
as O
golden O
references O
for O
automatic O
metrics O
. O
Introduction O
Image B-TaskName
captioning I-TaskName
is O
the O
task O
of O
automatically O
generating O
a O
fluent O
natural O
language O
description O
for O
a O
given O
image. O
This O
task O
is O
important O
for O

enabling O
accessibility O
for O
visually O
impaired O
users O
, O
and O
is O
a O
core O
task O
in O
multimodal O
research O
encompassing O
both O
vision O
and O
language O
modeling. O
However O
, O
datasets O
for O
this O
task O
are O
primarily O
available O
in O
English O
( O
Young O
et O
al. O
, O
2014 O
; O
Chen O
et O
al. O
, O
2015 O
; O
Krishna O
et O
al. O
, O
2017 O

; O
Sharma O
et O
al. O
, O
2018 O
; O
. O
Beyond O
English O
, O
there O
are O
a O
few O
datasets O
such O
as O
Multi30K O
with O
captions O
in O
German O
( O
Elliott O
et O
al. O
, O
2016 O
) O
, O
French O
( O
Elliott O
et O
al. O
, O
2017 O
) O
and O
Czech O
( O
Barrault O
et O
al. O
, O
2018 O
) O
, O
but O

they O
are O
limited O
to O
only O
a O
few O
languages O
that O
cover O
a O
small O
fraction O
of O
the O
world O
's O
population O
, O
while O
featuring O
images O
that O
severely O
under-represent O
the O
richness O
and O
diversity O
of O
cultures O
from O
across O
the O
globe. O
These O
aspects O
have O
hindered O
research O
on O
image O
captioning O
for O
a O
wide O
variety O
of O
languages O
, O

and O
directly O
hamper O
deploying O
accessibility O
solutions O
for O
a O
large O
potential O
audience O
around O
the O
world O
. O
Creating O
large O
training O
and O
evaluation O
datasets O
in O
multiple O
languages O
is O
a O
resource-intensive O
endeavor. O
Recent O
works O
( O
Thapliyal O
and O
Soricut O
, O
2020 O
) O
have O
shown O
that O
it O
is O
feasible O
to O
build O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models O
trained O

on O
machine-translated O
data O
( O
with O
English O
captions O
as O
the O
starting O
point O
) O
. O
This O
work O
also O
shows O
that O
the O
effectiveness O
of O
some O
of O
the O
most O
reliable O
automatic O
metrics O
for O
image O
captioning O
, O
such O
as O
CIDEr O
1 O
is O
severely O
diminished O
when O
applied O
to O
translated O
evaluation O
sets O
, O
resulting O
in O
poorer O
agreement O

with O
human O
evaluations O
compared O
to O
the O
English O
case. O
As O
such O
, O
the O
current O
situation O
is O
that O
trustworthy O
model O
evaluation O
can O
only O
be O
based O
on O
extensive O
and O
expensive O
human O
evaluations. O
However O
, O
such O
evaluations O
can O
not O
usually O
be O
replicated O
across O
different O
research O
efforts O
, O
and O
therefore O
do O
not O
offer O
a O
fast O

and O
robust O
mechanism O
for O
model O
hill-climbing O
and O
comparison O
of O
multiple O
lines O
of O
research O
. O
The O
proposed O
XM3600 B-DatasetName
image O
captioning O
evaluation O
dataset O
provides O
a O
robust O
benchmark O
for O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
, O
and O
can O
be O
reliably O
used O
to O
compare O
research O
contributions O
in O
this O
emerging O
field. O
Our O
contributions O
are O
as O
follows O
: O
( O

i O
) O
for O
human O
caption O
annotations O
, O
we O
have O
devised O
a O
protocol O
that O
allows O
annotators O
for O
a O
specific O
target O
language O
to O
produce O
image O
captions O
in O
a O
style O
that O
is O
consistent O
across O
languages O
; O
this O
protocol O
results O
in O
image-caption O
annotations O
that O
are O
free O
of O
direct O
translation O
artefacts O
, O
an O
issue O
that O

has O
plagued O
Machine O
Translation O
research O
for O
many O
years O
and O
is O
now O
well O
understood O
( O
Freitag O
et O
al. O
, O
2020 O
) O
; O
( O
ii O
) O
for O
image O
selection O
, O
we O
have O
devised O
an O
algorithmic O
approach O
to O
sample O
a O
set O
of O
3600 O
geographically-diverse O
images O
from O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O

al. O
, O
2020 O
) O
, O
aimed O
at O
creating O
a O
representative O
set O
of O
images O
from O
across O
the O
world O
; O
( O
iii O
) O
for O
the O
resulting O
XM3600 B-DatasetName
bench-Figure O
1 O
: O
Sample O
captions O
in O
three O
different O
languages O
( O
out O
of O
36 O
-see O
full O
list O
of O
captions O
in O
Appendix O
A O
) O
, O
showcasing O
the O

creation O
of O
annotations O
that O
are O
consistent O
in O
style O
across O
languages O
, O
while O
being O
free O
of O
directtranslation O
artefacts O
( O
e.g. O
the O
Spanish O
" O
number O
42 O
" O
or O
the O
Thai O
" O
convertibles O
" O
would O
not O
be O
possible O
when O
directly O
translating O
from O
the O
English O
versions O
) O
. O
mark O
, O
we O
empirically O
measure O
its O

ability O
to O
rank O
image O
captioning O
model O
variations O
, O
and O
show O
that O
it O
provides O
high O
levels O
of O
agreement O
with O
human O
judgements O
, O
therefore O
validating O
its O
usefulness O
as O
a O
benchmark O
and O
alleviating O
the O
need O
for O
human O
judgement O
in O
the O
future O
. O
Fig. O
1 O
shows O
a O
few O
sample O
captions O
for O
an O
image O
in O

XM3600 B-DatasetName
that O
exemplify O
point O
( O
i O
) O
above O
, O
and O
Fig. O
2 O
shows O
the O
variety O
of O
cultural O
aspects O
captured O
by O
the O
image O
sampling O
approach O
from O
point O
( O
ii O
) O
. O
We O
provide O
detailed O
explanations O
and O
results O
for O
each O
of O
the O
points O
above O
in O
the O
rest O
of O
the O
paper. O
We O
have O

released O
XM3600 B-DatasetName
under O
a O
CC-BY4.0 O
license O
at O
https O
: O
/ O
/ O
google.github.io O
/ O
crossmodal-3600 O
/ O
. O
The O
XM3600 B-DatasetName
Dataset O
In O
this O
section O
, O
we O
describe O
the O
heuristics O
used O
for O
language O
and O
image O
selection O
, O
the O
design O
of O
the O
caption O
annotation O
process O
, O
caption O
statistics O
including O
quality O
, O
and O
annotator O
details O

. O
Language O
Selection O
In O
this O
section O
, O
we O
describe O
the O
heuristic O
used O
for O
selecting O
the O
languages. O
As O
a O
first O
step O
, O
we O
take O
a O
quantitative O
stance O
and O
choose O
30 O
languages O
( O
L30 O
) O
roughly O
based O
on O
their O
percent O
of O
web O
content O
2 O
. O
As O
a O
second O
step O
, O
we O
consider O

an O
additional O
five O
languages O
( O
L5 O
) O
3 O
to O
cover O
low-resource O
languages O
with O
ish O
( O
da O
) O
, O
Dutch O
( O
nl O
) O
, O
Filipino O
( O
fil O
) O
, O
Finnish O
( O
fi O
) O
, O
French O
( O
fr O
) O
, O
German O
( O
de O
) O
, O
Greek O
( O
el O
) O
, O
Hebrew O
( O

he O
) O
, O
Hindi O
( O
hi O
) O
, O
Hungarian O
( O
hu O
) O
, O
Indonesian O
( O
id O
) O
, O
Italian O
( O
it O
) O
, O
Japanese O
( O
ja O
) O
, O
Korean O
( O
ko O
) O
, O
Norwegian O
( O
no O
) O
, O
Persian O
( O
fa O
) O
, O
Polish O
( O
pl O
) O
, O
Portuguese O
( O

pt O
) O
, O
Romanian O
( O
ro O
) O
, O
Russian O
( O
ru O
) O
, O
Spanish O
( O
es O
) O
, O
Swedish O
( O
sv O
) O
, O
Thai O
( O
th O
) O
, O
Turkish O
( O
tr O
) O
, O
Ukrainian O
( O
uk O
) O
, O
Vietnamese O
( O
vi O
) O
. O
3 O
Bengali O
( O
bn O
) O
, O
Cusco O

Quechua O
( O
quz O
) O
, O
Maori O
( O
mi O
) O
, O
Swahili O
( O
sw O
) O
, O
Telugu O
( O
te O
) O
. O
many O
native O
speakers O
, O
or O
major O
native O
languages O
from O
continents O
that O
would O
not O
be O
covered O
otherwise. O
The O
protocol O
for O
caption O
annotation O
( O
Sec. O
2.3 O
) O
has O
been O
applied O
to O
the O

resulting O
union O
of O
languages O
plus O
English O
, O
for O
a O
total O
of O
36 B-HyperparameterValue
languages B-HyperparameterName
. O
Image O
Selection O
In O
this O
section O
, O
we O
consider O
the O
heuristics O
used O
for O
selecting O
a O
geographically O
diverse O
set O
of O
images O
. O
For O
each O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
, O
we O
select O
100 O
images O
that O
, O
as O
far O
as O

it O
is O
possible O
for O
us O
to O
identify O
, O
are O
taken O
in O
an O
area O
where O
the O
given O
language O
is O
spoken. O
The O
images O
are O
selected O
among O
those O
in O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O
al. O
, O
2020 O
) O
that O
have O
GPS O
coordinates O
stored O
in O
their O
EXIF O
metadata. O
Since O
there O
are O
many O

regions O
where O
more O
than O
one O
language O
is O
spoken O
, O
and O
given O
that O
some O
areas O
are O
not O
well O
covered O
by O
Open B-DatasetName
Images I-DatasetName
, O
we O
design O
an O
algorithm O
that O
maximizes O
the O
percentage O
of O
selected O
images O
taken O
in O
an O
area O
in O
which O
the O
assigned O
language O
is O
spoken. O
This O
is O
a O
greedy O
algorithm O
that O

starts O
the O
selection O
of O
images O
by O
the O
languages O
for O
which O
we O
have O
the O
smallest O
pool O
( O
e.g. O
Persian O
) O
and O
processes O
them O
in O
increasing O
order O
of O
their O
candidate O
image O
pool O
size. O
Whenever O
there O
are O
not O
enough O
images O
in O
the O
area O
where O
a O
language O
is O
spoken O
, O
we O
have O
several O
back-off O

levels O
: O
( O
i O
) O
selecting O
from O
a O
country O
where O
the O
language O
is O
spoken O
; O
( O
ii O
) O
a O
continent O
where O
the O
language O
is O
spoken O
, O
and O
, O
as O
last O
resort O
, O
( O
iii O
) O
from O
anywhere O
in O
the O
world O
. O
This O
strategy O
succeeds O
in O
providing O
our O
target O
number O
of O

100 O
images O
from O
an O
appropriate O
region O
for O
most O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
except O
for O
Persian O
( O
where O
14 O
continent-level O
images O
are O
used O
) O
and O
Hindi O
( O
where O
all O
100 O
images O
are O
at O
the O
global O
level O
because O
the O
in-region O
images O
are O
assigned O
to O
Bengali O
and O
Telugu O
) O
. O
We O
keep O
the O

region O
each O
image O
is O
selected O
from O
as O
part O
of O
our O
data O
annotation O
, O
so O
that O
future O
evaluations O
can O
choose O
to O
either O
evaluate O
on O
images O
relevant O
to O
particular O
regions O
of O
interest O
or O
on O
the O
entire O
dataset O
. O
Caption B-TaskName
Annotation I-TaskName
In O
this O
section O
we O
detail O
the O
design O
of O
the O
caption B-TaskName
annotation I-TaskName
process. O

For O
a O
massively O
multilingual O
benchmark O
such O
as O
XM3600 B-DatasetName
, O
consistency O
in O
the O
style O
of O
the O
description O
language O
is O
critical O
, O
since O
language O
can O
serve O
multiple O
communication O
goals. O
For O
a O
more O
in-depth O
discussion O
on O
these O
issues O
as O
they O
relate O
to O
image O
captions O
, O
we O
refer O
the O
reader O
to O
( O
Alikhani O
et O

al. O
, O
2020 O
) O
. O
We O
borrow O
from O
their O
terminology O
, O
as O
it O
identifies O
coherence O
relations O
between O
image O
and O
captions O
such O
as O
VISIBLE O
, O
META O
, O
SUB-JECTIVE O
, O
and O
STORY. O
The O
goal O
for O
our O
caption B-TaskName
annotation I-TaskName
is O
to O
generate O
VISIBLE O
image O
captions O
, O
i.e. O
, O
use O
the O
target O
language O
to O

formulate O
a O
sentence O
that O
is O
intended O
to O
recognizably O
characterize O
what O
is O
visually O
depicted O
in O
the O
image O
. O
One O
possible O
approach O
to O
generating O
such O
captions O
is O
to O
generate O
them O
as O
such O
in O
English O
, O
and O
have O
them O
translated O
( O
automatically O
, O
semiautomatically O
, O
or O
manually O
) O
into O
all O
the O
other O
languages. O

However O
, O
this O
approach O
results O
in O
an O
English-language O
bias O
, O
as O
well O
as O
other O
problems O
that O
have O
been O
already O
identified O
in O
the O
literature. O
For O
instance O
, O
translations O
are O
often O
less O
fluent O
compared O
to O
natural O
target O
sentences O
, O
due O
to O
word O
order O
and O
lexical O
choices O
influenced O
by O
the O
source O
language. O
The O

impact O
of O
this O
phenomenon O
on O
metrics O
and O
modeling O
has O
recently O
received O
increased O
attention O
in O
the O
evaluation O
literature O
( O
Toral O
et O
al. O
, O
2018 O
; O
Zhang O
and O
Toral O
, O
2019 O
; O
Freitag O
et O
al. O
, O
2020 O
) O
, O
and O
references O
created O
in O
this O
style O
are O
thought O
to O
cause O
overlap-based O
metrics O
to O

favor O
model O
outputs O
that O
use O
such O
unnatural O
language O
. O
We O
have O
designed O
our O
caption B-TaskName
annotation I-TaskName
process O
to O
achieve O
two O
main O
goals O
: O
( O
i O
) O
produce O
caption O
annotations O
in O
a O
VISIBLE O
relation O
with O
respect O
to O
the O
image O
content O
, O
and O
, O
strongly O
, O
create O
consistency O
in O
the O
description O
style O
across O

languages O
; O
( O
ii O
) O
be O
free O
of O
translation O
artefacts. O
To O
achieve O
this O
, O
we O
use O
bi-lingual O
annotators O
with O
a O
requirement O
to O
be O
reading-proficient O
in O
English O
and O
fluent O
/ O
native O
in O
the O
target O
language. O
As O
a O
preliminary O
step O
, O
we O
train O
an O
image-captioning O
model O
on O
English-annotated O
data O
, O
which O
results O

in O
captions O
in O
the O
VISIBLE O
style O
of O
COCO-CAP O
( O
Chen O
et O
al. O
, O
2015 O
) O
. O
The O
annotation O
process O
proceeds O
as O
follows. O
Each O
annotation O
session O
is O
done O
over O
batches B-HyperparameterName
of O
N B-HyperparameterName
= O
15 B-HyperparameterValue
images O
, O
using O
the O
images O
selected O
as O
described O
in O
Sec. O
2.2. O
The O
first O
screen O
shows O
the O
N O

images O
with O
their O
captions O
in O
English O
as O
generated O
by O
the O
captioning O
model O
, O
and O
asks O
the O
annotators O
if O
the O
captions O
are O
EXCELLENT O
, O
GOOD O
, O
MEDIUM O
, O
BAD O
, O
or O
there O
is O
NOT-ENOUGH-INFO. O
We O
refer O
to O
this O
rating O
scale O
as O
the O
5-level O
quality O
scale O
in O
the O
subsequent O
text. O
We O
provide O

the O
annotators O
with O
clear O
guidelines O
about O
what O
constitutes O
an O
EXCEL-LENT O
caption O
, O
and O
how O
to O
evaluate O
degradations O
from O
that O
quality. O
This O
step O
forces O
the O
annotators O
to O
carefully O
assess O
caption O
quality O
and O
it O
primes O
them O
into O
internalizing O
the O
style O
of O
the O
captions O
without O
the O
need O
for O
complicated O
and O
lengthy O
annotation O
instructions O

. O
The O
second O
round O
shows O
the O
same O
N O
images O
again O
, O
but O
one O
image O
at O
a O
time O
without O
the O
English O
captions O
, O
and O
the O
annotators O
are O
asked O
to O
produce O
descriptive O
captions O
in O
the O
target O
language O
for O
each O
image. O
In O
the O
absence O
of O
the O
English O
captions O
, O
the O
annotators O
rely O
on O

the O
internalized O
caption O
style O
, O
and O
generate O
their O
annotations O
mostly O
based O
on O
the O
image O
content O
-with O
no O
support O
from O
the O
text O
modality O
, O
other O
than O
potentially O
from O
memory. O
Note O
, O
however O
, O
that O
we O
have O
designed O
the O
system O
to O
support O
N O
annotations O
simultaneously O
, O
and O
we O
have O
empirically O
selected O
the O

value O
of O
N O
as O
to O
be O
large O
enough O
to O
" O
overwrite O
" O
the O
memory O
of O
the O
annotators O
with O
respect O
to O
the O
exact O
textual O
formulation O
of O
the O
English O
captions. O
As O
a O
result O
, O
we O
observe O
that O
the O
produced O
annotations O
are O
free O
of O
translation O
artefacts O
: O
See O
the O
example O
in O
Fig. O
1 O

for O
Spanish O
mentioning O
" O
number O
42 O
" O
, O
and O
for O
Thai O
mentioning O
" O
convertibles O
" O
. O
We O
also O
provide O
the O
annotators O
with O
an O
annotation O
protocol O
to O
use O
when O
creating O
the O
captions O
, O
which O
provides O
useful O
guidance O
in O
achieving O
consistent O
annotations O
across O
all O
the O
targeted O
languages. O
We O
provide O
the O
annotation O
guidelines O

in O
Appendices O
B O
and O
C. O
For O
each O
language O
, O
we O
annotate O
all O
3600 O
images O
with O
captions O
using O
replication O
2 O
( O
two O
different O
annotators O
working O
independently O
) O
4 O
, O
except O
Bengali O
( O
bn O
) O
with O
replication O
1 O
and O
Maori O
( O
mi O
) O
with O
roughly O
1 O
for O
2 O
/ O
3 O
and O
2 O

for O
1 O
/ O
3 O
of O
the O
images O
, O
see O
Table O
1 O
. O
Caption O
Statistics O
In O
this O
section O
, O
we O
take O
a O
look O
at O
the O
the O
basic O
statistics O
of O
the O
captions O
in O
the O
dataset. O
captions O
per O
language O
. O
For O
languages O
with O
natural O
space O
tokenization O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
words I-HyperparameterName
per I-HyperparameterName
caption I-HyperparameterName

can O
be O
as O
low O
as O
5 B-HyperparameterValue
or O
6 B-HyperparameterValue
for O
some O
agglutinative O
languages O
like O
Cusco O
Quechua O
( O
quz O
) O
and O
Czech O
( O
cs O
) O
, O
and O
as O
high O
as O
18 B-HyperparameterValue
for O
an O
analytic O
language O
like O
Vietnamese O
( O
vi O
) O
. O
The O
number O
of O
characters O
per O
caption O
also O
varies O
drastically O
-from O
mid-20s O

for O
Korean O
( O
ko O
) O
to O
mid-90s O
for O
Indonesian O
( O
id O
) O
-depending O
on O
the O
alphabet O
and O
the O
script O
of O
the O
language O
. O
Caption O
Quality O
In O
this O
section O
, O
we O
describe O
the O
process O
for O
ensuring O
the O
creation O
of O
high O
quality O
annotations O
, O
and O
present O
quality O
statistics O
of O
the O
annotations O
produced O

. O
In O
order O
to O
ensure O
quality O
, O
the O
annotation O
process O
is O
initially O
started O
with O
pilot B-HyperparameterName
runs I-HyperparameterName
on O
150 B-HyperparameterValue
images. O
The O
caption O
ratings O
are O
spot O
checked O
by O
the O
authors O
to O
verify O
that O
the O
raters O
have O
a O
good O
understanding O
of O
the O
rating O
scale. O
Further O
, O
the O
generated O
captions O
go O
through O
a O
verification O

round O
where O
they O
are O
rated O
by O
the O
human O
annotators O
on O
the O
5-level O
quality O
scale O
described O
in O
Sec.2.3. O
If O
the O
annotations O
are O
below O
the O
desired O
quality O
, O
we O
clarify O
the O
guidelines O
and O
add O
more O
examples O
to O
provide O
feedback O
to O
the O
human O
annotators O
and O
then O
conduct O
another O
pilot. O
This O
process O
is O
repeated O

until O
very O
few O
low-quality O
captions O
are O
being O
produced O
5 O
. O
After O
this O
, O
for O
every O
language O
, O
we O
run O
the O
main O
annotation O
and O
finally O
a O
verification O
round O
where O
we O
select O
one O
caption O
for O
600 O
randomly O
selected O
images O
and O
have O
the O
annotator O
pool O
( O
per O
language O
) O
rate O
them O
on O
the O

5-level O
quality O
scale O
mentioned O
in O
Sec. O
2.3. O
The O
quality O
scores O
are O
presented O
in O
Table O
2 O
. O
Annotator O
Details O
We O
use O
an O
in-house O
annotation O
platform O
with O
professional O
( O
paid O
) O
annotators O
and O
quality O
assurance. O
Annotators O
are O
chosen O
to O
be O
native O
in O
the O
target O
language O
whenever O
possible O
, O
and O
fluent O
otherwise O
( O

for O
low-resource O
languages O
, O
they O
are O
usually O
linguists O
that O
have O
advanced-level O
knowledge O
of O
that O
language O
) O
. O
All O
annotators O
are O
required O
to O
be O
proficient O
in O
English O
since O
the O
instructions O
and O
guidelines O
are O
given O
in O
English O
. O
Model O
Comparison O
using O
XM3600 B-DatasetName
In O
this O
section O
, O
we O
detail O
our O
experiments O
for O
comparing O

several O
models O
using O
human O
evaluations O
, O
and O
also O
using O
XM3600 B-DatasetName
annotations O
as O
gold O
6 O
references O
for O
automated O
metrics O
. O
For O
model O
comparison O
, O
we O
train O
several O
multilingual O
image O
captioning O
models O
with O
different O
sizes O
over O
different O
datasets O
, O
and O
compare O
them O
on O
XM3600. B-DatasetName
As O
our O
main O
result O
, O
we O
show O
a O

high O
level O
of O
correlation O
between O
model O
rankings O
based O
on O
human-evaluation O
scores O
and O
the O
scores O
obtained O
using O
CIDEr B-MetricName
with O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
. O
Datasets O
We O
build O
two O
multilingual O
datasets O
for O
training O
, O
CC3M-35L B-DatasetName
and O
COCO-35L B-DatasetName
, O
by O
translating O
Conceptual O
Captions O
3M O
( O
Sharma O
et O
al. O
, O
2018 O
) O
and O

COCO O
Captions O
( O
Chen O
et O
al. O
, O
2015 O
) O
to O
the O
other O
34 O
languages O
using O
Google O
's O
machine O
translation O
API O
7 O
. O
The O
remaining O
language O
, O
Cusco O
Quechua O
( O
quz O
) O
, O
is O
not O
supported O
by O
the O
API O
8 O
. O
We O
use O
the O
standard O
train O
and O
validation O
splits O
for O
CC3M O

9 O
. O
For O
COCO O
, O
we O
use O
the O
Karpathy O
split O
( O
Karpathy O
and O
Fei-Fei O
, O
2014 O
) O
Models O
In O
this O
section O
we O
detail O
the O
model O
architecture O
we O
used O
for O
the O
experiments. O
Our O
Transformer-based O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
model O
architecture O
for O
image O
captioning O
is O
shown O
in O
Figure O
3. O

On O
the O
vision O
side O
, O
each O
input O
image O
is O
modeled O
by O
a O
Vision B-MethodName
Transformer I-MethodName
( O
ViT B-MethodName
) O
( O
Dosovitskiy O
et O
al. O
, O
2020 O
; O
Zhai O
et O
al. O
, O
2021 O
) O
. O
The O
visual O
features O
produced O
by O
ViT O
for O
every O
patch O
of O
the O
image O
are O
pooled O
into O
a O
single O
dense O
feature O

vector. O
On O
the O
text O
side O
, O
a O
Language O
Identifier O
( O
LangId O
) O
string O
is O
used O
to O
specify O
the O
language. O
The O
LangId O
string O
is O
tokenized O
and O
embedded O
into O
dense O
token O
embeddings O
, O
which O
are O
merged O
with O
the O
dense O
visual O
embeddings O
as O
the O
input O
to O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Encoder O

, O
followed O
by O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Decoder O
to O
generate O
the O
predicted O
captions O
. O
We O
take O
advantage O
of O
existing O
pretrained O
models O
to O
initialize O
different O
parts O
of O
our O
model O
: O
ViT O
( O
Zhai O
et O
al. O
, O
2021 O
) O
( O
green O
in O
Fig. O
3 O
) O
and O
mT5 O
( O
Xue O
et O

al. O
, O
2021 O
) O
( O
orange O
in O
Fig. O
3 O
) O
. O
We O
consider O
different O
model O
sizes O
: O
mT5-base O
, O
mT-large O
, O
ViT-B O
/ O
16 O
, O
and O
ViT-g O
/ O
14 O
, O
where O
16 O
and O
14 O
are O
the O
corresponding O
patch O
sizes. O
We O
choose O
three O
combinations O
resulting O
in O
three O
different O
model O
architectures O
: O

mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
, O
mT5-base B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
and O
mT5large B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
. O
We O
train O
these O
three O
models O
on O
COCO-35L. B-DatasetName
In O
addition O
, O
we O
consider O
a O
fourth O
model O
based O
on O
mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
and O
trained O
on O
CC3M-35L. B-DatasetName
The O
models O
are O
trained O
on O
a O

4x4x4 O
TPU-v4 O
architecture O
using O
an O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
optimizer O
with O
a O
constant B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
period I-HyperparameterName
between O
{ B-HyperparameterValue
1k I-HyperparameterValue
, I-HyperparameterValue
10k I-HyperparameterValue
} I-HyperparameterValue
steps I-HyperparameterValue
, O
followed O
by O
a O
reversed O
square-root O
decay O
with O
the O
number O
of O
steps. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
2048 B-HyperparameterValue
in O
all O
the O
experiments. O
The O
initial B-HyperparameterName

learning I-HyperparameterName
rate I-HyperparameterName
is O
between O
{ B-HyperparameterValue
1e-4 I-HyperparameterValue
, I-HyperparameterValue
3e-4 I-HyperparameterValue
} I-HyperparameterValue
. O
We O
use O
the O
same O
vocabulary B-HyperparameterName
( O
size B-HyperparameterValue
250k I-HyperparameterValue
) O
as O
mT5 O
( O
Xue O
et O
al. O
, O
2021 O
) O
. O
The O
model O
trained O
with O
CC3M-35L B-DatasetName
is O
subsequently O
finetuned O
on O
COCO-35L B-DatasetName
with O
constant O
learning B-HyperparameterName
rate I-HyperparameterName
3e-5 B-HyperparameterValue
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
. O
Human O
Evaluation O

In O
this O
section O
, O
we O
detail O
the O
process O
used O
for O
human O
evaluations O
comparing O
the O
performance O
of O
two O
models O
. O
Our O
main O
goal O
in O
creating O
XM3600 B-DatasetName
is O
to O
automate O
the O
evaluation O
of O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
by O
eliminating O
expensive O
and O
timeconsuming O
human O
evaluations. O
Our O
results O
indicate O
that O
they O
can O

be O
substituted O
by O
using O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
for O
automated O
metrics O
such O
as O
CIDEr B-MetricName
. O
To O
quantify O
the O
correlation O
between O
the O
two O
methods O
, O
we O
train O
four O
different O
models O
( O
Tab. O
3 O
) O
and O
conduct O
side-by-side O
human O
evaluations O
using O
the O
outputs O
of O
these O
models O
in O
several O
languages. O
We O

observe O
strong O
correlations O
( O
Sec. O
3.4 O
) O
between O
the O
human O
evaluations O
and O
the O
CIDEr B-MetricName
scores O
using O
the O
XM3600 B-DatasetName
references O
. O
Specifically O
, O
we O
use O
a O
randomly O
selected O
subset O
of O
600 O
images O
from O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
, O
which O
we O
call O
XM600. O
Image O
captions O
generated O
by O
a O
given O
pairing O
of O
models O

( O
m O
1 O
vs O
m O
2 O
, O
where O
m O
1 O
is O
considered O
as O
the O
base O
condition O
and O
m O
2 O
as O
the O
test O
condition O
) O
are O
compared O
and O
rated O
side-by-side O
, O
using O
a O
similar O
pool O
of O
annotators O
as O
described O
in O
Sec. O
2.6. O
Each O
side-by-side O
pair O
( O
shown O
in O
a O
random O
per-example O

left-vs-right O
order O
) O
is O
rated O
using O
a O
7-point O
scale O
: O
MUCH-BETTER O
, O
BETTER O
, O
SLIGHTLY-BETTER O
, O
SIMILAR O
, O
SLIGHTLY-WORSE O
, O
WORSE O
, O
MUCH-WORSE O
, O
with O
a O
replication O
factor O
of O
3 O
( O
three O
annotators O
rate O
each O
pair O
) O
. O
We O
denote O
by O
WINS B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O

raters O
( O
i.e. O
2 O
out O
of O
3 O
) O
mark O
m O
2 O
's O
captions O
as O
better O
, O
and O
by O
LOSSES B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O
raters O
mark O
m O
2 O
's O
captions O
as O
worse. O
We O
then O
define O
the O
overall B-MetricName
side-by-side I-MetricName
gain I-MetricName
of O
m O
2 O
over O
m O
1 O
as O
∆S×S O

= O
WINS O
-LOSSES O
. O
Conducting O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
for O
each O
pair O
of O
models O
over O
the O
35 O
languages O
would O
require O
210 O
human O
evaluation O
sessions. O
This O
is O
prohibitively O
expensive O
and O
time O
consuming. O
Thus O
, O
we O
conduct O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
of O
the O
pairs O
of O
models O

, O
on O
a O
core O
set O
of O
four O
languages O
called O
LCORE O
11 O
. O
We O
call O
this O
set O
of O
24 O
evaluation O
sessions O
OCORE. O
Furthermore O
, O
we O
also O
conduct O
a O
sparser O
set O
of O
side-by-side O
evaluations O
over O
languages O
where O
the O
CIDEr B-MetricName
differences O
on O
XM3600 B-DatasetName
and O
on O
COCO-DEV O
12 O
indicate O
11 O
Chinese-Simplified O
( O
zh O
) O

, O
English O
( O
en O
) O
, O
Hindi O
( O
hi O
) O
, O
Spanish O
( O
es O
) O
12 O
COCO O
validation O
split O
with O
machine-translated O
references O
disagreement O
or O
ambiguity O
( O
e.g. O
, O
opposite O
sign O
of O
the O
CIDEr O
differences O
, O
and O
/ O
or O
small O
CIDEr O
differences O
) O
; O
this O
gives O
us O
a O
set O
of O
28 O

languages O
called O
LEXT O
13 O
. O
We O
call O
the O
resulting O
set O
of O
41 O
evaluation O
sessions O
OEXT. O
The O
set O
of O
all O
evaluations O
is O
called O
OALL O
=OCORE O
+ O
OEXT O
, O
which O
are O
conducted O
over O
the O
languages O
LALL O
= O
LCORE O
+ O
LEXT O
. O
The O
choice O
of O
which O
model O
is O
called O
m O
1 O
and O
which O

model O
is O
called O
m O
2 O
is O
arbitrary O
in O
the O
sideby-side O
evaluations O
, O
since O
we O
randomly O
flip O
left O
vs O
right O
before O
presenting O
the O
captions O
to O
the O
raters. O
Hence O
a O
single O
side-by-side O
evaluation O
gives O
two O
points O
for O
the O
correlation O
calculations O
: O
one O
with O
the O
m O
1 O
and O
m O
2 O
assigned O
as O
per O

the O
actual O
evaluation O
conducted O
, O
and O
one O
more O
with O
the O
m O
1 O
and O
m O
2 O
assignment O
flipped O
and O
the O
∆S×S O
sign O
flipped O
correspondingly O
. O
Results O
We O
present O
results O
that O
show O
that O
it O
is O
feasible O
to O
use O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
with O
automated O
metrics O
such O
as O
CIDEr B-MetricName
to O
compare O

models O
in O
lieu O
of O
human B-MetricName
evaluations I-MetricName
, O
and O
that O
this O
option O
is O
superior O
to O
using O
silver O
references O
created O
via O
automated O
translation. O
Table O
5 O
presents O
the O
results O
for O
the O
OCORE O
set O
of O
evaluations O
on O
XM600 B-DatasetName
on O
the O
LCORE O
languages O
, O
while O
Table O
7 O
in O
the O
appendix O
shows O
the O
results O
on O
the O

LEXT O
languages. O
The O
reference O
for O
the O
relative O
strength O
of O
each O
pairing O
is O
given O
by O
∆S×S O
, O
with O
positive O
numbers O
indicating O
the O
superiority O
of O
m O
2 O
, O
and O
negative O
numbers O
indicating O
a O
superiority O
of O
m O
1 O
. O
As O
can O
be O
seen O
from O
the O
table O
, O
the O
model O
comparisons O
span O
a O
range O

of O
model O
differences O
, O
from O
low O
∆S×S O
to O
high O
∆S×S. O
∆CIDEr O
XM600and B-DatasetName
∆CIDEr O
XM3600 B-DatasetName
capture O
similar O
information O
, O
except O
these O
numbers O
are O
based O
on O
CIDEr O
scores O
using O
as O
references O
XM600 B-DatasetName
and O
XM3600 B-DatasetName
, O
respectively O
, O
while O
∆CIDEr O
COCO-DEV O
is O
based O
on O
machine-translated O
references O
from O
the O
validation O
split O
of O
COCO O
. O

We O
use O
the O
results O
from O
Table O
5 O
( O
and O
Table O
7 O
) O
to O
compute O
the O
correlation O
between O
human O
judgements O
of O
the O
relative O
quality O
of O
the O
captioning O
models O
and O
the O
ability O
of O
the O
CIDEr B-DatasetName
14 O
metric O
-or O
, O
rather O
, O
of O
the O
underlying O
references O
used O
by O
the O
metricto O
perform O
an O
equivalent O

task. O
Table O
6 O
presents O
the O
correlation O
results O
using O
three O
correlation O
metrics O
: O
Pearson B-MetricName
, O
Spearman B-MetricName
, O
and O
Kendall. B-MetricName
The O
first O
section O
shows O
the O
correlations O
over O
all O
the O
side-by-side O
evaluations O
( O
i.e. O
OCORE O
and O
OEXT O
) O
; O
These O
cover O
the O
LCORE O
and O
the O
LEXT O
languages. O
The O
second O
section O
shows O
the O
correlations O

for O
the O
OEXT O
covering O
the O
LEXT O
languages. O
The O
third O
section O
shows O
the O
correlations O
for O
the O
OCORE O
evaluations O
covering O
the O
LCORE O
languages O
. O
We O
observe O
that O
∆CIDEr O
XM3600 B-DatasetName
is O
highly O
correlated O
with O
human O
judgement O
according O
to O
all O
the O
correlation O
metrics O
( O
Bonett O
and O
Wright O
, O
2000 O
) O
, O
over O
all O
the O

evaluations O
OALL O
, O
over O
the O
OCORE O
evaluations O
, O
and O
also O
the O
OEXT O
evaluations. O
Furthermore O
, O
for O
the O
OEXT O
evaluations O
, O
where O
most O
of O
the O
instances O
have O
opposite O
signs O
for O
∆CIDEr O
COCO-DEV O
and O
∆CIDEr O
XM3600 B-DatasetName
, O
we O
find O
that O
the O
former O
is O
strongly O
anti-correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results O
while O
the O

latter O
is O
highly O
correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results. O
Overall O
, O
these O
results O
indicate O
that O
: O
( O
i O
) O
we O
can O
reliably O
substitute O
∆CIDEr O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
on O
XM600 B-DatasetName
when O
comparing O
models O
similar O
to O
the O
ones O
we O
used O
; O
( O
ii O
) O
the O
gold O
XM3600 B-DatasetName
references O
are O
preferable O
over O

the O
silver O
references O
obtained O
from O
translating O
COCO O
captions O
, O
in O
terms O
of O
approximating O
the O
judgements O
of O
the O
human O
evaluators O
15 O
. O
Based O
on O
the O
results O
from O
Table O
6 O
, O
we O
recommend O
the O
use O
of O
the O
XM3600 B-DatasetName
references O
as O
a O
means O
to O
achieve O
high-quality O
automatic O
comparisons O
between O
multilingual O
image O
captioning O
models. O

We O
have O
provided O
the O
CIDEr B-MetricName
scores O
for O
XM3600 B-DatasetName
in O
35 O
languages O
for O
all O
the O
models O
, O
in O
Tables O
8-11 O
in O
the O
Appendix. O
These O
can O
be O
used O
as O
baselines O
in O
future O
work. O
15 O
However O
, O
it O
is O
unclear O
whether O
machine O
translated O
references O
for O
one O
particular O
language O
in O
XM3600 B-DatasetName
translated O
to O
all O

others O
, O
are O
worse O
than O
using O
the O
human O
generated O
references. O
In O
particular O
, O
we O
studied O
the O
correlations O
of O
CIDEr B-MetricName
computed O
using O
XM3600-en-MT B-DatasetName
( O
i.e. O
the O
XM3600 O
English O
references O
, O
machine O
translated O
to O
all O
the O
other O
languages O
) O
, O
with O
the O
human B-MetricName
evaluations. I-MetricName
We O
found O
that O
even O
though O
the O
translations O
have O

artifacts O
and O
disfluencies O
, O
CIDEr B-MetricName
differences O
calculated O
using O
them O
show O
comparable O
correlations O
with O
human O
judgement O
observations. O
We O
also O
studied O
such O
correlations O
for O
machine O
translated O
references O
from O
German O
, O
Greek O
, O
Hebrew O
, O
Hungarian O
and O
Swahili. O
We O
found O
that O
the O
correlations O
are O
similar O
and O
sometimes O
even O
a O
bit O
higher O
than O
using O

the O
human O
generated O
references. O
We O
believe O
this O
happens O
because O
the O
rater O
guidelines O
weigh O
informativeness O
over O
fluency O
and O
the O
CIDEr B-MetricName
metric O
is O
also O
not O
as O
sensitive O
to O
fluency. O
Further O
work O
is O
needed O
to O
understand O
the O
use O
of O
translated O
references O
as O
compared O
to O
human O
generated O
references. O
We O
believe O
that O
using O
the O
human O

generated O
references O
along O
with O
the O
set O
of O
machine O
translated O
references O
from O
all O
the O
other O
languages O
may O
provide O
even O
stronger O
correlations O
and O
show O
greater O
diversity O
in O
the O
coverage O
of O
the O
image O
constituents O
. O
Conclusions O
We O
introduce O
the O
XM3600 B-DatasetName
dataset O
as O
a O
benchmark O
for O
evaluating O
the O
performance O
of O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models. O

The O
images O
in O
the O
dataset O
are O
geographically O
diverse O
, O
covering O
all O
inhabited O
continents O
and O
a O
large O
fraction O
of O
the O
world O
population. O
We O
believe O
this O
benchmark O
has O
the O
potential O
to O
positively O
impact O
both O
the O
research O
and O
the O
applications O
of O
this O
technology O
, O
and O
enable O
( O
among O
other O
things O
) O
better O
accessibility O

for O
visually-impaired O
users O
across O
the O
world O
, O
including O
speakers O
of O
lowresource O
languages O
. O
The O
main O
appeal O
of O
this O
benchmark O
is O
that O
it O
alleviates O
the O
need O
for O
extensive O
human B-MetricName
evaluation I-MetricName
, O
which O
is O
difficult O
to O
achieve O
across O
multiple O
languages O
and O
hinders O
direct O
comparison O
between O
different O
research O
ideas O
and O
results. O
We O
show O

significant O
improvements O
in O
correlation O
with O
human O
judgements O
when O
using O
the O
XM3600 B-DatasetName
dataset O
as O
references O
for O
automatic O
metrics O
, O
and O
therefore O
hope O
that O
the O
adoption O
of O
this O
dataset O
as O
a O
standard O
benchmark O
will O
facilitate O
faster O
progress O
and O
better O
comparisons O
among O
competing O
ideas O
. O
Our O
empirical O
observations O
are O
primarily O
on O
the O
full O

set O
of O
side-by-side O
comparisons O
over O
English O
and O
three O
other O
languages O
( O
Spanish O
, O
Hindi O
, O
Chinese O
) O
. O
Due O
to O
the O
similarity O
in O
the O
data O
collection O
and O
the O
quality O
control O
process O
, O
we O
expect O
similar O
results O
to O
hold O
for O
all O
the O
other O
languages O
as O
well O
; O
we O
validated O
this O
expectation O

with O
additional O
empirical O
observations O
covering O
an O
additional O
28 O
languages O
. O

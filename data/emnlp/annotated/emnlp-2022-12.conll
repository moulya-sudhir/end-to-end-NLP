Co-guiding B-MethodName
Net I-MethodName
: O
Achieving O
Mutual O
Guidances O
between O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
and O
Slot B-TaskName
Filling I-TaskName
via O
Heterogeneous O
Semantics-Label O
Graphs O
Recent O
graph-based O
models O
for O
joint B-TaskName
multiple I-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
have O
obtained O
promising O
results O
through O
modeling O
the O
guidance O
from O
the O
prediction O
of O
intents O
to O
the O
decoding O
of O
slot B-TaskName
filling I-TaskName
. O
However, O
existing O
methods O
(1) O
only O
model O
the O
unidirectional O
guidance O
from O
intent O
to O
slot; O
(2) O
adopt O
homogeneous O
graphs O
to O
model O
the O
interactions O
between O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes, O
which O
limit O
the O
performance. O
In O
this O
paper, O
we O
propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
achieving O
the O
mutual O
guidances O
between O
the O
two O
tasks. O
In O
the O
first O
stage, O
the O
initial O
estimated O
labels O
of O
both O
tasks O
are O
produced, O
and O
then O
they O
are O
leveraged O
in O
the O
second O
stage O
to O
model O
the O
mutual O
guidances. O
Specifically, O
we O
propose O
two O
heterogeneous O
graph O
attention O
networks O
working O
on O
the O
proposed O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
, O
which O
effectively O
represent O
the O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes. O
Experiment O
results O
show O
that O
our O
model O
outperforms O
existing O
models O
by O
a O
large O
margin, O
obtaining O
a O
relative O
improvement O
of O
19.3% B-MetricValue
over O
the O
previous O
best O
model O
on O
Mix-ATIS O
dataset O
in O
overall O
accuracy B-MetricName
. O
Introduction O
Spoken B-TaskName
language I-TaskName
understanding I-TaskName
( O
SLU B-TaskName
) O
(Young O
et O
al., O
2013) O
is O
a O
fundamental O
task O
in O
dialog O
systems. O
Its O
objective O
is O
to O
capture O
the O
comprehensive O
semantics O
of O
user O
utterances, O
and O
it O
typically O
includes O
two O
subtasks: O
intent B-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
(Tur O
and O
De O
Mori, O
2011). O
Intent B-TaskName
detection I-TaskName
aims O
to O
predict O
the O
intention O
of O
the O
user O
utterance O
and O
slot O
filling O
aims O
to O
extract O
additional O
information O
or O
constraints O
expressed O
in O
the O
utterance. O
Recently, O
researchers O
discovered O
that O
these O
two O
tasks O
are O
closely O
tied, O
and O
a O
bunch O
of O
models O
(Goo O
et O
al., O
2018;Liu O
et O
al., O
2019a;E O
et O
al., O
2019;Qin O
et O
al., O
2019) O
are O
proposed O
to O
combine O
the O
single-intent O
detection O
and O
slot B-TaskName
filling I-TaskName
in O
multi-task O
frameworks O
to O
leverage O
their O
correlations. O
However, O
in O
real-world O
scenarios, O
a O
user O
usually O
expresses O
multiple O
intents O
in O
a O
single O
utterance. O
To O
this O
end, O
(Kim O
et O
al., O
2017) O
begin O
to O
tackle O
the O
multi-intent B-TaskName
detection I-TaskName
task O
and O
(Gangadharaiah O
and O
Narayanaswamy, O
2019) O
make O
the O
first O
attempt O
to O
jointly O
model O
the O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
in O
a O
multi-task O
framework. O
(Qin O
et O
al., O
2020) O
propose O
an O
AGIF B-MethodName
model O
to O
adaptively O
integrate O
the O
fine-grained O
multi-intent O
prediction O
information O
into O
the O
autoregressive O
decoding O
process O
of O
slot B-TaskName
filling I-TaskName
via O
graph O
attention O
network O
(GAT) O
(Velickovic O
et O
al., O
2018). O
And O
(Qin O
et O
al., O
2021b) O
further O
propose O
a O
non-autoregressive B-MethodName
GAT-based I-MethodName
model I-MethodName
which O
enhances O
the O
interactions O
between O
the O
predicted O
multiple O
intents O
and O
the O
slot O
hidden O
states, O
obtaining O
state-of-the-art O
results O
and O
significant O
speedup. O
Despite O
the O
promising O
progress O
that O
existing O
multi-intent O
SLU O
joint O
models O
have O
achieved, O
we O
discover O
that O
they O
suffer O
from O
two O
main O
issues: O
(1) O
Ignoring O
the O
guidance O
from O
slot O
to O
intent. O
Since O
previous O
researchers O
realized O
that O
"slot O
labels O
could O
depend O
on O
the O
intent" O
(Gangadharaiah O
and O
Narayanaswamy, O
2019), O
existing O
models O
leverage O
the O
information O
of O
the O
predicted O
intents O
to O
guide O
slot B-TaskName
filling I-TaskName
, O
as O
shown O
in O
Fig. O
1(a). O
However, O
they O
ig- O
However, O
in O
previous O
works, O
the O
only O
guidance O
that O
the O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
task O
can O
get O
from O
the O
joint O
model O
is O
sharing O
the O
basic O
semantics O
with O
the O
slot B-TaskName
filling I-TaskName
task. O
As O
a O
result, O
the O
lack O
of O
guidance O
from O
slot O
to O
intent O
limits O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
so O
the O
joint O
task. O
(2) O
Node O
and O
edge O
ambiguity O
in O
the O
semanticslabel O
graph. O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b O
apply O
GATs B-MethodName
over O
the O
constructed O
graphs O
to O
model O
the O
interactions O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
However, O
their O
graphs O
are O
homogeneous, O
in O
which O
all O
nodes O
and O
edges O
are O
treated O
as O
the O
same O
type. O
For O
a O
slot O
semantics O
node, O
the O
information O
from O
intent O
label O
nodes O
and O
other O
slot O
semantics O
nodes O
play O
different O
roles, O
while O
the O
homogeneous O
graph O
cannot O
discriminate O
their O
specific O
contributions, O
causing O
ambiguity. O
Therefore, O
the O
heterogeneous O
graphs O
should O
be O
designed O
to O
represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes O
to O
facilitate O
better O
interactions. O
In O
this O
paper, O
we O
propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
to O
tackle O
the O
above O
two O
issues. O
For O
the O
first O
issue, O
Co-guiding B-MethodName
Net I-MethodName
implements O
a O
two-stage O
framework O
as O
shown O
in O
Fig. O
1 O
(b). O
The O
first O
stage O
produces O
the O
initial O
estimated O
labels O
for O
the O
two O
tasks O
and O
the O
second O
stage O
leverages O
the O
estimated O
labels O
as O
prior O
label O
information O
to O
allow O
the O
two O
tasks O
mutually O
guide O
each O
other. O
For O
the O
second O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
( O
HSLGs B-MethodName
): O
(1) O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
intent O
semantics O
nodes O
and O
slot O
label O
nodes; O
(2) O
an O
intent-to-slot B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
Moreover, O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
( O
HGATs B-MethodName
) O
are O
proposed O
to O
work O
on O
the O
two O
proposed O
graphs O
for O
modeling O
the O
guidances O
from O
slot O
to O
intent O
and O
intent O
to O
slot, O
respectively. O
Experiment O
results O
show O
that O
our O
Co-guiding B-MethodName
Net I-MethodName
significantly O
outperforms O
previous O
models, O
and O
model O
analysis O
further O
verifies O
the O
advantages O
of O
our O
model. O
The O
contributions O
of O
our O
work O
are O
three-fold: O
(1) O
We O
propose O
Co-guiding B-MethodName
Net I-MethodName
1 I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
allowing O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
mutually O
guide O
each O
other. O
We O
make O
the O
first O
attempt O
to O
achieve O
the O
mutual O
guidances O
between O
the O
two O
tasks. O
(2) O
We O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
as O
appropriate O
platforms O
for O
interactions O
between O
semantics O
nodes O
and O
label O
nodes. O
And O
we O
propose O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
to O
model O
the O
mutual O
guidances O
between O
the O
two O
tasks. O
(3) O
Experiment O
results O
demonstrate O
that O
our O
model O
achieves O
new O
state-of-the-art O
performance. O
Co-guiding O
Problem O
Definition O
Given O
a O
input O
utterance O
denoted O
as O
U O
= O
{u O
i O
} O
n O
1 O
, O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
can O
be O
formulated O
as O
a O
multi-label O
classification O
task O
that O
outputs O
multiple O
intent O
labels O
corresponding O
to O
the O
input O
utterance. O
And O
slot B-TaskName
filling I-TaskName
is O
a O
sequence O
labeling O
task O
that O
maps O
each O
u O
i O
into O
a O
slot O
label. O
Next, O
before O
diving O
into O
the O
details O
of O
Coguiding B-MethodName
Net I-MethodName
's O
architecture, O
we O
first O
introduce O
the O
construction O
of O
the O
two O
heterogeneous B-MethodName
graphs I-MethodName
. O
Slot-to-Intent B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
provide O
an O
appropriate O
platform O
for O
modeling O
the O
guidance O
from O
the O
estimated O
slot O
labels O
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
design O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
), O
which O
represents O
the O
relations O
among O
the O
semantics O
of O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
the O
estimated O
slot O
labels. O
S2I-SLG B-MethodName
is O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
3 O
(a). O
It O
contains O
two O
types O
of O
nodes: O
intent O
semantics O
nodes O
(e.g., O
I O
1 O
, O
..., O
I O
5 O
) O
and O
slot O
label O
(SL) O
nodes O
(e.g., O
SL O
1 O
, O
..., O
SL O
5 O
). O
And O
there O
are O
four O
types O
of O
edges O
in O
S2I-SLG B-MethodName
, O
as O
shown O
in O
Fig. O
3 O
(b). O
Each O
edge O
type O
corresponds O
to O
an O
individual O
kind O
of O
information O
aggregation O
on O
the O
graph. O
Mathematically, O
the O
S2I-SLG B-MethodName
can O
be O
denoted O
as O
G O
s2i O
= O
(V O
s2i O
, O
E O
s2i O
, O
A O
s2i O
, O
R O
s2i O
), O
in O
which O
V O
s2i O
is O
the O
set O
of O
all O
nodes, O
E O
s2i O
is O
the O
set O
of O
all O
edges, O
A O
s2i O
is O
the O
set O
of O
two O
node O
types O
and O
R O
s2i O
is O
the O
set O
of O
four O
edge O
types. O
Each O
node O
v O
s2i O
and O
each O
edge O
e O
s2i O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
s2i O
) O
: O
V O
s2i O
→ O
A O
s2i O
and O
ϕ(e O
s2i O
) O
: O
E O
s2i O
→ O
R O
s2i O
. O
For O
instance, O
in O
Fig. O
3, O
the O
SL O
2 O
node O
belongs O
to O
V O
s2i O
, O
while O
its O
node O
type O
SL O
belongs O
to O
A O
s2i O
; O
the O
edge O
from O
SL O
2 O
to O
I O
3 O
belongs O
to O
E O
s2i O
, O
while O
its O
edge O
type O
slot_to_intent_guidance O
belongs O
to O
R O
s2i O
. O
Besides, O
edges O
in O
S2I-SLG B-MethodName
are O
based O
on O
local O
connections. O
For O
example, O
node O
I O
i O
is O
connected O
to O
{I O
i−w O
, O
..., O
I O
i+w O
} O
and O
{SL O
i−w O
, O
..., O
SL O
i+w O
}, O
where O
w O
is O
a O
hyper-parameter O
of O
the O
local O
window O
size. O
Intent-to-Slot B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
present O
a O
platform O
for O
accommodating O
the O
guidance O
from O
the O
estimated O
intent O
labels O
to O
slot O
filling, O
we O
design O
an O
intent-to-slot B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
the O
intent O
label O
nodes. O
I2S-SLG B-MethodName
is O
also O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
4 O
(a). O
It O
contains O
two O
types O
of O
nodes: O
slot O
semantics O
nodes O
(e.g., O
S O
1 O
, O
..., O
S O
5 O
) O
and O
intent O
label O
(IL) O
nodes O
(e.g., O
IL O
1 O
, O
..., O
IL O
5 O
). O
And O
Fig. O
4 O
(b) O
shows O
the O
four O
edge O
types. O
Each O
edge O
type O
corresponds O
to O
an O
individual O
kind O
of O
information O
aggregation O
on O
the O
graph. O
Mathematically, O
the O
I2S-SLG B-MethodName
can O
be O
denoted O
as O
G O
i2s O
= O
(V O
i2s O
, O
E O
i2s O
, O
A O
i2s O
, O
R O
i2s O
). O
Each O
node O
v O
i2s O
and O
each O
edge O
e O
i2s O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
i2s O
) O
and O
ϕ(e O
i2s O
). O
The O
connections O
in O
I2S-SLG B-MethodName
are O
a O
little O
different O
from O
S2I-SLG B-MethodName
. O
Since O
intents O
are O
sentence-level, O
each O
IL O
node O
is O
globally O
connected O
with O
all O
nodes. O
For O
S O
i O
node, O
it O
is O
connected O
to O
{S O
i−w O
, O
..., O
S O
i+w O
} O
and O
{IL O
1 O
, O
..., O
IL O
m O
}, O
where O
w O
is O
the O
local O
window O
size O
and O
m O
is O
the O
number O
of O
estimated O
intents. O
Shared O
Self-Attentive O
Encoder O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
adopt O
a O
shared O
self-attentive O
encoder O
to O
produce O
the O
initial O
hidden O
states O
containing O
the O
basic O
semantics. O
It O
includes O
a O
BiLSTM O
and O
a O
self-attention O
module. O
BiLSTM O
captures O
the O
temporal O
dependencies: O
hi O
= O
BiLSTM O
xi, O
hi−1, O
hi+1(1) O
where O
x O
i O
is O
the O
word O
vector O
of O
u O
i O
. O
Now O
we O
obtain O
the O
context-sensitive O
hidden O
statesĤ O
= O
{ĥ O
i O
} O
n O
1 O
. O
Self-attention O
captures O
the O
global O
dependencies: O
H O
′ O
= O
softmax O
QK O
⊤ O
√ O
d O
k O
V O
(2) O
where O
H O
′ O
is O
the O
global O
contextual O
hidden O
states O
output O
by O
self-attention; O
Q, O
K O
and O
V O
are O
matrices O
obtained O
by O
applying O
different O
linear O
projections O
on O
the O
input O
utterance O
word O
vector O
matrix. O
Then O
we O
concatenate O
the O
output O
of O
BiLSTM O
and O
self-attention O
to O
form O
the O
output O
of O
the O
shared O
selfattentive O
encoder: O
H O
=Ĥ∥H O
′ O
, O
where O
H O
= O
{h O
i O
} O
n O
1 O
and O
∥ O
denotes O
concatenation O
operation. O
Initial O
Estimation O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
To O
obtain O
the O
taskspecific O
features O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
apply O
a O
BiLSTM O
layer O
over O
H: O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
conduct O
token-level O
multi-intent B-TaskName
detection I-TaskName
. O
Each O
h O
[I,0] O
i O
is O
fed O
into O
the O
intent O
decoder. O
Specifically, O
the O
intent O
label O
distributions O
of O
the O
i-th O
word O
are O
obtained O
by: O
h O
[I,0] O
i O
= O
BiLSTM O
I O
hi, O
h O
[I,0] O
i−1 O
, O
h O
[I,0] O
i+1 O
(3) O
Shared O
Self- O
y O
[I,0] O
i O
= O
sigmoid O
W O
1 O
I O
σ(W O
2 O
I O
h O
[I,0] O
i O
+b O
2 O
I O
) O
+b O
1 O
I O
(4) O
where O
σ O
denotes O
the O
non-linear O
activation O
function; O
W O
* O
and O
b O
* O
are O
model O
parameters. O
Then O
the O
estimated O
sentence-level O
intent O
labels O
{IL O
1 O
, O
..., O
IL O
m O
} O
are O
obtained O
by O
the O
token-level O
intent O
voting O
(Qin O
et O
al., O
2021b). O
Slot B-TaskName
Filling I-TaskName
(Qin O
et O
al., O
2021b) O
propose O
a O
nonautoregressive O
paradigm O
for O
slot B-TaskName
filling I-TaskName
decoding, O
which O
achieves O
significant O
speedup. O
In O
this O
paper, O
we O
also O
conduct O
parallel O
slot O
filling O
decoding. O
We O
first O
apply O
a O
BiLSTM O
over O
H O
to O
obtain O
the O
task-specific O
features O
for O
slot O
filling: O
h O
[S,0] O
i O
= O
BiLSTM O
S O
(hi, O
h O
[S,0] O
i−1 O
, O
h O
[S,0] O
i+1 O
)(5) O
Then O
use O
a O
softmax O
classifier O
to O
generate O
the O
slot O
label O
distribution O
for O
each O
word: O
y O
[S,0] O
i O
= O
softmax O
W O
1 O
S O
σ(W O
2 O
S O
h O
[S,0] O
i O
+b O
2 O
S O
) O
+b O
1 O
S O
(6) O
And O
the O
estimated O
slot O
label O
for O
each O
word O
is O
obtained O
by O
SL O
i O
= O
arg O
max(y O
[S,0] O
i O
). O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
State-of-the-art O
models O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b) O
use O
a O
homogeneous O
graph O
to O
connect O
the O
semantic O
nodes O
of O
slot O
filling O
and O
the O
intent O
label O
nodes. O
And O
GAT B-MethodName
(Velickovic O
et O
al., O
2018) O
is O
adopted O
to O
achieve O
information O
aggregation. O
In O
Sec. O
1, O
we O
propose O
that O
this O
manner O
cannot O
effectively O
learn O
the O
interactions O
between O
one O
task's O
semantics O
and O
the O
estimated O
labels O
of O
the O
other O
task. O
To O
tackle O
this O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
graphs I-MethodName
( O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
) O
to O
effectively O
represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes. O
To O
model O
the O
interactions O
between O
semantics O
and O
labels O
on O
the O
proposed O
graphs, O
we O
propose O
a O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
( O
HGAT B-MethodName
). O
When O
aggregating O
the O
information O
into O
a O
node, O
HGAT B-MethodName
can O
discriminate O
the O
specific O
information O
from O
different O
types O
of O
nodes O
along O
different O
relations. O
And O
two O
HGATs B-MethodName
( O
S2I-HGAT B-MethodName
and O
I2S-HGAT B-MethodName
) O
are O
applied O
on O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
, O
respectively. O
Specifically, O
S2I-HGAT B-MethodName
can O
be O
formulated O
as O
follows: O
h O
l+1 O
i O
= O
K O
∥ O
k=1 O
σ O
 O
 O
j∈N O
i O
s2i O
W O
[r,k,1] O
s2i O
α O
[r,k] O
ij O
h O
l O
j O
 O
 O
, O
r O
= O
ϕ O
e O
[j,i] O
s2i O
α O
[r,k] O
ij O
= O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
j O
T O
/ O
√ O
d O
u∈N O
r,i O
s2i O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
u O
T O
/ O
√ O
d O
(7) O
where O
K O
denotes O
the O
total O
head O
number; O
N O
i O
s2i O
denotes O
the O
set O
of O
incoming O
neighbors O
of O
node O
i O
on O
S2I-SLG B-MethodName
; O
W O
[r,k, O
* O
] O
s2i O
are O
weight O
matrices O
of O
edge O
type O
r O
on O
the O
k-th O
head; O
e O
[j,i] O
s2i O
denotes O
the O
edge O
from O
node O
j O
to O
node O
i O
on O
S2I-SLG B-MethodName
; O
N O
r,i O
s2i O
denotes O
the O
nodes O
connected O
to O
node O
i O
with O
r-type O
edges O
on O
S2I-SLG B-MethodName
; O
d O
is O
the O
dimension O
of O
node O
hidden O
state. O
I2S-HGAT B-MethodName
can O
be O
derived O
like O
Eq. O
7. O
Intent O
Decoding O
with O
Slot O
Guidance O
In O
the O
first O
stage, O
we O
obtain O
the O
initial O
intent O
features O
H O
[I,0] O
= O
{h O
I,0 O
i O
} O
n O
i O
and O
the O
initial O
estimated O
slot O
labels O
sequence O
{SL O
1 O
, O
..., O
SL O
n O
}. O
Now O
we O
project O
the O
slot O
labels O
into O
vector O
form O
using O
the O
slot O
label O
embedding O
matrix, O
obtaining O
E O
sl O
= O
{e O
1 O
sl O
, O
..., O
e O
n O
sl O
}. O
Then O
we O
feed O
H O
[I,0] O
and O
E O
sl O
into O
S2I-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
slot O
label O
information O
to O
guide O
the O
intent O
decoding: O
where O
[H O
[I,0] O
, O
E O
sl O
] O
denotes O
the O
input O
node O
representation; O
θ O
I O
denotes O
S2I-HGAT B-MethodName
's O
parameters. O
L O
denotes O
the O
total O
layer O
number. O
Finally, O
H O
[I,L] O
is O
fed O
to O
intent O
decoder, O
producing O
the O
intent O
label O
distributions O
for O
the O
utterance O
words: O
Y O
[I,1] O
= O
{y O
[I,1] O
i O
, O
..., O
y O
[I,1] O
n O
}. O
And O
the O
final O
output O
sentence-level O
intents O
are O
obtained O
via O
applying O
token-level O
intent O
voting O
over O
Y O
[I,1] O
. O
Slot O
Decoding O
with O
Intent O
Guidance O
Intent-aware O
BiLSTM O
Since O
the O
B-I-O O
tags O
of O
slot O
labels O
have O
temporal O
dependencies, O
we O
use O
an O
intent-aware O
BiLSTM O
to O
model O
the O
temporal O
dependencies O
among O
slot O
hidden O
states O
with O
the O
guidance O
of O
estimated O
intents: O
h O
[S,0] O
i O
= O
BiLSTM(y O
[I,0] O
i O
∥h O
[S,0] O
i O
,h O
[S,0] O
i−1 O
,h O
[S,0] O
i+1 O
)(9) O
I2S-HGAT B-MethodName
We O
first O
project O
the O
estimated O
intent O
labels O
{IL O
j O
} O
m O
1 O
into O
vectors O
using O
the O
intent O
label O
embedding O
matrix, O
obtaining O
E O
il O
= O
{e O
1 O
il O
, O
..., O
e O
m O
il O
}. O
Then O
we O
feedH O
S O
and O
E O
il O
into O
I2S-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
intent O
label O
information O
to O
guide O
the O
slot O
decoding: O
H O
[S,L] O
= O
I2S-HGAT B-MethodName
[H O
S O
, O
E O
il O
], O
Gi2s, O
θS(10) O
where O
[H O
[S] O
, O
E O
il O
] O
denotes O
the O
input O
node O
representation; O
θ O
S O
denotes O
I2S-HGAT B-MethodName
's O
parameters. O
Finally, O
H O
[S,L] O
is O
fed O
to O
slot O
decoder, O
producing O
the O
slot O
label O
distributions O
for O
each O
word: O
Y O
[S,1] O
= O
{y O
[S,1] O
i O
, O
..., O
y O
[S,1] O
n O
}. O
And O
the O
final O
output O
slot O
labels O
are O
obtained O
by O
applying O
arg O
max O
over O
Y O
[S,1] O
. O
Training O
Objective O
Loss O
Function O
The O
loss O
function O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
is: O
CE(ŷ, O
y) O
=ŷ O
log(y) O
+ O
(1 O
−ŷ) O
log(1 O
− O
y) O
LI O
= O
1 O
t=0 O
n O
i=1 O
N O
I O
j=1 O
CE O
ŷ O
I O
i O
[j], O
y O
[I,t] O
i O
[j](11) O
And O
the O
loss O
function O
for O
slot B-TaskName
filling I-TaskName
is: O
LS O
= O
1 O
t=0 O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
log O
y O
[S,t] O
i O
[j](12) O
where O
N O
I O
and O
N O
S O
denote O
the O
total O
numbers O
of O
intent O
labels O
and O
slot O
labels;ŷ O
I O
i O
andŷ O
S O
i O
denote O
the O
ground-truth O
intent O
labels O
and O
slot O
labels. O
Margin O
Penalty O
The O
core O
of O
our O
model O
is O
to O
let O
the O
two O
tasks O
mutually O
guide O
each O
other. O
Intuitively, O
the O
predictions O
in O
the O
second O
stage O
should O
be O
better O
than O
those O
in O
the O
first O
stage. O
To O
force O
our O
model O
obey O
this O
rule, O
we O
design O
a O
margin O
penalty O
(L O
mp O
) O
for O
each O
task, O
whose O
aim O
is O
to O
improve O
the O
probabilities O
of O
the O
correct O
labels. O
Specifically, O
the O
formulations O
of O
L O
mp O
I O
and O
L O
mp O
S O
are: O
L O
mp O
I O
= O
n O
i=1 O
N O
I O
j=1ŷ O
I O
i O
[j] O
max O
0, O
y O
[I,0] O
i O
[j] O
− O
y O
[I,1] O
i O
[j] O
L O
mp O
S O
= O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
max O
0, O
y O
[S,0] O
i O
[j] O
− O
y O
[S,1] O
i O
[j](13) O
Model O
Training O
The O
training O
objective O
L O
is O
the O
weighted O
sum O
of O
loss O
functions O
and O
margin O
regularizations O
of O
the O
two O
tasks: O
L O
= O
γ O
(LI O
+ O
βI O
L O
mp O
I O
) O
+ O
(1 O
− O
γ) O
(LS O
+ O
βSL O
mp O
S O
) O
(14) O
where O
γ O
is O
the O
coefficient O
balancing O
the O
two O
tasks; O
β O
I O
and O
β O
S O
are O
the O
coefficients O
of O
the O
margin O
regularization O
for O
the O
two O
tasks. O
3 O
Experiments O
Datasets O
and O
Metrics O
Following O
previous O
works, O
MixATIS B-DatasetName
and O
MixS-NIPS B-DatasetName
(Hemphill O
et O
al., O
1990;Coucke O
et O
al., O
2018;Qin O
et O
al., O
2020) O
As O
for O
evaluation O
metrics, O
following O
previous O
works, O
we O
adopt O
accuracy B-MetricName
( O
Acc B-MetricName
) O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
F1 B-MetricName
score O
for O
slot B-TaskName
filling I-TaskName
, O
and O
overall B-MetricName
accuracy I-MetricName
for O
the O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
. O
Overall O
accuracy O
denotes O
the O
ratio O
of O
sentences O
whose O
intents O
and O
slots O
are O
all O
correctly O
predicted. O
Implementation O
Details O
Following O
previous O
works, O
the O
word O
and O
label O
embeddings O
are O
trained O
from O
scratch O
2 O
. O
The O
dimensions O
of O
word B-HyperparameterName
embedding I-HyperparameterName
, O
label B-HyperparameterName
embedding I-HyperparameterName
, O
and O
hidden B-HyperparameterName
state I-HyperparameterName
are O
256 B-HyperparameterValue
on O
MixATIS B-DatasetName
, O
while O
on O
MixS-NIPS B-DatasetName
they O
are O
256 B-HyperparameterValue
, O
128 B-HyperparameterValue
, O
and O
256 B-HyperparameterValue
. O
The O
layer B-HyperparameterName
number I-HyperparameterName
of O
all O
GNNs O
is O
2 B-HyperparameterValue
. O
Adam O
(Kingma O
and O
Ba, O
2015) O
is O
used O
to O
train O
our O
model O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e B-HyperparameterValue
−3 I-HyperparameterValue
and O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e B-HyperparameterValue
−6 I-HyperparameterValue
. O
As O
for O
the O
coefficients O
Eq.14, O
γ B-HyperparameterName
is O
0.9 B-HyperparameterValue
on O
MixATIS B-DatasetName
and O
0.8 B-HyperparameterValue
on O
MixSNIPS B-DatasetName
; O
on O
both O
datasets, O
β B-HyperparameterName
I I-HyperparameterName
is O
1e B-HyperparameterValue
−6 I-HyperparameterValue
and O
β B-HyperparameterName
S I-HyperparameterName
is O
1e B-HyperparameterValue
0 I-HyperparameterValue
. O
The O
model O
performing O
best O
on O
the O
dev O
set O
is O
selected O
then O
we O
report O
its O
results O
on O
the O
test O
set. O
All O
experiments O
are O
conducted O
on O
RTX O
6000. O
Our O
source O
code O
will O
be O
released. O
Main O
Results O
The O
performance O
comparison O
of O
Co-guiding B-MethodName
Net I-MethodName
and O
baselines O
are O
shown O
in O
Table O
1, O
from O
which O
we O
have O
the O
following O
observations: O
(1) O
Co-guiding O
Net O
gains O
significant O
and O
consistent O
improvements O
on O
all O
tasks O
and O
datasets. O
Specifically, O
on O
MixATIS B-DatasetName
dataset, O
it O
overpasses O
the O
previous O
state-of-the-art O
model O
GL-GIN B-MethodName
by O
19.3% B-MetricValue
, O
1.8% B-MetricValue
, O
and O
3.7% B-MetricValue
on O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
, O
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively; O
on O
MixSNIPS B-DatasetName
dataset, O
it O
overpasses O
GL-GIN B-MethodName
by O
5.2% B-MetricValue
, O
1.2% B-MetricValue
and O
2.1% B-MetricValue
on O
sentencelevel B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively. O
This O
is O
because O
our O
model O
achieves O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
allowing O
the O
two O
tasks O
to O
provide O
crucial O
clues O
for O
each O
other. O
Besides, O
our O
designed O
HSLGs B-MethodName
and O
HGATs B-MethodName
can O
effectively O
model O
the O
interactions O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
extracting O
the O
indicative O
clues O
from O
initial O
predictions. O
(2) O
Co-guiding B-MethodName
Net I-MethodName
achieves O
a O
larger O
improvement O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
than O
slot B-TaskName
filling I-TaskName
. O
The O
reason O
is O
that O
except O
for O
the O
guidance O
from O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
to O
slot B-TaskName
filling I-TaskName
, O
our O
model O
also O
achieves O
the O
guidance O
from O
slot B-TaskName
filling I-TaskName
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
while O
previous O
models O
all O
ignore O
this. O
Besides, O
previous O
methods O
model O
the O
semantics-label O
interactions O
by O
homogeneous B-MethodName
graph I-MethodName
and O
GAT B-MethodName
, O
limiting O
the O
performance. O
Differently, O
our O
model O
uses O
the O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
to O
represent O
different O
relations O
among O
the O
semantic O
nodes O
and O
the O
label O
nodes, O
then O
applies O
the O
proposed O
HGATs B-MethodName
over O
the O
graphs O
to O
achieve O
the O
interactions. O
Consequently, O
their O
performances O
(especially O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
) O
are O
significantly O
inferior O
to O
our O
model. O
(3) O
The O
improvements O
in O
overall B-MetricName
accuracy I-MetricName
are O
much O
sharper. O
We O
suppose O
the O
reason O
is O
that O
the O
achieved O
mutual O
guidances O
make O
the O
two O
tasks O
deeply O
coupled O
and O
allow O
them O
to O
stimulate O
each O
other O
using O
their O
initial O
predictions. O
For O
each O
task, O
its O
final O
outputs O
are O
guided O
by O
its O
and O
another O
task's O
initial O
predictions. O
By O
this O
means, O
the O
correct O
predictions O
of O
the O
two O
tasks O
can O
be O
better O
aligned. O
As O
a O
result, O
more O
test O
samples O
get O
correct O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
results, O
and O
then O
overall B-MetricName
accuracy I-MetricName
is O
boosted. O
Model O
Analysis O
We O
conduct O
a O
set O
of O
ablation O
experiments O
to O
verify O
the O
advantages O
of O
our O
work O
from O
different O
perspectives, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
Effect O
of O
Slot-to-Intent O
Guidance O
One O
of O
the O
core O
contributions O
of O
our O
work O
is O
achieving O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
while O
previous O
works O
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot. O
Therefore, O
compared O
with O
previous O
works, O
one O
of O
the O
advantages O
of O
our O
work O
is O
modeling O
the O
slot-tointent O
guidance. O
To O
verify O
this, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
S2I-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O
can O
observe O
that O
Intent B-MetricName
Acc I-MetricName
drops O
by O
2.0% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.8% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
Moreover, O
Overall B-MetricName
Acc I-MetricName
drops O
more O
significantly: O
3.6% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.9% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
This O
proves O
that O
the O
guidance O
from O
slot O
to O
intent O
can O
effectively O
benefit O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
achieving O
the O
mutual O
guidances O
between O
the O
two O
tasks O
can O
significantly O
improve O
Overall B-MetricName
Acc I-MetricName
. O
Besides, O
although O
both O
of O
w/o B-MetricName
S2I-guidance I-MetricName
and O
GL-GIN B-MethodName
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot, O
w/o B-MetricName
S2I-guidance I-MetricName
outperforms O
GL-GIN B-MethodName
by O
large O
margins. O
We O
attribute O
this O
to O
our O
proposed O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
and O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
, O
whose O
advantages O
are O
verified O
in O
Sec. O
3.4.3. O
Effect O
of O
Intent-to-Slot O
Guidance O
To O
verify O
the O
effectiveness O
of O
intent-to-slot O
guidance, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
I2S-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O
can O
find O
that O
the O
intent-to-slot O
guidance O
has O
a O
significant O
impact O
on O
performance. O
Specifically, O
w/o B-MetricName
I2S-guidance I-MetricName
cause O
nearly O
the O
same O
extent O
of O
performance O
drop O
on O
Overall B-MetricName
Acc I-MetricName
, O
proving O
that O
both O
of O
the O
intent-toslot O
guidance O
and O
slot-to-intent O
guidance O
are O
indispensable O
and O
achieving O
the O
mutual O
guidances O
can O
significantly O
boost O
the O
performance. O
Effect O
of O
HSLGs B-MethodName
and O
HGATs B-MethodName
In O
this O
paper, O
we O
design O
two O
HSLGs B-MethodName
: O
(i.e., O
S2I-SLG B-MethodName
, O
I2S-SLG B-MethodName
) O
and O
two O
HGATs B-MethodName
(i.e., O
S2I-HGAT B-MethodName
, O
I2S-HGAT B-MethodName
). O
To O
verify O
their O
effectiveness, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
relations I-MetricName
by O
removing O
the O
relations O
on O
the O
two O
HSLGs B-MethodName
. O
In O
this O
case, O
S2I-SLG B-MethodName
/ O
I2S-SLG B-MethodName
collapses O
to O
a O
homogeneous B-MethodName
graph I-MethodName
, O
and O
S2I-HGAT B-MethodName
/ O
I2S-HGAT B-MethodName
collapses O
to O
a O
general O
GAT B-MethodName
based O
on O
multi-head O
attentions. O
From O
Table O
2, O
we O
can O
observe O
that O
w/o B-MetricName
relations I-MetricName
obtains O
dramatic O
drops O
on O
all O
metrics O
on O
both O
datasets. O
The O
apparent O
performance O
gap O
between O
w/o B-MetricName
relations I-MetricName
and O
Co-guiding B-MethodName
Net I-MethodName
verifies O
that O
(1) O
our O
proposed O
HSLGs B-MethodName
can O
effectively O
represent O
the O
different O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
providing O
appropriate O
platforms O
for O
modeling O
the O
mutual O
guidances O
between O
the O
two O
tasks; O
(2) O
our O
proposed O
HGATs B-MethodName
can O
sufficiently O
and O
effectively O
model O
interactions O
between O
the O
semantics O
and O
indicative O
label O
information O
via O
achieving O
the O
relation-specific O
attentive O
information O
aggregation O
on O
the O
HSLGs B-MethodName
. O
Besides, O
although O
w/o B-MetricName
relations I-MetricName
obviously O
un-derperforms O
Co-guiding B-MethodName
Ne I-MethodName
t, O
it O
still O
significantly O
outperforms O
all O
baselines. O
We O
attribute O
this O
to O
the O
fact O
that O
our O
model O
achieves O
the O
mutual O
guidances O
between O
the O
two O
tasks, O
which O
allows O
them O
to O
promote O
each O
other O
via O
cross-task O
correlations. O
et O
al. O
(2021b) O
propose O
a O
Local O
Slot-aware O
GAT O
module O
to O
alleviate O
the O
uncoordinated O
slot O
problem O
(e.g., O
B-singer O
followed O
by O
I-song) O
(Wu O
et O
al., O
2020) O
caused O
by O
the O
non-autoregressive O
fashion O
of O
slot O
filling. O
And O
the O
ablation O
study O
in O
(Qin O
et O
al., O
2021b) O
proves O
that O
this O
module O
effectively O
improves O
the O
slot O
filling O
performance O
by O
modeling O
the O
local O
dependencies O
among O
slot O
hidden O
states. O
Effect O
of O
I2S-HGAT B-MethodName
for O
Capturing O
Local O
Slot O
Dependencies O
Qin O
In O
their O
model O
( O
GL-GIN B-MethodName
), O
the O
local O
dependencies O
are O
modeled O
in O
both O
of O
the O
local B-MethodName
slot-aware I-MethodName
GAT I-MethodName
and O
subsequent O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
. O
We O
suppose O
the O
reason O
why O
GL-GIN B-MethodName
needs O
the O
local B-MethodName
Slotaware I-MethodName
GAT I-MethodName
is O
that O
the O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
in O
GL-GIN B-MethodName
cannot O
effectively O
capture O
the O
local O
slot O
dependencies. O
GL-GIN's B-MethodName
global B-MethodName
slot-intent I-MethodName
graph I-MethodName
is O
homogeneous, O
and O
the O
GAT B-MethodName
working O
on O
it O
treats O
the O
slot O
semantics O
nods O
and O
the O
intent O
label O
nodes O
equally O
without O
discrimination. O
Therefore, O
each O
slot O
hidden O
state O
receives O
indiscriminate O
information O
from O
both O
of O
its O
local O
slot O
hidden O
states O
and O
all O
intent O
labels, O
making O
it O
confusing O
to O
capture O
the O
local O
slot O
dependencies. O
In O
contrast, O
we O
believe O
our O
I2S-HLG B-MethodName
and O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
slot O
local O
dependencies O
along O
the O
specific O
slot_semantics_dependencies O
relation, O
which O
is O
modeled O
together O
with O
other O
relations. O
Therefore, O
our O
Co-guiding B-MethodName
Net I-MethodName
does O
not O
include O
another O
module O
to O
capture O
the O
slot O
local O
dependencies. O
To O
verify O
this, O
we O
design O
a O
variant O
termed O
+Local B-MetricName
Slot-aware I-MetricName
GAT I-MetricName
, O
which O
is O
implemented O
by O
augmenting O
Co-guiding B-MethodName
Net I-MethodName
with O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
(Qin O
et O
al., O
2021b) O
that O
not O
only O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
does O
not O
bring O
improvement, O
it O
even O
causes O
performance O
drops. O
This O
proves O
that O
our O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
local O
slot O
dependencies. O
A.1 O
Settings O
To O
evaluate O
Co-guiding B-MethodName
Net's I-MethodName
performance O
based O
on O
the O
pre-trained O
language O
model, O
we O
use O
the O
pretrained O
RoBERTa O
(Liu O
et O
al., O
2019b) O
encoder O
to O
replace O
the O
original O
self-attentive O
encoder. O
We O
adopt O
the O
pre-trained O
RoBERTa-base O
version O
provided O
by O
Transformers O
(Wolf O
et O
al., O
2020). O
For O
each O
word, O
its O
first O
subwords' O
hidden O
state O
generated O
by O
RoBERTa O
is O
taken O
as O
the O
word O
representation. O
AdamW O
(Loshchilov O
and O
Hutter, O
2019) O
optimizer O
is O
used O
for O
model O
training O
with O
the O
default O
setting, O
and O
RoBERTa O
is O
fine-tuned O
with O
model O
training. O
Other O
model O
components O
are O
identical O
to O
the O
Coguiding B-MethodName
Net I-MethodName
based O
on O
LSTM, O
and O
we O
use O
the O
same O
hyper-parameters O
of O
the O
model O
rather O
than O
search O
for O
the O
optimal O
ones O
for O
RoBERTa+Co-guiding O
Net O
due O
to O
our O
limited O
computation O
resource. O
A.2 O
Results O
Table O
3 O
shows O
the O
result O
comparison O
of O
Coguiding B-MethodName
Net I-MethodName
, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
, O
and O
their O
state-of-the-art O
counterparts: O
AGIF B-MethodName
, O
GL-GIN B-MethodName
, O
RoBERTa+AGIF B-MethodName
, O
and O
RoBERTa+GL-GIN B-MethodName
. O
We O
can O
find O
that O
although O
RoBERTa O
boosts O
the O
models' O
performance, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
 I-MethodName
still O
significantly O
outperforms O
RoBERTa+AGIF B-MethodName
and O
RoBERTa+GL-GIN B-MethodName
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
although O
the O
pre-trained O
language O
model O
(PTLM) O
can O
enhance O
the O
word O
representations, O
it O
cannot O
achieve O
the O
guidance O
between O
the O
two O
tasks O
or O
the O
interactions O
between O
the O
semantics O
and O
label O
information, O
which O
are O
exactly O
the O
advantages O
of O
our O
Co-guiding B-MethodName
Net I-MethodName
. O
Therefore, O
collaborating O
with O
PTLM O
that O
has O
strong O
ability O
of O
language O
modeling, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
gets O
its O
performance O
further O
boosted, O
achieving O
new O
state-of-the-art. O


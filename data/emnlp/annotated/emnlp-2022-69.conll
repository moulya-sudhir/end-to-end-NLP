Multilingual B-TaskName
Relation I-TaskName
Classification I-TaskName
via O
Efficient O
and O
Effective O
Prompting O
Prompting O
pre-trained O
language O
models O
has O
achieved O
impressive O
performance O
on O
various O
NLP O
tasks, O
especially O
in O
low O
data O
regimes. O
Despite O
the O
success O
of O
prompting O
in O
monolingual O
settings, O
applying O
prompt-based O
methods O
in O
multilingual O
scenarios O
has O
been O
limited O
to O
a O
narrow O
set O
of O
tasks, O
due O
to O
the O
high O
cost O
of O
handcrafting O
multilingual O
prompts. O
In O
this O
paper, O
we O
present O
the O
first O
work O
on O
prompt-based O
multilingual B-TaskName
relation I-TaskName
classification I-TaskName
( O
RC B-TaskName
), O
by O
introducing O
an O
efficient O
and O
effective O
method O
that O
constructs O
prompts O
from O
relation O
triples O
and O
involves O
only O
minimal O
translation O
for O
the O
class O
labels. O
We O
evaluate O
its O
performance O
in O
fully O
supervised, O
few-shot O
and O
zero-shot O
scenarios, O
and O
analyze O
its O
effectiveness O
across O
14 O
languages, O
prompt O
variants, O
and O
English-task O
training O
in O
cross-lingual O
settings. O
We O
find O
that O
in O
both O
fully O
supervised O
and O
few-shot O
scenarios, O
our O
prompt O
method O
beats O
competitive O
baselines: O
fine-tuning O
XLM-R B-MethodName
EM I-MethodName
and O
null O
prompts. O
It O
also O
outperforms O
the O
random O
baseline O
by O
a O
large O
margin O
in O
zero-shot O
experiments. O
Our O
method O
requires O
little O
in-language O
knowledge O
and O
can O
be O
used O
as O
a O
strong O
baseline O
for O
similar O
multilingual O
classification O
tasks. O
Introduction O
Relation B-TaskName
classification I-TaskName
( O
RC B-TaskName
) O
is O
a O
crucial O
task O
in O
information O
extraction O
(IE), O
aiming O
to O
identify O
the O
relation O
between O
entities O
in O
a O
text O
(Alt O
et O
al., O
2019). O
Extending O
RC B-TaskName
to O
multilingual O
settings O
has O
recently O
received O
increased O
interest O
(Zou O
et O
al., O
2018;Kolluru O
et O
al., O
2022), O
but O
the O
majority O
of O
prior O
work O
still O
focuses O
on O
English O
(Baldini O
Soares O
et O
al., O
2019;Lyu O
and O
Chen, O
2021). O
A O
main O
bottleneck O
for O
multilingual B-TaskName
RC I-TaskName
is O
the O
lack O
of O
supervised O
resources, O
comparable O
in O
size O
to O
large O
English O
datasets O
(Riedel O
et O
al., O
2010;Zhang O
et O
al., O
2017). O
The O
SMiLER B-DatasetName
dataset O
(Seganti O
et O
al., O
2021) O
provides O
a O
starting O
point O
to O
test O
fully O
supervised O
and O
more O
efficient O
approaches O
due O
to O
different O
resource O
availability O
for O
different O
languages. O
Previous O
studies O
have O
shown O
the O
promising O
performance O
of O
prompting O
PLMs O
compared O
to O
the O
datahungry O
fine-tuning, O
especially O
in O
low-resource O
scenarios O
(Gao O
et O
al., O
2021;Le O
Scao O
and O
Rush, O
2021;. O
Multilingual O
pre-trained O
language O
models O
(Conneau O
et O
al., O
2020;Xue O
et O
al., O
2021) O
further O
enable O
multiple O
languages O
to O
be O
represented O
in O
a O
shared O
semantic O
space, O
thus O
making O
prompting O
in O
multilingual O
scenarios O
feasible. O
However, O
the O
study O
of O
prompting O
for O
multilingual O
tasks O
so O
far O
remains O
limited O
to O
a O
small O
range O
of O
tasks O
such O
as O
text O
classification O
(Winata O
et O
al., O
2021) O
and O
natural O
language O
inference O
(Lin O
et O
al., O
2022). O
To O
our O
knowledge, O
the O
effectiveness O
of O
prompt-based O
methods O
for O
multilingual B-TaskName
RC I-TaskName
is O
still O
unexplored. O
To O
analyse O
this O
gap, O
we O
pose O
two O
research O
questions O
for O
multilingual B-TaskName
RC I-TaskName
with O
prompts: O
RQ1. O
What O
is O
the O
most O
effective O
way O
to O
prompt? O
We O
investigate O
whether O
prompting O
should O
be O
done O
in O
English O
or O
the O
target O
language O
and O
whether O
to O
use O
soft O
prompt O
tokens. O
RQ2. O
How O
well O
do O
prompts O
perform O
in O
different O
data O
regimes O
and O
languages? O
We O
investigate O
the O
effectiveness O
of O
our O
prompting O
approach O
in O
three O
scenarios: O
fully O
supervised, O
few-shot O
and O
zero-shot. O
We O
explore O
to O
what O
extent O
the O
results O
are O
related O
to O
the O
available O
language O
resources. O
We O
present O
an O
efficient O
and O
effective O
prompt O
method O
for O
multilingual B-TaskName
RC I-TaskName
(see O
Figure O
1) O
that O
derives O
prompts O
from O
relation O
triplets O
(see O
Section O
3.1). O
The O
derived O
prompts O
include O
the O
original O
sentence O
and O
entities O
and O
are O
supposed O
to O
be O
filled O
with O
the O
relation O
label. O
We O
evaluate O
the O
prompts O
with O
three O
variants, O
two O
of O
which O
require O
no O
translation, O
and O
one O
of O
which O
requires O
minimal O
translation, O
i.e., O
of O
the O
relation O
labels O
only. O
We O
find O
that O
our O
method O
outperforms O
fine-tuning O
and O
a O
strong O
taskagnostic O
prompt O
baseline O
in O
fully O
supervised O
and O
few-shot O
scenarios, O
especially O
for O
relatively O
lowresource O
languages. O
Our O
method O
also O
improves O
over O
the O
random O
baseline O
in O
zero-shot O
settings, O
and O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
_______ O
Cameron O
. O
Goethe O
schrieb O
die O
Tragödie O
Faust O
. O
Relation B-TaskName
Classification I-TaskName
Task O
Definition O
Relation B-TaskName
classification I-TaskName
is O
the O
task O
of O
classifying O
the O
relationship O
such O
as O
date_of_birth, O
founded_by O
or O
parents O
between O
pairs O
of O
entities O
in O
a O
given O
context. O
Formally, O
given O
a O
relation O
set O
R O
and O
a O
text O
x O
= O
[x O
1 O
, O
x O
2 O
, O
. O
. O
. O
, O
x O
n O
] O
(where O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
are O
tokens) O
with O
two O
disjoint O
spans O
e O
h O
and O
e O
t O
denoting O
the O
head O
and O
tail O
entity, O
RC B-TaskName
aims O
to O
predict O
the O
relation O
r O
∈ O
R O
between O
e O
h O
and O
e O
t O
, O
or O
give O
a O
no_relation O
prediction O
if O
no O
relation O
in O
R O
holds. O
RC B-TaskName
is O
a O
multilingual O
task O
if O
the O
token O
sequences O
come O
from O
different O
languages. O
Fine-tuning O
for O
Relation B-TaskName
Classification I-TaskName
In O
fine-tuning, O
a O
task-specific O
linear O
classifier O
is O
added O
on O
top O
of O
the O
PLM. O
Fine-tuning O
hence O
introduces O
a O
different O
scenario O
from O
pre-training, O
since O
language O
model O
(LM) O
pre-training O
is O
usually O
formalized O
as O
a O
cloze-style O
task O
to O
predict O
target O
tokens O
at O
[MASK] O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019) O
or O
a O
corrupted O
span O
(Raffel O
et O
al., O
2020;Lewis O
et O
al., O
2020). O
For O
the O
RC B-TaskName
task, O
the O
classifier O
aims O
to O
predict O
the O
target O
class O
r O
at O
[CLS] O
or O
at O
the O
entity O
spans O
denoted O
by O
MARKER O
(Baldini O
Soares O
et O
al., O
2019). O
Prompting O
for O
Relation B-TaskName
Classification I-TaskName
Prompting O
is O
proposed O
to O
bridge O
the O
gap O
between O
pre-training O
and O
fine-tuning O
. O
The O
essence O
of O
prompting O
is, O
by O
appending O
extra O
text O
to O
the O
original O
text O
according O
to O
a O
task-specific O
template O
T O
(•), O
to O
reformulate O
the O
downstream O
task O
to O
an O
LM O
pre-training O
task O
such O
as O
masked O
language O
modeling O
(MLM), O
and O
apply O
the O
same O
training O
objective O
during O
the O
task-specific O
training. O
For O
the O
RC B-TaskName
task, O
to O
identify O
the O
relation O
between O
"Angela O
Merkel" O
and O
"Joachim O
Sauer" O
in O
the O
text O
"Angela O
Merkel's O
current O
husband O
is O
quantum O
chemist O
Joachim O
Sauer," O
an O
intuitive O
template O
for O
prompting O
can O
be O
"The O
relation O
between O
Angela O
Merkel O
and O
Joachim O
Sauer O
is O
[MASK]," O
and O
the O
LM O
is O
supposed O
to O
assign O
a O
higher O
likelihood O
to O
the O
term O
couple O
than O
to O
e.g. O
friends O
or O
colleagues O
at O
[MASK]. O
This O
"fill-in O
the O
blank" O
paradigm O
is O
well O
aligned O
with O
the O
pre-training O
scenario, O
and O
enables O
prompting O
to O
better O
coax O
the O
PLMs O
for O
pre-trained O
knowledge O
(Petroni O
et O
al., O
2019). O
Methods O
We O
now O
present O
our O
method, O
as O
shown O
in O
Figure O
1. O
We O
introduce O
its O
template O
and O
verbalizer, O
and O
propose O
several O
variants O
of O
the O
prompt. O
Lastly, O
we O
explain O
the O
training O
and O
inference O
process. O
Template O
For O
prompting O
, O
a O
prompt O
often O
consists O
of O
a O
template O
T O
(•) O
and O
a O
verbalizer O
V. O
Given O
a O
plain O
text O
x, O
the O
template O
T O
adds O
taskrelated O
instruction O
to O
x O
to O
yield O
the O
prompt O
input O
x O
prompt O
= O
T O
(x).(1) O
Following O
and O
Han O
et O
al. O
(2021), O
we O
treat O
relations O
as O
predicates O
and O
use O
the O
cloze O
"e O
h O
{relation} O
e O
t O
" O
for O
the O
LM O
to O
fill O
in. O
Our O
template O
is O
formulated O
as O
T O
(x) O
:= O
"x. O
e O
h O
____ O
e O
t O
". O
(2) O
In O
the O
template O
T O
(x), O
x O
is O
the O
original O
text O
and O
the O
two O
entities O
e O
h O
and O
e O
t O
come O
from O
x. O
Therefore, O
our O
template O
does O
not O
introduce O
extra O
tokens, O
thus O
involves O
no O
translation O
at O
all. O
Verbalizer O
After O
being O
prompted O
by O
x O
prompt O
, O
the O
PLM O
M O
predicts O
the O
masked O
text O
y O
at O
the O
blank. O
To O
complete O
an O
NLP O
classification O
task, O
a O
verbalizer O
ϕ O
is O
required O
to O
bridge O
the O
set O
of O
labels O
Y O
and O
the O
set O
of O
predicted O
texts O
(verbalizations O
V). O
For O
the O
simplicity O
of O
our O
prompt, O
we O
use O
the O
one-to-one O
verbalizer: O
ϕ O
: O
Y O
→ O
V, O
r O
→ O
ϕ(r),(3) O
where O
r O
is O
a O
relation, O
and O
ϕ(r) O
is O
the O
simple O
verbalization O
of O
r. O
ϕ(•) O
normally O
only O
involves O
splitting O
r O
by O
"-" O
or O
"_" O
and O
replacing O
abbreviations O
such O
as O
org O
with O
organization. O
E.g., O
the O
relation O
org-has-member O
corresponds O
to O
the O
verbalization O
"organization O
has O
member". O
Then O
the O
prediction O
is O
formalized O
as O
p(r|x) O
∝ O
p(y O
= O
ϕ(r)|x O
prompt O
; O
θ O
M O
), O
(4 O
) O
where O
θ O
M O
denotes O
the O
parameters O
of O
model O
M. O
p(r|x) O
is O
normalized O
by O
the O
likelihood O
sum O
over O
all O
relations. O
Variants O
To O
find O
the O
optimal O
way O
to O
prompt, O
we O
investigate O
three O
variants O
as O
follows. O
Hard O
prompt O
vs O
soft O
prompt O
(SP) O
Hard O
prompts O
(a.k.a. O
discrete O
prompts) O
are O
entirely O
formulated O
in O
natural O
language. O
Soft O
prompts O
(a.k.a. O
continuous O
prompts) O
consist O
of O
learnable O
tokens O
(Lester O
et O
al., O
2021) O
that O
are O
not O
contained O
in O
the O
PLM O
vocabulary. O
Following O
Han O
et O
al. O
(2021), O
we O
insert O
soft O
tokens O
before O
entities O
and O
blanks O
as O
shown O
for O
SP O
in O
Table O
1. O
Code-switch O
(CS) O
vs O
in-language O
(IL) O
Relation O
labels O
are O
in O
English O
across O
almost O
all O
RC O
datasets. O
Given O
a O
text O
from O
a O
non-English O
input O
L O
with O
a O
blank, O
the O
recovered O
text O
is O
code-mixed O
after O
being O
completed O
with O
an O
English O
verbalization, O
corresponding O
to O
code-switch O
prompting. O
It O
is O
probably O
more O
reasonable O
for O
the O
PLM O
to O
fill O
in O
the O
blank O
in O
language O
L. O
Inspired O
by O
Lin O
et O
al. O
(2022) O
(Hendrickx O
et O
al., O
2010) O
10 O
cause O
effect, O
entity O
origin, O
product O
producer, O
... O
2.50 O
0.81 O
NYT O
(Riedel O
et O
al., O
2010) O
24 O
ethnicity, O
major O
shareholder O
of, O
religion, O
... O
show O
that O
the O
label O
space O
of O
the O
RC B-TaskName
task O
is O
more O
complex O
than O
most O
few-class O
classification O
tasks. O
The O
verbalizations O
of O
RC O
datasets O
are O
listed O
in O
Appendix O
B. O
For O
SemEval B-DatasetName
, O
the O
two O
possible O
directions O
of O
a O
relation O
are O
combined. O
For O
NYT B-DatasetName
, O
we O
use O
the O
version O
from O
Zeng O
et O
al. O
(2018). O
For O
SMiLER B-DatasetName
, O
"EN" O
is O
the O
English O
split; O
"ALL" O
contains O
all O
data O
from O
14 O
languages. O
Table O
1 O
visualizes O
both O
code-switch O
(CS) O
and O
inlanguage O
(IL) O
prompting. O
For O
English, O
CS-and O
ILprompting O
are O
equivalent, O
since O
L O
is O
English O
itself. O
Word O
order O
of O
prompting O
For O
the O
RC O
task, O
head-relation-tail O
triples O
involve O
three O
elements. O
Therefore, O
deriving O
natural O
language O
prompts O
from O
them O
requires O
handling O
where O
to O
put O
the O
predicate O
(relation). O
In O
the O
case O
of O
SOV O
languages, O
filling O
in O
a O
relation O
that O
occurs O
between O
e O
h O
and O
e O
t O
seems O
less O
intuitive. O
Therefore, O
to O
investigate O
if O
the O
word O
order O
of O
prompting O
affects O
prediction O
accuracy, O
we O
swap O
the O
entities O
and O
the O
blank O
in O
the O
SVOtemplate O
"x. O
e O
h O
____ O
e O
t O
" O
and O
get O
"x. O
e O
h O
e O
t O
____" O
as O
the O
SOV-template. O
Training O
and O
Inference O
The O
training O
and O
inference O
setups O
depend O
on O
the O
employed O
model. O
Prompting O
autoencoding O
language O
models O
requires O
the O
verbalizations O
to O
be O
of O
fixed O
length, O
since O
the O
length O
of O
masks, O
which O
is O
identical O
with O
verbalization O
length, O
is O
unknown O
during O
inference. O
Encoder-decoders O
can O
handle O
verbalizations O
of O
varying O
length O
by O
nature O
. O
Han O
et O
al. O
(2021) O
adjust O
all O
the O
verbalizations O
in O
TACRED B-DatasetName
to O
a O
length O
of O
3, O
to O
enable O
prompting O
with O
RoBERTa B-MethodName
for O
RC B-TaskName
. O
We O
argue O
that O
for O
multilingual B-TaskName
RC I-TaskName
, O
this O
fix O
is O
largely O
infeasible, O
because: O
(1) O
in O
case O
of O
in-language O
prompting O
on O
SMiLER B-DatasetName
, O
the O
variance B-MetricName
of O
the O
length O
of O
the O
verbalizations O
increases O
from O
0.68 B-HyperparameterValue
to O
1.44 B-HyperparameterValue
after O
translation O
(see O
Table O
2), O
and O
surpasses O
most O
of O
listed O
monolingual O
RC O
datasets O
( O
SemEval B-DatasetName
, O
NYT B-DatasetName
and O
SCIERC B-DatasetName
), O
making O
it O
harder O
to O
unify O
the O
length; O
(2) O
manually O
adjusting O
the O
translated O
prompts O
requires O
manual O
effort O
per O
target O
language, O
making O
it O
much O
more O
expensive O
than O
adjusting O
only O
English O
verbalizations. O
Therefore, O
we O
use O
an O
encoder-decoder O
PLM O
for O
prompting O
Song O
et O
al., O
2022). O
Training O
objective O
For O
an O
encoder-decoder O
PLM O
M, O
given O
the O
prompt O
input O
T O
(x) O
and O
the O
target O
sequence O
ϕ(r) O
(i.e. O
label O
verbalization), O
we O
denote O
the O
output O
sequence O
as O
y. O
The O
probability O
of O
an O
exact-match O
decoding O
is O
calculated O
as O
follows: O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
,(5) O
where O
y O
t O
, O
ϕ O
t O
(r) O
denote O
the O
t-th O
token O
of O
y O
and O
ϕ(r), O
respectively. O
y O
<t O
denotes O
the O
decoded O
sequence O
on O
the O
left. O
θ O
represents O
the O
set O
of O
all O
the O
learnable O
parameters, O
including O
those O
of O
the O
PLM O
θ O
M O
, O
and O
those O
of O
the O
soft O
tokens O
θ O
sp O
in O
case O
of O
variant O
"soft O
prompt". O
Hence, O
the O
final O
objective O
over O
the O
training O
set O
X O
is O
to O
minimize O
the O
negative O
loglikelihood: O
argmin O
θ O
− O
1 O
|X O
| O
x∈X O
|ϕ(r)| O
t=1 O
log O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
.(6) O
Inference O
We O
collect O
the O
output O
logits O
of O
the O
decoder, O
L O
∈ O
R O
|V O
|×L O
, O
where O
|V O
| O
is O
the O
vocabulary O
size O
of O
M, O
and O
L O
is O
the O
maximum O
decode O
length. O
For O
each O
relation O
r O
∈ O
R, O
its O
score O
is O
given O
by O
: O
where O
we O
compute O
P O
by O
looking O
up O
in O
the O
t-th O
column O
of O
L O
and O
applying O
softmax O
at O
each O
time O
step O
t. O
We O
aggregate O
P O
by O
addition O
to O
encourage O
partial O
matches O
as O
well, O
instead O
of O
enforcing O
exact O
matches. O
The O
score O
is O
normalized O
by O
the O
length O
of O
verbalization O
in O
order O
to O
avoid O
predictions O
favoring O
longer O
relations. O
Finally, O
we O
select O
the O
relation O
with O
the O
highest O
score O
as O
prediction. O
score O
θ O
(r) O
:= O
1 O
|ϕ(r)| O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)), O
(7) O
Experiments O
We O
implement O
our O
experiments O
using O
the O
Hugging O
Face O
Transformers O
library O
(Wolf O
et O
al., O
2020), O
Hydra O
(Yadan, O
2019) O
and O
PyTorch O
(Paszke O
et O
al., O
2019). O
2 O
We O
use O
micro-F1 B-MetricName
as O
the O
evaluation O
metric, O
as O
the O
SMiLER B-DatasetName
paper O
(Seganti O
et O
al., O
2021) O
suggests. O
To O
measure O
the O
overall O
performance O
over O
multiple O
languages, O
we O
report O
the O
macro B-MetricName
average I-MetricName
across O
languages, O
following O
Zhao O
and O
Schütze O
(2021) O
and O
Lin O
et O
al. O
(2022). O
We O
also O
group O
the O
languages O
by O
their O
available O
resources O
in O
both O
pretraining O
and O
fine-tuning O
datasets O
for O
additional O
aggregate O
results. O
Details O
of O
the O
dataset, O
the O
models, O
and O
the O
experimental O
setups O
are O
as O
follows. O
Further O
experimental O
details O
are O
listed O
in O
Appendix O
A. O
Dataset O
We O
conduct O
an O
experimental O
evaluation O
of O
our O
multilingual O
prompt O
methods O
on O
the O
SMiLER B-DatasetName
(Seganti O
2 O
We O
make O
our O
code O
publicly O
available O
at O
https://github. O
com/DFKI-NLP/meffi-prompt O
for O
better O
reproducibility. O
Grouping O
of O
the O
languages O
We O
visualize O
the O
languages O
in O
Figure O
2 O
based O
on O
the O
sizes O
of O
RC B-TaskName
training O
data, O
but O
include O
the O
pre-training O
data O
as O
well, O
to O
give O
a O
more O
comprehensive O
overview O
of O
the O
availability O
of O
resources O
for O
each O
language. O
We O
divide O
the O
14 O
languages O
into O
4 O
groups, O
according O
to O
the O
detectable O
clusters O
in O
Figure O
2 O
and O
language O
origins. O
Model O
For O
prompting, O
we O
use O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021), O
an O
encoder-decoder O
PLM O
that O
supports O
101 O
languages, O
including O
all O
languages O
in O
SMiLER B-DatasetName
. O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021) O
has O
220M B-HyperparameterValue
parameters B-HyperparameterName
. O
XLM-R B-MethodName
EM I-MethodName
To O
provide O
a O
fine-tuning O
baseline, O
we O
re-implement O
BERT B-MethodName
EM I-MethodName
(Baldini O
Soares O
et O
al., O
2019) O
with O
the O
ENTITY O
START O
variant. O
4 O
In O
this O
method, O
the O
top-layer O
representations O
at O
the O
starts O
of O
the O
two O
entities O
are O
concatenated O
for O
linear O
classification. O
To O
adapt O
BERT B-MethodName
EM I-MethodName
to O
multilingual O
tasks, O
we O
change O
the O
PLM O
from O
BERT O
to O
a O
multilingual O
autoencoder, O
XLM-R B-MethodName
BASE I-MethodName
(Conneau O
et O
al., O
2020), O
and O
refer O
to O
this O
model O
as O
XLM-R B-MethodName
EM I-MethodName
. O
XLM-R B-MethodName
BASE I-MethodName
has O
125M B-HyperparameterValue
parameters B-HyperparameterName
. O
Null O
prompts O
(Logan O
IV O
et O
al., O
2022) O
To O
better O
verify O
the O
effectiveness O
of O
our O
method, O
we O
implement O
null O
prompts O
as O
a O
strong O
task-agnostic O
prompt O
baseline. O
Null O
prompts O
involve O
minimal O
prompt O
engineering O
by O
directly O
asking O
the O
LM O
about O
the O
relation, O
without O
giving O
any O
task O
instruction O
(see O
Table O
1). O
Logan O
IV O
et O
al. O
(2022) O
show O
that O
null O
prompts O
surprisingly O
achieve O
on-par O
performance O
with O
handcrafted O
prompts O
on O
many O
tasks. O
For O
best O
comparability, O
we O
use O
the O
same O
PLM O
mT5 B-MethodName
BASE I-MethodName
. O
Fully O
Supervised O
Setup O
We O
evaluate O
the O
performance O
of O
XLM-R B-MethodName
EM I-MethodName
, O
null O
prompts, O
and O
our O
method O
on O
each O
of O
the O
14 O
languages, O
after O
training O
on O
the O
full O
train O
split O
from O
that O
language. O
The O
prompt O
input O
and O
target O
of O
null O
prompts O
and O
our O
prompts O
are O
listed O
in O
Table O
1. O
We O
employ O
the O
randomly O
generated O
seed B-HyperparameterName
319 B-HyperparameterValue
for O
all O
the O
evaluated O
methods. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
follow O
Baldini O
Soares O
et O
al. O
(2019) O
and O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
be O
64 B-HyperparameterValue
, O
the O
optimizer O
to O
be O
Adam O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
number O
of O
epochs B-HyperparameterName
to O
be O
5 B-HyperparameterValue
. O
For O
null O
prompts O
and O
ours, O
we O
use O
AdamW O
as O
the O
optimizer O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
as O
suggest O
for O
most O
of O
the O
sequence-to-sequence O
tasks, O
the O
number O
of O
epochs B-HyperparameterName
to O
5 B-HyperparameterValue
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
. O
The O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
for O
all O
methods. O
Few-shot O
Setup O
Few-shot O
learning O
is O
normally O
cast O
as O
a O
K-shot O
problem, O
where O
K O
labelled O
examples O
per O
class O
are O
available. O
We O
follow O
and O
Han O
et O
al. O
(2021), O
and O
evaluate O
on O
8, O
16 O
and O
32 O
shots. O
The O
few-shot O
training O
set O
D O
train O
is O
generated O
by O
randomly O
sampling O
K O
instances O
per O
relation O
from O
the O
training O
split. O
The O
test O
set O
D O
test O
is O
the O
original O
test O
split O
from O
that O
language. O
We O
follow O
Gao O
et O
al. O
(2021) O
and O
sample O
another O
K-shot O
set O
from O
the O
English O
train O
split O
as O
validation O
set O
D O
val O
. O
We O
tune O
hyperparameters O
on O
D O
val O
for O
the O
English O
task, O
and O
apply O
these O
to O
all O
languages. O
We O
evaluate O
the O
same O
methods O
as O
in O
the O
fully O
supervised O
scenarios, O
but O
repeat O
5 O
runs O
as O
suggested O
in O
Gao O
et O
al. O
(2021), O
and O
report O
the O
mean O
and O
standard O
deviation O
of O
micro-F1 B-MetricName
. O
We O
use O
a O
fixed O
set O
of O
random O
seeds B-HyperparameterName
{13, B-HyperparameterValue
36, I-HyperparameterValue
121, I-HyperparameterValue
223, I-HyperparameterValue
319} I-HyperparameterValue
for O
data O
generation O
and O
training O
across O
the O
5 O
runs. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
use O
the O
same O
hyperparameters O
as O
Baldini O
Soares O
et O
al. O
( O
2019), O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
, O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
. O
For O
null O
prompts O
and O
our O
prompts, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
and O
the O
number O
of O
epochs B-HyperparameterName
to O
20 B-HyperparameterValue
. O
Zero-shot O
Setup O
We O
consider O
two O
scenarios O
for O
zero-shot O
multilingual O
relation O
classification. O
Zero-shot O
in-context O
learning O
Following O
Kojima O
et O
al. O
( O
2022), O
we O
investigate O
whether O
PLMs O
are O
also O
decent O
zero-shot O
reasoners O
for O
RC B-TaskName
. O
This O
scenario O
does O
not O
require O
any O
samples O
or O
training. O
We O
test O
the O
out-of-the-box O
performance O
of O
the O
PLM O
by O
directly O
prompting O
it O
with O
x O
prompt O
. O
Zero-shot O
in-context O
learning O
does O
not O
specify O
further O
hyperparameters O
since O
it O
is O
training-free. O
Zero-shot O
cross-lingual O
transfer O
In O
this O
scenario, O
following O
Krishnan O
et O
al. O
(2021), O
we O
finetune O
the O
model O
with O
in-language O
prompting O
on O
the O
English O
train O
split, O
and O
then O
conduct O
zero-shot O
incontext O
tests O
with O
this O
fine-tuned O
model O
on O
other O
languages O
using O
code-switch O
prompting. O
Through O
this O
setting, O
we O
want O
to O
verify O
if O
task-specific O
pretraining O
in O
a O
high-resource O
language O
such O
as O
English O
helps O
in O
other O
languages. O
In O
zero-shot O
crosslingual O
transfer, O
we O
use O
the O
same O
hyperparameters O
and O
random O
seed O
to O
fine-tune O
on O
the O
English O
task. O
Fully O
Supervised O
Results O
the O
three O
variants O
of O
our O
method O
beat O
the O
finetuning O
baseline O
XLM-R B-MethodName
EM I-MethodName
and O
the O
prompting O
baseline O
null O
prompts, O
according O
to O
the O
macroaveraged O
performance O
across O
14 O
languages. O
Inlanguage O
prompting O
delivers O
the O
most O
promising O
result, O
achieving O
an O
average O
F B-MetricName
1 I-MetricName
of O
85.0 B-MetricValue
, O
which O
is O
higher O
than O
XLM-R B-MethodName
EM I-MethodName
( O
68.2 B-MetricValue
) O
and O
null B-MethodName
prompts I-MethodName
( O
66.2 B-MetricValue
). O
The O
other O
two O
variants, O
code-switch B-MethodName
prompting I-MethodName
with O
and O
w/o B-MethodName
soft I-MethodName
tokens I-MethodName
, O
achieve O
F B-MetricName
1 I-MetricName
scores O
of O
84.1 B-MetricValue
and O
82.7 B-MetricValue
, O
respectively, O
only O
0.9 O
and O
2.3 O
lower O
than O
in-language. O
All O
three O
prompt O
variants O
are O
hence O
effective O
in O
fully O
supervised O
scenarios. O
On O
a O
per-group O
basis, O
we O
find O
that O
the O
lowerresourced O
a O
language O
is, O
the O
greater O
an O
advantage O
prompting O
enjoys O
against O
fine-tuning. O
In O
particular, O
in-language O
prompts O
shows O
better O
robustness O
compared O
to O
XLM-R B-MethodName
EM I-MethodName
in O
low-resource O
languages. O
They O
both O
yield O
95.9 B-MetricValue
- O
96.0 B-MetricValue
F B-MetricName
1 I-MetricName
scores O
for O
English, O
but O
XLM-R B-MethodName
EM I-MethodName
decreases O
to O
54.3 B-MetricValue
and O
3.7 B-MetricValue
F B-MetricName
1 I-MetricName
in O
Group-M O
and O
-L, O
while O
in-language O
prompting O
still O
delivers O
83.5 B-MetricValue
and O
65.2 B-MetricValue
F B-MetricValue
1 I-MetricValue
. O
Few-shot O
Results O
Table O
5 O
presents O
the O
per-group O
results O
in O
few-shot O
experiments. O
All O
the O
methods O
benefit O
from O
larger O
K. O
Similarly, O
in-language O
prompting O
still O
turns O
out O
to O
be O
the O
best O
contender, O
performing O
1st O
in O
8-and O
32-shot, O
and O
the O
2nd O
in O
16-shot. O
We O
see O
that O
inlanguage O
outperforms O
XLM-R B-MethodName
EM I-MethodName
in O
all O
K-shots, O
while O
code-switch B-MethodName
achieves O
comparable O
or O
even O
lower O
F B-MetricName
1 I-MetricName
to O
XLM-R B-MethodName
EM I-MethodName
for O
K O
= O
8, O
suggesting O
that O
the O
choice O
of O
prompt O
affects O
the O
few-shot O
performance O
greatly, O
thus O
needs O
careful O
consideration. O
On O
a O
per-group O
basis, O
we O
find O
that O
in-language O
prompting O
outperforms O
other O
methods O
for O
middleand O
low-resourced O
languages. O
Similar O
observations O
can O
also O
be O
drawn O
from O
fully O
supervised O
results. O
We O
conclude O
that, O
with O
sufficient O
supervision, O
inlanguage O
is O
the O
optimal O
variant O
to O
prompt O
rather O
Table O
5: O
Few-shot O
results O
by O
group O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
(Seganti O
et O
al., O
2021) O
dataset O
averaged O
over O
five O
runs. O
We O
macro-average B-MetricName
results O
for O
each O
language O
group O
(see O
Figure O
2) O
and O
over O
all O
languages O
(X). O
Inlanguage O
prompting O
performs O
best O
in O
most O
settings O
and O
language O
groups. O
Our O
variants O
are O
especially O
strong O
for O
medium-and O
lower-resource O
language O
groups. O
See O
Table O
7 O
in O
Appendix O
C O
for O
detailed O
results O
with O
mean O
and O
std. O
for O
each O
language. O
than O
code-switch B-MethodName
. O
We O
hypothesize O
it O
is O
due O
to O
the O
pre-training O
scenario, O
where O
the O
PLM O
rarely O
sees O
code-mixed O
text O
(Santy O
et O
al., O
2021). O
Zero-shot O
Results O
Table O
6 O
presents O
the O
per-language O
results O
in O
zeroshot O
scenarios. O
We O
consider O
the O
random O
baseline O
for O
comparison O
(Zhao O
and O
Schütze, O
2021;Winata O
et O
al., O
2021). O
We O
notice O
that O
performance O
of O
the O
random O
baseline O
varies O
a O
lot O
across O
languages, O
since O
the O
languages O
have O
different O
number O
of O
classes O
in O
the O
dataset O
(cf. O
Table O
6: O
Zero-shot O
results O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
dataset. O
"SVO" O
and O
"SOV": O
word O
order O
of O
prompting. O
Overall, O
Code-switch B-MethodName
prompting I-MethodName
performs O
the O
best O
in O
the O
zero-shot O
in-context O
scenario. O
In O
cross-lingual O
transfer O
experiments, O
English-task O
training O
greatly O
improves O
the O
performance O
on O
all O
the O
other O
13 O
languages. O
margin, O
in O
both O
word O
orders, O
while O
in-language B-MethodName
prompting I-MethodName
performs O
worse O
than O
the O
random O
baseline O
in O
6 O
languages. O
Code-switch B-MethodName
prompting I-MethodName
outperforms O
in-language O
prompting O
across O
all O
the O
13 O
non-English O
languages, O
using O
SVO-template. O
We O
assume O
that, O
without O
in-language O
training, O
the O
PLM O
understands O
the O
task O
best O
when O
prompted O
in O
English. O
The O
impressive O
performance O
of O
code-switch B-MethodName
shows O
the O
PLM O
is O
able O
to O
transfer O
its O
pre-trained O
knowledge O
in O
English O
to O
other O
languages. O
We O
also O
find O
that O
the O
performance O
is O
also O
highly O
indicated O
by O
the O
number O
of O
classes, O
with O
worst O
F B-MetricName
1 I-MetricName
scores O
achieved O
in O
EN, O
KO O
and O
PT O
(36, O
28 O
and O
22 O
classes), O
and O
best O
scores O
in O
AR, O
RU O
and O
UK O
(9, O
8 O
and O
7 O
classes). O
In O
addition, O
we O
observe O
that O
word O
order O
does O
not O
play O
a O
significant O
role O
for O
most O
languages, O
except O
for O
FA, O
which O
is O
an O
SOV-language O
and O
has O
54.5 B-MetricValue
F B-MetricName
1 I-MetricName
gain O
from O
in-language O
prompting O
with O
an O
SOV-template. O
For O
zero-shot O
cross-lingual O
transfer, O
we O
see O
that O
non-English O
tasks O
benefit O
from O
English O
in-domain B-MethodName
prompt-based I-MethodName
fine-tuning I-MethodName
, O
and O
the O
F B-MetricName
1 I-MetricName
gain O
improves O
with O
the O
English O
data O
size. O
For O
5 O
languages O
(ES, O
FA, O
NL, O
SV, O
and O
UK), O
zero-shot O
transfer O
after O
training O
on O
268k O
English O
examples O
delivers O
even O
better O
results O
than O
in-language O
fully O
supervised O
training O
(cf. O
Table O
4). O
Sanh O
et O
al. O
(2022) O
show O
that O
including O
RC B-TaskName
-specific O
prompt O
input O
in O
English O
during O
pre-training O
can O
help O
in O
other O
languages. O
Discussion O
Based O
on O
the O
results O
above, O
we O
answer O
the O
research O
questions O
from O
Section O
1. O
RQ1. O
Which O
is O
the O
most O
effective O
way O
to O
prompt? O
In O
the O
fully-supervised O
and O
few-shot O
scenario, O
in-language B-MethodName
prompting I-MethodName
displays O
the O
best O
re-sults. O
This O
appears O
to O
stem O
from O
a O
solid O
performance O
across O
all O
languages O
in O
both O
settings. O
Its O
worst O
performance O
is O
31.8 O
F O
1 O
for O
Polish O
8-shot O
(see O
Table O
7 O
in O
Appendix O
C). O
All O
other O
methods O
have O
results O
lower O
than O
15.0 B-MetricValue
F B-MetricName
1 I-MetricName
for O
some O
language. O
This O
indicates O
that O
with O
little O
supervision O
mT5 B-MethodName
is O
able O
to O
perform O
the O
task O
when O
prompted O
in O
the O
language O
of O
the O
original O
text. O
However, O
zero-shot O
results O
strongly O
prefer O
code-switch B-MethodName
prompting I-MethodName
. O
It O
could O
follow O
that, O
without O
fine-tuning, O
the O
model's O
understanding O
of O
this O
task O
is O
much O
better O
in O
English. O
RQ2. O
How O
well O
does O
our O
method O
perform O
in O
different O
data O
regimes O
and O
languages? O
Averaged O
over O
all O
languages, O
all O
our O
variants O
outperform O
the O
baselines, O
except O
for O
8-shot. O
For O
some O
high-resource O
languages, O
XLM-R B-MethodName
EM I-MethodName
is O
able O
to O
outperform O
our O
method. O
On O
the O
other O
hand, O
for O
low-resource O
languages O
null O
prompts O
are O
a O
better O
baseline O
which O
we O
consistently O
outperform. O
This O
could O
indicate O
that O
prompting O
the O
underlying O
mT5 B-MethodName
model O
is O
better O
suited O
for O
multilingual B-TaskName
RC I-TaskName
on O
SMiLER B-DatasetName
. O
Overall, O
the O
results O
suggest O
that O
minimal O
translation O
can O
be O
very O
helpful O
for O
multilingual O
relation O
classification. O
B O
Verbalizers O
for O
SMiLER B-DatasetName
• O
EN O
"birth-place": O
"birth O
place", O
"eats": O
"eats", O
"event-year": O
"event O
year", O
"firstproduct": O
"first O
product", O
"from-country": O
"from O
country", O
"has-author": O
"has O
author", O
"has-child": O
"has O
child", O
"has-edu": O
"has O
education", O
"has-genre": O
"has O
genre", O
"has-height": O
"has O
height", O
"has-highestmountain": O
"has O
highest O
mountain", O
"haslength": O
"has O
length", O
"has-lifespan": O
"has O
lifespan", O
"has-nationality": O
"has O
nationality", O
"has-occupation": O
"has O
occupation", O
"has-parent": O
"has O
parent", O
"has-population": O
"has O
population", O
"has-sibling": O
"has O
sibling", O
"has-spouse": O
"has O
spouse", O
"hastourist-attraction": O
"has O
tourist O
attraction", O
"has-type": O
"has O
type", O
"has-weight": O
"has O
weight", O
"headquarters": O
"headquarters", O
"invented-by": O
"invented O
by", O
"inventedwhen": O
"invented O
when", O
"is-member-of": O
"is O
member O
of", O
"is-where": O
"located O
in", O
"loc-leader": O
"location O
leader", O
"movie-hasdirector": O
"movie O
has O
director", O
"no_relation": O
"no O
relation", O
"org-has-founder": O
"organization O
has O
founder", O
"org-has-member": O
"organization O
has O
member", O
"org-leader": O
"organization O
leader", O
"post-code": O
"post O
code", O
"starring": O
"starring", O
"won-award": O
"won O
award"; O
1 O
Introduction O
010 O
• O
AR O
"event-year": O
" O
", O
"hasedu": O
" O
", O
"has-genre": O
" O
"has-population": O
" O
", O
"has-type": O
" O
", O
"is-member-of": O
" O
", O
• O
DE O
"birth-place": O
"Geburtsort", O
"eventyear": O
"Veranstaltungsjahr", O
"from-country": O
"vom O
Land", O
"has-author": O
"hat O
Autor", O
"haschild": O
"hat O
Kind", O
"has-edu": O
"hat O
Bildung", O
"has-genre": O
"hat O
Genre", O
"has-occupation": O
"hat O
Beruf", O
"has-parent": O
"hat O
Elternteil", O
"has-population": O
"hat O
Bevölkerung", O
"has-spouse": O
"hat O
Ehepartner", O
"has-type": O
"hat O
Typ", O
"headquarters": O
"Hauptsitz", O
"ismember-of": O
"ist O
Mitglied O
von", O
"is-where": O
"gelegen O
in", O
"loc-leader": O
"Standortleiter", O
"movie-has-director": O
"Film O
hat O
Regisseur", O
"no_relation": O
"keine O
Beziehung", O
"org-hasfounder": O
"Organisation O
hat O
Gründer", O
"orghas-member": O
"Organisation O
hat O
Mitglied", O
"org-leader": O
"Organisationsleiter", O
"wonaward": O
"gewann O
eine O
Auszeichnung"; O
• O
ES O
"birth-place": O
"lugar O
de O
nacimiento", O
"event-year": O
"año O
del O
evento", O
"fromcountry": O
"del O
país", O
"has-author": O
"tiene O
autor", O
"has-child": O
"tiene O
hijo", O
"hasedu": O
"tiene O
educación", O
"has-genre": O
"tiene O
género", O
"has-occupation": O
"tiene O
ocupación", O
"has-parent": O
"tiene O
padre", O
"has-population": O
"tiene O
población", O
"has-spouse": O
"tiene O
cónyuge", O
"has-type": O
"tiene O
tipo", O
"headquarters": O
"sede O
central", O
"is-member-of": O
"es O
miembro O
de", O
"is-where": O
"situado O
en", O
"loc-leader": O
"líder O
de O
ubicación", O
"moviehas-director": O
"película O
cuenta O
con O
el O
director", O
"no_relation": O
"sin O
relación", O
"orghas-founder": O
"organización O
cuenta O
con O
el O
fundador", O
"org-has-member": O
"organización O
tiene O
miembro", O
"won-award": O
"ganó O
el O
premio"; O
• O
AR O
"event-year": O
" O
", O
"has-011 O
edu": O
" O
", O
"has-genre": O
" O
A.1 O
Hyperparameter O
Search O
We O
investigated O
the O
following O
possible O
hyperparameters O
for O
few-shot O
settings. O
For O
fully-supervised, O
we O
take O
hyperparameters O
from O
literature O
(see O
Section O
4.4). O
Number O
of O
epochs B-HyperparameterName
: O
[10,20] B-HyperparameterValue
; O
Learning B-HyperparameterName
rate I-HyperparameterName
: O
[1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
1 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
] I-HyperparameterValue
. O
Batch B-HyperparameterName
size I-HyperparameterName
: O
[16,64,256] B-HyperparameterValue
, O
not O
tuned O
but O
selected O
based O
on O
available O
GPU O
VRAM. O
We O
manually O
tune O
these O
hyperparameters, O
based O
on O
the O
micro-F B-MetricName
1 I-MetricName
score O
on O
the O
validation O
set. O
A.2 O
Computing O
Infrastructure O
Fully O
supervised O
experiments O
are O
conducted O
on O
a O
single O
A100-80GB O
GPU. O
Few-shot O
and O
zero-shot O
experiments O
are O
conducted O
on O
a O
single O
A100 O
GPU. O
A.3 O
Average O
Running O
Time O
Fully O
supervised O
It O
takes O
5 O
hours O
to O
train O
for O
1 O
run O
with O
mT5 B-MethodName
BASE I-MethodName
and O
a O
prompt O
method O
(null O
prompts, O
CS, O
SP O
and O
IL) O
on O
either O
English, O
or O
all O
other O
languages O
in O
total. O
With O
XLM-R B-MethodName
EM I-MethodName
the O
running O
time O
is O
3 O
hours. O
Few O


Inducer-tuning B-MethodName
: O
Connecting O
Prefix-tuning B-TaskName
and O
Adapter-tuning B-TaskName
Prefix-tuning B-TaskName
, O
or O
more O
generally O
continuous O
prompt O
tuning, O
has O
become O
an O
essential O
paradigm O
of O
parameter-efficient O
transfer O
learning. O
Using O
a O
large O
pre-trained O
language O
model O
(PLM), O
prefix-tuning B-TaskName
can O
obtain O
strong O
performance O
by O
training O
only O
a O
small O
portion O
of O
parameters. O
In O
this O
paper, O
we O
propose O
to O
understand O
and O
further O
develop O
prefix-tuning B-TaskName
through O
the O
kernel O
lens. O
Specifically, O
we O
make O
an O
analogy O
between O
prefixes O
and O
inducing O
variables O
in O
kernel O
methods O
and O
hypothesize O
that O
prefixes O
serving O
as O
inducing O
variables O
would O
improve O
their O
overall O
mechanism. O
From O
the O
kernel O
estimator O
perspective, O
we O
suggest O
a O
new O
variant O
of O
prefix-tuning-inducer-tuning B-MethodName
, O
which O
shares O
the O
exact O
mechanism O
as O
prefix-tuning B-TaskName
while O
leveraging O
the O
residual O
form O
found O
in O
adaptertuning B-TaskName
. O
This O
mitigates O
the O
initialization O
issue O
in O
prefix-tuning B-TaskName
. O
Through O
comprehensive O
empirical O
experiments O
on O
natural O
language O
understanding O
and O
generation O
tasks, O
we O
demonstrate O
that O
inducer-tuning B-MethodName
can O
close O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
fine-tuning. O
* O
Equal O
contribution. O
This O
work O
was O
performed O
while O
the O
first O
author O
was O
interning O
at O
Amazon O
Alexa O
AI. O
Introduction O
Transfer O
learning O
from O
large O
pre-trained O
language O
models O
(PLMs) O
has O
been O
the O
de-facto O
method O
to O
tackle O
downstream O
natural O
language O
processing O
(NLP) O
tasks O
with O
proven O
performance O
and O
scalability O
(Peters O
et O
al., O
2018). O
Among O
all O
the O
adaption O
techniques, O
fine-tuning O
(Howard O
and O
Ruder, O
2018;Kale O
and O
Rastogi, O
2020) O
is O
predominant O
for O
PLMs O
and O
maintains O
the O
models' O
architecture O
while O
updating O
all O
the O
parameters O
within. O
Though O
powerful, O
fine-tuning O
is O
considered O
parameter-inefficient O
since O
it O
results O
in O
separate O
copies O
of O
model O
parameters O
for O
each O
task/client O
after O
training. O
With O
the O
sizes O
of O
PLMs O
increasing O
to O
hundreds O
of O
millions O
(Brown O
et O
al., O
2020) O
or O
even O
up O
to O
tril-lion O
(Fedus O
et O
al., O
2021) O
parameters, O
the O
trend O
motivates O
a O
range O
of O
parameter-efficient O
adaptation O
techniques, O
including O
adapter-tuning B-TaskName
and O
prompting, O
as O
promising O
lightweight O
alternatives O
to O
finetuning O
to O
reduce O
computational O
consumption O
and O
storage O
space. O
Adapter-tuning B-TaskName
inserts O
bottlenecked O
Multi-layer O
Perception O
(MLP) O
modules O
between O
the O
pre-trained O
layers O
of O
PLMs O
and O
tunes O
only O
these O
new O
parameters O
for O
task O
adaptation O
(Houlsby O
et O
al., O
2019;Pfeiffer O
et O
al., O
2020a). O
Prompting, O
instead, O
aims O
to O
adapt O
the O
general-purpose O
PLMs O
through O
prompts, O
whose O
effectiveness O
has O
been O
shown O
on O
a O
frozen O
GPT-3 O
model O
(Brown O
et O
al., O
2020). O
An O
implicit O
drawback O
of O
the O
prompt-based O
adaptation O
is O
the O
difficulty O
of O
searching O
for O
the O
proper O
prompt. O
To O
avoid O
manually O
designing O
the O
prompts, O
Shin O
et O
al. O
(2020) O
propose O
a O
search O
algorithm O
to O
find O
the O
effective O
prompt O
over O
discrete O
space O
of O
vocabularies; O
prefix-tuning O
(Li O
and O
Liang, O
2021) O
and O
other O
concurrent O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b,a) O
further O
extend O
the O
discrete O
search O
to O
continuous O
prompts, O
attaining O
performance O
close O
to O
fine-tuning O
in O
some O
tasks. O
Despite O
the O
effort, O
there O
is O
still O
a O
performance O
gap O
between O
" O
prefixtuning B-TaskName
" O
and O
"fine-tuning" O
in O
many O
tasks, O
especially O
when O
the O
model O
size O
is O
small O
(Lester O
et O
al., O
2021;He O
et O
al., O
2021a). O
In O
addition, O
the O
mechanism O
of O
prefix-tuning O
is O
still O
poorly O
understood O
and O
underexplored. O
Prefix-tuning B-TaskName
is O
also O
similar O
to O
adaptertuning B-TaskName
, O
since O
they O
both O
insert O
additional O
modules O
into O
each O
transformer O
layer O
(classical O
prompt-based O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b) O
only O
add O
prompts O
to O
the O
embedding O
layer). O
Scrutinizing O
the O
evolution O
of O
prompt-based O
methods, O
we O
can O
observe O
they O
have O
gradually O
deviated O
from O
the O
concept O
of O
"prompts". O
Compared O
to O
the O
manually O
designed O
prompts, O
the O
discrete O
search O
usually O
results O
in O
counter-intuitive O
prompt O
tokens, O
which O
vaguely O
match O
the O
topic O
but O
are O
not O
as O
sensible O
as O
the O
manual O
one; O
for O
continuous O
prompt O
tuning, O
it O
even O
breaks O
the O
limit O
of O
the O
existing O
vo-cabulary. O
All O
these O
pieces O
imply O
that O
the O
mechanism O
behind O
prompt-based O
tuning O
might O
be O
more O
complicated O
than O
guiding O
the O
output O
through O
hint O
prompts. O
To O
open O
the O
black O
box O
of O
"prompts", O
in O
this O
work, O
we O
propose O
to O
consider O
the O
prompts O
(either O
hard O
or O
soft) O
as O
"inducing O
variables" O
in O
kernel O
methods O
(Titsias, O
2009). O
This O
analogy O
is O
justified O
due O
to O
the O
close O
connection O
between O
attention O
modules O
in O
PLMs O
and O
kernel O
estimators O
(Choromanski O
et O
al., O
2020;Tsai O
et O
al., O
2019). O
This O
kernel O
perspective O
explains O
the O
potential O
mechanism O
of O
prefix-tuning B-TaskName
and O
motivates O
a O
new O
method, O
inducer-tuning B-MethodName
. O
Specifically, O
inducertuning B-MethodName
freezes O
all O
the O
original O
parameters O
in O
the O
PLMs O
as O
other O
prompt-based O
methods; O
when O
computing O
the O
attention O
output O
for O
a O
certain O
input O
token O
in O
each O
layer, O
inducer-tuning O
utilizes O
a O
point O
close O
to O
the O
query O
vector O
as O
the O
"inducer". O
This O
unique O
"soft O
prompt" O
eases O
the O
search O
for O
appropriate O
prompts O
and O
builds O
a O
new O
connection O
between O
"prompting" O
and O
" O
adapter-tuning B-TaskName
". O
In O
summary, O
the O
contribution O
of O
this O
work O
is O
three-fold: O
1 O
We O
explain O
the O
underlying O
mechanism O
of O
prefix-tuning B-TaskName
as O
the O
inducing O
variables O
in O
kernel O
learning. O
2 O
We O
propose O
a O
new O
parameterefficient O
adaptation O
technique, O
inducer-tuning B-MethodName
, O
to O
further O
improve O
prefix-tuning B-TaskName
. O
3 O
Through O
comprehensive O
empirical O
studies, O
we O
verify O
our O
proposed O
method O
can O
close O
the O
gap O
between O
" O
prefix-tuning B-TaskName
" O
and O
"fine-tuning" O
on O
relatively O
small O
PLMs, O
and O
provide O
a O
tighter O
lower O
bound O
on O
the O
potential O
of O
continuous O
prompt O
tuning. O
Related O
Work O
In O
this O
section, O
we O
briefly O
introduce O
the O
classical O
form O
of O
adapter-tuning B-TaskName
and O
mainly O
focus O
on O
the O
different O
variants O
of O
prompting. O
Adapter-tuning B-TaskName
. O
Compared O
to O
fine-tuning O
all O
the O
parameters O
in O
the O
PLMs, O
Houlsby O
et O
al. O
(2019), O
Pfeiffer O
et O
al. O
(2020a) O
propose O
to O
modulate O
the O
output O
of O
a O
transformer O
layer O
through O
inserting O
additional O
small-bottleneck O
MLP O
layers O
(adapters) O
(Houlsby O
et O
al., O
2019) O
1 O
: O
Adapter(h) O
= O
h O
+ O
ReLU(hW O
1 O
)W O
2 O
, O
(1) O
where O
h O
is O
the O
dimension-d O
hidden O
state O
in O
the O
transformer O
and O
W O
1 O
, O
W O
2 O
are O
d-by-r O
and O
r-by-d O
projection O
matrices. O
Adapters O
have O
a O
residual O
form O
similar O
to O
skip O
connection, O
while O
only O
W O
1 O
, O
W O
2 O
will O
be O
trained, O
greatly O
decreasing O
the O
size O
of O
tunable O
parameters. O
Up O
to O
now, O
the O
adapter-based O
method O
has O
been O
widely O
used O
for O
multiple O
NLP O
tasks O
(Stickland O
and O
Murray, O
2019;Pfeiffer O
et O
al., O
2020a;Wang O
et O
al., O
2020;Pfeiffer O
et O
al., O
2020b;Üstün O
et O
al., O
2020;Vidoni O
et O
al., O
2020;Pfeiffer O
et O
al., O
2021;He O
et O
al., O
2021b;Xu O
et O
al., O
2021;Rücklé O
et O
al., O
2020;Karimi O
Mahabadi O
et O
al., O
2021), O
and O
adapters O
are O
also O
intrinsically O
connected O
to O
many O
other O
parameter-efficient O
adaptation O
techniques, O
as O
detailed O
in O
He O
et O
al. O
(2021a). O
Prompting. O
Prompting O
prepends O
task-specific O
instructions O
to O
the O
task O
input O
and O
was O
originally O
demonstrated O
in O
Brown O
et O
al. O
(2020). O
As O
manual O
prompts O
rely O
on O
trial O
and O
error, O
, O
Shin O
et O
al. O
(2020) O
suggests O
search O
algorithms O
to O
specify O
the O
prompts O
among O
all O
the O
tokens O
in O
the O
vocabulary. O
Prompt-tuning B-TaskName
(Lester O
et O
al., O
2021) O
and O
P-tuning O
(Liu O
et O
al., O
2021b) O
remove O
the O
vocabulary O
restriction O
on O
prompts O
by O
using O
trainable O
"soft O
prompts". O
The O
prompts O
in O
the O
aforementioned O
methods O
are O
only O
inserted O
into O
the O
bottom O
embedding O
layer O
of O
PLMs, O
while O
Prefix-tuning B-TaskName
(Li O
and O
Liang, O
2021;Liu O
et O
al., O
2021a) O
adds O
soft O
prompts O
to O
all O
the O
transformer O
layers O
to O
further O
increase O
the O
capacity O
of O
prompting. O
Though O
effective, O
proper O
initialization O
of O
the O
soft O
prompts O
remains O
challenging. O
To O
mitigate O
the O
issue, O
Li O
and O
Liang O
(2021) O
used O
an O
extra O
MLP O
to O
reparameterize O
the O
prompts O
in O
each O
layer, O
thus O
adding O
more O
parameters O
that O
need O
training; O
SPoT O
(Vu O
et O
al., O
2021) O
suggests O
performing O
pre-training O
for O
soft O
prompts O
using O
a O
wide O
range O
of O
NLP O
tasks, O
which O
requires O
additional O
computational O
resources. O
In O
contrast, O
though O
adapters O
have O
a O
similar O
expression O
form O
to O
prefix-tuning B-TaskName
(He O
et O
al., O
2021a), O
adaptertuning B-TaskName
only O
requires O
regular O
initialization. O
We O
speculate O
that O
the O
residual O
form O
of O
adapters O
mitigates O
the O
initialization O
issue O
since O
the O
output O
of O
each O
layer O
in O
the O
new O
model O
would O
be O
centered O
around O
the O
output O
in O
the O
frozen O
PLMs, O
and O
the O
residual O
form O
contributes O
to O
gradient O
back-propagation O
as O
in O
skip O
connection. O
We O
rely O
on O
this O
intuition O
and O
utilize O
the O
above-mentioned O
advantages O
of O
adapters O
to O
guide O
the O
design O
of O
our O
proposed O
inducer-tuning B-MethodName
. O
Preliminaries: O
Transformer O
Layers O
Before O
discussing O
the O
mechanism O
of O
prompt-tuning, O
we O
introduce O
the O
structure O
of O
transformer O
layers O
and O
necessary O
notations O
in O
this O
section. O
A O
general O
transformer-based O
PLM O
is O
mainly O
composed O
of O
L O
stacked O
layers. O
Each O
layer O
contains O
a O
multi-headed O
self-attention O
and O
a O
fully O
connected O
feed-forward O
network O
(FFN) O
sub-layer, O
both O
followed O
by O
an O
"Add O
& O
Norm" O
module O
(Vaswani O
et O
al., O
2017). O
2 O
Hereon, O
we O
shall O
focus O
on O
the O
structure O
of O
the O
attention O
sub-layer O
since O
prefix-tuning B-TaskName
directly O
works O
on O
this O
sub-layer. O
Passing O
a O
length-n O
input O
sequence O
X O
∈ O
R O
n×N O
h O
p O
to O
an O
attention O
sub-layer O
(assuming O
N O
h O
heads O
and O
dimension O
size O
p O
for O
each O
head), O
we O
first O
perform O
linear O
transforms O
to O
the O
input O
X O
and O
obtain O
the O
query O
matrix O
(Q), O
the O
key O
matrix O
(K), O
and O
the O
value O
matrix O
(V O
) O
as: O
Q/K/V O
= O
XW O
[q/k/v] O
+ O
1b O
T O
[q/k/v] O
, O
(2) O
where O
Q, O
K, O
V O
∈ O
R O
n×N O
h O
p O
are O
the O
query/ O
key/ O
value O
matrix; O
W O
[q/k/v] O
∈ O
R O
N O
h O
p×N O
h O
p O
are O
the O
weight O
matrices, O
and O
b O
[q/k/v] O
∈ O
R O
N O
h O
p O
are O
the O
bias O
terms O
in O
the O
corresponding O
transformations. O
3 O
To O
increase O
the O
model O
capacity, O
the O
three O
components O
Q, O
K, O
V O
are O
respectively O
divided O
into O
N O
h O
blocks, O
contributing O
to O
the O
attention O
output O
in O
each O
head O
of O
the O
multi-headed O
self-attention O
module. O
For O
instance, O
we O
represent O
Q O
as O
Q O
= O
Q O
(1) O
, O
• O
• O
• O
, O
Q O
(N O
h O
) O
, O
where O
each O
block O
Q O
(h) O
= O
XW O
(h) O
q O
+ O
1(b O
(h) O
q O
) O
T O
is O
an O
n-by-p O
matrix, O
and O
W O
(h) O
q O
, O
b O
(h) O
q O
are O
the O
corresponding O
parts O
in O
W O
q O
, O
b O
q O
. O
The O
attention O
output O
for O
the O
h O
th O
head O
is: O
L O
(h) O
V O
(h) O
:= O
softmax(Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p)V O
(h) O
= O
(D O
(h) O
) O
−1 O
M O
(h) O
V O
(h) O
,(3) O
where O
M O
(h) O
:= O
exp O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
and O
D O
(h) O
is O
a O
diagonal O
matrix O
in O
which O
D O
(h) O
ii O
is O
the O
sum O
of O
the O
i-th O
row O
in O
M O
(h) O
, O
serving O
as O
the O
normalization O
procedure O
in O
softmax. O
The O
attention O
outputs O
in O
each O
head O
are O
then O
concatenated O
as O
L O
:= O
(L O
(1) O
V O
(1) O
, O
. O
. O
. O
, O
L O
(N O
h O
) O
V O
(N O
h O
) O
). O
After O
concatenating O
the O
heads, O
there O
is O
a O
linear O
transform O
following O
the O
output O
LW O
o O
+ O
1b O
T O
o O
,(4) O
where O
W O
o O
and O
b O
o O
are O
similarly O
sized O
as O
the O
other O
matrices O
in O
Equation O
(2). O
This O
is O
the O
overall O
output O
of O
the O
attention O
sub-layer, O
which O
we O
shall O
revisit O
in O
§ O
4.4. O
Attention O
as O
Kernel O
Estimators O
Traditionally, O
attention O
operation O
(Equation O
( O
3)) O
is O
viewed O
as O
a O
transformation O
g(•) O
of O
the O
input O
sequence O
X. O
However, O
in O
prefix-tuning B-TaskName
, O
parameters O
within O
PLMs O
are O
frozen, O
which O
implies O
that O
given O
the O
input O
X, O
the O
represenattions O
Q, O
K, O
and O
V O
are O
invariant. O
4 O
This O
observation O
allows O
us O
to O
reinterpret O
attention O
as O
a O
kernel O
estimator O
f O
(•) O
with O
Q O
as O
its O
input. O
Specifically, O
we O
denote O
the O
i-th O
input O
vector O
X O
i O
's O
attention O
operation O
as O
f O
(Q O
i O
) O
:= O
g(X O
i O
). O
This O
attention O
representation O
can O
be O
seen O
as O
modifying O
the O
input O
query O
vector O
Q O
i O
to O
f O
(Q O
i O
) O
via O
supporting O
points O
{K O
j O
} O
n O
j=1 O
(Choromanski O
et O
al., O
2020;Peng O
et O
al., O
2020;, O
which O
can O
be O
considered O
as O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39): O
row-normalize O
(κ O
(Q, O
K)) O
V O
, O
where O
κ(•, O
•) O
is O
a O
kernel O
function. O
(Refer O
to O
Appendix O
C O
for O
more O
details O
on O
this O
claim.) O
Prefix-Tuning B-TaskName
and O
Inducing O
Variables O
Prefix-tuning O
(Li O
and O
Liang, O
2021) O
alters O
the O
attention O
output O
in O
each O
layer. O
Concretely, O
it O
prepends O
length-l O
prefix O
vectors O
P O
k O
, O
P O
v O
∈ O
R O
l×p O
to O
K O
and O
V O
, O
respectively; O
for O
a O
certain O
query O
token O
Q O
i O
(the O
i-th O
row O
of O
the O
query O
matrix O
Q), O
its O
attention O
output O
f O
(Q O
i O
) O
:= O
Attn(Q O
i O
, O
K, O
V O
) O
is O
updated O
as O
a O
weighted O
sum O
of O
f O
(Q O
i O
) O
and O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
) O
(He O
et O
al., O
2021a, O
Equation O
(7)). O
Remark. O
From O
the O
kernel O
estimator O
perspective, O
the O
two O
categories O
of O
virtual O
tokens O
play O
different O
roles. O
The O
virtual O
key O
vectors O
P O
k O
apply O
to O
the O
empirical O
kernel O
matrix O
part O
and O
can O
alter O
the O
attention O
scores O
(and O
thus O
the O
weights O
for O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
)); O
whereas O
P O
v O
takes O
effect O
in O
the O
value O
part. O
It O
might O
not O
be O
optimal O
for O
prefix-tuning B-TaskName
to O
model O
the O
two O
categories O
of O
virtual O
tokens O
similarly. O
In O
§ O
4.3 O
we O
will O
show O
how O
inducer-tuning O
addresses O
the O
two O
parts O
through O
different O
residual O
forms. O
We O
suggest O
that O
the O
mechanism O
of O
prefix-tuning B-TaskName
can O
be O
further O
understood O
through O
the O
concept O
of O
inducing O
variables O
in O
kernel O
learning O
literature O
(Titsias, O
2009). O
Many O
computational O
methods O
in O
kernel O
learning O
utilize O
a O
small O
set O
of O
support O
points O
(inducing O
variables) O
to O
improve O
the O
inference O
performance O
(Musco O
and O
Musco, O
2017;. O
Snelson O
and O
Ghahramani O
(2005) O
specifically O
consider O
the O
inducing O
variables O
as O
auxiliary O
pseudo-inputs O
and O
infer O
them O
using O
continuous O
optimization, O
which O
is O
similar O
to O
prefix-tuning B-TaskName
. O
We O
emphasize O
that O
from O
the O
first O
sight O
the O
main O
character O
of O
inducing-point O
methods O
is O
representing O
a O
vast O
amount O
of O
training O
examples O
through O
a O
small O
number O
of O
points, O
so O
as O
to O
reduce O
the O
computational O
cost; O
however, O
here O
we O
instead O
aim O
to O
leverage O
the O
mechanism O
of O
inducing O
variables O
to O
well-steer O
the O
estimation: O
the O
goal O
we O
try O
to O
attain O
is O
to O
strengthen O
prefix-tuning B-TaskName
by O
making O
the O
prefixes O
better O
modulate O
the O
attention O
output. O
We O
introduce O
and O
analyze O
the O
mechanism O
as O
follows. O
Mechanism O
for O
well-steering O
inference O
outputs O
in O
inducing-point O
methods. O
Conceptually, O
inducing O
variables O
help O
the O
inference O
because O
they O
can O
represent O
the O
distribution O
of O
the O
query O
inputs O
and O
steer O
the O
kernel O
methods O
without O
changing O
the O
kernel O
in O
use. O
In O
particular, O
we O
consider O
the O
distribution O
pattern O
of O
unconstrained O
inducing O
points O
X O
M O
(Snelson O
and O
Ghahramani, O
2005, O
Figure O
1). O
We O
observe O
that O
most O
of O
them O
are O
close O
to O
the O
testing O
examples O
X O
* O
, O
and O
in O
the O
new O
estimation O
(Snelson O
and O
Ghahramani, O
2005, O
Equation O
( O
8)) O
the O
inducers O
X O
M O
will O
receive O
great O
weights O
through O
the O
weights O
assignment O
mechanism O
in O
kernel O
methods O
(we O
recall O
kernel O
methods O
can O
assign O
the O
weights O
of O
samples O
as O
attention O
(Choromanski O
et O
al., O
2020;Tsai O
et O
al., O
2019); O
for O
inducing O
variables O
close O
to O
the O
query, O
they O
would O
automatically O
receive O
more O
attention), O
and O
thus O
effectively O
modulate O
the O
output. O
From O
this O
mechanism, O
we O
draw O
an O
inductive O
bias O
"the O
prefix O
should O
be O
close O
to O
the O
query" O
(which O
is O
not O
enforced O
in O
the O
method O
of O
prefix-tuning B-TaskName
) O
and O
accordingly O
propose O
inducer-tuning B-MethodName
. O
We O
remark O
since O
we O
are O
not O
pursuing O
the O
original O
goal, O
reducing O
computational O
cost, O
of O
inducing O
variables, O
it O
is O
ordinary O
that O
the O
concrete O
design O
in O
the O
next O
subsection O
is O
different O
from O
the O
usual O
form O
of O
inducing O
points, O
a O
small O
number O
of O
samples. O
We O
speculate O
prefix-tuning B-TaskName
partially O
benefits O
from O
the O
above O
mechanism O
as O
well. O
Furthermore, O
some O
indirect O
evidence O
is O
stated O
as O
follows. O
As O
discussed O
in O
previous O
studies, O
to O
make O
the O
full O
potential O
of O
prompting, O
the O
manually O
designed O
prompts O
are O
expected O
to O
be O
related O
to O
the O
topic O
of O
the O
input O
sequence O
(Brown O
et O
al., O
2020) O
(close O
to O
the O
query); O
even O
for O
the O
soft O
prompts O
they O
are O
recommended O
to O
be O
initialized O
with O
the O
token O
relevant O
to O
the O
specific O
tasks O
(Li O
and O
Liang, O
2021), O
which O
also O
requires O
the O
prompts O
to O
be O
close O
to O
the O
query O
to O
provide O
effective O
adaptation. O
With O
this O
belief, O
we O
propose O
inducer-tuning B-MethodName
to O
exploit O
further O
the O
mechanism O
of O
inducing O
variables O
and O
improve O
upon O
prefix-tuning B-TaskName
. O
Method O
Inducer-tuning B-MethodName
follows O
the O
same O
design O
principle O
as O
prefix-tuning B-TaskName
, O
which O
modulates O
the O
attention O
output O
through O
inserting O
virtual O
tokens O
(vectors). O
However, O
unlike O
prefix-tuning B-TaskName
, O
our O
virtual O
tokens O
are O
not O
shared O
among O
the O
input O
sequences. O
Inducertuning O
also O
incorporates O
the O
benefits O
of O
residual O
forms O
to O
ease O
the O
initialization O
and O
remove O
the O
reparametrization O
trick O
in O
prefix-tuning B-TaskName
. O
Specifically, O
we O
suggest O
the O
following O
modifications: O
1 O
The O
"inducers" O
are O
adaptive O
to O
and O
customized O
for O
each O
input O
token O
to O
strengthen O
the O
expressiveness O
of O
the O
new O
attention O
output. O
2 O
We O
propose O
to O
model O
the O
virtual O
vectors O
in O
a O
residual O
form O
as O
an O
adapter, O
which O
makes O
the O
final O
attention O
output O
be O
in O
a O
residual O
form O
as O
well. O
We O
now O
dive O
into O
discussing O
the O
intuitions O
behind O
the O
modifications O
in O
detail. O
Adaptive B-TaskName
inducers I-TaskName
. O
There O
is O
an O
important O
difference O
between O
language O
models O
and O
kernel O
methods, O
making O
fixed O
prefixes O
less O
effective O
than O
inducing O
variables O
in O
kernel O
methods. O
In O
language O
models, O
the O
distribution O
of O
the O
input O
queries O
keeps O
changing, O
and O
for O
some O
inputs, O
the O
fixed O
prefixes O
fail O
to O
be O
qualified O
as O
"inducing O
variables". O
Even O
worse, O
for O
a O
long O
input, O
there O
probably O
exists O
some O
query O
vectors O
away O
(regarding O
ℓ O
2 O
distance) O
from O
all O
the O
virtual O
vectors O
in O
the O
fixed O
prefixes, O
which O
are O
thus O
unable O
to O
modulate O
the O
attention O
output O
well. O
The O
phenomenon O
that O
prefix-tuning B-TaskName
has O
a O
relatively O
poorer O
performance O
on O
tasks O
with O
longer O
inputs O
can O
be O
observed O
in O
our O
experiments O
( O
§ O
6). O
To O
alleviate O
the O
above O
issue, O
we O
propose O
adaptive O
modeling O
of O
the O
virtual O
key O
vectors. O
For O
a O
query O
Q O
i O
, O
we O
suggest O
taking O
a O
vector O
close O
to O
Q O
i O
itself O
as O
the O
corresponding O
virtual O
key O
vector O
(the O
length O
of O
the O
new O
prefix O
is O
thus O
1), O
in O
the O
hope O
of O
leading O
to O
better O
inference. O
As O
for O
the O
virtual O
value O
vectors, O
we O
relate O
them O
to O
the O
corresponding O
virtual O
key O
vectors. O
The O
motivation O
comes O
from O
traditional O
(non-self-)attention, O
whose O
mechanism O
coincides O
with O
a O
kernel O
estimator: O
the O
value O
V O
is O
independent O
of O
the O
query O
sequence O
Q O
and O
related O
to O
the O
supporting O
points O
K. O
Specifically, O
considering O
our O
design O
above O
that O
the O
virtual O
key O
vectors O
are O
close O
to O
Q O
i O
(we O
take O
the O
virtual O
key O
vectors O
as O
transforms O
of O
the O
input O
query O
vectors O
Q O
i O
's), O
we O
propose O
to O
accordingly O
model O
the O
virtual O
value O
vectors O
as O
a O
map O
of O
Q O
i O
as O
well, O
which O
implies O
the O
virtual O
value O
vectors O
are O
also O
adaptive O
to O
the O
input O
query O
vectors. O
Adapter O
Structures. O
To O
stabilize O
the O
training O
procedure, O
we O
propose O
incorporating O
the O
adapter O
structures O
into O
modeling O
the O
virtual O
key/value O
vectors. O
Specifically, O
for O
the O
i-th O
token O
Q O
i O
(in O
a O
certain O
head), O
we O
represent O
the O
corresponding O
virtual O
key/value O
vectors O
respectively O
as O
P O
k,i O
= O
Q O
i O
+ O
MLP O
k O
(Q O
i O
)(5) O
P O
v,i O
= O
f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
),(6) O
where O
MLP O
k/v O
will O
both O
return O
a O
vector O
of O
the O
same O
dimension O
as O
the O
input O
Q O
i O
. O
5 O
It O
is O
natural O
to O
model O
P O
k,i O
in O
a O
residual O
form O
as O
in O
Equation O
( O
1), O
considering O
P O
k,i O
is O
expected O
to O
center O
around O
Q O
i O
; O
as O
for O
P O
v,i O
, O
we O
claim O
the O
specific O
form O
in O
Equation O
( O
6) O
allows O
the O
complete O
expression O
of O
inducer-tuning B-MethodName
to O
be O
adapter-like, O
and O
the O
justification O
is O
stated O
as O
the O
following O
derivation. O
To O
derive O
the O
expression O
for O
inducer-tuning B-MethodName
, O
we O
denote O
the O
new O
key O
matrix O
and O
value O
matrix O
(specific O
to O
the O
input O
query O
vector O
Q O
i O
) O
as O
K O
(i) O
= O
P O
T O
k,i O
K O
, O
V O
(i) O
= O
P O
T O
v,i O
V O
T O
. O
The O
new O
attention O
outputf O
(Q O
i O
) O
for O
the O
query O
Q O
i O
is O
thus O
(omitting O
the O
factor O
1/ O
√ O
p O
for O
clarity) O
Attn(Q O
i O
, O
K O
(i) O
, O
V O
(i) O
) O
= O
exp(⟨Q O
i O
, O
P O
k,i O
⟩)P O
v,i O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩)V O
j O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
=λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
)(7) O
where O
we O
define O
the O
weight O
λ O
i O
as, O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
. O
Combining O
the O
pieces, O
we O
state O
the O
complete O
equation O
for O
the O
new O
attention O
outputf O
(Q O
i O
) O
as, O
λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=λ O
i O
(f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
)) O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
(8 O
) O
We O
observe O
inducer-tuning B-MethodName
now O
perturbs O
the O
output O
f O
(Q O
i O
) O
in O
a O
residual O
form, O
which O
therefore O
connects O
prefix-tuning B-TaskName
and O
adapter-tuning B-TaskName
. O
The O
procedure O
of O
inducer-tuning B-MethodName
is O
summarized O
in O
Figure O
1, O
and O
§ O
6.2 O
shows O
the O
residual O
form O
greatly O
impacts O
the O
model O
performance. O
Extending O
the O
Scope O
of O
Value O
Besides O
the O
representation O
of O
the O
virtual O
vectors, O
we O
propose O
another O
improvement O
via O
the O
self-attention O
decomposition O
proposed O
by O
Hou O
et O
al. O
(2020). O
Considering O
the O
linear O
transform O
right O
after O
the O
attention O
module, O
we O
can O
accordingly O
rewrite O
the O
attention O
sub-layer O
as O
(ignoring O
the O
bias O
term O
in O
the O
linear O
transform) O
N O
h O
h=1 O
softmax(Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p)V O
(h) O
W O
(h) O
o O
, O
where O
W O
(h) O
o O
is O
the O
h-th O
row O
block O
in O
W O
o O
. O
No- O
tably, O
W O
(h) O
o O
is O
attached O
to O
the O
value O
matrix O
V O
(h) O
, O
suggesting O
that O
W O
(h) O
o O
's O
should O
be O
counted O
into O
the O
complete O
kernel O
structure O
of O
a O
head. O
We O
therefore O
define O
the O
complete O
attention O
outputf O
(Q O
(h) O
) O
as O
softmax O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
V O
(h) O
W O
(h) O
o O
, O
(9) O
and O
align O
the O
prefix O
vectors O
P O
v,i O
's O
with O
the O
rows O
in O
(h) O
as O
in O
prefixtuning. O
The O
detailed O
implementation O
of O
the O
extended O
P O
v,i O
is O
provided O
in O
Appendix O
B.3. O
We O
can O
verify O
the O
improvement O
by O
this O
extension O
through O
the O
ablation O
studies O
in O
§ O
6.2. O
V O
(h) O
W O
(h) O
o O
, O
instead O
of O
solely O
V O
A O
Potential O
Limitation O
of O
Prompting O
A O
potential O
limitation O
of O
prompt-based O
methods O
comes O
from O
the O
frozen O
weight O
matrices O
W O
q O
and O
W O
k O
. O
For O
all O
the O
n(n O
+ O
l) O
pairs O
of O
query O
/ O
key O
vectors O
in O
a O
head, O
most O
of O
the O
pairs O
(corresponding O
to O
the O
elements O
within O
QK O
T O
) O
have O
invariant O
. O
. O
MLPk O
+ O
MLPv O
+ O
P O
k O
K O
V O
P O
V O
Q O
i O
Q O
i O
P O
k,i O
K O
P O
v,i O
V O
f(Q O
i O
) O
Prefix-Tuning B-TaskName
Inducer-Tuning B-MethodName
Attention O
Scores O
Attention O
Scores O
Figure O
1: O
The O
mechanisms O
of O
prefix-tuning B-TaskName
(left) O
and O
inducer-tuning B-MethodName
(right) O
in O
inference O
(the O
MLP O
module O
for O
reparameterization O
in O
prefix-tuning O
is O
dropped). O
For O
prefix-tuning B-TaskName
, O
the O
virtual O
tokens O
(P O
k O
, O
P O
v O
) O
are O
shared O
among O
all O
the O
query O
vectors; O
inducer-tuning O
instead O
prepends O
customized O
inducers O
(P O
k,i O
, O
P O
v,i O
) O
for O
a O
certain O
vector O
Q O
i O
. O
pairwise O
positional O
interactions O
due O
to O
the O
frozen O
weight O
matrices O
W O
q O
and O
W O
k O
. O
However, O
on O
downstream O
tasks, O
there O
can O
be O
a O
mismatch O
between O
W O
q O
and O
W O
k O
maintained O
from O
pre-training: O
the O
distribution O
of O
Q, O
K O
will O
substantially O
change O
due O
to O
the O
distinct O
task-specific O
datasets O
as O
well O
as O
the O
virtual O
tokens O
added O
in O
the O
previous O
layers. O
There O
is O
no O
adaptation O
to O
ensure O
the O
positional O
interactions O
between O
Q, O
K O
still O
contribute O
to O
the O
proper O
representation O
f O
(Q O
i O
). O
To O
resolve O
the O
potential O
issue O
of O
prefixtuning B-TaskName
, O
we O
suggest O
applying O
low-rank O
adaptation O
(LoRA) O
(Hu O
et O
al., O
2021) O
to O
W O
q O
as O
a O
complement O
to O
prompt-based O
methods, O
including O
inducer-tuning B-MethodName
. O
Specifically, O
before O
we O
compute O
the O
attention O
output O
in O
each O
layer, O
W O
q O
will O
be O
updated O
as O
W O
q O
← O
W O
q O
+ O
BA,(10) O
where O
W O
q O
is O
kept O
frozen O
and O
B O
∈ O
R O
N O
h O
p×r O
, O
A O
∈ O
R O
r×N O
h O
p O
will O
be O
tunable O
in O
training. O
We O
report O
in O
§ O
6 O
that O
combining O
both O
inducer-tuning O
and O
LoRA O
outperforms O
their O
individual O
counterparts. O
Final O
Model. O
Our O
final O
proposed O
model O
does O
the O
inferencex O
as O
follows: O
1 O
in O
each O
layer, O
we O
first O
apply O
Equation O
(10) O
to O
update O
W O
q O
before O
obtaining O
Q, O
K, O
V O
; O
2 O
construct O
the O
inducer O
matrices O
P O
k O
= O
Q O
+ O
MLP O
k O
(Q), O
and O
compute O
the O
vector O
a O
with O
the O
i-th O
component O
a O
i O
= O
⟨Q O
i O
, O
P O
k,i O
⟩; O
3 O
compute O
the O
matrix O
product O
[a; O
QK O
T O
]/ O
√ O
p O
and O
then O
perform O
softmax O
over O
the O
product-the O
first O
column O
(denoted O
as O
p) O
is O
the O
weights O
λ O
i O
's O
in O
Equation O
(7); O
4 O
obtainf O
(Q) O
as O
in O
Equation O
( O
9), O
and O
returnf O
(Q) O
+ O
diag(p)MLP O
v O
(Q) O
(corresponding O
to O
Equation O
( O
8)) O
as O
the O
complete O
attention O
output. O
Experiments O
While O
prefix-tuning B-TaskName
has O
been O
shown O
comparable O
to O
fine-tuning O
on O
some O
natural O
language O
understanding O
(NLU) O
tasks O
(Liu O
et O
al., O
2021a), O
there O
is O
still O
a O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
finetuning O
on O
natural O
language O
generation O
(NLG) O
tasks, O
especially O
for O
those O
tasks O
with O
long O
input O
sequences. O
Complete O
settings O
of O
the O
experiments O
below O
can O
be O
found O
in O
Appendix O
A O
and O
Appendix O
B. O
The O
code O
for O
our O
algorithms O
is O
publicly O
available O
at O
https://github.com/ychen-stat-ml/kerneladapters. O
Sketch O
of O
the O
Tasks O
We O
test O
the O
performance O
of O
our O
methods O
on O
both O
NLU B-TaskName
and O
NLG B-TaskName
tasks. O
For O
NLU B-TaskName
tasks, O
we O
follow O
(He O
et O
al., O
2021a) O
to O
use O
RoBERTa B-MethodName
BASE I-MethodName
(Liu O
et O
al., O
2019) O
on O
MNLI B-DatasetName
(Williams O
et O
al., O
2018) O
and O
SST2 B-DatasetName
(Socher O
et O
al., O
2013) O
from O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019); O
in O
SST2, O
the O
models O
predict O
the O
two-way O
sentiment O
(positive/negative) O
of O
a O
given O
sentence, O
and O
the O
MNLI B-DatasetName
task O
is O
to O
decide, O
given O
a O
premise O
and O
a O
hypothesis, O
whether O
there O
is O
entailment, O
contradiction, O
or O
neither. O
We O
use O
GPT-2 B-MethodName
SMALL I-MethodName
(Radford O
et O
al., O
2019) O
for O
NLG B-TaskName
tasks: O
WebNLG-challenge B-DatasetName
(Gardent O
et O
al., O
2017) O
focuses O
on O
table-to-text O
tasks, O
in O
which O
the O
language O
models O
generate O
some O
relatively O
long O
and O
sensible O
sentences O
based O
on O
the O
triples O
with O
solely O
a O
few O
words; O
in O
contrast, O
CoQA B-DatasetName
(Reddy O
et O
al., O
2019) O
provides O
the O
data O
for O
conversational O
question O
answering O
6 O
, O
which O
requires O
the O
language O
model O
to O
return O
short O
answers O
to O
questions O
based O
on O
long O
conversational O
materials. O
More O
details O
about O
the O
datasets O
(including O
the O
average O
sequence O
length) O
and O
the O
evaluation O
metrics O
used O
are O
provided O
in O
Appendix O
A. O
Baselines O
We O
compare O
our O
method O
with O
other O
representative O
methods: O
Fine-Tuning O
(Howard O
and O
Ruder, O
2018) O
We O
differentiate O
the O
number O
of O
parameters O
to O
store O
and O
tune, O
as O
for O
prefix-tuning B-TaskName
, O
the O
two O
numbers O
are O
inconsistent O
due O
to O
a O
re-parametrization O
trick O
(Li O
and O
Liang, O
2021) O
to O
mitigate O
the O
initialization O
issue. O
Instead O
of O
directly O
setting O
up O
an O
embedding O
matrix O
for O
virtual O
tokens, O
an O
additional O
MLP O
module O
in O
each O
layer O
is O
used O
in O
prefix-tuning O
to O
model O
the O
representation O
for O
those O
virtual O
tokens; O
after O
the O
fine-tuning O
stage, O
the O
additional O
MLP O
modules O
are O
dropped O
and O
only O
the O
output O
embedding O
for O
virtual O
tokens O
needs O
storing, O
which O
leads O
to O
a O
regular O
number O
of O
parameters O
to O
store. O
For O
the O
proposed O
inducer-tuning B-MethodName
, O
we O
adopt O
the O
residual O
form O
to O
address O
the O
initialization O
issue O
and O
avoid O
the O
usage O
of O
the O
extra O
MLP, O
which O
makes O
inducer-tuning B-MethodName
have O
the O
same O
number O
of O
parameters O
to O
store O
as O
to O
train O
and O
behave O
more O
like O
a O
regular O
adapter. O
To O
make O
a O
fair O
comparison, O
we O
intentionally O
choose O
the O
number O
of O
parameters O
to O
store O
in O
prefixtuning B-TaskName
roughly O
the O
same O
as O
its O
adapter O
counterpart O
by O
adjusting O
the O
prefix O
length. O
Detailed O
settings O
are O
available O
in O
Appendix O
B.4. O
Main O
Results O
We O
conclude O
our O
experimental O
results O
in O
Tables O
1 O
and O
2, O
comparing O
the O
proposed O
inducertuning B-MethodName
( O
§ O
4.3), O
or O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
( O
§ O
4.5), O
against O
other O
baselines. O
The O
benefit O
of O
using O
Mix-And-Match O
(MAM) O
techniques O
(He O
et O
al., O
2021a) O
The O
MAM O
technique O
benefits O
inducer-tuning B-MethodName
. O
As O
remarked O
by O
He O
et O
al. O
(2021a), O
the O
"Mix-And-Match" O
of O
adapters O
in O
both O
self-attention O
and O
FFN O
sub-layers O
can O
better O
exploit O
parameter-efficient O
transfer O
learning O
than O
only O
modulating O
a O
single O
sublayer. O
We O
obtain O
a O
similar O
conclusion O
by O
replacing O
prefix-tuning B-TaskName
with O
inducer-tuning B-MethodName
(+ I-MethodName
LoRA) I-MethodName
in O
self-attention O
sub-layers. O
The O
combination O
( O
MAM B-MethodName
inducer-tuning I-MethodName
) O
performs O
well O
on O
most O
of O
the O
tasks; O
especially O
on O
the O
tasks O
with O
relatively O
longer O
sequences, O
MNLI B-DatasetName
and O
CoQA B-DatasetName
, O
MAM O
inducer-tuning O
attains O
respectively O
0.6% B-MetricValue
and O
1.2% B-MetricValue
performance O
improvement O
over O
vanilla O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
. O
Long O
inputs O
deteriorate O
prefix-tuning. O
Notably, O
the O
performance O
of O
prefix-tuning B-TaskName
is O
sensitive O
to O
the O
input O
length O
(c.f. O
§ O
4.3). O
For O
WebNLG B-DatasetName
with O
short O
inputs, O
prefix-tuning B-TaskName
attains O
comparable O
performance O
with O
fine-tuning O
and O
other O
parameter-efficient O
methods. O
On O
CoQA B-DatasetName
, O
however, O
prefix-tuning B-TaskName
has O
a O
substantially O
lower O
exact-match O
/ O
F1 B-MetricName
score O
than O
others O
(e.g., O
over O
7% B-MetricValue
decrease O
in O
F1 B-MetricName
score O
compared O
with O
fine-tuning). O
The O
similar O
pattern O
can O
be O
observed O
on O
the O
two O
NLU B-TaskName
tasks O
as O
well: O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
other O
candidate O
methods O
is O
much O
smaller O
on O
SST2 B-DatasetName
, O
whose O
mean O
sequence O
length O
is O
shorter O
than O
MNLI B-DatasetName
. O
We O
remark O
our O
proposed O
adaptive O
inducers O
somewhat O
resolve O
the O
issue: O
both O
variants O
of O
inducer-tuning B-DatasetName
in O
Table O
1 O
obtain O
a O
5%+ B-MetricValue
improvement O
on O
CoQA B-DatasetName
. O
Enhance O
inducer-tuning B-MethodName
through O
adapting O
pairwise O
positional O
interactions. O
In O
§ O
4.5, O
we O
speculate O
the O
prompt-based O
methods O
can O
benefit O
from O
adapting O
pairwise O
positional O
interactions, O
and O
we O
investigate O
it O
on O
both O
NLU B-TaskName
and O
NLG B-TaskName
tasks. O
With O
the O
same O
parameter O
budgets, O
the O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
outperforms O
the O
pure O
inducer-tuning B-MethodName
on O
all O
tasks. O
The O
improvement O
is O
more O
evident O
in O
CoQA B-DatasetName
, O
the O
more O
challenging O
generation O
task O
with O
longer O
input O
sequences. O
We O
remark O
that O
inducer-tuning O
more O
effectively O
exploits O
the O
tunable O
parameters O
than O
LoRA-54 O
for O
the O
value O
part, O
as O
the O
combination O
variant O
also O
performs O
better O
than O
pure O
LoRA. O
Ablation O
Studies O
We O
perform O
ablation O
studies O
on O
generation O
tasks O
to O
analyze O
the O
efficacy O
of O
the O
different O
components O
in O
our O
proposed O
method. O
We O
recall O
there O
are O
four O
different O
features O
in O
inducer-tuning B-MethodName
compared O
to O
prefix-tuning B-TaskName
, O
including O
the O
usage O
of O
adaptive O
inducers, O
the O
extension O
of O
virtual O
value O
vectors, O
the O
residual O
form O
of O
P O
k O
, O
and O
the O
design O
for O
P O
v,i O
to O
concentrate O
around O
attention O
output. O
Accordingly, O
we O
implement O
three O
other O
variants O
of O
inducer-tuning O
to O
help O
ablate O
the O
effects O
of O
the O
above-mentioned O
components. O
Among O
them, O
Adaptive O
directly O
takes O
Q O
i O
as O
P O
k,i O
but O
still O
models O
P O
v,i O
as O
f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
upon O
Adaptive, O
Extension O
changes O
P O
v,i O
tof O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
compared O
to O
Extension, O
Inducer-tuning B-MethodName
just O
mod-ifies O
P O
k,i O
to O
Q O
i O
+ O
MLP O
k O
(Q O
i O
); O
to O
justify O
the O
design O
that O
P O
v,i O
centers O
around O
the O
attention O
output, O
Gating O
models O
P O
v,i O
simply O
as O
MLP O
v O
(Q O
i O
), O
and O
the O
new O
complete O
attention O
output O
thus O
becomes O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
The O
concrete O
setting O
of O
each O
variant O
is O
deferred O
to O
Appendix O
B.4 O
due O
to O
limited O
space. O
The O
usage O
of O
adaptive B-TaskName
inducers I-TaskName
. O
To O
demonstrate O
the O
benefits O
of O
adaptive B-TaskName
inducers I-TaskName
, O
we O
compare O
Prefix-tuning-108 O
with O
the O
basic O
counterpart-Adaptive. O
Table O
3 O
shows O
Adaptive O
attains O
close O
performance O
to O
Prefix-tuning B-TaskName
-108 O
on O
WebNLG B-DatasetName
while O
obtaining O
a O
substantial O
improvement O
on O
CoQA B-DatasetName
, O
which O
has O
longer O
inputs. O
The O
extension O
of O
virtual O
value O
vectors. O
We O
observe O
an O
obvious O
improvement O
attributed O
to O
extending O
the O
scope O
of O
virtual O
value O
vectors O
by O
comparing O
the O
performance O
of O
Adaptive O
and O
Extension. O
For O
almost O
all O
the O
metrics, O
Extension O
obtains O
better O
performance O
than O
Adaptive, O
with O
the O
same O
number O
of O
tunable O
parameters. O
The O
residual O
form O
of O
P O
k O
. O
A O
natural O
design O
for O
P O
k O
is O
to O
directly O
model O
it O
as O
Q, O
which O
would O
automatically O
be O
the O
closest O
vectors O
to O
the O
ones O
in O
Q. O
To O
ablate O
the O
usage O
of O
MLP O
k O
, O
we O
compare O
Inducertuning B-MethodName
against O
Extension, O
which O
follows O
the O
natural O
design O
to O
model O
P O
k O
. O
Through O
the O
empirical O
results, O
we O
find O
assigning O
parameters O
to O
MLP O
k O
can O
still O
slightly O
help O
the O
performance O
of O
inducer-tuning B-MethodName
. O
P O
v,i O
centers O
aroundf O
(Q O
i O
). O
Lastly, O
to O
show O
the O
benefits O
of O
modeling O
P O
v,i O
as O
centering O
around O
f O
(Q O
i O
), O
we O
compare O
the O
variant O
Gating B-MethodName
against O
Inducer-tuning B-MethodName
. O
While O
Gating B-MethodName
has O
a O
weighted O
sum O
form O
similar O
to O
prefix-tuning B-TaskName
, O
it O
suffers O
from O
a O
great O
performance O
drop O
on O
both O
tasks, O
which O
justifies O
the O
effectiveness O
of O
our O
design O
for O
P O
v,i O
's. O
Ethics O
Statement O
As O
an O
efficient O
method O
for O
NLP, O
we O
consider O
our O
work O
to O
have O
a O
low O
ethical O
risk O
since O
the O
outcomes O
of O
the O
algorithm O
mainly O
depend O
on O
the O
downstream O
applications. O
The O
usage O
of O
the O
method O
would O
be O
the O
same O
as O
some O
previous O
methods, O
i.e., O
the O
practical O
deployment O
for O
some O
applications. O
Also, O
our O
method O
doesn't O
assume O
any O
specific O
structure O
of O
the O
input O
and O
thus O
doesn't O
leverage O
biases O
in O
the O
data. O
We O
conclude O
that O
our O
work O
will O
not O
likely O
have O
a O
negative O
ethical O
impact. O
A O
Dataset O
Details O
• O
The O
Multi-Genre B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
Corpus O
(Williams O
et O
al., O
2018, O
MNLI B-DatasetName
) O
involves O
433k O
sentence O
pairs O
of O
premises O
and O
hypotheses, O
labeled O
with O
textual O
entailment O
annotations. O
The O
premise O
sentences O
include O
ten O
distinct O
genres, O
and O
the O
classification O
can O
be O
performed O
on O
both O
the O
matched O
(in-domain) O
and O
mismatched O
(cross-domain) O
sections. O
Concatenating O
premises O
and O
hypothesis O
as O
the O
inputs, O
we O
obtain O
the O
sequence O
lengths O
are O
on O
average O
39.9 O
and O
max O
444. O
For O
the O
results O
reported O
in O
Table O
2, O
we O
follow O
Hu O
et O
al. O
( O
2021) O
and O
take O
mismatched O
accuracy O
as O
the O
metric. O
• O
The O
Stanford B-DatasetName
Sentiment I-DatasetName
Treebank I-DatasetName
(Socher O
et O
al., O
2013, O
SST2) O
is O
a O
corpus O
of O
movie O
reviews O
and O
human O
annotations O
of O
their O
sentiment. O
This O
task O
is O
incorporated O
into O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019), O
and O
the O
dataset O
split O
assigns O
67k O
sentences O
to O
the O
training O
set O
and O
0.9k O
to O
the O
dev O
set. O
In O
SST2 B-DatasetName
, O
the O
sequence O
lengths O
are O
on O
average O
13.3 O
and O
max O
66, O
much O
shorter O
than O
in O
MNLI B-DatasetName
. O
As O
specified O
in O
the O
GLUE B-DatasetName
benchmark, O
we O
test O
the O
accuracy O
metric O
on O
whether O
the O
sentiment O
of O
a O
review O
sentence O
is O
positive O
or O
negative. O
• O
The O
instances O
in O
WebNLG B-DatasetName
dataset O
are O
the O
mapping O
set O
of O
RDF O
triples O
to O
text. O
They O
are O
Data/Text O
pairs, O
where O
the O
"Data" O
is O
in O
a O
format O
of O
(subject, O
property, O
object) O
triples. O
For O
the O
train O
and O
the O
validation O
set, O
they O
involve O
nine O
categories O
which O
are O
extracted O
from O
DBpedia B-DatasetName
; O
while O
in O
the O
test O
set, O
there O
are O
five O
extra O
unseen O
categories, O
which O
can O
partially O
reflect O
the O
generalization O
of O
the O
adaptation O
methods. O
The O
input O
sequences O
in O
the O
training O
set O
consist O
of O
1 O
to O
7 O
triples, O
and O
the O
lengths O
of O
most O
sequences O
are O
bounded O
by O
50 O
(as O
each O
triple O
only O
includes O
three O
short O
phrases). O
The O
official O
evaluation O
script O
is O
used O
in O
our O
experiments, O
and O
we O
report O
BLEU B-MetricName
(Papineni O
et O
al., O
2002), O
METEOR B-MetricName
, O
(Lavie O
and O
Agarwal, O
2007) O
and O
TER B-MetricName
(Snover O
et O
al., O
2006) O
as O
the O
metrics. O
• O
CoQA B-DatasetName
is O
a O
large-scale O
dataset, O
mainly O
for O
conversational O
question O
answering. O
It O
collects O
more O
than O
8K O
conversations O
over O
text O
passages, O
involving O
over O
127K O
questions O
with O
answers O
in O
5 O
domains. O
The O
average O
conversation O
length O
is O
15 O
turns O
(each O
turn O
consists O
of O
a O
question O
and O
an O
answer). O
The O
task O
requires O
the O
language O
model O
to O
generate O
answers O
to O
the O
given O
questions O
based O
on O
related O
conversation O
histories O
and O
documents O
in O
the O
dataset. O
The O
average O
passage O
length O
in O
CoQA B-DatasetName
is O
271 O
(Reddy O
et O
al., O
2019, O
Table O
3). O
We O
simply O
follow O
the O
evaluation O
script O
provided O
on O
the O
official O
website, O
reporting O
both O
the O
macro-average B-MetricName
F1 I-MetricName
score O
of O
word O
overlap O
and O
the O
exact-match O
metric O
(Reddy O
et O
al., O
2019). O
B O
Training O
Details O
We O
mainly O
implement O
our O
methods O
based O
on O
the O
GitHub O
repositories O
provided O
by O
Lin O
et O
al. O
(2020) O
and O
He O
et O
al. O
(2021a). O
Our O
code O
will O
be O
made O
public O
after O
the O
review O
procedure. O
B.1 O
General O
Training O
Settings O
For O
the O
NLU B-TaskName
tasks, O
we O
exactly O
follow O
the O
experimental O
setup O
used O
by O
He O
et O
al. O
(2021a), O
and O
more O
details O
can O
be O
found O
in O
Appendix O
B.2. O
For O
the O
two O
NLG B-TaskName
tasks, O
we O
mainly O
follow O
the O
experimental O
setting O
adopted O
by O
Lin O
et O
al. O
(2020), O
and O
specifically, O
keep O
using O
"task O
embeddings" O
in O
our O
experiments, O
as O
they O
are O
also O
applied O
in O
the O
original O
GPT-2 B-MethodName
model. O
These O
task O
embeddings O
are O
specialized O
segment O
embeddings O
used O
to O
indicate O
the O
different O
components O
of O
the O
text O
input O
(e.g., O
the O
three O
components O
of O
a O
triple O
in O
WebNLG B-DatasetName
, O
questions, O
and O
answers O
in O
CoQA B-DatasetName
, O
etc.). O
8 O
We O
list O
the O
task O
embedding O
used O
in O
each O
NLG B-TaskName
task: O
for O
CoQA B-DatasetName
, O
we O
follow O
the O
task O
embedding O
suggested O
by O
Lin O
et O
al. O
(2020); O
for O
WebNLG B-DatasetName
, O
we O
simply O
use O
the O
special O
tokens O
to O
indicate O
the O
different O
components O
in O
the O
triples. O
The O
details O
of O
the O
special O
tokens O
in O
each O
task O
are O
summarized O
in O
Table O
4. O
Notably, O
the O
parameter O
budget O
for O
task O
embedding O
is O
much O
smaller O
than O
the O
number O
of O
tunable O
parameters O
in O
the O
aforementioned O
parameter-efficient O
adaptation O
methods O
(around O
2M). O
B.2 O
Hyper-parameters O
for O
Training O
For O
NLU B-TaskName
tasks, O
we O
train O
the O
models O
with O
Adam O
(Kingma O
and O
Ba, O
2015) O
optimizer O
and O
use O
a O
polynomial O
learning O
rate O
scheduler O
to O
make O
the O
learning O
rate O
linearly O
decay; O
specifically, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
linearly O
warmed O
up O
from O
0 O
for O
the O
first O
6For O
NLG B-TaskName
tasks, O
an O
AdamW O
(Loshchilov O
and O
Hutter, O
2018) O
optimizer O
is O
applied O
to O
train O
the O
models, O
and O
a O
linear O
learning O
rate O
scheduler O
with O
a O
500-step B-HyperparameterValue
warmup B-HyperparameterName
duration I-HyperparameterName
is O
used. O
For O
the O
evaluation O
of O
NLG B-TaskName
tasks, O
we O
follow O
the O
script O
provided O
by O
Lin O
et O
al. O
(2020) O
to O
generate O
the O
texts O
through O
a O
greedy O
search O
for O
both O
WebNLG B-DatasetName
and O
CoQA B-DatasetName
. O
As O
for O
the O
number O
of O
epochs O
and O
the O
argument O
for O
weight O
decay, O
we O
mainly O
follow O
the O
setting O
used O
by O
Lin O
et O
al. O
(2020); O
Hu O
et O
al. O
( O
2021): O
for O
WebNLG B-DatasetName
, O
we O
train O
the O
model O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
; O
for O
CoQA B-DatasetName
, O
we O
train O
the O
model O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
the O
model-specific O
hyper-parameters, O
namely O
batch O
size O
(gradient O
accumulation O
is O
used O
if O
necessary) O
and O
learning O
rate, O
we O
decide O
them O
for O
different O
methods O
based O
on O
the O
loss O
on O
the O
validation O
set. O
For O
the O
proposed O
method O
inducer-tuning B-MethodName
with/without O
LoRA O
and O
MAM B-MethodName
inducer-tuning I-MethodName
in O
Table O
1, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.00125 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
for O
WebNLG B-DatasetName
; O
for O
CoQA B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
is O
0.001 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
. O
On O
MNLI B-DatasetName
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.0002 B-HyperparameterValue
for O
both O
two O
methods O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
32 B-HyperparameterValue
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
and O
16 B-HyperparameterValue
for O
MAM B-MethodName
inducer-tuning I-MethodName
; O
on O
SST2 B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
similarly O
set O
as O
0.0002 B-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
is O
16 B-HyperparameterValue
, O
and O
for O
MAM B-MethodName
inducer-tuning I-MethodName
64 B-HyperparameterValue
. O
To O
reduce O
the O
random O
variability O
in O
the O
results, O
all O
the O
methods O
reported O
are O
trained O
for O
multiple O
independent O
runs. O
In O
particular, O
for O
WebNLG B-DatasetName
, O
we O
train O
models O
over O
5 B-HyperparameterValue
runs B-HyperparameterName
, O
and O
for O
CoQA B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SST2 B-DatasetName
3 B-HyperparameterValue
runs B-HyperparameterName
. O
The O
reported O
numbers O
in O
the O
cells O
in O
Tables O
1 O
and O
2 O
Rücklé O
et O
al. O
(2020). O
For O
the O
implementation O
of O
MLP O
v O
, O
we O
provide O
the O
exact O
expression O
for O
MLP O
(h) O
v O
(Q O
(h) O
) O
in O
head O
h O
as O
follows: O
σ O
Q O
(h) O
W O
(h) O
1 O
+ O
1(b O
(h) O
1 O
) O
T O
W O
(h) O
2 O
+ O
1b O
T O
2 O
, O
(11 O
) O
where O
σ O
is O
the O
activation O
function. O
As O
the O
superscript O
suggests, O
W O
h) O
1 O
∈ O
R O
p×r O
, O
b O
(h) O
1 O
∈ O
R O
r O
,( O
and O
W O
(h) O
2 O
∈ O
R O
r×N O
h O
p O
are O
specific O
to O
the O
head O
h, O
while O
b O
2 O
∈ O
R O
N O
h O
p O
are O
shared O
among O
all O
the O
heads, O
which O
is O
the O
same O
case O
as O
in O
Equation O
( O
4) O
(in O
the O
original O
attention O
sub-layer, O
the O
bias O
term O
b O
o O
applies O
to O
all O
the O
heads O
as O
well). O
B.4 O
Specific O
settings O
for O
baseline O
methods O
In O
this O
subsection, O
we O
provide O
the O
detailed O
setting O
for O
the O
methods O
in O
Tables O
1, O
2, O
and O
3 O
that O
need O
further O
specification. O
In O
Table O
1, O
the O
settings O
for O
Adapter-108 O
and O
Prefix-tuning-108 O
are O
clear, O
as O
the O
only O
arguments O
are O
the O
bottleneck O
size O
/ O
prefix O
length; O
for O
LoRA-54, O
we O
apply O
rank-54 O
updates O
for O
both O
W O
q O
and O
W O
v O
, O
as O
suggested O
by O
Hu O
et O
al. O
(2021); O
for O
MAM O
adapter, O
we O
mimic O
the O
parameter O
assignment O
scheme O
( O
bottleneck B-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
for O
FFN O
and O
prefix B-HyperparameterName
length I-HyperparameterName
30 B-HyperparameterValue
) O
by O
He O
et O
al. O
(2021a), O
and O
use O
the O
ratio O
102 O
: O
6 O
to O
implement O
MAM O
adapters O
with O
1.61% O
tunable O
parameters. O
For O
the O
variants O
of O
inducer O
tuning, O
their O
settings O
are O
summarized O
in O
Table O
5. O
In O
this O
table, O
the O
numbers O
in O
column O
MLP O
k O
and O
MLP O
v O
are O
the O
bottleneck O
sizes O
used O
for O
computing O
P O
k O
and O
P O
v O
; O
notice O
for O
Adaptive, O
the O
scope O
of O
virtual O
value O
tokens O
is O
not O
extended O
and O
thus O
has O
a O
larger O
bottleneck O
size O
than O
others. O
(Recall O
for O
MLP O
v O
, O
the O
size O
of O
W O
(h) O
2 O
in O
Equation O
( O
11) O
is O
larger O
than O
the O
counterparts O
in O
MLP O
v O
. O
For O
the O
numbers O
in O
column O
LoRA, O
they O
are O
the O
rank O
of O
the O
update O
used O
in O
the O
LoRA O
component O
to O
adjust O
W O
q O
; O
only O
for O
our O
proposed O
method O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
, O
the O
number O
will O
be O
nonzero. O
C O
Attention O
as O
Kernels O
To O
justify O
the O
claim O
that O
attention O
is O
a O
kernel O
operation, O
we O
construct O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39) O
of O
a O
query O
vector O
Q O
i O
(taking O
{K O
j O
} O
n O
j=1 O
as O
the O
supporting O
points) O
as O
follows: O
f O
(Q O
i O
) O
= O
n O
j=1 O
ℓ O
j O
(Q O
i O
)C O
j O
,(12) O
where O
ℓ O
j O
(Q O
i O
) O
:= O
κ(Q O
i O
, O
K O
j O
) O
n O
k=1 O
κ(Q O
i O
, O
K O
j O
) O
. O
κ(•, O
•) O
is O
a O
kernel O
function, O
and O
C O
j O
's O
are O
the O
coefficients O
corresponding O
to O
the O
rows O
V O
j O
's O
in O
the O
value O
matrix O
V O
. O
Take O
kernel O
function O
κ(x, O
y) O
= O
exp O
⟨x, O
y⟩ O
/ O
√ O
p O
. O
We O
slightly O
abuse O
the O
notation O
κ(Q, O
K) O
to O
represent O
the O
n-by-n O
empirical O
kernel O
matrix O
M O
, O
in O
which O
the O
i-th O
row O
and O
the O
j-th O
column O
is O
κ(Q O
i O
, O
K O
j O
), O
∀i O
∈ O
[n], O
j O
∈ O
[N O
]. O
With O
these O
notations, O
the O
output O
of O
the O
kernel O
estimator O
will O
be, O
D O
−1 O
M O
C, O
(13 O
) O
where O
D O
is O
a O
diagonal O
matrix O
serving O
as O
the O
row O
normalization O
in O
Equation O
( O
12), O
and O
C O
is O
an O
nby-p O
matrix O
with O
C O
j O
as O
its O
j-th O
row. O
We O
observe O
an O
obvious O
correspondence O
between O
Equation O
( O
13) O
and O
the O
standard O
attention O
in O
Equation O
(3). O
The O
correspondence O
implies O
a O
finer O
division O
of O
the O
attention O
module: O
the O
empirical O
kernel O
matrix O
M O
(D O
is O
decided O
by O
κ(Q, O
K)) O
and O
the O
value O
part O
C. O
(In O
Section O
4.4, O
we O
show O
that O
C O
includes O
but O
is O
not O
limited O
to O
the O
value O
matrix O
in O
attention.) O
D O
Example O
We O
provide O
an O
example O
answer O
generated O
by O
finetuning O
and O
our O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
on O
CoQA B-DatasetName
in O
Table O
6. O
Acknowledgements O
We O
appreciate O
all O
the O
valuable O
feedback O
from O
the O
anonymous O
reviewers. O


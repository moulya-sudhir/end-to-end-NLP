Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning
Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose to understand and further develop prefix-tuning through the kernel lens. Specifically, we make an analogy between prefixes and inducing variables in kernel methods and hypothesize that prefixes serving as inducing variables would improve their overall mechanism. From the kernel estimator perspective, we suggest a new variant of prefix-tuning-inducer-tuning, which shares the exact mechanism as prefix-tuning while leveraging the residual form found in adaptertuning. This mitigates the initialization issue in prefix-tuning. Through comprehensive empirical experiments on natural language understanding and generation tasks, we demonstrate that inducer-tuning can close the performance gap between prefix-tuning and fine-tuning. * Equal contribution. This work was performed while the first author was interning at Amazon Alexa AI.
Introduction Transfer learning from large pre-trained language models (PLMs) has been the de-facto method to tackle downstream natural language processing (NLP) tasks with proven performance and scalability (Peters et al., 2018). Among all the adaption techniques, fine-tuning (Howard and Ruder, 2018;Kale and Rastogi, 2020) is predominant for PLMs and maintains the models' architecture while updating all the parameters within. Though powerful, fine-tuning is considered parameter-inefficient since it results in separate copies of model parameters for each task/client after training.
With the sizes of PLMs increasing to hundreds of millions (Brown et al., 2020) or even up to tril-lion (Fedus et al., 2021) parameters, the trend motivates a range of parameter-efficient adaptation techniques, including adapter-tuning and prompting, as promising lightweight alternatives to finetuning to reduce computational consumption and storage space. Adapter-tuning inserts bottlenecked Multi-layer Perception (MLP) modules between the pre-trained layers of PLMs and tunes only these new parameters for task adaptation (Houlsby et al., 2019;Pfeiffer et al., 2020a). Prompting, instead, aims to adapt the general-purpose PLMs through prompts, whose effectiveness has been shown on a frozen GPT-3 model (Brown et al., 2020).
An implicit drawback of the prompt-based adaptation is the difficulty of searching for the proper prompt. To avoid manually designing the prompts, Shin et al. (2020) propose a search algorithm to find the effective prompt over discrete space of vocabularies; prefix-tuning (Li and Liang, 2021) and other concurrent methods (Lester et al., 2021;Liu et al., 2021b,a) further extend the discrete search to continuous prompts, attaining performance close to fine-tuning in some tasks. Despite the effort, there is still a performance gap between "prefixtuning" and "fine-tuning" in many tasks, especially when the model size is small (Lester et al., 2021;He et al., 2021a). In addition, the mechanism of prefix-tuning is still poorly understood and underexplored. Prefix-tuning is also similar to adaptertuning, since they both insert additional modules into each transformer layer (classical prompt-based methods (Lester et al., 2021;Liu et al., 2021b) only add prompts to the embedding layer).
Scrutinizing the evolution of prompt-based methods, we can observe they have gradually deviated from the concept of "prompts". Compared to the manually designed prompts, the discrete search usually results in counter-intuitive prompt tokens, which vaguely match the topic but are not as sensible as the manual one; for continuous prompt tuning, it even breaks the limit of the existing vo-cabulary. All these pieces imply that the mechanism behind prompt-based tuning might be more complicated than guiding the output through hint prompts. To open the black box of "prompts", in this work, we propose to consider the prompts (either hard or soft) as "inducing variables" in kernel methods (Titsias, 2009). This analogy is justified due to the close connection between attention modules in PLMs and kernel estimators (Choromanski et al., 2020;Tsai et al., 2019). This kernel perspective explains the potential mechanism of prefix-tuning and motivates a new method, inducer-tuning. Specifically, inducertuning freezes all the original parameters in the PLMs as other prompt-based methods; when computing the attention output for a certain input token in each layer, inducer-tuning utilizes a point close to the query vector as the "inducer". This unique "soft prompt" eases the search for appropriate prompts and builds a new connection between "prompting" and "adapter-tuning".
In summary, the contribution of this work is three-fold: 1 We explain the underlying mechanism of prefix-tuning as the inducing variables in kernel learning. 2 We propose a new parameterefficient adaptation technique, inducer-tuning, to further improve prefix-tuning. 3 Through comprehensive empirical studies, we verify our proposed method can close the gap between "prefix-tuning" and "fine-tuning" on relatively small PLMs, and provide a tighter lower bound on the potential of continuous prompt tuning.
Related Work In this section, we briefly introduce the classical form of adapter-tuning and mainly focus on the different variants of prompting.
Adapter-tuning. Compared to fine-tuning all the parameters in the PLMs, Houlsby et al. (2019), Pfeiffer et al. (2020a) propose to modulate the output of a transformer layer through inserting additional small-bottleneck MLP layers (adapters) (Houlsby et al., 2019) 
1 : Adapter(h) = h + ReLU(hW 1 )W 2 , (1)
where h is the dimension-d hidden state in the transformer and W 1 , W 2 are d-by-r and r-by-d projection matrices. Adapters have a residual form similar to skip connection, while only W 1 , W 2 will be trained, greatly decreasing the size of tunable parameters. Up to now, the adapter-based method has been widely used for multiple NLP tasks (Stickland and Murray, 2019;Pfeiffer et al., 2020a;Wang et al., 2020;Pfeiffer et al., 2020b;Üstün et al., 2020;Vidoni et al., 2020;Pfeiffer et al., 2021;He et al., 2021b;Xu et al., 2021;Rücklé et al., 2020;Karimi Mahabadi et al., 2021), and adapters are also intrinsically connected to many other parameter-efficient adaptation techniques, as detailed in He et al. (2021a).
Prompting. Prompting prepends task-specific instructions to the task input and was originally demonstrated in Brown et al. (2020). As manual prompts rely on trial and error, , Shin et al. (2020) suggests search algorithms to specify the prompts among all the tokens in the vocabulary. Prompt-tuning (Lester et al., 2021) and P-tuning (Liu et al., 2021b) remove the vocabulary restriction on prompts by using trainable "soft prompts". The prompts in the aforementioned methods are only inserted into the bottom embedding layer of PLMs, while Prefix-tuning (Li and Liang, 2021;Liu et al., 2021a) adds soft prompts to all the transformer layers to further increase the capacity of prompting.
Though effective, proper initialization of the soft prompts remains challenging. To mitigate the issue, Li and Liang (2021) used an extra MLP to reparameterize the prompts in each layer, thus adding more parameters that need training; SPoT (Vu et al., 2021) suggests performing pre-training for soft prompts using a wide range of NLP tasks, which requires additional computational resources. In contrast, though adapters have a similar expression form to prefix-tuning (He et al., 2021a), adaptertuning only requires regular initialization. We speculate that the residual form of adapters mitigates the initialization issue since the output of each layer in the new model would be centered around the output in the frozen PLMs, and the residual form contributes to gradient back-propagation as in skip connection. We rely on this intuition and utilize the above-mentioned advantages of adapters to guide the design of our proposed inducer-tuning.
Preliminaries: Transformer Layers Before discussing the mechanism of prompt-tuning, we introduce the structure of transformer layers and necessary notations in this section.
A general transformer-based PLM is mainly composed of L stacked layers. Each layer contains a multi-headed self-attention and a fully connected feed-forward network (FFN) sub-layer, both followed by an "Add & Norm" module (Vaswani et al., 2017). 2 Hereon, we shall focus on the structure of the attention sub-layer since prefix-tuning directly works on this sub-layer. Passing a length-n input sequence X ∈ R n×N h p to an attention sub-layer (assuming N h heads and dimension size p for each head), we first perform linear transforms to the input X and obtain the query matrix (Q), the key matrix (K), and the value matrix (V ) as:
Q/K/V = XW [q/k/v] + 1b T [q/k/v] , (2)
where Q, K, V ∈ R n×N h p are the query/ key/ value matrix; W [q/k/v] ∈ R N h p×N h p are the weight matrices, and b [q/k/v] ∈ R N h p are the bias terms in the corresponding transformations. 3 To increase the model capacity, the three components Q, K, V are respectively divided into N h blocks, contributing to the attention output in each head of the multi-headed self-attention module. For instance, we represent Q as
Q = Q (1) , • • • , Q (N h ) , where each block Q (h) = XW (h) q + 1(b (h) q )
T is an n-by-p matrix, and
W (h) q , b (h) q are the corresponding parts in W q , b q .
The attention output for the h th head is:
L (h) V (h) := softmax(Q (h) (K (h) ) T / √ p)V (h) = (D (h) ) −1 M (h) V (h) ,(3)
where
M (h) := exp Q (h) (K (h) ) T / √ p and D (h) is a diagonal matrix in which D (h)
ii is the sum of the i-th row in M (h) , serving as the normalization procedure in softmax. The attention outputs in each head are then concatenated as
L := (L (1) V (1) , . . . , L (N h ) V (N h ) ).
After concatenating the heads, there is a linear transform following the output
LW o + 1b T o ,(4)
where W o and b o are similarly sized as the other matrices in Equation (2). This is the overall output of the attention sub-layer, which we shall revisit in § 4.4.
Attention as Kernel Estimators Traditionally, attention operation (Equation ( 3)) is viewed as a transformation g(•) of the input sequence X. However, in prefix-tuning, parameters within PLMs are frozen, which implies that given the input X, the represenattions Q, K, and V are invariant. 4 This observation allows us to reinterpret attention as a kernel estimator f (•) with Q as its input. Specifically, we denote the i-th input vector X i 's attention operation as f (Q i ) := g(X i ).
This attention representation can be seen as modifying the input query vector Q i to f (Q i ) via supporting points {K j } n j=1 (Choromanski et al., 2020;Peng et al., 2020;, which can be considered as a Nadaraya-Watson kernel estimator (Wasserman, 2006, Definition 5.39): row-normalize (κ (Q, K)) V , where κ(•, •) is a kernel function. (Refer to Appendix C for more details on this claim.)
Prefix-Tuning and Inducing Variables Prefix-tuning (Li and Liang, 2021) alters the attention output in each layer. Concretely, it prepends length-l prefix vectors P k , P v ∈ R l×p to K and V , respectively; for a certain query token Q i (the i-th row of the query matrix Q), its attention output f (Q i ) := Attn(Q i , K, V ) is updated as a weighted sum of f (Q i ) and Attn(Q i , P k , P v ) (He et al., 2021a, Equation (7)).
Remark. From the kernel estimator perspective, the two categories of virtual tokens play different roles. The virtual key vectors P k apply to the empirical kernel matrix part and can alter the attention scores (and thus the weights for Attn(Q i , P k , P v )); whereas P v takes effect in the value part. It might not be optimal for prefix-tuning to model the two categories of virtual tokens similarly. In § 4.3 we will show how inducer-tuning addresses the two parts through different residual forms.
We suggest that the mechanism of prefix-tuning can be further understood through the concept of inducing variables in kernel learning literature (Titsias, 2009). Many computational methods in kernel learning utilize a small set of support points (inducing variables) to improve the inference performance (Musco and Musco, 2017;. Snelson and Ghahramani (2005) specifically consider the inducing variables as auxiliary pseudo-inputs and infer them using continuous optimization, which is similar to prefix-tuning. We emphasize that from the first sight the main character of inducing-point methods is representing a vast amount of training examples through a small number of points, so as to reduce the computational cost; however, here we instead aim to leverage the mechanism of inducing variables to well-steer the estimation: the goal we try to attain is to strengthen prefix-tuning by making the prefixes better modulate the attention output. We introduce and analyze the mechanism as follows.
Mechanism for well-steering inference outputs in inducing-point methods. Conceptually, inducing variables help the inference because they can represent the distribution of the query inputs and steer the kernel methods without changing the kernel in use. In particular, we consider the distribution pattern of unconstrained inducing points X M (Snelson and Ghahramani, 2005, Figure 1). We observe that most of them are close to the testing examples X * , and in the new estimation (Snelson and Ghahramani, 2005, Equation ( 8)) the inducers X M will receive great weights through the weights assignment mechanism in kernel methods (we recall kernel methods can assign the weights of samples as attention (Choromanski et al., 2020;Tsai et al., 2019); for inducing variables close to the query, they would automatically receive more attention), and thus effectively modulate the output.
From this mechanism, we draw an inductive bias "the prefix should be close to the query" (which is not enforced in the method of prefix-tuning) and accordingly propose inducer-tuning. We remark since we are not pursuing the original goal, reducing computational cost, of inducing variables, it is ordinary that the concrete design in the next subsection is different from the usual form of inducing points, a small number of samples.
We speculate prefix-tuning partially benefits from the above mechanism as well. Furthermore, some indirect evidence is stated as follows. As discussed in previous studies, to make the full potential of prompting, the manually designed prompts are expected to be related to the topic of the input sequence (Brown et al., 2020) (close to the query); even for the soft prompts they are recommended to be initialized with the token relevant to the specific tasks (Li and Liang, 2021), which also requires the prompts to be close to the query to provide effective adaptation. With this belief, we propose inducer-tuning to exploit further the mechanism of inducing variables and improve upon prefix-tuning.
Method Inducer-tuning follows the same design principle as prefix-tuning, which modulates the attention output through inserting virtual tokens (vectors). However, unlike prefix-tuning, our virtual tokens are not shared among the input sequences. Inducertuning also incorporates the benefits of residual forms to ease the initialization and remove the reparametrization trick in prefix-tuning. Specifically, we suggest the following modifications: 1 The "inducers" are adaptive to and customized for each input token to strengthen the expressiveness of the new attention output. 2 We propose to model the virtual vectors in a residual form as an adapter, which makes the final attention output be in a residual form as well. We now dive into discussing the intuitions behind the modifications in detail.
Adaptive inducers. There is an important difference between language models and kernel methods, making fixed prefixes less effective than inducing variables in kernel methods. In language models, the distribution of the input queries keeps changing, and for some inputs, the fixed prefixes fail to be qualified as "inducing variables". Even worse, for a long input, there probably exists some query vectors away (regarding ℓ 2 distance) from all the virtual vectors in the fixed prefixes, which are thus unable to modulate the attention output well. The phenomenon that prefix-tuning has a relatively poorer performance on tasks with longer inputs can be observed in our experiments ( § 6).
To alleviate the above issue, we propose adaptive modeling of the virtual key vectors. For a query Q i , we suggest taking a vector close to Q i itself as the corresponding virtual key vector (the length of the new prefix is thus 1), in the hope of leading to better inference.
As for the virtual value vectors, we relate them to the corresponding virtual key vectors. The motivation comes from traditional (non-self-)attention, whose mechanism coincides with a kernel estimator: the value V is independent of the query sequence Q and related to the supporting points K. Specifically, considering our design above that the virtual key vectors are close to Q i (we take the virtual key vectors as transforms of the input query vectors Q i 's), we propose to accordingly model the virtual value vectors as a map of Q i as well, which implies the virtual value vectors are also adaptive to the input query vectors.
Adapter Structures. To stabilize the training procedure, we propose incorporating the adapter structures into modeling the virtual key/value vectors. Specifically, for the i-th token Q i (in a certain head), we represent the corresponding virtual key/value vectors respectively as
P k,i = Q i + MLP k (Q i )(5)
P v,i = f (Q i ) + MLP v (Q i ),(6)
where MLP k/v will both return a vector of the same dimension as the input Q i . 5 It is natural to model P k,i in a residual form as in Equation ( 1), considering P k,i is expected to center around Q i ; as for P v,i , we claim the specific form in Equation ( 6) allows the complete expression of inducer-tuning to be adapter-like, and the justification is stated as the following derivation.
To derive the expression for inducer-tuning, we denote the new key matrix and value matrix (specific to the input query vector Q i ) as
K (i) = P T k,i K , V (i) = P T v,i V T .
The new attention outputf (Q i ) for the query Q i is thus (omitting the factor 1/ √ p for clarity)
Attn(Q i , K (i) , V (i) ) = exp(⟨Q i , P k,i ⟩)P v,i + j exp(⟨Q i , K j ⟩)V j exp(⟨Q i , P k,i ⟩) + j exp(⟨Q i , K j ⟩) =λ i P v,i + (1 − λ i )f (Q i )(7)
where we define the weight λ i as,
exp(⟨Q i , P k,i ⟩) exp(⟨Q i , P k,i ⟩) + j exp(⟨Q i , K j ⟩)
.
Combining the pieces, we state the complete equation for the new attention outputf (Q i ) as,
λ i P v,i + (1 − λ i )f (Q i ) =λ i (f (Q i ) + MLP v (Q i )) + (1 − λ i )f (Q i ) =f (Q i ) + λ i MLP v (Q i ). (8
)
We observe inducer-tuning now perturbs the output f (Q i ) in a residual form, which therefore connects prefix-tuning and adapter-tuning.
The procedure of inducer-tuning is summarized in Figure 1, and § 6.2 shows the residual form greatly impacts the model performance.
Extending the Scope of Value Besides the representation of the virtual vectors, we propose another improvement via the self-attention decomposition proposed by Hou et al. (2020).
Considering the linear transform right after the attention module, we can accordingly rewrite the attention sub-layer as (ignoring the bias term in the linear transform)
N h h=1 softmax(Q (h) (K (h) ) T / √ p)V (h) W (h) o ,
where
W (h) o is the h-th row block in W o . No- tably, W (h) o is attached to the value matrix V (h) , suggesting that W (h)
o 's should be counted into the complete kernel structure of a head. We therefore define the complete attention outputf (Q
(h) ) as softmax Q (h) (K (h) ) T / √ p V (h) W (h) o , (9)
and align the prefix vectors P v,i 's with the rows in (h) as in prefixtuning. The detailed implementation of the extended P v,i is provided in Appendix B.3. We can verify the improvement by this extension through the ablation studies in § 6.2.
V (h) W (h) o , instead of solely V
A Potential Limitation of Prompting A potential limitation of prompt-based methods comes from the frozen weight matrices W q and W k . For all the n(n + l) pairs of query / key vectors in a head, most of the pairs (corresponding to the elements within QK T ) have invariant . .
MLPk + MLPv + P k K V P V Q i Q i P k,i K P v,i V f(Q i )
Prefix-Tuning Inducer-Tuning
Attention Scores Attention Scores Figure 1: The mechanisms of prefix-tuning (left) and inducer-tuning (right) in inference (the MLP module for reparameterization in prefix-tuning is dropped). For prefix-tuning, the virtual tokens (P k , P v ) are shared among all the query vectors; inducer-tuning instead prepends customized inducers (P k,i , P v,i ) for a certain vector Q i .
pairwise positional interactions due to the frozen weight matrices W q and W k . However, on downstream tasks, there can be a mismatch between W q and W k maintained from pre-training: the distribution of Q, K will substantially change due to the distinct task-specific datasets as well as the virtual tokens added in the previous layers. There is no adaptation to ensure the positional interactions between Q, K still contribute to the proper representation f (Q i ).
To resolve the potential issue of prefixtuning, we suggest applying low-rank adaptation (LoRA) (Hu et al., 2021) to W q as a complement to prompt-based methods, including inducer-tuning. Specifically, before we compute the attention output in each layer, W q will be updated as
W q ← W q + BA,(10)
where W q is kept frozen and B ∈ R N h p×r , A ∈ R r×N h p will be tunable in training. We report in § 6 that combining both inducer-tuning and LoRA outperforms their individual counterparts.
Final Model. Our final proposed model does the inferencex as follows: 1 in each layer, we first apply Equation (10) to update W q before obtaining Q, K, V ; 2 construct the inducer matrices P k = Q + MLP k (Q), and compute the vector a with the i-th component a i = ⟨Q i , P k,i ⟩; 3 compute the matrix product [a; QK T ]/ √ p and then perform softmax over the product-the first column (denoted as p) is the weights λ i 's in Equation (7); 4 obtainf (Q) as in Equation ( 9), and returnf (Q) + diag(p)MLP v (Q) (corresponding to Equation ( 8)) as the complete attention output.
Experiments While prefix-tuning has been shown comparable to fine-tuning on some natural language understanding (NLU) tasks (Liu et al., 2021a), there is still a performance gap between prefix-tuning and finetuning on natural language generation (NLG) tasks, especially for those tasks with long input sequences. Complete settings of the experiments below can be found in Appendix A and Appendix B. The code for our algorithms is publicly available at https://github.com/ychen-stat-ml/kerneladapters.
Sketch of the Tasks We test the performance of our methods on both NLU and NLG tasks. For NLU tasks, we follow (He et al., 2021a) to use RoBERTa BASE (Liu et al., 2019) on MNLI (Williams et al., 2018) and SST2 (Socher et al., 2013) from the GLUE benchmark (Wang et al., 2019); in SST2, the models predict the two-way sentiment (positive/negative) of a given sentence, and the MNLI task is to decide, given a premise and a hypothesis, whether there is entailment, contradiction, or neither. We use GPT-2 SMALL (Radford et al., 2019) for NLG tasks: WebNLG-challenge (Gardent et al., 2017) focuses on table-to-text tasks, in which the language models generate some relatively long and sensible sentences based on the triples with solely a few words; in contrast, CoQA (Reddy et al., 2019) provides the data for conversational question answering 6 , which requires the language model to return short answers to questions based on long   conversational materials. More details about the datasets (including the average sequence length) and the evaluation metrics used are provided in Appendix A.
Baselines We compare our method with other representative methods: Fine-Tuning (Howard and Ruder, 2018) We differentiate the number of parameters to store and tune, as for prefix-tuning, the two numbers are inconsistent due to a re-parametrization trick (Li and Liang, 2021) to mitigate the initialization issue. Instead of directly setting up an embedding matrix for virtual tokens, an additional MLP module in each layer is used in prefix-tuning to model the representation for those virtual tokens; after the fine-tuning stage, the additional MLP modules are dropped and only the output embedding for virtual tokens needs storing, which leads to a regular number of parameters to store. For the proposed inducer-tuning, we adopt the residual form to address the initialization issue and avoid the usage of the extra MLP, which makes inducer-tuning have the same number of parameters to store as to train and behave more like a regular adapter.
To make a fair comparison, we intentionally choose the number of parameters to store in prefixtuning roughly the same as its adapter counterpart by adjusting the prefix length. Detailed settings are available in Appendix B.4.
Main Results We conclude our experimental results in Tables 1 and 2, comparing the proposed inducertuning ( § 4.3), or inducer-tuning with LoRA ( § 4.5), against other baselines. The benefit of using Mix-And-Match (MAM) techniques (He et al., 2021a)  The MAM technique benefits inducer-tuning.
As remarked by He et al. (2021a), the "Mix-And-Match" of adapters in both self-attention and FFN sub-layers can better exploit parameter-efficient transfer learning than only modulating a single sublayer. We obtain a similar conclusion by replacing prefix-tuning with inducer-tuning (+ LoRA) in self-attention sub-layers. The combination (MAM inducer-tuning) performs well on most of the tasks; especially on the tasks with relatively longer sequences, MNLI and CoQA, MAM inducer-tuning attains respectively 0.6% and 1.2% performance improvement over vanilla inducer-tuning + LoRA.
Long inputs deteriorate prefix-tuning. Notably, the performance of prefix-tuning is sensitive to the input length (c.f. § 4.3). For WebNLG with short inputs, prefix-tuning attains comparable performance with fine-tuning and other parameter-efficient methods. On CoQA, however, prefix-tuning has a substantially lower exact-match / F1 score than others (e.g., over 7% decrease in F1 score compared with fine-tuning). The similar pattern can be observed on the two NLU tasks as well: the performance gap between prefix-tuning and other candidate methods is much smaller on SST2, whose mean sequence length is shorter than MNLI. We remark our proposed adaptive inducers somewhat resolve the issue: both variants of inducer-tuning in Table 1 obtain a 5%+ improvement on CoQA.
Enhance inducer-tuning through adapting pairwise positional interactions. In § 4.5, we speculate the prompt-based methods can benefit from adapting pairwise positional interactions, and we investigate it on both NLU and NLG tasks. With the same parameter budgets, the inducer-tuning + LoRA outperforms the pure inducer-tuning on all tasks. The improvement is more evident in CoQA, the more challenging generation task with longer input sequences. We remark that inducer-tuning more effectively exploits the tunable parameters than LoRA-54 for the value part, as the combination variant also performs better than pure LoRA.
Ablation Studies We perform ablation studies on generation tasks to analyze the efficacy of the different components in our proposed method. We recall there are four different features in inducer-tuning compared to prefix-tuning, including the usage of adaptive inducers, the extension of virtual value vectors, the residual form of P k , and the design for P v,i to concentrate around attention output. Accordingly, we implement three other variants of inducer-tuning to help ablate the effects of the above-mentioned components. Among them, Adaptive directly takes Q i as P k,i but still models P v,i as f (Q i ) + MLP v (Q i ); upon Adaptive, Extension changes P v,i tof (Q i ) + MLP v (Q i ); compared to Extension, Inducer-tuning just mod-ifies P k,i to Q i + MLP k (Q i ); to justify the design that P v,i centers around the attention output, Gating models P v,i simply as MLP v (Q i ), and the new complete attention output thus becomes
(1 − λ i )f (Q i ) + λ i MLP v (Q i ).
The concrete setting of each variant is deferred to Appendix B.4 due to limited space.
The usage of adaptive inducers. To demonstrate the benefits of adaptive inducers, we compare Prefix-tuning-108 with the basic counterpart-Adaptive. Table 3 shows Adaptive attains close performance to Prefix-tuning-108 on WebNLG while obtaining a substantial improvement on CoQA, which has longer inputs.
The extension of virtual value vectors. We observe an obvious improvement attributed to extending the scope of virtual value vectors by comparing the performance of Adaptive and Extension.
For almost all the metrics, Extension obtains better performance than Adaptive, with the same number of tunable parameters.
The residual form of P k . A natural design for P k is to directly model it as Q, which would automatically be the closest vectors to the ones in Q.
To ablate the usage of MLP k , we compare Inducertuning against Extension, which follows the natural design to model P k . Through the empirical results, we find assigning parameters to MLP k can still slightly help the performance of inducer-tuning.
P v,i centers aroundf (Q i ). Lastly, to show the benefits of modeling P v,i as centering around f (Q i ), we compare the variant Gating against Inducer-tuning. While Gating has a weighted sum form similar to prefix-tuning, it suffers from a great performance drop on both tasks, which justifies the effectiveness of our design for P v,i 's.
Ethics Statement As an efficient method for NLP, we consider our work to have a low ethical risk since the outcomes of the algorithm mainly depend on the downstream applications. The usage of the method would be the same as some previous methods, i.e., the practical deployment for some applications. Also, our method doesn't assume any specific structure of the input and thus doesn't leverage biases in the data. We conclude that our work will not likely have a negative ethical impact.
A Dataset Details • The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018, MNLI) involves 433k sentence pairs of premises and hypotheses, labeled with textual entailment annotations. The premise sentences include ten distinct genres, and the classification can be performed on both the matched (in-domain) and mismatched (cross-domain) sections. Concatenating premises and hypothesis as the inputs, we obtain the sequence lengths are on average 39.9 and max 444.
For the results reported in Table 2, we follow Hu et al. ( 2021) and take mismatched accuracy as the metric.
• The Stanford Sentiment Treebank (Socher et al., 2013, SST2) is a corpus of movie reviews and human annotations of their sentiment. This task is incorporated into the GLUE benchmark (Wang et al., 2019), and the dataset split assigns 67k sentences to the training set and 0.9k to the dev set. In SST2, the sequence lengths are on average 13.3 and max 66, much shorter than in MNLI. As specified in the GLUE benchmark, we test the accuracy metric on whether the sentiment of a review sentence is positive or negative.
• The instances in WebNLG dataset are the mapping set of RDF triples to text. They are Data/Text pairs, where the "Data" is in a format of (subject, property, object) triples. For the train and the validation set, they involve nine categories which are extracted from DBpedia; while in the test set, there are five extra unseen categories, which can partially reflect the generalization of the adaptation methods. The input sequences in the training set consist of 1 to 7 triples, and the lengths of most sequences are bounded by 50 (as each triple only includes three short phrases). The official evaluation script is used in our experiments, and we report BLEU (Papineni et al., 2002), METEOR, (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as the metrics.
• CoQA is a large-scale dataset, mainly for conversational question answering. It collects more than 8K conversations over text passages, involving over 127K questions with answers in 5 domains. The average conversation length is 15 turns (each turn consists of a question and an answer). The task requires the language model to generate answers to the given questions based on related conversation histories and documents in the dataset. The average passage length in CoQA is 271 (Reddy et al., 2019, Table 3). We simply follow the evaluation script provided on the official website, reporting both the macro-average F1 score of word overlap and the exact-match metric (Reddy et al., 2019).
B Training Details We mainly implement our methods based on the GitHub repositories provided by Lin et al. (2020) and He et al. (2021a). Our code will be made public after the review procedure.
B.1 General Training Settings For the NLU tasks, we exactly follow the experimental setup used by He et al. (2021a), and more details can be found in Appendix B.2. For the two NLG tasks, we mainly follow the experimental setting adopted by Lin et al. (2020), and specifically, keep using "task embeddings" in our experiments, as they are also applied in the original GPT-2 model. These task embeddings are specialized segment embeddings used to indicate the different components of the text input (e.g., the three components of a triple in WebNLG, questions, and answers in CoQA, etc.). 8 We list the task embedding used in each NLG task: for CoQA, we follow the task embedding suggested by Lin et al. (2020); for WebNLG, we simply use the special tokens to indicate the different components in the triples. The details of the special tokens in each task are summarized in Table 4. Notably, the parameter budget for task embedding is much smaller than the number of tunable parameters in the aforementioned parameter-efficient adaptation methods (around 2M).
B.2 Hyper-parameters for Training For NLU tasks, we train the models with Adam (Kingma and Ba, 2015) optimizer and use a polynomial learning rate scheduler to make the learning rate linearly decay; specifically, the learning rate is linearly warmed up from 0 for the first 6For NLG tasks, an AdamW (Loshchilov and Hutter, 2018) optimizer is applied to train the models, and a linear learning rate scheduler with a 500-step warmup duration is used.  For the evaluation of NLG tasks, we follow the script provided by Lin et al. (2020) to generate the texts through a greedy search for both WebNLG and CoQA. As for the number of epochs and the argument for weight decay, we mainly follow the setting used by Lin et al. (2020); Hu et al. ( 2021): for WebNLG, we train the model for 10 epochs; for CoQA, we train the model for 5 epochs.
For the model-specific hyper-parameters, namely batch size (gradient accumulation is used if necessary) and learning rate, we decide them for different methods based on the loss on the validation set. For the proposed method inducer-tuning with/without LoRA and MAM inducer-tuning in Table 1, we set the learning rate as 0.00125, and the batch size is 16 for WebNLG; for CoQA, the learning rate we use is 0.001, and the batch size is 16. On MNLI, we set the learning rate as 0.0002 for both two methods and the batch size as 32 for inducer-tuning with LoRA and 16 for MAM inducer-tuning; on SST2, the learning rate is similarly set as 0.0002, the batch size for inducer-tuning with LoRA is 16, and for MAM inducer-tuning 64.
To reduce the random variability in the results, all the methods reported are trained for multiple independent runs. In particular, for WebNLG, we train models over 5 runs, and for CoQA, MNLI, and SST2 3 runs. The reported numbers in the cells in Tables 1 and 2 Rücklé et al. (2020).
For the implementation of MLP v , we provide the exact expression for MLP
(h) v (Q (h) ) in head h as follows: σ Q (h) W (h) 1 + 1(b (h) 1 ) T W (h) 2 + 1b T 2 , (11
)
where σ is the activation function. As the superscript suggests, W
h) 1 ∈ R p×r , b (h) 1 ∈ R r ,(
and W (h) 2 ∈ R r×N h p are specific to the head h, while b 2 ∈ R N h p are shared among all the heads, which is the same case as in Equation ( 4) (in the original attention sub-layer, the bias term b o applies to all the heads as well).
B.4 Specific settings for baseline methods In this subsection, we provide the detailed setting for the methods in Tables 1, 2, and 3 that need further specification.
In Table 1, the settings for Adapter-108 and Prefix-tuning-108 are clear, as the only arguments are the bottleneck size / prefix length; for LoRA-54, we apply rank-54 updates for both W q and W v , as suggested by Hu et al. (2021); for MAM adapter, we mimic the parameter assignment scheme (bottleneck size 512 for FFN and prefix length 30) by He et al. (2021a), and use the ratio 102 : 6 to implement MAM adapters with 1.61% tunable parameters.
For the variants of inducer tuning, their settings are summarized in Table 5. In this table, the numbers in column MLP k and MLP v are the bottleneck sizes used for computing P k and P v ; notice for Adaptive, the scope of virtual value tokens is not extended and thus has a larger bottleneck size than others. (Recall for MLP v , the size of W (h) 2 in Equation ( 11) is larger than the counterparts in MLP v . For the numbers in column LoRA, they are the rank of the update used in the LoRA component to adjust W q ; only for our proposed method inducer-tuning with LoRA, the number will be nonzero.
C Attention as Kernels To justify the claim that attention is a kernel operation, we construct a Nadaraya-Watson kernel estimator (Wasserman, 2006, Definition 5.39) of a query vector Q i (taking {K j } n j=1 as the supporting points) as follows:
f (Q i ) = n j=1 ℓ j (Q i )C j ,(12)
where ℓ j (Q i ) := κ(Q i , K j ) n k=1 κ(Q i , K j )
. κ(•, •) is a kernel function, and C j 's are the coefficients corresponding to the rows V j 's in the value matrix V . Take kernel function κ(x, y) = exp ⟨x, y⟩ / √ p . We slightly abuse the notation κ(Q, K) to represent the n-by-n empirical kernel matrix M , in which the i-th row and the j-th column is κ(Q i , K j ), ∀i ∈ [n], j ∈ [N ]. With these notations, the output of the kernel estimator will be,
D −1 M C, (13
)
where D is a diagonal matrix serving as the row normalization in Equation ( 12), and C is an nby-p matrix with C j as its j-th row. We observe an obvious correspondence between Equation ( 13) and the standard attention in Equation (3). The correspondence implies a finer division of the attention module: the empirical kernel matrix M (D is decided by κ(Q, K)) and the value part C. (In Section 4.4, we show that C includes but is not limited to the value matrix in attention.)
D Example We provide an example answer generated by finetuning and our inducer-tuning with LoRA on CoQA in Table 6.  
Acknowledgements We appreciate all the valuable feedback from the anonymous reviewers.

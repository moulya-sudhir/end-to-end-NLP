Learning a Grammar Inducer from Massive Uncurated Instructional Videos
Video-aided grammar induction aims to leverage video information for finding more accurate syntactic grammars for accompanying text. While previous work focuses on building systems for inducing grammars on text that are well-aligned with video content, we investigate the scenario, in which text and video are only in loose correspondence. Such data can be found in abundance online, and the weak correspondence is similar to the indeterminacy problem studied in language acquisition. Furthermore, we build a new model that can better learn video-span correlation without manually designed features adopted by previous work. Experiments show that our model trained only on large-scale YouTube data with no textvideo alignment reports strong and robust performances across three unseen datasets, despite domain shift and noisy label issues. Furthermore our model yields higher F1 scores than the previous state-of-the-art systems trained on in-domain data.
Introduction Grammar induction is a fundamental and longlasting (Lari and Young, 1990;Clark, 2001;Klein and Manning, 2002) problem in computational linguistics, which aims to find hierarchical syntactic structures from plain sentences. Unlike supervised methods (Charniak, 2000;Collins, 2003;Petrov and Klein, 2007;Zhang and Clark, 2011;Cross and Huang, 2016;Kitaev and Klein, 2018) that require human annotated treebanks, e.g., Penn Treebank (Marcus et al., 1993), grammar inducers do not rely on any human annotations for training. Grammar induction is attractive since annotating syntactic trees by human language experts is expensive and time consuming, while the current treebanks are limited to several major languages and domains.
Recently, deep learning models have achieved remarkable success across NLP tasks, and neural models have been designed (Shen et al., 2018b,a;Kim et al., 2019a,b;Jin et al., 2018) for grammar induction, which greatly advanced model performance on induction with raw text. Recent efforts have started to consider other useful information from multiple modalities, such as images (Shi et al., 2019;Jin and Schuler, 2020) and videos (Zhang et al., 2021). Specifically, Zhang et al. (2021) show that multi-modal information (e.g. motion, sound and objects) from videos can significantly improve the induction accuracy on verb and noun phrases. Such work uses curated multi-modal data publicly available on the web, which all assume that the meaning of a sentence needs to be identical (e.g., being a caption) to the corresponding video or image. This assumption limits usable data to several small-scale benchmarks (Lin et al., 2014;Xu et al., 2016;Hendricks et al., 2017) with expensive human annotations on image/video captions.
The noisy correspondence between form and meaning is one of the main research questions in language acquisition (Akhtar and Montague, 1999;Gentner et al., 2001;Dominey and Dodane, 2004), where different proposals attempt to address this indeterminacy faced by children. There has been computational work incorporating such indeterminacy into their models (Yu and Siskind, 2013;Huang et al., 2021). For modeling empirical grammar learning with multi-modal inputs, two important questions still remain open: 1) how can a grammar inducer benefit from large-scale multi-media data (e.g., YouTube videos) with noisy text-to-video correspondence? and 2) how can a grammar inducer show robust performances across multiple domains and datasets? By using data with only weak cross-modal correspondence, such as YouTube videos and their automatically generated subtitles, we allow the computational models to face a similar indeterminacy problem, and exam-ine how indeterminacy interacts with data size to influence learning behavior and performance of the induction models.
In this paper, we conduct the first investigation on both questions. Specifically, we collect 2.4 million video clips and the corresponding subtitles from instructional YouTube videos (HowTo100M Miech et al. 2019) to train multi-modal grammar inducers, instead of using the training data from a benchmark where text and video are in alignment. We then propose a novel model, named Pre-Trained Compound Probabilistic Context-Free Grammars (PTC-PCFG), that extends previous work (Shi et al., 2019;Zhang et al., 2021) by incorporating a videospan matching loss term into the Compound PCFG (Kim et al., 2019a) model. To better capture the video-span correlation, it leverages CLIP (Miech et al., 2020), a state-of-the-art model pretrained on video subtitle retrieval, as the encoders for both video and text. Compared with previous work (Zhang et al., 2021) that independently extracts features from each modality before merging them using a simple Transformer (Vaswani et al., 2017) encoder, the encoders of our model have been pretrained to merge such multi-modal information, and no human efforts are needed to select useful modalities from the full set.
Experiments on three benchmarks show that our model, which is trained on noisy YouTube video clips and no data from these benchmarks, produces substantial gains over the previous state-of-the-art system (Zhang et al., 2021) trained on in-domain video clips with human annotated captions. Furthermore, our model demonstrates robust performances across all three datasets. We suggest the limitations of our model and future directions for improvements through analysis and discussions. Code will be released upon paper acceptance.
In summary, the main contributions are:
• We are the first to study training a grammar inducer with massive general-domain noisy video clips instead of benchmark data, introducing the indeterminacy problem to the induction model.
• We propose PTC-PCFG, a novel model for unsupervised grammar induction. It is simpler in design than previous models and can better capture the video-text matching information.
• Trained only on noisy YouTube videos without finetuning on benchmark data, PTC-PCFG reports stronger performances than previous mod-els trained on benchmark data across three benchmarks.
Background and Motivation 
Compound PCFGs A PCFG model in Chomsky Normal Form can be defined as a tuple of 6 terms (S, N , P, Σ, R, Π), where they correspond to the start symbol, the sets of non-terminals, pre-terminals, terminals, production rules and their probabilities. Given pre-defined numbers of non-terminals and pre-terminals, a PCFG induction model tries to estimate the probabilities for all production rules. The compound PCFG (C-PCFG) model (Kim et al., 2019a) adopts a mixture of PCFGs. Instead of a corpus-level prior used in previous work (Kurihara and Sato, 2006;Johnson et al., 2007;Wang and Blunsom, 2013;Jin et al., 2018), C-PCFG imposes a sentence-specific prior on the distribution of possible PCFGs. Specifically in the generative story, the probability π r for production rule r is estimated by model g that assigns a latent variable z for each sentence σ, and z is drawn from a prior distribution:
π r = g(r, z; θ), z ∼ p(z).
(
)1
where θ represents the model parameters. The probabilities for all three types of CFG rules are defined as follows:
π S→A = exp(u ⊤ A f s ([w S ; z])) A ′ ∈N exp(u ⊤ A ′ f s ([w S ; z])) , π A→BC = exp(u ⊤ BC [w A ; z]) B ′ ,C ′ ∈N ∪P exp(u ⊤ B ′ C ′ [w A ; z])) , π T →w = exp(u ⊤ w f t ([w T ; z])) w ′ ∈Σ exp(u ⊤ w ′ f t ([w T ; z])) ,(2)
where A ∈ N , B and C ∈ N ∪ P, T ∈ P, w ∈ Σ. Both w and u are dense vectors representing words and all types of non-terminals, and f s and f t are neural encoding functions.
Optimizing the C-PCFG model involves maximizing the marginal likelihood p(σ) of each training sentence σ for all possible z:
log p θ (σ) = log z t∈T G (σ) p θ (t|z)p(z)dz (3)
where T G (σ) indicates all possible parsing trees for sentence σ. Since computing the integral over z is intractable, this objective is optimized by maximizing its evidence lower bound ELBO(σ; ϕ, θ):
ELBO(σ; ϕ, θ) = E q ϕ (z|σ) [log p θ (σ|z)] −KL[q ϕ (z|σ)||p(z)],(4)
where q ϕ (z|σ) is the variational posterior calculated by another neural network with parameters ϕ. Given a sampled z, the log-likelihood term log p θ (σ|z) is calculated via the inside algorithm.
The KL term can be computed analytically when both the prior p(z) and the variational posterior q ϕ (z|σ) are Gaussian (Kingma and Welling, 2014).
Multi-Modal Compound PCFGs Multi-Modal Compound PCFGs (MMC-PCFG) (Zhang et al., 2021) extends C-PCFG with a model to match a video v with a span c in a parse tree t of a sentence σ. It extracts M visual and audio features from a video v and encodes them via a multi-modal transformer (Gabeur et al., 2020), denoted as Ψ = {ψ i } M i=1 . The word representation h i of the ith word is computed by BiLSTM. Given a particular span c = w i , . . . , w j , its representation c is the weighted sum of all label-specific span representations:
c = |N | k=1 p(k|c, σ)f k 1 j − i + 1 j l=i h l , (5)
where {p(k|c, σ)|1 ≤ k ≤ |N |} are the phrasal label probabilities of span c. The representation of a span c is then correspondingly projected to M separate embeddings via gated embedding (Miech et al., 2018), denoted as Ξ = {ξ i } M i=1 . Finally the video-text matching loss is defined as a sum over all video-span matching losses weighted by the marginal probability of a span from the parser:
s mm (v, σ) = c∈σ p(c|σ)h mm (Ξ, Ψ), (6)
where h mm (Ξ, Ψ) is a hinge loss measuring the distances from video v to the matched and unmatched (i.e. span from another sentence) span c and c ′ and the distances from span c to the matched and unmatched (i.e. another video) video v and v ′ :
ω i (c) = exp(u ⊤ i c) M j=1 exp(u ⊤ j c) ,(7)
o(Ξ,Ψ) = M i=1 ω i (c)cos(ξ i , ψ i ),(8)
h mm (Ξ,Ψ) = E c ′ [o(Ξ ′ , Ψ) − o(Ξ, Ψ)) + ϵ] + + E v ′ [o(Ξ, Ψ ′ ) − o(Ξ, Ψ) + ϵ] + , (9)
where Ξ ′ is a set of unmatched span expert embeddings of Ψ, Ψ ′ is a set of unmatched video representations of Ξ, ϵ is a positive margin, [•] + = max(0, •), {u i } M i=1 are learned weights, and the expectations are approximated with one sample drawn from the training data. During training, both ELBO and the video-text matching loss are jointly optimized.
Limitation and Motivation Existing work on multi-modal grammar induction aims at leveraging strict correspondence between image/video and text for information about syntactic categories and structures of the words and spans in the text. However, such datasets are expensive to annotate. Besides, the ambiguous correspondence between language and real-world context, observed in language acquisition, is not really reflected in such training setups.
As a result, we believe that the previous work fails to answer the following important questions: 1) how well a grammar inducer would perform when it is trained only on noisy multi-media data; 2) how the scale of training data would affect the performance and cross-domain robustness?
Training a Grammar Inducer with Massive YouTube Videos We make the first investigation into the above questions by leveraging massive video clips from instructional YouTube videos to train our grammar inducer. Different from the benchmark data used by previous work, the YouTube video clips do not contain paired sentences. This section will first introduce the method for generating noisy training instances (video clip and sentence pairs) from YouTube videos ( §3.1), before describing a novel grammar induction model ( §3.2) with pre-trained text and video encoders. 
Harvesting Training Instances from YouTube Videos Given a YouTube video, we would like to generate a set of video clip and subtitle pairs Ω = {(v, σ)}, where each subtitle σ is a complete sentence and is aligned in time with its paired video clip v. To this end, the YouTube API is chosen to obtain all subtitles of the video. But, our observation finds that most obtained subtitles are not complete sentences, and in some cases, a complete sentence can last for several continuous video fragments. Meanwhile, they do not contain any punctuation, which is a key factor for sentence segmentation. As shown in the top part of Figure 1, we design an algorithm that takes the following steps to find each complete sentence and its corresponding video clip. Sentence segmentation. In the first step, we try to find complete sentences from the subtitles. We first concatenate all subtitles from the same video are concatenated into a very long sequence of tokens. Next, a punctuation restoration model 1 (Tilk and Alumäe, 2016) is adopted to insert punctuation into the sequence. Lastly, sentences are segmented based on certain punctuation (e.g., ".", "?", "!").
Video clip extraction. In the second step, we trim the corresponding video clips. Each raw subtitle contains its start and an end times. We assume each word within the raw subtitle occupies equal time and record the start and end times for 1 We manually punctuate subtitles from 10 videos randomly selected from HowTo100M, which contains 461 sentences after annotation. The punctuation restoration model has an overall F1 score of 74.1% with the manual labels. each word. After that, given a complete sentence σ = w 1 , w 2 , ..., w N , we use the start time of its first word w 1 and the end time of its last word w N as the start and end times of σ. Lastly, we segment a complete sentence σ's corresponding video clip v based on its start and end times.
Model: Pre-Trained Compound PCFGs After harvesting large-scale sentence and video pairs, the next step is to build a strong grammar induction model that can benefit from them. In this section, we introduce our Pre-Trained Compound PCFGs (PTC-PCFG) model for unsupervised grammar induction. As shown in the lower part of Figure 1, the PTC-PCFG model composes of a video encoder, a span encoder and a parsing model. Both the video encoder and the span encoder are initialized from the MIL-NCE model (Miech et al., 2020), a pre-trained video-text matching model that takes a simple design and has shown superior zero-shot results on many video understanding tasks, such as video retrieval, video question answering, etc. We first introduce the pre-trained video and span encoders, before covering the training and inference details of PTC-PCFG. Video encoding. The first step is to encode a video v to its representation v. To do this, we first segment v into small video clips, where each video clip v i consists of T frames. Following Zhang et al. (2021), we sample L video clips with equal interval for efficiency. We use the video encoder from the MIL-NCE model (Miech et al., 2020) as our video encoder and only fine-tune its last fully connected layer f v for efficiency. In more detail, for each sampled video clip, we pre-compute the input of f v as its representation, denoted as {h v i } L i=1 . Then we feed them into f v and average the output as its representation v, denoted as,
v = AvgPool({f v (h v i )} L i=1 ),(10)
where AvgPool indicates average pooling. Span encoding. The next step is to compute a span representation c for each particular span c = w i , . . . , w j (1 ≤ i < j ≤ N ) in sentence σ = w 1 , w 2 , . . . , w N . The pre-trained text encoder of MIL-NCE consists of a word embedding layer and two stacked fully connected layers, f c 0 and f c 1 . Motivated by Zhao and Titov (2020); Zhang et al. (2021), we expect to learn |N | different span representations, each is specified for one non-terminal node. However, directly applying the pre-trained text encoder is not feasible, since it has only one output layer f c 1 . Therefore, we duplicate f c 1 for
|N | times, denoted as {f c k } |N |
k=1 , and compose |N | label-specific output layers. In more detail, we first encode each word w i with the word embedding layer, denoted as h c i . Then we feed the word embeddings to f c 0 , ReLU, maximum pooling and each label-specific output layer sequentially. we also compute the probabilities of its phrasal labels {p(k|c, σ)|1 ≤ k ≤ |N |}, as illustrated in Section 2.1. Lastly, the span representation c is the sum of all label-specific span representations weighted by the probabilities we predicted, denoted as:
τ = MaxPool(ReLU(f c 0 (h c i ))) c = |N | k=1 p(k|c, σ)f c k (τ ),(11)
where MaxPool is a maximum pooling operation and ReLU is a ReLU activation function.
Training. As shown in lower left of Figure 1, we optimize both the video-text matching loss and evidence lower bound during training. We first compute the similarity between a video clip v and a particular span c via dot product and then compute a triplet hinge loss as following,
h(v, c) = E c ′ [c ′ • v − c • v + ϵ] + + E v ′ [c • v ′ − c • v + ϵ] + , (12
)
where ϵ is a positive margin,
[•] + = max(0, •), v ′
is a clip from a different video and c ′ is a span from a different sentence. The video-text matching loss is correspondingly defined as,
s(v, σ) = Σ c∈σ p(c|σ)h(v, c),(13)
where p(c|σ) is the probability of a particular span c being a syntactic phrase. Finally, the overall loss function is composed by the ELBO and the videotext matching loss:
L(ϕ, θ) = (v,σ)∈Ω −ELBO(σ; ϕ, θ) + αs(v, σ),(14)
where α is a constant balancing these two terms. Inference. During inference, given a sentence σ, we predict the most likely tree t * without accessing videos, as shown in the lower right of Figure 1. Since computing the integral over z is intractable, we estimate t * with the following approximation,
t * = arg max t z p θ (t|z)p θ (z|σ)dz ≈ arg max t p θ (t|σ, µ ϕ (σ)),(15)
where µ ϕ (σ) is the mean vector of the variational posterior q ϕ (z|σ), and t * is obtained by the CYK algo. (Cocke, 1969;Younger, 1967;Kasami, 1966). 
Evaluation We discard punctuation, lowercase all words, replace numbers with a special token and ignore trivial single-word and sentence-level spans during testing following Kim et al. (2019a). Besides, we follow previous work (Shi et al., 2019;Zhang et al., 2021) by using a state-of-the-art constituency parser (Benepar Kitaev et al. 2019) to obtain the reference trees for evaluation 2 . Following Shi et al. (2020); Zhang et al. (2021), all models are run 5 times for 1 epoch with different random seeds. For each model, we report the averaged sentence-level F1 (S-F1) and corpus-level F1 (C-F1) of its runs on each testing set.
Implementation Details We use Spacy 3 for tokenization and keep sentences with fewer than 40 words for training due to the limited computational resources. Each video is decoded at 16 fps and L = 8 video clips are sampled in total, where each clip contains T = 16 frames. We train baseline models, C-PCFG and MMC-PCFG with the same hyper-parameters suggested by Kim et al. (2019a) and Zhang et al. (2021). The parsing model of PTC-PCFG has the same hyperparameter setting as C-PCFG and MMC-PCFG (Please refer their papers for details). The constant α is set to 1. We select the top 20 000 most common words in HowTo100M as vocabulary for all datasets. All baseline methods and ours are optimized by Adam (Kingma and Ba, 2015) with a learning rate of 0.001, β 1 = 0.75 and β 2 = 0.999. All parameters (except the video-text matching model in PTC-PCFG) are initialized with Xavier uniform initializer (Glorot and Bengio, 2010). All our models in experiments are trained for 1 epoch with batch size of 32, without finetuning on the 2 For each dataset, we randomly select 50 sentences and manually label their constituency parse trees. Benepar has S-F1 scores of 98.1% (DiDeMo), 97.2% (YouCook2) and 98.1% (MSRVTT) with manual labels.
3 https://spacy.io/ target dataset.
Main Results Figure    
Cross-dataset Evaluation We evaluate the robustness of models across different datasets, as shown in Table 1. Comparing MMC-PCFG trained on in-domain datasets (Row 1-3), we can observe that MMC-PCFG trained on MSRVTT achieves the best overall performance, while MMC-PCFG trained on YouCook2 is the worst. We believe this is due to the different number of training instances 5 and the domain gap between different datasets. Comparing Rows 1-4, we can observe that the MMC-PCFG model trained on HT(592k) (Row 4) is the best or the second place regarding C-F1 and S-F1 compared with its variants trained on in-domain datasets (Rows 1-3). This demonstrates that the our processed videotext training instances are abundant, rich in content and can serve for general purpose. Comparing Rows 4 and 5, PTC-PCFG outperforms MMC-PCFG in both C-F1 and S-F1 in all three datasets and has smaller variance. This demonstrate that our model can leverage pre-trained video-text matching knowledge and learn consistent grammar induction.
Effectiveness of Pre-Training In this section, we explore how different pretrained video and text encoders can affect the parsing performance, and the results are shown in Table 2. In particular, we study different  (Zhang et al., 2021;Zhao and Titov, 2020), a pre-trained TinyBERT (Jiao et al., 2020) model, the text encoder from MIL-NCE (Miech et al., 2020), and the text encoder from CLIP (Radford et al., 2021).
Comparing Rows 1 with 2, we can observe that MM is better than the video encoder of MIL-NCE regarding C-F1 and S-F1 on all three datasets, as MM provides more comprehensive video features. By comparing row 1 with 3, we can also observe that TinyBERT, which is distilled from BERT (Devlin et al., 2019), outperforms the randomly initialized LSTM encoder. However, both MM and TinyBERT are independently trained only on vision or language tasks, where the vision-language correspondences are not considered during pretraining. Therefore, we further investigate the encoders jointly pre-trained on large scale multimedia datasets, including the video-text matching model MIL-NCE (Row 4) and the image-text matching model CLIP (Row 5). We can observe that by leveraging both video and text encoders in MIL-NCE can improve the parsing performance by a large margin on all three datasets. On the other hand, CLIP does not perform well, since it is designed for static images and other multi-modal information (e.g., motion) is ignored.
Qualitative Analysis In figure 5, we visualize a parser tree predicted by the best run of C-PCFG trained on MSRVTT, MMC-PCFG trained on MSRVTT, MMC-PCFG trained on HT(296k) and PTC-PCFG trained on HT(296k), as well as its reference tree. We can observe that C-PCFG trained on MSRVTT fails at noun phrase "a lady", while MMC-PCFG trained on MSRVTT succeeds. MMC-PCFG can be further improved by training on HT(296k), however, fails at noun phrase "the groceries she had kept in her refrigerator". Our PTC-PCFG can leverage the pretrained matching knowledge and make the correct prediction.

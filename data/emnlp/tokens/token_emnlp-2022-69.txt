Multilingual Relation Classification via Efficient and Effective Prompting
Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the high cost of handcrafting multilingual prompts. In this paper, we present the first work on prompt-based multilingual relation classification (RC), by introducing an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels. We evaluate its performance in fully supervised, few-shot and zero-shot scenarios, and analyze its effectiveness across 14 languages, prompt variants, and English-task training in cross-lingual settings. We find that in both fully supervised and few-shot scenarios, our prompt method beats competitive baselines: fine-tuning XLM-R EM and null prompts. It also outperforms the random baseline by a large margin in zero-shot experiments. Our method requires little in-language knowledge and can be used as a strong baseline for similar multilingual classification tasks.
Introduction Relation classification (RC) is a crucial task in information extraction (IE), aiming to identify the relation between entities in a text (Alt et al., 2019). Extending RC to multilingual settings has recently received increased interest (Zou et al., 2018;Kolluru et al., 2022), but the majority of prior work still focuses on English (Baldini Soares et al., 2019;Lyu and Chen, 2021). A main bottleneck for multilingual RC is the lack of supervised resources, comparable in size to large English datasets (Riedel et al., 2010;Zhang et al., 2017). The SMiLER dataset (Seganti et al., 2021) provides a starting point to test fully supervised and more efficient approaches due to different resource availability for different languages.
Previous studies have shown the promising performance of prompting PLMs compared to the datahungry fine-tuning, especially in low-resource scenarios (Gao et al., 2021;Le Scao and Rush, 2021;. Multilingual pre-trained language models (Conneau et al., 2020;Xue et al., 2021) further enable multiple languages to be represented in a shared semantic space, thus making prompting in multilingual scenarios feasible. However, the study of prompting for multilingual tasks so far remains limited to a small range of tasks such as text classification (Winata et al., 2021) and natural language inference (Lin et al., 2022). To our knowledge, the effectiveness of prompt-based methods for multilingual RC is still unexplored.
To analyse this gap, we pose two research questions for multilingual RC with prompts: RQ1. What is the most effective way to prompt? We investigate whether prompting should be done in English or the target language and whether to use soft prompt tokens. RQ2. How well do prompts perform in different data regimes and languages? We investigate the effectiveness of our prompting approach in three scenarios: fully supervised, few-shot and zero-shot. We explore to what extent the results are related to the available language resources.
We present an efficient and effective prompt method for multilingual RC (see Figure 1) that derives prompts from relation triplets (see Section 3.1). The derived prompts include the original sentence and entities and are supposed to be filled with the relation label. We evaluate the prompts with three variants, two of which require no translation, and one of which requires minimal translation, i.e., of the relation labels only. We find that our method outperforms fine-tuning and a strong taskagnostic prompt baseline in fully supervised and few-shot scenarios, especially for relatively lowresource languages. Our method also improves over the random baseline in zero-shot settings, and Titanic is directed by Cameron .
Titanic is directed by Cameron . Titanic _______ Cameron .
Goethe schrieb die Tragödie Faust . 
Relation Classification Task Definition Relation classification is the task of classifying the relationship such as date_of_birth, founded_by or parents between pairs of entities in a given context. Formally, given a relation set R and a text x = [x 1 , x 2 , . . . , x n ] (where x 1 , • • • , x n are tokens) with two disjoint spans e h and e t denoting the head and tail entity, RC aims to predict the relation r ∈ R between e h and e t , or give a no_relation prediction if no relation in R holds.
RC is a multilingual task if the token sequences come from different languages.
Fine-tuning for Relation Classification In fine-tuning, a task-specific linear classifier is added on top of the PLM. Fine-tuning hence introduces a different scenario from pre-training, since language model (LM) pre-training is usually formalized as a cloze-style task to predict target tokens at [MASK] (Devlin et al., 2019;Liu et al., 2019) or a corrupted span (Raffel et al., 2020;Lewis et al., 2020). For the RC task, the classifier aims to predict the target class r at [CLS] or at the entity spans denoted by MARKER (Baldini Soares et al., 2019).
Prompting for Relation Classification Prompting is proposed to bridge the gap between pre-training and fine-tuning . The essence of prompting is, by appending extra text to the original text according to a task-specific template T (•), to reformulate the downstream task to an LM pre-training task such as masked language modeling (MLM), and apply the same training objective during the task-specific training. For the RC task, to identify the relation between "Angela Merkel" and "Joachim Sauer" in the text "Angela Merkel's current husband is quantum chemist Joachim Sauer," an intuitive template for prompting can be "The relation between Angela Merkel and Joachim Sauer is [MASK]," and the LM is supposed to assign a higher likelihood to the term couple than to e.g. friends or colleagues at [MASK]. This "fill-in the blank" paradigm is well  aligned with the pre-training scenario, and enables prompting to better coax the PLMs for pre-trained knowledge (Petroni et al., 2019).
Methods We now present our method, as shown in Figure 1. We introduce its template and verbalizer, and propose several variants of the prompt. Lastly, we explain the training and inference process.
Template For prompting , a prompt often consists of a template T (•) and a verbalizer V. Given a plain text x, the template T adds taskrelated instruction to x to yield the prompt input
x prompt = T (x).(1)
Following  and Han et al. (2021), we treat relations as predicates and use the cloze "e h {relation} e t " for the LM to fill in. Our template is formulated as T (x) := "x. e h ____ e t ".
(2)
In the template T (x), x is the original text and the two entities e h and e t come from x. Therefore, our template does not introduce extra tokens, thus involves no translation at all.
Verbalizer After being prompted by x prompt , the PLM M predicts the masked text y at the blank. To complete an NLP classification task, a verbalizer ϕ is required to bridge the set of labels Y and the set of predicted texts (verbalizations V). For the simplicity of our prompt, we use the one-to-one verbalizer:
ϕ : Y → V, r → ϕ(r),(3)
where r is a relation, and ϕ(r) is the simple verbalization of r. ϕ(•) normally only involves splitting r by "-" or "_" and replacing abbreviations such as org with organization. E.g., the relation org-has-member corresponds to the verbalization "organization has member". Then the prediction is formalized as
p(r|x) ∝ p(y = ϕ(r)|x prompt ; θ M ), (4
)
where θ M denotes the parameters of model M. p(r|x) is normalized by the likelihood sum over all relations.
Variants To find the optimal way to prompt, we investigate three variants as follows.
Hard prompt vs soft prompt (SP) Hard prompts (a.k.a. discrete prompts)  are entirely formulated in natural language. Soft prompts (a.k.a. continuous prompts) consist of learnable tokens (Lester et al., 2021) that are not contained in the PLM vocabulary. Following Han et al. (2021), we insert soft tokens before entities and blanks as shown for SP in Table 1.
Code-switch (CS) vs in-language (IL) Relation labels are in English across almost all RC datasets. Given a text from a non-English input L with a blank, the recovered text is code-mixed after being completed with an English verbalization, corresponding to code-switch prompting. It is probably more reasonable for the PLM to fill in the blank in language L. Inspired by Lin et al. (2022) (Hendrickx et al., 2010) 10 cause effect, entity origin, product producer, ... 2.50 0.81 NYT (Riedel et al., 2010) 24 ethnicity, major shareholder of, religion, ...  show that the label space of the RC task is more complex than most few-class classification tasks. The verbalizations of RC datasets are listed in Appendix B. For SemEval, the two possible directions of a relation are combined. For NYT, we use the version from Zeng et al. (2018). For SMiLER, "EN" is the English split; "ALL" contains all data from 14 languages.
Table 1 visualizes both code-switch (CS) and inlanguage (IL) prompting. For English, CS-and ILprompting are equivalent, since L is English itself. Word order of prompting For the RC task, head-relation-tail triples involve three elements. Therefore, deriving natural language prompts from them requires handling where to put the predicate (relation). In the case of SOV languages, filling in a relation that occurs between e h and e t seems less intuitive. Therefore, to investigate if the word order of prompting affects prediction accuracy, we swap the entities and the blank in the SVOtemplate "x. e h ____ e t " and get "x. e h e t ____" as the SOV-template.
Training and Inference The training and inference setups depend on the employed model. Prompting autoencoding language models requires the verbalizations to be of fixed length, since the length of masks, which is identical with verbalization length, is unknown during inference. Encoder-decoders can handle verbalizations of varying length by nature . Han et al. (2021) adjust all the verbalizations in TACRED to a length of 3, to enable prompting with RoBERTa for RC. We argue that for multilingual RC, this fix is largely infeasible, because: (1) in case of in-language prompting on SMiLER, the variance of the length of the verbalizations increases from 0.68 to 1.44 after translation (see Table 2), and surpasses most of listed monolingual RC datasets (SemEval, NYT and SCIERC), making it harder to unify the length; (2) manually adjusting the translated prompts requires manual effort per target language, making it much more expensive than adjusting only English verbalizations. Therefore, we use an encoder-decoder PLM for prompting Song et al., 2022).
Training objective For an encoder-decoder PLM M, given the prompt input T (x) and the target sequence ϕ(r) (i.e. label verbalization), we denote the output sequence as y. The probability of an exact-match decoding is calculated as follows:
|ϕ(r)| t=1 P θ (y t = ϕ t (r)|y <t , T (x)) ,(5)
where y t , ϕ t (r) denote the t-th token of y and ϕ(r), respectively. y <t denotes the decoded sequence on the left. θ represents the set of all the learnable parameters, including those of the PLM θ M , and those of the soft tokens θ sp in case of variant "soft prompt". Hence, the final objective over the training set X is to minimize the negative loglikelihood:
argmin θ − 1 |X | x∈X |ϕ(r)| t=1 log P θ (y t = ϕ t (r)|y <t , T (x)) .(6)
Inference We collect the output logits of the decoder, L ∈ R |V |×L , where |V | is the vocabulary size of M, and L is the maximum decode length. For each relation r ∈ R, its score is given by :  where we compute P by looking up in the t-th column of L and applying softmax at each time step t. We aggregate P by addition to encourage partial matches as well, instead of enforcing exact matches. The score is normalized by the length of verbalization in order to avoid predictions favoring longer relations. Finally, we select the relation with the highest score as prediction.
score θ (r) := 1 |ϕ(r)| |ϕ(r)| t=1 P θ (y t = ϕ t (r)), (7)
Experiments We implement our experiments using the Hugging Face Transformers library (Wolf et al., 2020), Hydra (Yadan, 2019) and PyTorch (Paszke et al., 2019). 2 We use micro-F1 as the evaluation metric, as the SMiLER paper (Seganti et al., 2021) suggests. To measure the overall performance over multiple languages, we report the macro average across languages, following Zhao and Schütze (2021) and Lin et al. (2022). We also group the languages by their available resources in both pretraining and fine-tuning datasets for additional aggregate results. Details of the dataset, the models, and the experimental setups are as follows. Further experimental details are listed in Appendix A.
Dataset We conduct an experimental evaluation of our multilingual prompt methods on the SMiLER (Seganti 2 We make our code publicly available at https://github. com/DFKI-NLP/meffi-prompt for better reproducibility. Grouping of the languages We visualize the languages in Figure 2 based on the sizes of RC training data, but include the pre-training data as well, to give a more comprehensive overview of the availability of resources for each language. We divide the 14 languages into 4 groups, according to the detectable clusters in Figure 2 and language origins.
Model For prompting, we use mT5 BASE (Xue et al., 2021), an encoder-decoder PLM that supports 101 languages, including all languages in SMiLER. mT5 BASE (Xue et al., 2021) has 220M parameters. XLM-R EM To provide a fine-tuning baseline, we re-implement BERT EM (Baldini Soares et al., 2019) with the ENTITY START variant. 4 In this method, the top-layer representations at the starts of the two entities are concatenated for linear classification. To adapt BERT EM to multilingual tasks, we change the PLM from BERT to a multilingual autoencoder, XLM-R BASE (Conneau et al., 2020), and refer to this model as XLM-R EM . XLM-R BASE has 125M parameters.
Null prompts (Logan IV et al., 2022) To better verify the effectiveness of our method, we implement null prompts as a strong task-agnostic prompt baseline. Null prompts involve minimal prompt engineering by directly asking the LM about the relation, without giving any task instruction (see Table 1). Logan IV et al. (2022) show that null prompts surprisingly achieve on-par performance with handcrafted prompts on many tasks. For best comparability, we use the same PLM mT5 BASE .
Fully Supervised Setup We evaluate the performance of XLM-R EM , null prompts, and our method on each of the 14 languages, after training on the full train split from that language. The prompt input and target of null prompts and our prompts are listed in Table 1.
We employ the randomly generated seed 319 for all the evaluated methods. For XLM-R EM , we follow Baldini Soares et al. (2019) and set the batch size to be 64, the optimizer to be Adam with the learning rate 3 × 10 −5 and the number of epochs to be 5. For null prompts and ours, we use AdamW as the optimizer with the learning rate 3 × 10 −5 , as  suggest for most of the sequence-to-sequence tasks, the number of epochs to 5, and batch size to 16. The maximum sequence length is 256 for all methods.
Few-shot Setup Few-shot learning is normally cast as a K-shot problem, where K labelled examples per class are available. We follow  and Han et al. (2021), and evaluate on 8, 16 and 32 shots.
The few-shot training set D train is generated by randomly sampling K instances per relation from the training split. The test set D test is the original test split from that language. We follow Gao et al. (2021) and sample another K-shot set from the English train split as validation set D val . We tune hyperparameters on D val for the English task, and apply these to all languages.
We evaluate the same methods as in the fully supervised scenarios, but repeat 5 runs as suggested in Gao et al. (2021), and report the mean and standard deviation of micro-F1. We use a fixed set of random seeds {13, 36, 121, 223, 319} for data generation and training across the 5 runs. For XLM-R EM , we use the same hyperparameters as Baldini Soares et al. ( 2019), a batch size of 256, and a learning rate of 1 × 10 −4 . For null prompts and our prompts, we set the learning rate to 3 × 10 −4 , batch size to 16, and the number of epochs to 20.
Zero-shot Setup We consider two scenarios for zero-shot multilingual relation classification.
Zero-shot in-context learning Following Kojima et al. ( 2022), we investigate whether PLMs are also decent zero-shot reasoners for RC. This scenario does not require any samples or training. We test the out-of-the-box performance of the PLM by directly prompting it with x prompt . Zero-shot in-context learning does not specify further hyperparameters since it is training-free.
Zero-shot cross-lingual transfer In this scenario, following Krishnan et al. (2021), we finetune the model with in-language prompting on the English train split, and then conduct zero-shot incontext tests with this fine-tuned model on other languages using code-switch prompting. Through this setting, we want to verify if task-specific pretraining in a high-resource language such as English helps in other languages. In zero-shot crosslingual transfer, we use the same hyperparameters and random seed to fine-tune on the English task.
Fully Supervised Results the three variants of our method beat the finetuning baseline XLM-R EM and the prompting baseline null prompts, according to the macroaveraged performance across 14 languages. Inlanguage prompting delivers the most promising result, achieving an average F 1 of 85.0, which is higher than XLM-R EM (68.2) and null prompts (66.2). The other two variants, code-switch prompting with and w/o soft tokens, achieve F 1 scores of 84.1 and 82.7, respectively, only 0.9 and 2.3 lower than in-language. All three prompt variants are hence effective in fully supervised scenarios.
On a per-group basis, we find that the lowerresourced a language is, the greater an advantage prompting enjoys against fine-tuning. In particular, in-language prompts shows better robustness compared to XLM-R EM in low-resource languages. They both yield 95.9-96.0 F 1 scores for English, but XLM-R EM decreases to 54.3 and 3.7 F 1 in Group-M and -L, while in-language prompting still delivers 83.5 and 65.2 F 1 .
Few-shot Results Table 5 presents the per-group results in few-shot experiments. All the methods benefit from larger K. Similarly, in-language prompting still turns out to be the best contender, performing 1st in 8-and 32-shot, and the 2nd in 16-shot. We see that inlanguage outperforms XLM-R EM in all K-shots, while code-switch achieves comparable or even lower F 1 to XLM-R EM for K = 8, suggesting that the choice of prompt affects the few-shot performance greatly, thus needs careful consideration.
On a per-group basis, we find that in-language prompting outperforms other methods for middleand low-resourced languages. Similar observations can also be drawn from fully supervised results. We conclude that, with sufficient supervision, inlanguage is the optimal variant to prompt rather Table 5: Few-shot results by group in micro-F1 (%) on the SMiLER (Seganti et al., 2021) dataset averaged over five runs. We macro-average results for each language group (see Figure 2) and over all languages (X). Inlanguage prompting performs best in most settings and language groups. Our variants are especially strong for medium-and lower-resource language groups. See Table 7 in Appendix C for detailed results with mean and std. for each language.
than code-switch. We hypothesize it is due to the pre-training scenario, where the PLM rarely sees code-mixed text (Santy et al., 2021).
Zero-shot Results Table 6 presents the per-language results in zeroshot scenarios. We consider the random baseline for comparison (Zhao and Schütze, 2021;Winata et al., 2021). We notice that performance of the random baseline varies a lot across languages, since the languages have different number of classes in the dataset (cf. Table 6: Zero-shot results in micro-F1 (%) on the SMiLER dataset. "SVO" and "SOV": word order of prompting.
Overall, Code-switch prompting performs the best in the zero-shot in-context scenario. In cross-lingual transfer experiments, English-task training greatly improves the performance on all the other 13 languages.
margin, in both word orders, while in-language prompting performs worse than the random baseline in 6 languages. Code-switch prompting outperforms in-language prompting across all the 13 non-English languages, using SVO-template. We assume that, without in-language training, the PLM understands the task best when prompted in English. The impressive performance of code-switch shows the PLM is able to transfer its pre-trained knowledge in English to other languages. We also find that the performance is also highly indicated by the number of classes, with worst F 1 scores achieved in EN, KO and PT (36, 28 and 22 classes), and best scores in AR, RU and UK (9, 8 and 7 classes). In addition, we observe that word order does not play a significant role for most languages, except for FA, which is an SOV-language and has 54.5 F 1 gain from in-language prompting with an SOV-template. For zero-shot cross-lingual transfer, we see that non-English tasks benefit from English in-domain prompt-based fine-tuning, and the F 1 gain improves with the English data size. For 5 languages (ES, FA, NL, SV, and UK), zero-shot transfer after training on 268k English examples delivers even better results than in-language fully supervised training (cf. Table 4). Sanh et al. (2022) show that including RC-specific prompt input in English during pre-training can help in other languages.
Discussion Based on the results above, we answer the research questions from Section 1.
RQ1. Which is the most effective way to prompt? In the fully-supervised and few-shot scenario, in-language prompting displays the best re-sults. This appears to stem from a solid performance across all languages in both settings. Its worst performance is 31.8 F 1 for Polish 8-shot (see Table 7 in Appendix C). All other methods have results lower than 15.0 F 1 for some language. This indicates that with little supervision mT5 is able to perform the task when prompted in the language of the original text. However, zero-shot results strongly prefer code-switch prompting. It could follow that, without fine-tuning, the model's understanding of this task is much better in English.
RQ2. How well does our method perform in different data regimes and languages? Averaged over all languages, all our variants outperform the baselines, except for 8-shot. For some high-resource languages, XLM-R EM is able to outperform our method. On the other hand, for low-resource languages null prompts are a better baseline which we consistently outperform. This could indicate that prompting the underlying mT5 model is better suited for multilingual RC on SMiLER. Overall, the results suggest that minimal translation can be very helpful for multilingual relation classification.
B Verbalizers for SMiLER • EN "birth-place": "birth place", "eats": "eats", "event-year": "event year", "firstproduct": "first product", "from-country": "from country", "has-author": "has author", "has-child": "has child", "has-edu": "has education", "has-genre": "has genre", "has-height": "has height", "has-highestmountain": "has highest mountain", "haslength": "has length", "has-lifespan": "has lifespan", "has-nationality": "has nationality", "has-occupation": "has occupation", "has-parent": "has parent", "has-population": "has population", "has-sibling": "has sibling", "has-spouse": "has spouse", "hastourist-attraction": "has tourist attraction", "has-type": "has type", "has-weight": "has weight", "headquarters": "headquarters", "invented-by": "invented by", "inventedwhen": "invented when", "is-member-of": "is member of", "is-where": "located in", "loc-leader": "location leader", "movie-hasdirector": "movie has director", "no_relation": "no relation", "org-has-founder": "organization has founder", "org-has-member": "organization has member", "org-leader": "organization leader", "post-code": "post code", "starring": "starring", "won-award": "won award"; 1 Introduction 010 • AR "event-year": "
", "hasedu": " ", "has-genre": "
"has-population": " ", "has-type":
" ", "is-member-of": " ",
• DE "birth-place": "Geburtsort", "eventyear": "Veranstaltungsjahr", "from-country": "vom Land", "has-author": "hat Autor", "haschild": "hat Kind", "has-edu": "hat Bildung", "has-genre": "hat Genre", "has-occupation": "hat Beruf", "has-parent": "hat Elternteil", "has-population": "hat Bevölkerung", "has-spouse": "hat Ehepartner", "has-type": "hat Typ", "headquarters": "Hauptsitz", "ismember-of": "ist Mitglied von", "is-where": "gelegen in", "loc-leader": "Standortleiter", "movie-has-director": "Film hat Regisseur", "no_relation": "keine Beziehung", "org-hasfounder": "Organisation hat Gründer", "orghas-member": "Organisation hat Mitglied", "org-leader": "Organisationsleiter", "wonaward": "gewann eine Auszeichnung";
• ES "birth-place": "lugar de nacimiento", "event-year": "año del evento", "fromcountry": "del país", "has-author": "tiene autor", "has-child": "tiene hijo", "hasedu": "tiene educación", "has-genre": "tiene género", "has-occupation": "tiene ocupación", "has-parent": "tiene padre", "has-population": "tiene población", "has-spouse": "tiene cónyuge", "has-type": "tiene tipo", "headquarters": "sede central", "is-member-of": "es miembro de", "is-where": "situado en", "loc-leader": "líder de ubicación", "moviehas-director": "película cuenta con el director", "no_relation": "sin relación", "orghas-founder": "organización cuenta con el fundador", "org-has-member": "organización tiene miembro", "won-award": "ganó el premio"; • AR "event-year": " ", "has-011 edu": " ", "has-genre": "
A.1 Hyperparameter Search We investigated the following possible hyperparameters for few-shot settings. For fully-supervised, we take hyperparameters from literature (see Section 4.4).
Number of epochs: [10,20]; Learning rate: [1 × 10 −5 , 3 × 10 −5 , 1 × 10 −4 , 3 × 10 −4 ]. Batch size: [16,64,256], not tuned but selected based on available GPU VRAM.
We manually tune these hyperparameters, based on the micro-F 1 score on the validation set.
A.2 Computing Infrastructure Fully supervised experiments are conducted on a single A100-80GB GPU. Few-shot and zero-shot experiments are conducted on a single A100 GPU.
A.3 Average Running Time Fully supervised It takes 5 hours to train for 1 run with mT5 BASE and a prompt method (null prompts, CS, SP and IL) on either English, or all other languages in total. With XLM-R EM the running time is 3 hours.
Few 

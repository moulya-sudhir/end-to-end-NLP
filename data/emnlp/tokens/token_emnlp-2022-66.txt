Learning to Adapt to Low-Resource Paraphrase Generation
Paraphrase generation is a longstanding NLP task and achieves great success with the aid of large corpora. However, transferring a paraphrasing model to another domain encounters the problem of domain shifting especially when the data is sparse. At the same time, widely using large pre-trained language models (PLMs) faces the overfitting problem when training on scarce labeled data. To mitigate these two issues, we propose, LAPA, an effective adapter for PLMs optimized by meta-learning. LAPA has three-stage training on three types of related resources to solve this problem: 1. pre-training PLMs on unsupervised corpora, 2. inserting an adapter layer and meta-training on source domain labeled data, and 3. fine-tuning adapters on a small amount of target domain labeled data. This method enables paraphrase generation models to learn basic language knowledge first, then learn the paraphrasing task itself later, and finally adapt to the target task. Our experimental results demonstrate that LAPA achieves state-of-the-art in supervised, unsupervised, and low-resource settings on three benchmark datasets. With only 2% of trainable parameters and 1% labeled data of the target task, our approach can achieve a competitive performance with previous work.
Introduction Paraphrase generation can comprehend a sentence and generate another with the same semantics but with variations in lexicon or syntax, which has various applications on downstream tasks including query rewriting (Dong et al., 2017), data augmentation (Iyyer et al., 2018) and language model pre-training (Lewis et al., 2020a). Conventional approaches (Prakash et al., 2016;Chowdhury et al., 2022) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. However, the success of these methods often relies on a large number of parallel paraphrases, whose collection is timeconsuming and requires a lot of domain knowledge. Therefore, in real scenarios with a small amount of parallel data, the model suffers from performance drops facing domain gaps. This phenomenon, known as domain shift problem (Pan and Yang, 2009), comes from the representation gap between training and testing domains with different writing styles or forms.
To tackle this problem, unsupervised methods such as editing-based approaches (Bowman et al., 2016;Miao et al., 2019) or reinforcement learning (Li et al., 2018;Siddique et al., 2020), and weakly-supervised methods such as retrievalenhanced (Ding et al., 2021;Yin et al., 2022) or prompt-based  do not introduce or only introduce a small number of supervised signals, which limits their performance such that underperforms supervised methods. In fact, largescale unlabeled corpus data (UCD) and labeled source domain data (LSDD), as well as a few labeled target domain data (LTDD), can be easily achieved. Therefore, we propose a new three-stage learning paradigm: pre-training, meta-learning, and fine-tuning, aiming to leverage the pre-trained knowledge on UCD, source domain knowledge on LSDD, and adapt to target domain on LSDD to improve the performance of low-resource paraphrase generation. In order to successfully implement this learning paradigm, we propose a simple yet effective model which combined pre-trained language model (PLM) and MAML (Finn et al., 2017), named Learning to Adapt to low-resource PAraphrase generation (LAPA). Specifically, before meta-learning, we insert an adapter layer into each transformer layer of PLM. An adapter layer is composed of a few parameters of feedforward layer and residual connection. During meta-training and fine-tuning, only the adapter layer and normalization layer are trainable. Parameter freezing and residual connection can retain the prior knowledge of PLM to avoid negative transfer effects. Smaller-scale parameter updating can prevent MAML from gradient explosion or diminishing problems when the number of MAML inner loop iterations and model depth increase (Antoniou et al., 2019) or training data is extremely scarce.
Overall, we hold the idea that paraphrasing is a fundamental ability of human beings. The paraphrase model should not rely on domain and seen data. Therefore, we are committed to characterizing the basic ability of the paraphrase model, obtaining gains from each domain, and applying it to a specific domain. Our contributions are summarized as follows:
• We define a novel three stages learning paradigm for low-resource paraphrase generation in data scarcity scenarios.
• We propose that LAPA implement this learning paradigm, which transferred the PLM knowledge and source domain knowledge to complete the low-resource learning in the target domain quickly and with high quality.
• The supervised, unsupervised and weakly supervised experimental results of LAPA on three benchmark datasets achieve state-of-theart (SOTA). LAPA with only 2% of trainable parameters and 1% target task labeled data can achieve a competitive performance with previous works.
Related Work While the paraphrase generation performance is greatly improved with various supervised techniques (Zhao et al., 2008;Prakash et al., 2016;Egonmwan and Chali, 2019;Cao and Wan, 2020;Hosking and Lapata, 2021;Chowdhury et al., 2022), there are few studies regarding the lowresource setting. West et al. (2021) and Meng et al. (2021) proposed novel unsupervised paraphrasing strategies by data augmentation based on reflective decoding or diverse decoding. Ding et al. (2021) and Yin et al. (2022) achieved improvements on various low-resource datasets with retrieved data and meta reinforcement learning. However, these studies only use a single large corpus for training the full PLM, which suffers from domainshifting problems (Wang et al., 2019). Besides, under the extreme low-resource setting, directly fine-tuning the full PLM will cause an over-fitting problem (Antoniou et al., 2019). Meta-learning helps improve low-resource performance in various recent studies, such as image classification (Soh et al., 2020), vehicle tracking  and natural language processing (Park et al., 2021;Chen and Shuai, 2021;Hong and Jang, 2022). Finn et al. (2017) proposed a meta learner named MAML, which uses other example tasks to learn how to effectively initialize a basic learner, which can be quickly generalized to new tasks. Adapter modules have been mainly used for parameter-efficient and quick fine-tuning of a basic PLMs to new tasks (Houlsby et al., 2019;Bapna and Firat, 2019;Pfeiffer et al., 2020Pfeiffer et al., , 2021. Our paper proposes to incorporate meta-learning approaches to realize multi-domain migration and task adapter to realize parameter effective transfer learning (i.e., limited trainable parameters) to mitigate the above problems of paraphrase generation.
3 The Approach
Learning Paradigm As shown in Figure 1, the workflow of our learning paradigm including three stages: 1. Backbone model pre-training on large unlabeled corpora 2. Adapter model meta-training on large source corpora using the meta-learning and 3. Adapter model fine-tuning on target corpora and evaluate model performance. The prior knowledge K pri comes from first two stages: pre-training and meta-learning. We denote our backbone model by f (θ) with parameters θ. The first stage is pretraining on unlabeled corpora D pre , and we get f (θ pre ). The second stage is meta-training on adapter model f [θ pre , Φ] with additional parameters Φ and frozen θ pre on related source corpora D src , and we got f [θ pre , Φ src ]. Finally, we initialize the adapter model with [θ pre , Φ src ] and finetune Φ src on the target corpus D tgt to obtain a target model f [θ pre , Φ tgt ] which are model parameters after target adapter, i.e., the posterior knowledge K por .
Backbone Model Because PLM is equipped with prior knowledge K pri and exhibits strong capabilities in a range of different generative tasks, we choose the pretrained BART (Lewis et al., 2020b) as the backbone model for paraphrase generation. Specifically, given a labeled paraphrase pair i = (x,ŷ), where x = [x 1 , . . . , x N ],ŷ = [ŷ 1 , . . . ,ŷ M ], and inputting x, the model has produced a predicted segment sequence y <t = [y 1 , . . . , y t−1 ] before time t, then the probability that the token generated at time t is y t is p(y t |y <t , x, θ). The model is optimized by minimizing the negative log-likelihood:
L i (f (θ)) = − M t=1 log p(ŷ t |y <t , x, θ).
Adapter Model The adapter model is obtained by inserting the adapter layer into each transformer layer of the backbone model. An adapter layer is a bottlenecked feed-forward network consisting of a downproject layer, a nonlinearity function and an upproject layer. In addition, a skip connection layer from input to output prevents the noised initialization from interference with the training initially. For the adapter in layer l, the function can be formulated as: Adapter(z l ) = W l u ReLU (W l d z l ) + z l where z l represents the inputs of the adapter in layer l. Besides, the normalization layers are trainable and initialized from the previous training stage.
Meta-Learning The second stage is adapter model meta traning based on MAML (Finn et al., 2017). The learning process is shown in Algorithm 1. First, we freeze the backbone model parameters θ pre that have been pre-trained in the pre-training stage, then, add new adapters with parameters Φ to get adapter model f [θ pre , Φ]. Based on Algorithm 1, we first complete the meta-learning of the adapter model on the source corpus D src to help the adapters Φ find the initialization parameters Φ src suitable for paraphrase generation to adapt faster target task. At this Compute adapted parameters with gradient descent: 
[θ,Φ] = [θ, Φ] − α∇ Φ L i (f [θ, Φ]) 8: end for 9: Update [θ, Φ] ← [θ, Φ] − β∇ Φ T i ∼p(T ) L i (f [θ,Φ]
Datasets We conducted experiments on Quora 1 , Twitter (Lan et al., 2017) and MSCOCO (Lin et al., 2014) benchmark datasets, and followed the same setting in previous works (Lin et al., 2014;Liu et al., 2020;Ding et al., 2021). For meta-learning, we choose a different source task's labeld train-set from the target task to randomly construct meta tasks. Appendix Table 4 describes more details.
Baselines Supervised methods are trained with all parallel sentences of target task. Unsupervised baselines   (Lewis et al., 2020b). Like our work, they all used BART as PLM. To compare the performance of our method against the previous works, we use BLEU (Papineni et al., 2002), iBLEU (Sun and Zhou, 2012) and ROUGE (Hovy et al., 2006) metrics. All metrics are computed between the generated and the reference paraphrases in the test set (Kumar et al., 2020). We also separately analyze the impact of target task Example Input Can we ever store energy produced in lightning?
Experimental Results How does a pencil and a liquid eyeliner differ?
How come there's no physical evidence for sea dragons existing if they're the largestanimal in the sea. Table 2: Examples of the generated paraphrases on Quora dataset. We highlight the key phrases in the paraphrases generated and use wavy underline to show the matched parts between LAPA and reference.
labeled data scale under low-resource setting. Figure 2 shows the experimental results on the Quora dataset. It can be conclused that LAPA has a significant effect compared with BART under the same small data size. LAPA can achieve the effect of 89% to 93% of the full amount of data when not using any target task labeled data; when using a very small amount of data such as 0.5k (i.e 0.5% of the full data), it can be improved to 94% to 96%; when the amount of data increases to 10k (i.e 10% of the full data), the performance is almost the same as the full amount of data 100k. It should be pointed out that which dataset is selected as the source data can not have a substantial impact on the migration results, as shown in Figure 3. The results independent of the source dataset prove that LAPA can learn the paraphrasing task itself on any dataset, so it has strong adaptability to the target task.  We conduct an ablation study with three variants under the low-resource setting of the Quora dataset to investigate the contribution of each component in the proposed method. The experimental results are shown in Table 3. We can get: first, using pre-trained BART can get good results; second, by adding the source task dataset for pre-trained BART, the knowledge of the source domain can be effectively learned, thereby improving the performance of the model in the target domain; third, adding our proposed meta-learning framework can again effectively improve the speed and quality of learning the source domain (LAPA only has 2.8% training parameters compared with BART) and achieve the best performance.
Case Study Table 2 lists some paraphrases generated by LAPA and BART with different experimental settings. We can observe that paraphrases produced by LAPA are not only grammatically correct but preserve the semantics of Input more completely, and the expression is closer to Reference than the other methods. This benefits from the fact that our LAPA approach can make full use of source domain data and task features, and better preserve the prior knowledge of PLM, so as to adapt to new target tasks quickly and efficiently.
Conclusion In this work, we investigate the problem of paraphrase generation under the low-resource setting and propose a simple yet effective approach LAPA. We effectively combine transfer learning and meta-learning by using adapter modules as the bridge. Whether in supervised, unsupervised or low-resource setting, the results that our approach achieves the SOTA results on benchmark datasets. In the future, we plan to explore how to choose a smaller but suitable high-quality source corpus for learning in the source domain to improve the effect of transferring to the target domain, because not all source domain data has a positive effect. Second, we plan to extend this framework to other AI fields to solve low-resource problems in other scenarios and enable more industrial applications.
Limitations The major limitation of present study is the need for source domain annotated data that can adapt to the target domain. Because this is the source of data for the knowledge of the learning task itself, it cannot be avoided. In the real world, we can find it from public free datasets, exchange it commercially with other institutions, or annotate a batch of raw data ourselves as a cold start to solve this problem. Secondly, this study also has insufficient research on related variables. Due to the limitation of time and article length, we have not been able to study. These findings provide the following insights for future research: What is the lower bound of the amount of source domain data that can be well adapted to the target task? Whether we can apply weak supervision, data augmentation and other methods to create source domain data? How to select high-quality source domain data to get a better adapter model? We leave these questions to future research. Twitter The twitter URL paraphrasing corpus is built by Lan et al. (2017) for paraphrase identification. We follow the setting in Li et al. (2018), Kazemnejad et al. (2020) and Siddique et al. (2020).
The detailed dataset statistics are summarized in Table 4 .  
A.2 Evaluation Details To make a fair and comprehensive assessment, we follow the same experiment setting of each comparison work (Li et al., 2018;Liu et al., 2020;Ding et al., 2021) and conduct the comparison respectively. For data preprocessing, all the sentences are lower cased, and truncate all sentences to up to 20 words. <s> and </s> are spliced to the front and back end of the sentence as start and end markers.
For evaluation metrics, we use BLEU, i-BLEU and ROUGE that have been widely used in the previous work to measure the quality of the paraphrases. The i-BLUE aims to measure the diversity of expression in the generated paraphrases by penalizing copying words from input sentences. Specifically, we follow the unsupervised paraphrase generation baselines and set the balancing parameter α = 0.9.
A.3 Implementation Our experiments were conducted with PyToch on NVIDIA Tesla V100 16GB GPU. Following the comparison methods, we used BART-large as the pre-trained language model and use its pre-trained parameters. For adapter modules, the hidden size is 128. For meta-training, unless otherwise specified, a meta batch includes 3 tasks, and the batch size of each task is 10. Both basic learners and meta learners use the AdamW (Loshchilov and Hutter, 2019) optimizer for optimization, and the learning rate is set by grid search in 1e-5, 5e-5, 1e-6 and 5e-6. The internal gradient step size is 4, and the whole model has enough step size for training. For meta verification, we use a corpus excluded from the source task and the target task. For fine-tuning, we use validation set to select the best model for metrics calculation.

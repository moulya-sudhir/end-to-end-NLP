Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs
Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling. However, existing methods (1) only model the unidirectional guidance from intent to slot; (2) adopt homogeneous graphs to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance. In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the mutual guidances between the two tasks. In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances. Specifically, we propose two heterogeneous graph attention networks working on the proposed two heterogeneous semantics-label graphs, which effectively represent the relations among the semantics nodes and label nodes. Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3% over the previous best model on Mix-ATIS dataset in overall accuracy.
Introduction Spoken language understanding (SLU) (Young et al., 2013) is a fundamental task in dialog systems. Its objective is to capture the comprehensive semantics of user utterances, and it typically includes two subtasks: intent detection and slot filling (Tur and De Mori, 2011). Intent detection aims to predict the intention of the user utterance and slot filling aims to extract additional information or constraints expressed in the utterance.
Recently, researchers discovered that these two tasks are closely tied, and a bunch of models (Goo et al., 2018;Liu et al., 2019a;E et al., 2019;Qin et al., 2019) are proposed to combine the single-intent detection and slot filling in multi-task frameworks to leverage their correlations. However, in real-world scenarios, a user usually expresses multiple intents in a single utterance. To this end, (Kim et al., 2017) begin to tackle the multi-intent detection task and (Gangadharaiah and Narayanaswamy, 2019) make the first attempt to jointly model the multiple intent detection and slot filling in a multi-task framework. (Qin et al., 2020) propose an AGIF model to adaptively integrate the fine-grained multi-intent prediction information into the autoregressive decoding process of slot filling via graph attention network (GAT) (Velickovic et al., 2018). And (Qin et al., 2021b) further propose a non-autoregressive GAT-based model which enhances the interactions between the predicted multiple intents and the slot hidden states, obtaining state-of-the-art results and significant speedup.
Despite the promising progress that existing multi-intent SLU joint models have achieved, we discover that they suffer from two main issues:
(1) Ignoring the guidance from slot to intent. Since previous researchers realized that "slot labels could depend on the intent" (Gangadharaiah and Narayanaswamy, 2019), existing models leverage the information of the predicted intents to guide slot filling, as shown in Fig. 1(a). However, they ig- However, in previous works, the only guidance that the multiple intent detection task can get from the joint model is sharing the basic semantics with the slot filling task. As a result, the lack of guidance from slot to intent limits multiple intent detection, and so the joint task.
(2) Node and edge ambiguity in the semanticslabel graph. (Qin et al., 2020(Qin et al., , 2021b apply GATs over the constructed graphs to model the interactions among the slot semantics nodes and intent label nodes. However, their graphs are homogeneous, in which all nodes and edges are treated as the same type. For a slot semantics node, the information from intent label nodes and other slot semantics nodes play different roles, while the homogeneous graph cannot discriminate their specific contributions, causing ambiguity. Therefore, the heterogeneous graphs should be designed to represent the relations among the semantic nodes and label nodes to facilitate better interactions.
In this paper, we propose a novel model termed Co-guiding Net to tackle the above two issues. For the first issue, Co-guiding Net implements a two-stage framework as shown in Fig. 1 (b). The first stage produces the initial estimated labels for the two tasks and the second stage leverages the estimated labels as prior label information to allow the two tasks mutually guide each other. For the second issue, we propose two heterogeneous semantics-label graphs (HSLGs): (1) a slot-to-intent semantics-label graph (S2I-SLG) that effectively represents the relations among the intent semantics nodes and slot label nodes; (2) an intent-to-slot semantics-label graph (I2S-SLG) that effectively represents the relations among the slot semantics nodes and intent label nodes. Moreover, two heterogeneous graph attention networks (HGATs) are proposed to work on the two proposed graphs for modeling the guidances from slot to intent and intent to slot, respectively. Experiment results show that our Co-guiding Net significantly outperforms previous models, and model analysis further verifies the advantages of our model.
The contributions of our work are three-fold: (1) We propose Co-guiding Net 1 , which implements a two-stage framework allowing multiple intent detection and slot filling mutually guide each other. We make the first attempt to achieve the mutual guidances between the two tasks. (2) We propose two heterogeneous semantics-label graphs as appropriate platforms for interactions between semantics nodes and label nodes. And we propose two heterogeneous graph attention networks to model the mutual guidances between the two tasks. (3) Experiment results demonstrate that our model achieves new state-of-the-art performance.
Co-guiding Problem Definition Given a input utterance denoted as U = {u i } n 1 , multiple intent detection can be formulated as a multi-label classification task that outputs multiple intent labels corresponding to the input utterance. And slot filling is a sequence labeling task that maps each u i into a slot label.
Next, before diving into the details of Coguiding Net's architecture, we first introduce the construction of the two heterogeneous graphs.
Slot-to-Intent Semantics-Label Graph To provide an appropriate platform for modeling the guidance from the estimated slot labels to multiple intent detection, we design a slot-to-intent semantics-label graph (S2I-SLG), which represents the relations among the semantics of multiple intent detection and the estimated slot labels. S2I-SLG is a heterogeneous graph and an example is shown in Fig. 3 (a). It contains two types of nodes: intent semantics nodes (e.g., I 1 , ..., I 5 ) and slot label (SL) nodes (e.g., SL 1 , ..., SL 5 ). And there are four types of edges in S2I-SLG, as shown in Fig. 3 (b). Each edge type corresponds to an individual kind of information aggregation on the graph.  Mathematically, the S2I-SLG can be denoted as
G s2i = (V s2i , E s2i , A s2i , R s2i ), in which V s2i
is the set of all nodes, E s2i is the set of all edges, A s2i is the set of two node types and R s2i is the set of four edge types. Each node v s2i and each edge e s2i are associated with their type mapping functions τ (v s2i ) : V s2i → A s2i and ϕ(e s2i ) : E s2i → R s2i . For instance, in Fig. 3, the SL 2 node belongs to V s2i , while its node type SL belongs to A s2i ; the edge from SL 2 to I 3 belongs to E s2i , while its edge type slot_to_intent_guidance belongs to R s2i . Besides, edges in S2I-SLG are based on local connections. For example, node I i is connected to {I i−w , ..., I i+w } and {SL i−w , ..., SL i+w }, where w is a hyper-parameter of the local window size.
Intent-to-Slot Semantics-Label Graph To present a platform for accommodating the guidance from the estimated intent labels to slot filling, we design an intent-to-slot semantics-label graph (I2S-SLG) that represents the relations among the slot semantics nodes and the intent label nodes. I2S-SLG is also a heterogeneous graph and an example is shown in Fig. 4 (a). It contains two types of nodes: slot semantics nodes (e.g., S 1 , ..., S 5 ) and intent label (IL) nodes (e.g., IL 1 , ..., IL 5 ). And Fig. 4 (b) shows the four edge types. Each edge type corresponds to an individual kind of information aggregation on the graph.
Mathematically, the I2S-SLG can be denoted as G i2s = (V i2s , E i2s , A i2s , R i2s ). Each node v i2s and each edge e i2s are associated with their type mapping functions τ (v i2s ) and ϕ(e i2s ). The connections in I2S-SLG are a little different from S2I-SLG. Since intents are sentence-level, each IL node is globally connected with all nodes. For S i node, it is connected to {S i−w , ..., S i+w } and {IL 1 , ..., IL m }, where w is the local window size and m is the number of estimated intents.
Shared Self-Attentive Encoder Following (Qin et al., 2020(Qin et al., , 2021b, we adopt a shared self-attentive encoder to produce the initial hidden states containing the basic semantics. It includes a BiLSTM and a self-attention module. BiLSTM captures the temporal dependencies:
hi = BiLSTM xi, hi−1, hi+1(1)
where x i is the word vector of u i . Now we obtain the context-sensitive hidden statesĤ = {ĥ i } n 1 . Self-attention captures the global dependencies:
H ′ = softmax QK ⊤ √ d k V (2)
where H ′ is the global contextual hidden states output by self-attention; Q, K and V are matrices obtained by applying different linear projections on the input utterance word vector matrix.
Then we concatenate the output of BiLSTM and self-attention to form the output of the shared selfattentive encoder: H =Ĥ∥H ′ , where H = {h i } n 1 and ∥ denotes concatenation operation.
Initial Estimation Multiple Intent Detection To obtain the taskspecific features for multiple intent detection, we apply a BiLSTM layer over H:  Following (Qin et al., 2020(Qin et al., , 2021b, we conduct token-level multi-intent detection. Each h [I,0] i is fed into the intent decoder. Specifically, the intent label distributions of the i-th word are obtained by:
h [I,0] i = BiLSTM I hi, h [I,0] i−1 , h [I,0] i+1 (3) Shared Self-
y [I,0] i = sigmoid W 1 I σ(W 2 I h [I,0] i +b 2 I ) +b 1 I (4)
where σ denotes the non-linear activation function; W * and b * are model parameters.
Then the estimated sentence-level intent labels {IL 1 , ..., IL m } are obtained by the token-level intent voting (Qin et al., 2021b).
Slot Filling (Qin et al., 2021b) propose a nonautoregressive paradigm for slot filling decoding, which achieves significant speedup. In this paper, we also conduct parallel slot filling decoding.
We first apply a BiLSTM over H to obtain the task-specific features for slot filling:
h [S,0] i = BiLSTM S (hi, h [S,0] i−1 , h [S,0] i+1 )(5)
Then use a softmax classifier to generate the slot label distribution for each word:
y [S,0] i = softmax W 1 S σ(W 2 S h [S,0] i +b 2 S ) +b 1 S (6)
And the estimated slot label for each word is obtained by SL i = arg max(y
[S,0] i
).
Heterogeneous Graph Attention Network State-of-the-art models (Qin et al., 2020(Qin et al., , 2021b) use a homogeneous graph to connect the semantic nodes of slot filling and the intent label nodes. And GAT (Velickovic et al., 2018) is adopted to achieve information aggregation. In Sec. 1, we propose that this manner cannot effectively learn the interactions between one task's semantics and the estimated labels of the other task. To tackle this issue, we propose two heterogeneous graphs (S2I-SLG and I2S-SLG) to effectively represent the relations among the semantic nodes and label nodes.
To model the interactions between semantics and labels on the proposed graphs, we propose a Heterogeneous Graph Attention Network (HGAT). When aggregating the information into a node, HGAT can discriminate the specific information from different types of nodes along different relations. And two HGATs (S2I-HGAT and I2S-HGAT) are applied on S2I-SLG and I2S-SLG, respectively. Specifically, S2I-HGAT can be formulated as follows:
h l+1 i = K ∥ k=1 σ   j∈N i s2i W [r,k,1] s2i α [r,k] ij h l j   , r = ϕ e [j,i] s2i α [r,k] ij = exp W [r,k,2] s2i h l i W [r,k,3] s2i h l j T / √ d u∈N r,i s2i exp W [r,k,2] s2i h l i W [r,k,3] s2i h l u T / √ d (7)
where K denotes the total head number; N i s2i denotes the set of incoming neighbors of node i on S2I-SLG; W [r,k, * ] s2i are weight matrices of edge type r on the k-th head; e [j,i] s2i denotes the edge from node j to node i on S2I-SLG; N r,i s2i denotes the nodes connected to node i with r-type edges on S2I-SLG; d is the dimension of node hidden state.
I2S-HGAT can be derived like Eq. 7.
Intent Decoding with Slot Guidance In the first stage, we obtain the initial intent features H [I,0] = {h I,0 i } n i and the initial estimated slot labels sequence {SL 1 , ..., SL n }. Now we project the slot labels into vector form using the slot label embedding matrix, obtaining E sl = {e 1 sl , ..., e n sl }. Then we feed H [I,0] and E sl into S2I-HGAT to model their interactions, allowing the estimated slot label information to guide the intent decoding:
where [H [I,0] , E sl ] denotes the input node representation; θ I denotes S2I-HGAT's parameters. L denotes the total layer number.
Finally, H [I,L] is fed to intent decoder, producing the intent label distributions for the utterance words:
Y [I,1] = {y [I,1] i , ..., y [I,1] n }.
And the final output sentence-level intents are obtained via applying token-level intent voting over Y [I,1] .
Slot Decoding with Intent Guidance Intent-aware BiLSTM Since the B-I-O tags of slot labels have temporal dependencies, we use an intent-aware BiLSTM to model the temporal dependencies among slot hidden states with the guidance of estimated intents:
h [S,0] i = BiLSTM(y [I,0] i ∥h [S,0] i ,h [S,0] i−1 ,h [S,0] i+1 )(9)
I2S-HGAT We first project the estimated intent labels {IL j } m 1 into vectors using the intent label embedding matrix, obtaining E il = {e 1 il , ..., e m il }. Then we feedH S and E il into I2S-HGAT to model their interactions, allowing the estimated intent label information to guide the slot decoding:
H [S,L] = I2S-HGAT [H S , E il ], Gi2s, θS(10)
where [H [S] , E il ] denotes the input node representation; θ S denotes I2S-HGAT's parameters.
Finally, H [S,L] is fed to slot decoder, producing the slot label distributions for each word:
Y [S,1] = {y [S,1] i , ..., y [S,1] n }.
And the final output slot labels are obtained by applying arg max over Y [S,1] .
Training Objective Loss Function The loss function for multiple intent detection is:
CE(ŷ, y) =ŷ log(y) + (1 −ŷ) log(1 − y) LI = 1 t=0 n i=1 N I j=1 CE ŷ I i [j], y [I,t] i [j](11)
And the loss function for slot filling is:
LS = 1 t=0 n i=1 N S j=1ŷ S i [j] log y [S,t] i [j](12)
where N I and N S denote the total numbers of intent labels and slot labels;ŷ I i andŷ S i denote the ground-truth intent labels and slot labels.
Margin Penalty The core of our model is to let the two tasks mutually guide each other. Intuitively, the predictions in the second stage should be better than those in the first stage. To force our model obey this rule, we design a margin penalty (L mp ) for each task, whose aim is to improve the probabilities of the correct labels. Specifically, the formulations of L mp I and L mp S are:
L mp I = n i=1 N I j=1ŷ I i [j] max 0, y [I,0] i [j] − y [I,1] i [j] L mp S = n i=1 N S j=1ŷ S i [j] max 0, y [S,0] i [j] − y [S,1] i [j](13)
Model Training The training objective L is the weighted sum of loss functions and margin regularizations of the two tasks:
L = γ (LI + βI L mp I ) + (1 − γ) (LS + βSL mp S ) (14)
where γ is the coefficient balancing the two tasks; β I and β S are the coefficients of the margin regularization for the two tasks.
3 Experiments
Datasets and Metrics Following previous works, MixATIS and MixS-NIPS (Hemphill et al., 1990;Coucke et al., 2018;Qin et al., 2020)  As for evaluation metrics, following previous works, we adopt accuracy (Acc) for multiple intent detection, F1 score for slot filling, and overall accuracy for the sentence-level semantic frame parsing. Overall accuracy denotes the ratio of sentences whose intents and slots are all correctly predicted.
Implementation Details Following previous works, the word and label embeddings are trained from scratch 2 . The dimensions of word embedding, label embedding, and hidden state are 256 on MixATIS, while on MixS-NIPS they are 256, 128, and 256. The layer number of all GNNs is 2. Adam (Kingma and Ba, 2015) is used to train our model with a learning rate of 1e −3 and a weight decay of 1e −6 . As for the coefficients Eq.14, γ is 0.9 on MixATIS and 0.8 on  MixSNIPS; on both datasets, β I is 1e −6 and β S is 1e 0 . The model performing best on the dev set is selected then we report its results on the test set.
All experiments are conducted on RTX 6000. Our source code will be released.
Main Results The performance comparison of Co-guiding Net and baselines are shown in Table 1, from which we have the following observations:
(1) Co-guiding Net gains significant and consistent improvements on all tasks and datasets. Specifically, on MixATIS dataset, it overpasses the previous state-of-the-art model GL-GIN by 19.3%, 1.8%, and 3.7% on sentence-level semantic frame parsing, slot filling, and multiple intent detection, respectively; on MixSNIPS dataset, it overpasses GL-GIN by 5.2%, 1.2% and 2.1% on sentencelevel semantic frame parsing, slot filling and multiple intent detection, respectively. This is because our model achieves the mutual guidances between multiple intent detection and slot filling, allowing the two tasks to provide crucial clues for each other. Besides, our designed HSLGs and HGATs can effectively model the interactions among the semantics nodes and label nodes, extracting the indicative clues from initial predictions.
(2) Co-guiding Net achieves a larger improvement on multiple intent detection than slot filling. The reason is that except for the guidance from multiple intent detection to slot filling, our model also achieves the guidance from slot filling to multiple intent detection, while previous models all ignore this. Besides, previous methods model the semantics-label interactions by homogeneous graph and GAT, limiting the performance. Differently, our model uses the heterogeneous semantics-label graphs to represent different relations among the semantic nodes and the label nodes, then applies the proposed HGATs over the graphs to achieve the interactions. Consequently, their performances (especially on multiple intent detection) are significantly inferior to our model.
(3) The improvements in overall accuracy are much sharper. We suppose the reason is that the achieved mutual guidances make the two tasks deeply coupled and allow them to stimulate each other using their initial predictions. For each task, its final outputs are guided by its and another task's initial predictions. By this means, the correct predictions of the two tasks can be better aligned. As a result, more test samples get correct sentence-level semantic frame parsing results, and then overall accuracy is boosted.
Model Analysis We conduct a set of ablation experiments to verify the advantages of our work from different perspectives, and the results are shown in Table 2.
Effect of Slot-to-Intent Guidance One of the core contributions of our work is achieving the mutual guidances between multiple intent detection and slot filling, while previous works only leverage the one-way message from intent to slot. Therefore, compared with previous works, one of the advantages of our work is modeling the slot-tointent guidance. To verify this, we design a variant termed w/o S2I-guidance and its result is shown in Table 2. We can observe that Intent Acc drops by 2.0% on MixATIS and 0.8% on MixSNIPS. Moreover, Overall Acc drops more significantly: 3.6% on MixATIS and 0.9% on MixSNIPS. This proves that the guidance from slot to intent can effectively benefit multiple intent detection, and achieving the mutual guidances between the two tasks can significantly improve Overall Acc.
Besides, although both of w/o S2I-guidance and  GL-GIN only leverage the one-way message from intent to slot, w/o S2I-guidance outperforms GL-GIN by large margins. We attribute this to our proposed heterogeneous semantics-label graphs and heterogeneous graph attention networks, whose advantages are verified in Sec. 3.4.3.
Effect of Intent-to-Slot Guidance To verify the effectiveness of intent-to-slot guidance, we design a variant termed w/o I2S-guidance and its result is shown in Table 2. We can find that the intent-to-slot guidance has a significant impact on performance. Specifically, w/o I2S-guidance cause nearly the same extent of performance drop on Overall Acc, proving that both of the intent-toslot guidance and slot-to-intent guidance are indispensable and achieving the mutual guidances can significantly boost the performance.
Effect of HSLGs and HGATs In this paper, we design two HSLGs: (i.e., S2I-SLG, I2S-SLG) and two HGATs (i.e., S2I-HGAT, I2S-HGAT). To verify their effectiveness, we design a variant termed w/o relations by removing the relations on the two HSLGs. In this case, S2I-SLG/I2S-SLG collapses to a homogeneous graph, and S2I-HGAT/I2S-HGAT collapses to a general GAT based on multi-head attentions. From Table 2, we can observe that w/o relations obtains dramatic drops on all metrics on both datasets. The apparent performance gap between w/o relations and Co-guiding Net verifies that (1) our proposed HSLGs can effectively represent the different relations among the semantics nodes and label nodes, providing appropriate platforms for modeling the mutual guidances between the two tasks;
(2) our proposed HGATs can sufficiently and effectively model interactions between the semantics and indicative label information via achieving the relation-specific attentive information aggregation on the HSLGs. Besides, although w/o relations obviously un-derperforms Co-guiding Net, it still significantly outperforms all baselines. We attribute this to the fact that our model achieves the mutual guidances between the two tasks, which allows them to promote each other via cross-task correlations. et al. (2021b) propose a Local Slot-aware GAT module to alleviate the uncoordinated slot problem (e.g., B-singer followed by I-song) (Wu et al., 2020) caused by the non-autoregressive fashion of slot filling. And the ablation study in (Qin et al., 2021b) proves that this module effectively improves the slot filling performance by modeling the local dependencies among slot hidden states.
Effect of I2S-HGAT for Capturing Local Slot Dependencies Qin
In their model (GL-GIN), the local dependencies are modeled in both of the local slot-aware GAT and subsequent global intent-slot GAT. We suppose the reason why GL-GIN needs the local Slotaware GAT is that the global intent-slot GAT in GL-GIN cannot effectively capture the local slot dependencies. GL-GIN's global slot-intent graph is homogeneous, and the GAT working on it treats the slot semantics nods and the intent label nodes equally without discrimination. Therefore, each slot hidden state receives indiscriminate information from both of its local slot hidden states and all intent labels, making it confusing to capture the local slot dependencies. In contrast, we believe our I2S-HLG and I2S-HGAT can effectively capture the slot local dependencies along the specific slot_semantics_dependencies relation, which is modeled together with other relations. Therefore, our Co-guiding Net does not include another module to capture the slot local dependencies.
To verify this, we design a variant termed +Local Slot-aware GAT, which is implemented by augmenting Co-guiding Net with the Local Slot-aware GAT (Qin et al., 2021b)  that not only the Local Slot-aware GAT does not bring improvement, it even causes performance drops. This proves that our I2S-HGAT can effectively capture the local slot dependencies.
A.1 Settings To evaluate Co-guiding Net's performance based on the pre-trained language model, we use the pretrained RoBERTa (Liu et al., 2019b) encoder to replace the original self-attentive encoder. We adopt the pre-trained RoBERTa-base version provided by Transformers (Wolf et al., 2020). For each word, its first subwords' hidden state generated by RoBERTa is taken as the word representation.
AdamW (Loshchilov and Hutter, 2019) optimizer is used for model training with the default setting, and RoBERTa is fine-tuned with model training.
Other model components are identical to the Coguiding Net based on LSTM, and we use the same hyper-parameters of the model rather than search for the optimal ones for RoBERTa+Co-guiding Net due to our limited computation resource.
A.2 Results Table 3 shows the result comparison of Coguiding Net, RoBERTa+Co-guiding Net, and their state-of-the-art counterparts: AGIF, GL-GIN, RoBERTa+AGIF, and RoBERTa+GL-GIN. We can find that although RoBERTa boosts the models' performance, RoBERTa+Co-guiding Net still significantly outperforms RoBERTa+AGIF and RoBERTa+GL-GIN. This can be attributed to the fact that although the pre-trained language model (PTLM) can enhance the word representations, it cannot achieve the guidance between the two tasks or the interactions between the semantics and label information, which are exactly the advantages of our Co-guiding Net. Therefore, collaborating with PTLM that has strong ability of language modeling, RoBERTa+Co-guiding Net gets its performance further boosted, achieving new state-of-the-art.  

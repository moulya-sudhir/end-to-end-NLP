-DOCSTART- -X- O
GLM -X- _ B-MethodName
: -X- _ O
General -X- _ B-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
Pretraining -X- _ I-MethodName
with -X- _ O
Autoregressive -X- _ O
Blank -X- _ O
Infilling -X- _ O

There -X- _ O
have -X- _ O
been -X- _ O
various -X- _ O
types -X- _ O
of -X- _ O
pretraining -X- _ O
architectures -X- _ O
including -X- _ O
autoencoding -X- _ O
models -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
BERT -X- _ O
) -X- _ O
, -X- _ O
autoregressive -X- _ O
models -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
GPT -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
encoder-decoder -X- _ O
models -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
T5 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
none -X- _ O
of -X- _ O
the -X- _ O
pretraining -X- _ O
frameworks -X- _ O
performs -X- _ O
the -X- _ O
best -X- _ O
for -X- _ O
all -X- _ O
tasks -X- _ O
of -X- _ O
three -X- _ O
main -X- _ O
categories -X- _ O
including -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
( -X- _ I-TaskName
NLU -X- _ I-TaskName
) -X- _ I-TaskName
, -X- _ O
unconditional -X- _ B-TaskName
generation -X- _ I-TaskName
, -X- _ O
and -X- _ O
conditional -X- _ B-TaskName
generation. -X- _ I-TaskName
We -X- _ O
propose -X- _ O
a -X- _ O
General -X- _ B-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
( -X- _ O
GLM -X- _ B-MethodName
) -X- _ O
based -X- _ O
on -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling -X- _ O
to -X- _ O
address -X- _ O
this -X- _ O
challenge. -X- _ O
GLM -X- _ B-MethodName
improves -X- _ O
blank -X- _ O
filling -X- _ O
pretraining -X- _ O
by -X- _ O
adding -X- _ O
2D -X- _ O
positional -X- _ O
encodings -X- _ O
and -X- _ O
allowing -X- _ O
an -X- _ O
arbitrary -X- _ O
order -X- _ O
to -X- _ O
predict -X- _ O
spans -X- _ O
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
performance -X- _ O
gains -X- _ O
over -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
on -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
Meanwhile -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
can -X- _ O
be -X- _ O
pretrained -X- _ O
for -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
tasks -X- _ O
by -X- _ O
varying -X- _ O
the -X- _ O
number -X- _ O
and -X- _ O
lengths -X- _ O
of -X- _ O
blanks. -X- _ O
On -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
tasks -X- _ O
across -X- _ O
NLU -X- _ B-TaskName
, -X- _ O
conditional -X- _ O
and -X- _ O
unconditional -X- _ O
generation -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
outperforms -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
and -X- _ O
GPT -X- _ B-MethodName
given -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
sizes -X- _ O
and -X- _ O
data -X- _ O
, -X- _ O
and -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
pretrained -X- _ O
model -X- _ O
with -X- _ O
1.25× -X- _ B-MethodName
parameters -X- _ O
of -X- _ O
BERT -X- _ O
Large -X- _ O
, -X- _ O
demonstrating -X- _ O
its -X- _ O
generalizability -X- _ O
to -X- _ O
different -X- _ O
downstream -X- _ O
tasks. -X- _ O
1 -X- _ O

Introduction -X- _ O
Language -X- _ O
models -X- _ O
pretrained -X- _ O
on -X- _ O
unlabeled -X- _ O
texts -X- _ O
have -X- _ O
substantially -X- _ O
advanced -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
in -X- _ O
various -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
ranging -X- _ O
from -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
( -X- _ O
NLU -X- _ B-TaskName
) -X- _ O
to -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
( -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
; -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
; -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Brown -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Downstream -X- _ O
task -X- _ O
performance -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
scale -X- _ O
of -X- _ O
the -X- _ O
parameters -X- _ O
have -X- _ O
also -X- _ O
constantly -X- _ O
increased -X- _ O
in -X- _ O
the -X- _ O
past -X- _ O
few -X- _ O
years. -X- _ O
* -X- _ O
The -X- _ O
first -X- _ O
two -X- _ O
authors -X- _ O
contributed -X- _ O
equally -X- _ O
. -X- _ O

† -X- _ O
Corresponding -X- _ O
authors. -X- _ O
1 -X- _ O
The -X- _ O
code -X- _ O
and -X- _ O
pre-trained -X- _ O
models -X- _ O
are -X- _ O
available -X- _ O
at -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
THUDM -X- _ O
/ -X- _ O
GLM -X- _ O
In -X- _ O
general -X- _ O
, -X- _ O
existing -X- _ O
pretraining -X- _ O
frameworks -X- _ O
can -X- _ O
be -X- _ O
categorized -X- _ O
into -X- _ O
three -X- _ O
families -X- _ O
: -X- _ O
autoregressive -X- _ B-MethodName
, -X- _ O
autoencoding -X- _ B-MethodName
, -X- _ O
and -X- _ O
encoder-decoder -X- _ B-MethodName
models. -X- _ O
Autoregressive -X- _ O
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
GPT -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
, -X- _ O
learn -X- _ O
left-to-right -X- _ O
language -X- _ O
models. -X- _ O
While -X- _ O
they -X- _ O
succeed -X- _ O
in -X- _ O
long-text -X- _ O
generation -X- _ O
and -X- _ O
show -X- _ O
fewshot -X- _ O
learning -X- _ O
ability -X- _ O
when -X- _ O
scaled -X- _ O
to -X- _ O
billions -X- _ O
of -X- _ O
parameters -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
; -X- _ O
Brown -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
inherent -X- _ O
disadvantage -X- _ O
is -X- _ O
the -X- _ O
unidirectional -X- _ O
attention -X- _ O
mechanism -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
not -X- _ O
fully -X- _ O
capture -X- _ O
the -X- _ O
dependencies -X- _ O
between -X- _ O
the -X- _ O
context -X- _ O
words -X- _ O
in -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
Autoencoding -X- _ B-MethodName
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
learn -X- _ O
bidirectional -X- _ O
context -X- _ O
encoders -X- _ O
via -X- _ O
denoising -X- _ B-TaskName
objectives -X- _ I-TaskName
, -X- _ O
e.g. -X- _ O
Masked -X- _ B-TaskName
Language -X- _ I-TaskName
Model -X- _ I-TaskName
( -X- _ O
MLM -X- _ B-TaskName
) -X- _ O
. -X- _ O
The -X- _ O
encoders -X- _ O
produce -X- _ O
contextualized -X- _ O
representations -X- _ O
that -X- _ O
suit -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
but -X- _ O
could -X- _ O
not -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
for -X- _ O
text -X- _ B-TaskName
generation. -X- _ I-TaskName
Encoder-decoder -X- _ B-MethodName
models -X- _ O
adopt -X- _ O
bidirectional -X- _ O
attention -X- _ O
for -X- _ O
the -X- _ O
encoder -X- _ O
, -X- _ O
unidirectional -X- _ O
attention -X- _ O
for -X- _ O
the -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
cross -X- _ O
attention -X- _ O
between -X- _ O
them -X- _ O
( -X- _ O
Song -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Bi -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
. -X- _ O
They -X- _ O
are -X- _ O
typically -X- _ O
deployed -X- _ O
in -X- _ O
conditional -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
text -X- _ B-TaskName
summarization -X- _ I-TaskName
and -X- _ O
response -X- _ B-TaskName
generation. -X- _ I-TaskName
2 -X- _ O
. -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
unifies -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
conditional -X- _ B-TaskName
generation -X- _ I-TaskName
via -X- _ O
encoder-decoder -X- _ O
models -X- _ O
but -X- _ O
requires -X- _ O
more -X- _ O
parameters -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
BRET-based -X- _ B-MethodName
models -X- _ O
such -X- _ O
as -X- _ O
RoBERTa -X- _ B-MethodName
and -X- _ O
DeBERTa -X- _ B-MethodName
( -X- _ O
He -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

None -X- _ O
of -X- _ O
these -X- _ O
pretraining -X- _ O
frameworks -X- _ O
is -X- _ O
flexible -X- _ O
enough -X- _ O
to -X- _ O
perform -X- _ O
competitively -X- _ O
across -X- _ O
all -X- _ O
NLP -X- _ O
tasks. -X- _ O
Previous -X- _ O
works -X- _ O
have -X- _ O
tried -X- _ O
to -X- _ O
unify -X- _ O
different -X- _ O
frameworks -X- _ O
by -X- _ O
combining -X- _ O
their -X- _ O
objectives -X- _ O
via -X- _ O
multi-task -X- _ B-TaskName
learning -X- _ I-TaskName
( -X- _ O
Dong -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Bao -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
autoencoding -X- _ O
and -X- _ O
autoregressive -X- _ O
objectives -X- _ O
differ -X- _ O
by -X- _ O
nature -X- _ O
, -X- _ O
a -X- _ O
simple -X- _ O
unification -X- _ O
can -X- _ O
not -X- _ O
fully -X- _ O
inherit -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
both -X- _ O
frameworks -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
pretraining -X- _ O
framework -X- _ O
named -X- _ O
GLM -X- _ B-MethodName
( -X- _ O
General -X- _ B-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
) -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling. -X- _ O
We -X- _ O
randomly -X- _ O
blank -X- _ O
out -X- _ O
continuous -X- _ O
spans -X- _ O
of -X- _ O
tokens -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
, -X- _ O
following -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
autoencoding -X- _ O
, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
sequentially -X- _ O
reconstruct -X- _ O
the -X- _ O
spans -X- _ O
, -X- _ O
following -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
autoregressive -X- _ O
pretraining -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
While -X- _ O
blanking -X- _ O
filling -X- _ O
has -X- _ O
been -X- _ O
used -X- _ O
in -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
for -X- _ O
text-to-text -X- _ O
pretraining -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
two -X- _ O
improvements -X- _ O
, -X- _ O
namely -X- _ O
span -X- _ O
shuffling -X- _ O
and -X- _ O
2D -X- _ O
positional -X- _ O
encoding. -X- _ O
Empirically -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
parameters -X- _ O
and -X- _ O
computational -X- _ O
cost -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
significantly -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
the -X- _ O
SuperGLUE -X- _ B-DatasetName
benchmark -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
of -X- _ O
4.6 -X- _ B-MetricValue
% -X- _ I-MetricValue
-5.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
and -X- _ O
outperforms -X- _ O
RoBERTa -X- _ B-MethodName
and -X- _ O
BART -X- _ B-MethodName
when -X- _ O
pretrained -X- _ O
on -X- _ O
a -X- _ O
corpus -X- _ O
of -X- _ O
similar -X- _ O
size -X- _ O
( -X- _ O
158GB -X- _ O
) -X- _ O
. -X- _ O
GLM -X- _ B-MethodName
also -X- _ O
significantly -X- _ O
outperforms -X- _ O
T5 -X- _ B-MethodName
on -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
generation -X- _ O
tasks -X- _ O
with -X- _ O
fewer -X- _ O
parameters -X- _ O
and -X- _ O
data -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
Pattern-Exploiting -X- _ B-MethodName
Training -X- _ I-MethodName
( -X- _ O
PET -X- _ B-MethodName
) -X- _ O
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
reformulate -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
as -X- _ O
manually-crafted -X- _ O
cloze -X- _ O
questions -X- _ O
that -X- _ O
mimic -X- _ O
human -X- _ O
language. -X- _ O
Different -X- _ O
from -X- _ O
the -X- _ O
BERTbased -X- _ B-MethodName
models -X- _ O
used -X- _ O
by -X- _ O
PET -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
can -X- _ O
naturally -X- _ O
handle -X- _ O
multi-token -X- _ O
answers -X- _ O
to -X- _ O
the -X- _ O
cloze -X- _ O
question -X- _ O
via -X- _ O
autoregressive -X- _ O
blank -X- _ O
filling -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
by -X- _ O
varying -X- _ O
the -X- _ O
number -X- _ O
and -X- _ O
lengths -X- _ O
of -X- _ O
missing -X- _ O
spans -X- _ O
, -X- _ O
the -X- _ O
autoregressive -X- _ B-TaskName
blank -X- _ I-TaskName
filling -X- _ I-TaskName
objective -X- _ O
can -X- _ O
pretrain -X- _ O
language -X- _ O
models -X- _ O
for -X- _ O
conditional -X- _ B-TaskName
and -X- _ O
unconditional -X- _ B-TaskName
generation. -X- _ I-TaskName
Through -X- _ O
multi-task -X- _ B-TaskName
learning -X- _ I-TaskName
of -X- _ O
different -X- _ O
pretraining -X- _ O
objectives -X- _ O
, -X- _ O
a -X- _ O
single -X- _ O
GLM -X- _ B-MethodName
can -X- _ O
excel -X- _ O
in -X- _ O
both -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
( -X- _ O
conditional -X- _ B-TaskName
and -X- _ I-TaskName
unconditional -X- _ I-TaskName
) -X- _ I-TaskName
text -X- _ I-TaskName
generation. -X- _ I-TaskName
Empirically -X- _ O
, -X- _ O
compared -X- _ O
with -X- _ O
standalone -X- _ O
baselines -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
with -X- _ O
multi-task -X- _ B-TaskName
pretraining -X- _ I-TaskName
achieves -X- _ O
improvements -X- _ O
in -X- _ O
NLU -X- _ B-TaskName
, -X- _ O
conditional -X- _ B-TaskName
text -X- _ I-TaskName
generation -X- _ I-TaskName
, -X- _ O
and -X- _ O
language -X- _ B-TaskName
modeling -X- _ I-TaskName
tasks -X- _ O
altogether -X- _ O
by -X- _ O
sharing -X- _ O
the -X- _ O
parameters -X- _ O
. -X- _ O

GLM -X- _ B-MethodName
Pretraining -X- _ O
Framework -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
general -X- _ O
pretraining -X- _ O
framework -X- _ O
GLM -X- _ B-MethodName
based -X- _ O
on -X- _ O
a -X- _ O
novel -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling -X- _ O
objective. -X- _ O
GLM -X- _ B-MethodName
formulates -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
as -X- _ O
cloze -X- _ O
questions -X- _ O
that -X- _ O
contain -X- _ O
task -X- _ O
descriptions -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
answered -X- _ O
by -X- _ O
autoregressive -X- _ O
generation -X- _ O
. -X- _ O

Pretraining -X- _ O
Objective -X- _ O

Autoregressive -X- _ O
Blank -X- _ O
Infilling -X- _ O
GLM -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
optimizing -X- _ O
an -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling -X- _ O
objective. -X- _ O
Given -X- _ O
an -X- _ O
input -X- _ O
text -X- _ O

Each -X- _ O
span -X- _ O
is -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
token -X- _ O
, -X- _ O
forming -X- _ O
a -X- _ O
corrupted -X- _ O
text -X- _ O
x -X- _ O
corrupt -X- _ O
. -X- _ O
The -X- _ O
model -X- _ O
predicts -X- _ O
the -X- _ O
missing -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
spans -X- _ O
from -X- _ O
the -X- _ O
corrupted -X- _ O
text -X- _ O
in -X- _ O
an -X- _ O
autoregressive -X- _ O
manner -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
when -X- _ O
predicting -X- _ O
the -X- _ O
missing -X- _ O
tokens -X- _ O
in -X- _ O
a -X- _ O
span -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
corrupted -X- _ O
text -X- _ O
and -X- _ O
the -X- _ O
previously -X- _ O
predicted -X- _ O
spans. -X- _ O
To -X- _ O
fully -X- _ O
capture -X- _ O
the -X- _ O
interdependencies -X- _ O
between -X- _ O
different -X- _ O
spans -X- _ O
, -X- _ O
we -X- _ O
randomly -X- _ O
permute -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
spans -X- _ O
, -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
model -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Formally -X- _ O
, -X- _ O
let -X- _ O
Z -X- _ O
m -X- _ O
be -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
all -X- _ O
possible -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
length-m -X- _ O
index -X- _ O
sequence -X- _ O
[ -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
m -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
s -X- _ O
z -X- _ O
< -X- _ O
i -X- _ O
be -X- _ O
[ -X- _ O
s -X- _ O
z -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
s -X- _ O
z -X- _ O
i−1 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
the -X- _ O
pretraining -X- _ O
objective -X- _ O
as -X- _ O

Position -X- _ O
1 -X- _ O
1 -X- _ O
2 -X- _ O
3 -X- _ O
4 -X- _ O
5 -X- _ O
5 -X- _ O
5 -X- _ O
5 -X- _ O
3 -X- _ O
3 -X- _ O
Position -X- _ O
2 -X- _ O
0 -X- _ O
0 -X- _ O
0 -X- _ O
0 -X- _ O
0 -X- _ O
1 -X- _ O
2 -X- _ O
3 -X- _ O
1 -X- _ O
2 -X- _ O
output -X- _ O
respectively. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
automatically -X- _ O
learns -X- _ O
a -X- _ O
bidirectional -X- _ O
encoder -X- _ O
( -X- _ O
for -X- _ O
Part -X- _ O
A -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
unidirectional -X- _ O
decoder -X- _ O
( -X- _ O
for -X- _ O
Part -X- _ O
B -X- _ O
) -X- _ O
in -X- _ O
a -X- _ O
unified -X- _ O
model. -X- _ O
The -X- _ O
implementation -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
is -X- _ O
illustrated -X- _ O
in -X- _ O
Figure -X- _ O
2. -X- _ O
We -X- _ O
randomly -X- _ O
sample -X- _ O
spans -X- _ O
of -X- _ O
length -X- _ O
drawn -X- _ O
from -X- _ O
a -X- _ O
Poisson -X- _ O
distribution -X- _ O
with -X- _ O
λ -X- _ B-HyperparameterName
= -X- _ O
3. -X- _ B-HyperparameterValue
We -X- _ O
repeatedly -X- _ O
sample -X- _ O
new -X- _ O
spans -X- _ O
until -X- _ O
at -X- _ O
least -X- _ O
15 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
original -X- _ O
tokens -X- _ O
are -X- _ O
masked. -X- _ O
Empirically -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
15 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
ratio -X- _ O
is -X- _ O
critical -X- _ O
for -X- _ O
good -X- _ O
performance -X- _ O
on -X- _ O
downstream -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

Multi-Task -X- _ O
Pretraining -X- _ O
In -X- _ O
the -X- _ O
previous -X- _ O
section -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
masks -X- _ O
short -X- _ O
spans -X- _ O
and -X- _ O
is -X- _ O
suited -X- _ O
for -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
interested -X- _ O
in -X- _ O
pretraining -X- _ O
a -X- _ O
single -X- _ O
model -X- _ O
that -X- _ O
can -X- _ O
handle -X- _ O
both -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
text -X- _ B-TaskName
generation. -X- _ I-TaskName
We -X- _ O
then -X- _ O
study -X- _ O
a -X- _ O
multi-task -X- _ B-TaskName
pretraining -X- _ I-TaskName
setup -X- _ O
, -X- _ O
in -X- _ O
which -X- _ O
a -X- _ O
second -X- _ O
objective -X- _ O
of -X- _ O
generating -X- _ O
longer -X- _ O
text -X- _ O
is -X- _ O
jointly -X- _ O
optimized -X- _ O
with -X- _ O
the -X- _ O
blank -X- _ O
infilling -X- _ O
objective. -X- _ O
We -X- _ O
consider -X- _ O
the -X- _ O
following -X- _ O
two -X- _ O
objectives -X- _ O
: -X- _ O

• -X- _ O
Document-level. -X- _ O
We -X- _ O
sample -X- _ O
a -X- _ O
single -X- _ O
span -X- _ O
whose -X- _ O
length -X- _ O
is -X- _ O
sampled -X- _ O
from -X- _ O
a -X- _ O
uniform -X- _ O
distribution -X- _ O
over -X- _ O
50 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
-100 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
original -X- _ O
length. -X- _ O
The -X- _ O
objective -X- _ O
aims -X- _ O
for -X- _ O
long -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O

• -X- _ O
Sentence-level. -X- _ O
We -X- _ O
restrict -X- _ O
that -X- _ O
the -X- _ O
masked -X- _ O
spans -X- _ O
must -X- _ O
be -X- _ O
full -X- _ O
sentences. -X- _ O
Multiple -X- _ O
spans -X- _ O
( -X- _ O
sentences -X- _ O
) -X- _ O
are -X- _ O
sampled -X- _ O
to -X- _ O
cover -X- _ O
15 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
original -X- _ O
tokens. -X- _ O
This -X- _ O
objective -X- _ O
aims -X- _ O
for -X- _ O
seq2seq -X- _ O
tasks -X- _ O
whose -X- _ O
predictions -X- _ O
are -X- _ O
often -X- _ O
complete -X- _ O
sentences -X- _ O
or -X- _ O
paragraphs -X- _ O
. -X- _ O

Model -X- _ O
Architecture -X- _ O
GLM -X- _ B-MethodName
uses -X- _ O
a -X- _ O
single -X- _ O
Transformer -X- _ O
with -X- _ O
several -X- _ O
modifications -X- _ O
to -X- _ O
the -X- _ O
architecture -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
we -X- _ O
rearrange -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
layer -X- _ O
normalization -X- _ O
and -X- _ O
the -X- _ O
residual -X- _ O
connection -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
critical -X- _ O
for -X- _ O
large-scale -X- _ O
language -X- _ O
models -X- _ O
to -X- _ O
avoid -X- _ O
numerical -X- _ O
errors -X- _ O
( -X- _ O
Shoeybi -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
single -X- _ O
linear -X- _ O
layer -X- _ O
for -X- _ O
the -X- _ O
output -X- _ O
token -X- _ O
prediction -X- _ O
; -X- _ O

Finetuning -X- _ O
GLM -X- _ B-MethodName
Typically -X- _ O
, -X- _ O
for -X- _ O
downstream -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
a -X- _ O
linear -X- _ O
classifier -X- _ O
takes -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
sequences -X- _ O
or -X- _ O
tokens -X- _ O
produced -X- _ O
by -X- _ O
pretrained -X- _ O
models -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
predicts -X- _ O
the -X- _ O
correct -X- _ O
labels. -X- _ O
The -X- _ O
practices -X- _ O
are -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
generative -X- _ O
pretraining -X- _ O
task -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
inconsistency -X- _ O
between -X- _ O
pretraining -X- _ O
and -X- _ O
finetuning -X- _ O
. -X- _ O

Instead -X- _ O
, -X- _ O
we -X- _ O
reformulate -X- _ O
NLU -X- _ B-TaskName
classification -X- _ O
tasks -X- _ O
as -X- _ O
generation -X- _ O
tasks -X- _ O
of -X- _ O
blank -X- _ B-TaskName
infilling -X- _ I-TaskName
, -X- _ O
following -X- _ O
PET -X- _ B-TaskName
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
labeled -X- _ O
example -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
convert -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
x -X- _ O
to -X- _ O
a -X- _ O
cloze -X- _ O
question -X- _ O
c -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
via -X- _ O
a -X- _ O
pattern -X- _ O
containing -X- _ O
a -X- _ O
single -X- _ O
mask -X- _ O
token. -X- _ O
The -X- _ O
pattern -X- _ O
is -X- _ O
written -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
to -X- _ O
represent -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
the -X- _ O
task. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
a -X- _ O
sentiment -X- _ O
classification -X- _ O
task -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
" -X- _ O
{ -X- _ O
SENTENCE -X- _ O
} -X- _ O
. -X- _ O
It -X- _ O
's -X- _ O
really -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
candidate -X- _ O
labels -X- _ O
y -X- _ O
∈ -X- _ O
Y -X- _ O
are -X- _ O
also -X- _ O
mapped -X- _ O
to -X- _ O
answers -X- _ O
to -X- _ O
the -X- _ O
cloze -X- _ O
, -X- _ O
called -X- _ O
verbalizer -X- _ O
v -X- _ O
( -X- _ O
y -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
sentiment -X- _ O
classification -X- _ O
, -X- _ O
the -X- _ O
labels -X- _ O
" -X- _ O
positive -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
negative -X- _ O
" -X- _ O
are -X- _ O
mapped -X- _ O
to -X- _ O
the -X- _ O
words -X- _ O
" -X- _ O
good -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
bad -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
predicting -X- _ O
y -X- _ O
given -X- _ O
x -X- _ O
is -X- _ O

Then -X- _ O
we -X- _ O
finetune -X- _ O
GLM -X- _ B-MethodName
with -X- _ O
a -X- _ O
cross-entropy -X- _ O
loss -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
text -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
the -X- _ O
given -X- _ O
context -X- _ O
constitutes -X- _ O
the -X- _ O
Part -X- _ O
A -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
mask -X- _ O
token -X- _ O
appended -X- _ O
at -X- _ O
the -X- _ O
end. -X- _ O
The -X- _ O
model -X- _ O
generates -X- _ O
the -X- _ O
text -X- _ O
of -X- _ O
Part -X- _ O
B -X- _ O
autoregressively. -X- _ O
We -X- _ O
can -X- _ O
directly -X- _ O
apply -X- _ O
the -X- _ O
pretrained -X- _ O
GLM -X- _ B-MethodName
for -X- _ O
unconditional -X- _ O
generation -X- _ O
, -X- _ O
or -X- _ O
finetune -X- _ O
it -X- _ O
on -X- _ O
downstream -X- _ O
conditional -X- _ O
generation -X- _ O
tasks -X- _ O
. -X- _ O

Discussion -X- _ O
and -X- _ O
Analysis -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
discuss -X- _ O
the -X- _ O
differences -X- _ O
between -X- _ O
GLM -X- _ O
and -X- _ O
other -X- _ O
pretraining -X- _ O
models. -X- _ O
We -X- _ O
are -X- _ O
mainly -X- _ O
concerned -X- _ O
with -X- _ O
how -X- _ O
they -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
downstream -X- _ O
blank -X- _ O
infilling -X- _ O
tasks -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
pointed -X- _ O
out -X- _ O
by -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
fails -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
interdependencies -X- _ O
of -X- _ O
masked -X- _ O
tokens -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
of -X- _ O
MLM. -X- _ B-TaskName
Another -X- _ O
disadvantage -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
that -X- _ O
it -X- _ O
can -X- _ O
not -X- _ O
fill -X- _ O
in -X- _ O
the -X- _ O
blanks -X- _ O
of -X- _ O
multiple -X- _ O
tokens -X- _ O
properly. -X- _ O
To -X- _ O
infer -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
an -X- _ O
answer -X- _ O
of -X- _ O
length -X- _ O
l -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
needs -X- _ O
to -X- _ O
perform -X- _ O
l -X- _ O
consecutive -X- _ O
predictions. -X- _ O
If -X- _ O
the -X- _ O
length -X- _ O
l -X- _ O
is -X- _ O
unknown -X- _ O
, -X- _ O
we -X- _ O
may -X- _ O
need -X- _ O
to -X- _ O
enumerate -X- _ O
all -X- _ O
possible -X- _ O
lengths -X- _ O
, -X- _ O
since -X- _ O
BERT -X- _ B-MethodName
needs -X- _ O
to -X- _ O
change -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
tokens -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
XLNet -X- _ B-MethodName
( -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Both -X- _ O
GLM -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
are -X- _ O
pretrained -X- _ O
with -X- _ O
autoregressive -X- _ O
objectives -X- _ O
, -X- _ O
but -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
differences -X- _ O
between -X- _ O
them. -X- _ O
First -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
uses -X- _ O
the -X- _ O
original -X- _ O
position -X- _ O
encodings -X- _ O
before -X- _ O
corruption. -X- _ O
During -X- _ O
inference -X- _ O
, -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
either -X- _ O
know -X- _ O
or -X- _ O
enumerate -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
answer -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
problem -X- _ O
as -X- _ O
BERT. -X- _ B-MethodName
Second -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
uses -X- _ O
a -X- _ O
two-stream -X- _ O
self-attention -X- _ O
mechanism -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
right-shift -X- _ O
, -X- _ O
to -X- _ O
avoid -X- _ O
the -X- _ O
information -X- _ O
leak -X- _ O
within -X- _ O
Transformer. -X- _ O
It -X- _ O
doubles -X- _ O
the -X- _ O
time -X- _ O
cost -X- _ O
of -X- _ O
pretraining -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
T5 -X- _ B-MethodName
proposes -X- _ O
a -X- _ O
similar -X- _ O
blank -X- _ O
infilling -X- _ O
objective -X- _ O
to -X- _ O
pretrain -X- _ O
an -X- _ O
encoder-decoder -X- _ O
Transformer. -X- _ O
T5 -X- _ B-MethodName
uses -X- _ O
independent -X- _ O
positional -X- _ O
encodings -X- _ O
for -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
relies -X- _ O
on -X- _ O
multiple -X- _ O
sentinel -X- _ O
tokens -X- _ O
to -X- _ O
differentiate -X- _ O
the -X- _ O
masked -X- _ O
spans. -X- _ O
In -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
only -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
sentinel -X- _ O
tokens -X- _ O
is -X- _ O
used -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
a -X- _ O
waste -X- _ O
of -X- _ O
model -X- _ O
capacity -X- _ O
and -X- _ O
inconsistency -X- _ O
between -X- _ O
pretraining -X- _ O
and -X- _ O
finetuning. -X- _ O
Moreover -X- _ O
, -X- _ O
T5 -X- _ B-MethodName
always -X- _ O
predicts -X- _ O
spans -X- _ O
in -X- _ O
a -X- _ O
fixed -X- _ O
left-to-right -X- _ O
order. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
can -X- _ O
significantly -X- _ O
outperform -X- _ O
T5 -X- _ B-MethodName
on -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
seq2seq -X- _ B-TaskName
tasks -X- _ O
with -X- _ O
fewer -X- _ O
parameters -X- _ O
and -X- _ O
data -X- _ O
, -X- _ O
as -X- _ O
stated -X- _ O
in -X- _ O
Sections -X- _ O
3.2 -X- _ O
and -X- _ O
3.3 -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
UniLM -X- _ B-MethodName
( -X- _ O
Dong -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
UniLM -X- _ B-MethodName
combines -X- _ O
different -X- _ O
pretraining -X- _ O
objectives -X- _ O
under -X- _ O
the -X- _ O
autoencoding -X- _ O
framework -X- _ O
by -X- _ O
changing -X- _ O
the -X- _ O
attention -X- _ O
mask -X- _ O
among -X- _ O
bidirectional -X- _ O
, -X- _ O
unidirectional -X- _ O
, -X- _ O
and -X- _ O
cross -X- _ O
attention. -X- _ O
However -X- _ O
, -X- _ O
UniLM -X- _ B-MethodName
always -X- _ O
replaces -X- _ O
masked -X- _ O
spans -X- _ O
with -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
tokens -X- _ O
, -X- _ O
which -X- _ O
limits -X- _ O
its -X- _ O
ability -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
dependencies -X- _ O
between -X- _ O
the -X- _ O
masked -X- _ O
spans -X- _ O
and -X- _ O
their -X- _ O
context. -X- _ O
GLM -X- _ B-MethodName
feeds -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
token -X- _ O
and -X- _ O
autoregressively -X- _ O
generates -X- _ O
the -X- _ O
next -X- _ O
token. -X- _ O
Finetuning -X- _ O
UniLM -X- _ B-MethodName
on -X- _ O
downstream -X- _ O
generation -X- _ O
tasks -X- _ O
also -X- _ O
relies -X- _ O
on -X- _ O
masked -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
, -X- _ O
which -X- _ O
is -X- _ O
less -X- _ O
efficient. -X- _ O
UniLMv2 -X- _ B-MethodName
( -X- _ O
Bao -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
adopts -X- _ O
partially -X- _ O
autoregressive -X- _ O
modeling -X- _ O
for -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
autoencoding -X- _ O
objective -X- _ O
for -X- _ O
NLU -X- _ B-MethodName
tasks. -X- _ O
Instead -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
unifies -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
with -X- _ O
autoregressive -X- _ O
pretraining -X- _ O
. -X- _ O

Pretraining -X- _ O
Setup -X- _ O
For -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
BooksCorpus -X- _ B-DatasetName
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
English -X- _ B-DatasetName
Wikipedia -X- _ I-DatasetName
as -X- _ O
our -X- _ O
pretraining -X- _ O
data. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
uncased -X- _ O
wordpiece -X- _ O
tokenizer -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
30k -X- _ O
vocabulary. -X- _ O
We -X- _ O
train -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
with -X- _ O
the -X- _ O
same -X- _ O
architectures -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
containing -X- _ O
110M -X- _ B-HyperparameterValue
and -X- _ O
340M -X- _ B-HyperparameterValue
parameters -X- _ O
respectively -X- _ O
. -X- _ O

For -X- _ O
multi-task -X- _ B-TaskName
pretraining -X- _ I-TaskName
, -X- _ O
we -X- _ O
train -X- _ O
two -X- _ O
Largesized -X- _ O
models -X- _ O
with -X- _ O
a -X- _ O
mixture -X- _ O
of -X- _ O
the -X- _ O
blank -X- _ O
infilling -X- _ O
objective -X- _ O
and -X- _ O
the -X- _ O
document-level -X- _ O
or -X- _ O
sentencelevel -X- _ O
objective -X- _ O
, -X- _ O
denoted -X- _ O
as -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Sent -X- _ I-MethodName
. -X- _ O
Additionally -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
two -X- _ O
larger -X- _ O
GLM -X- _ B-MethodName
models -X- _ O
of -X- _ O
410M -X- _ B-HyperparameterValue
( -X- _ O
30 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
, -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
1024 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
16 -X- _ B-HyperparameterValue
attention -X- _ B-HyperparameterName
heads -X- _ I-HyperparameterName
) -X- _ O
and -X- _ O
515M -X- _ B-HyperparameterValue
( -X- _ O
30 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
, -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
1152 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
18 -X- _ B-HyperparameterValue
attention -X- _ B-HyperparameterName
heads -X- _ I-HyperparameterName
) -X- _ O
parameters -X- _ O
with -X- _ O
documentlevel -X- _ B-TaskName
multi-task -X- _ I-TaskName
pretraining -X- _ I-TaskName
, -X- _ O
denoted -X- _ O
as -X- _ O
GLM -X- _ B-MethodName
410M -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
515M -X- _ I-MethodName
. -X- _ O

To -X- _ O
compare -X- _ O
with -X- _ O
SOTA -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
train -X- _ O
a -X- _ O
Large-sized -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
, -X- _ O
tokenization -X- _ O
, -X- _ O
and -X- _ O
hyperparameters -X- _ O
as -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
denoted -X- _ O
as -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
. -X- _ O
Due -X- _ O
to -X- _ O
resource -X- _ O
limitations -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
pretrain -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
250,000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
are -X- _ O
half -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
and -X- _ O
BART -X- _ B-MethodName
's -X- _ I-MethodName
training -X- _ O
steps -X- _ O
and -X- _ O
close -X- _ O
to -X- _ O
T5 -X- _ B-MethodName
in -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
trained -X- _ O
tokens. -X- _ O
More -X- _ O
experiment -X- _ O
details -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

SuperGLUE -X- _ B-DatasetName
To -X- _ O
evaluate -X- _ O
our -X- _ O
pretrained -X- _ O
GLM -X- _ B-MethodName
models -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
SuperGLUE -X- _ B-DatasetName
bench-mark -X- _ O
and -X- _ O
report -X- _ O
the -X- _ O
standard -X- _ O
metrics. -X- _ O
SuperGLUE -X- _ B-DatasetName
consists -X- _ O
of -X- _ O
8 -X- _ O
challenging -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
We -X- _ O
reformulate -X- _ O
the -X- _ O
classification -X- _ O
tasks -X- _ O
as -X- _ O
blank -X- _ B-TaskName
infilling -X- _ I-TaskName
with -X- _ I-TaskName
human-crafted -X- _ I-TaskName
cloze -X- _ I-TaskName
questions -X- _ I-TaskName
, -X- _ O
following -X- _ O
PET -X- _ B-TaskName
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2020b -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
we -X- _ O
finetune -X- _ O
the -X- _ O
pretrained -X- _ O
GLM -X- _ B-MethodName
models -X- _ O
on -X- _ O
each -X- _ O
task -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
2.3. -X- _ O
The -X- _ O
cloze -X- _ O
questions -X- _ O
and -X- _ O
other -X- _ O
details -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
B.1 -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
we -X- _ O
choose -X- _ O
BERT -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
as -X- _ O
our -X- _ O
baselines -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
pretrained -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
corpus -X- _ O
and -X- _ O
for -X- _ O
a -X- _ O
similar -X- _ O
amount -X- _ O
of -X- _ O
time. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
standard -X- _ O
finetuning -X- _ O
( -X- _ O
i.e. -X- _ O
classification -X- _ O
on -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
representation -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
performance -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
cloze -X- _ O
questions -X- _ O
is -X- _ O
reported -X- _ O
in -X- _ O
Section -X- _ O
3.4. -X- _ O
To -X- _ O
compare -X- _ O
with -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
, -X- _ O
we -X- _ O
choose -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
BART -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
and -X- _ O
RoBERTa -X- _ B-MethodName
Large -X- _ I-MethodName
as -X- _ O
our -X- _ O
baselines. -X- _ O
T5 -X- _ B-MethodName
has -X- _ O
no -X- _ O
direct -X- _ O
match -X- _ O
in -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
so -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
both -X- _ O
T5 -X- _ B-MethodName
Base -X- _ I-MethodName
( -X- _ O
220M -X- _ B-HyperparameterValue
parameters -X- _ B-HyperparameterName
) -X- _ O
and -X- _ O
T5 -X- _ B-MethodName
Large -X- _ I-MethodName
( -X- _ O
770M -X- _ B-HyperparameterValue
parameters -X- _ B-HyperparameterName
) -X- _ O
. -X- _ O
All -X- _ O
the -X- _ O
other -X- _ O
baselines -X- _ O
are -X- _ O
of -X- _ O
similar -X- _ O
size -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
the -X- _ O
results. -X- _ O
With -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
training -X- _ O
data -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
consistently -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
most -X- _ O
tasks -X- _ O
with -X- _ O
either -X- _ O
base -X- _ O
or -X- _ O
large -X- _ O
architecture. -X- _ O
The -X- _ O
only -X- _ O
exception -X- _ O
is -X- _ O
WiC -X- _ B-DatasetName
( -X- _ O
word -X- _ O
sense -X- _ O
disambiguation -X- _ O
) -X- _ O
. -X- _ O
On -X- _ O
average -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
scores -X- _ O
4.6 -X- _ B-MetricValue
% -X- _ I-MetricValue
higher -X- _ O
than -X- _ O
BERT -X- _ B-MethodName
Base -X- _ I-MethodName
, -X- _ O
and -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
scores -X- _ O
5.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
higher -X- _ O
than -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
It -X- _ O
clearly -X- _ O
demonstrates -X- _ O
the -X- _ O
advantage -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
in -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
In -X- _ O
the -X- _ O
setting -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
can -X- _ O
still -X- _ O
achieve -X- _ O
improvements -X- _ O
over -X- _ O
the -X- _ O
baselines -X- _ O
, -X- _ O
but -X- _ O
with -X- _ O
a -X- _ O
smaller -X- _ O
margin. -X- _ O
Specifically -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
outperforms -X- _ O
T5 -X- _ B-MethodName
Large -X- _ I-MethodName
but -X- _ O
is -X- _ O
only -X- _ O
half -X- _ O
its -X- _ O
size. -X- _ O
We -X- _ O
also -X- _ O
find -X- _ O
that -X- _ O
BART -X- _ B-MethodName
does -X- _ O
not -X- _ O
perform -X- _ O
well -X- _ O
on -X- _ O
the -X- _ O
challenging -X- _ O
SuperGLUE -X- _ B-DatasetName
benchmark. -X- _ O
We -X- _ O
conjecture -X- _ O
this -X- _ O
can -X- _ O
be -X- _ O
attributed -X- _ O
to -X- _ O
the -X- _ O
low -X- _ O
parameter -X- _ O
efficiency -X- _ O
of -X- _ O
the -X- _ O
encoder-decoder -X- _ O
architecture -X- _ O
and -X- _ O
the -X- _ O
denoising -X- _ B-TaskName
sequence-to-sequence -X- _ I-TaskName
objective -X- _ O
. -X- _ O

Multi-Task -X- _ O
Pretraining -X- _ O
Then -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
GLM -X- _ B-MethodName
's -X- _ I-MethodName
performance -X- _ O
in -X- _ O
a -X- _ O
multi-task -X- _ O
setting -X- _ O
( -X- _ O
Section -X- _ O
2.1 -X- _ O
) -X- _ O
. -X- _ O
Within -X- _ O
one -X- _ O
training -X- _ O
batch -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
short -X- _ O
spans -X- _ O
and -X- _ O
longer -X- _ O
spans -X- _ O
( -X- _ O
document-level -X- _ O
or -X- _ O
sentence-level -X- _ O
) -X- _ O
with -X- _ O
equal -X- _ O
chances. -X- _ O
We -X- _ O
evaluate -X- _ O
the -X- _ O
multi-task -X- _ O
model -X- _ O
for -X- _ O
NLU -X- _ B-TaskName
, -X- _ O
seq2seq -X- _ B-TaskName
, -X- _ O
blank -X- _ B-TaskName
infilling -X- _ I-TaskName
, -X- _ O
and -X- _ O
zero-shot -X- _ B-TaskName
language -X- _ I-TaskName
modeling. -X- _ I-TaskName
SuperGLUE. -X- _ B-DatasetName
For -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
SuperGLUE -X- _ B-DatasetName
benchmark. -X- _ O
The -X- _ O
results -X- _ O
Sequence-to-Sequence -X- _ O
. -X- _ O

Considering -X- _ O
the -X- _ O
available -X- _ O
baseline -X- _ O
results -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
Gigaword -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Rush -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
for -X- _ O
abstractive -X- _ B-TaskName
summarization -X- _ I-TaskName
and -X- _ O
the -X- _ O
SQuAD -X- _ B-DatasetName
1.1 -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
for -X- _ O
question -X- _ B-TaskName
generation -X- _ I-TaskName
( -X- _ O
Du -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
benchmarks -X- _ O
for -X- _ O
models -X- _ O
pretrained -X- _ O
on -X- _ O
BookCorpus -X- _ B-DatasetName
and -X- _ O
Wikipedia. -X- _ B-DatasetName
Additionally -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DailyMail -X- _ I-DatasetName
( -X- _ O
See -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
XSum -X- _ B-DatasetName
( -X- _ O
Narayan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
datasets -X- _ O
for -X- _ O
abstractive -X- _ B-TaskName
summarization -X- _ I-TaskName
as -X- _ O
the -X- _ O
benchmarks -X- _ O
for -X- _ O
models -X- _ O
pretrained -X- _ O
on -X- _ O
larger -X- _ O
corpora -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
for -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
BookCorpus -X- _ B-DatasetName
and -X- _ O
Wikipedia -X- _ B-DatasetName
are -X- _ O
shown -X- _ O
in -X- _ O
Tables -X- _ O
3 -X- _ O
and -X- _ O
4. -X- _ O
We -X- _ O
observe -X- _ O
that -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
can -X- _ O
achieve -X- _ O
performance -X- _ O
matching -X- _ O
the -X- _ O
other -X- _ O
pretraining -X- _ O
models -X- _ O
on -X- _ O
the -X- _ O
two -X- _ O
generation -X- _ O
tasks. -X- _ O
GLM -X- _ B-MethodName
Sent -X- _ I-MethodName
can -X- _ O
perform -X- _ O
better -X- _ O
than -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
while -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
performs -X- _ O
slightly -X- _ O
worse -X- _ O
than -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
This -X- _ O
indicates -X- _ O
that -X- _ O
the -X- _ O
documentlevel -X- _ O
objective -X- _ O
, -X- _ O
which -X- _ O
teaches -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
given -X- _ O
contexts -X- _ O
, -X- _ O
is -X- _ O
less -X- _ O
helpful -X- _ O
to -X- _ O
conditional -X- _ O
generation -X- _ O
, -X- _ O
which -X- _ O
aims -X- _ O
to -X- _ O
extract -X- _ O
useful -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
context. -X- _ O
Increasing -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
's -X- _ O
parameters -X- _ O
to -X- _ O
410M -X- _ B-HyperparameterValue
leads -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
both -X- _ O
tasks. -X- _ O
The -X- _ O
results -X- _ O
for -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
larger -X- _ O
corpora -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
2. -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
can -X- _ O
achieve -X- _ O
performance -X- _ O
matching -X- _ O
the -X- _ O
seq2seq -X- _ O
BART -X- _ B-MethodName
model -X- _ O
, -X- _ O
and -X- _ O
outperform -X- _ O
T5 -X- _ B-MethodName
and -X- _ O
UniLMv2 -X- _ B-MethodName
. -X- _ O

Text -X- _ O
Infilling. -X- _ O
Text -X- _ B-TaskName
infilling -X- _ I-TaskName
is -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
predicting -X- _ O
missing -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
which -X- _ O
are -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
surrounding -X- _ O
context -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Donahue -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
GLM -X- _ B-MethodName
is -X- _ O
trained -X- _ O
with -X- _ O
an -X- _ O
autoregressive -X- _ B-TaskName
blank -X- _ I-TaskName
infilling -X- _ I-TaskName
objective -X- _ O
, -X- _ O
thus -X- _ O
can -X- _ O
straightforwardly -X- _ O
solve -X- _ O
this -X- _ O
task. -X- _ O
We -X- _ O
evaluate -X- _ O
GLM -X- _ B-MethodName
on -X- _ O
the -X- _ O
Yahoo -X- _ B-DatasetName
Answers -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
compare -X- _ O
it -X- _ O
with -X- _ O
Blank -X- _ B-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
( -X- _ O
BLM -X- _ B-MethodName
) -X- _ O
( -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
specifically -X- _ O
designed -X- _ O
model -X- _ O
for -X- _ O
text -X- _ O
infilling. -X- _ O
From -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
methods -X- _ O
by -X- _ O
large -X- _ O
margins -X- _ O
( -X- _ O
1.3 -X- _ B-MetricValue
to -X- _ O
3.9 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
) -X- _ O
and -X- _ O
achieves -X- _ O
the -X- _ O
state-of-the-art -X- _ O
result -X- _ O
on -X- _ O
this -X- _ O
dataset. -X- _ O
We -X- _ O
notice -X- _ O
that -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
slightly -X- _ O
underperforms -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
our -X- _ O
observations -X- _ O
in -X- _ O
the -X- _ O
seq2seq -X- _ O
experiments. -X- _ O
Language -X- _ O
Modeling. -X- _ O
Most -X- _ O
language -X- _ O
modeling -X- _ O
datasets -X- _ O
such -X- _ O
as -X- _ O
WikiText103 -X- _ B-DatasetName
are -X- _ O
constructed -X- _ O
from -X- _ O
Wikipedia -X- _ O
documents -X- _ O
, -X- _ O
which -X- _ O
our -X- _ O
pretraining -X- _ O
dataset -X- _ O
already -X- _ O
contains. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
language -X- _ O
modeling -X- _ O
perplexity -X- _ O
on -X- _ O
a -X- _ O
held-out -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
our -X- _ O
pretraining -X- _ O
dataset -X- _ O
, -X- _ O
which -X- _ O
contains -X- _ O
about -X- _ O
20M -X- _ B-HyperparameterValue
tokens -X- _ B-HyperparameterName
, -X- _ O
denoted -X- _ O
as -X- _ O
BookWiki. -X- _ B-DatasetName
We -X- _ O
also -X- _ O
evaluate -X- _ O
GLM -X- _ B-MethodName
on -X- _ O
the -X- _ O
LAMBADA -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Paperno -X- _ O
The -X- _ O
task -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
final -X- _ O
word -X- _ O
of -X- _ O
a -X- _ O
passage. -X- _ O
As -X- _ O
the -X- _ O
baseline -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
model -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
; -X- _ O
Brown -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
and -X- _ O
tokenization -X- _ O
as -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
4. -X- _ O
All -X- _ O
the -X- _ O
models -X- _ O
are -X- _ O
evaluated -X- _ O
in -X- _ O
the -X- _ O
zero-shot -X- _ O
setting. -X- _ O
Since -X- _ O
GLM -X- _ B-MethodName
learns -X- _ O
the -X- _ O
bidirectional -X- _ O
attention -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
evaluate -X- _ O
GLM -X- _ B-MethodName
under -X- _ O
the -X- _ O
setting -X- _ O
in -X- _ O
which -X- _ O
the -X- _ O
contexts -X- _ O
are -X- _ O
encoded -X- _ O
with -X- _ O
bidirectional -X- _ O
attention. -X- _ O
Without -X- _ O
generative -X- _ O
objective -X- _ O
during -X- _ O
pretraining -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
Large -X- _ O
can -X- _ O
not -X- _ O
complete -X- _ O
the -X- _ O
language -X- _ O
modeling -X- _ O
tasks -X- _ O
, -X- _ O
with -X- _ O
perplexity -X- _ O
larger -X- _ O
than -X- _ O
100. -X- _ O
With -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
parameters -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
performs -X- _ O
worse -X- _ O
than -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
This -X- _ O
is -X- _ O
expected -X- _ O
since -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
also -X- _ O
optimizes -X- _ O
the -X- _ O
blank -X- _ O
infilling -X- _ O
objective. -X- _ O
Increasing -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
parameters -X- _ O
to -X- _ O
410M -X- _ O
( -X- _ O
1.25× -X- _ O
of -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
) -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
performance -X- _ O
close -X- _ O
to -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
GLM -X- _ B-MethodName
515M -X- _ I-MethodName
( -X- _ O
1.5× -X- _ O
of -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
) -X- _ O
can -X- _ O
further -X- _ O
outperform -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
With -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
parameters -X- _ O
, -X- _ O
encoding -X- _ O
the -X- _ O
context -X- _ O
with -X- _ O
bidirectional -X- _ O
attention -X- _ O
can -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
language -X- _ O
modeling. -X- _ O
Under -X- _ O
this -X- _ O
setting -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
410M -X- _ I-MethodName
outperforms -X- _ O
GPT -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
This -X- _ O
is -X- _ O
the -X- _ O
advantage -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
over -X- _ O
unidirectional -X- _ O
GPT. -X- _ B-MethodName
We -X- _ O
also -X- _ O
study -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
2D -X- _ O
positional -X- _ O
encoding -X- _ O
to -X- _ O
long -X- _ O
text -X- _ O
generation. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
removing -X- _ O
the -X- _ O
2D -X- _ O
positional -X- _ O
encoding -X- _ O
leads -X- _ O
to -X- _ O
lower -X- _ O
accuracy -X- _ O
and -X- _ O
higher -X- _ O
perplexity -X- _ O
in -X- _ O
language -X- _ O
modeling. -X- _ O
Summary. -X- _ O
Above -X- _ O
all -X- _ O
, -X- _ O
we -X- _ O
conclude -X- _ O
that -X- _ O
GLM -X- _ B-MethodName
effectively -X- _ O
shares -X- _ O
model -X- _ O
parameters -X- _ O
across -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
and -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
achieving -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
a -X- _ O
standalone -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
encoder-decoder -X- _ O
, -X- _ O
or -X- _ O
GPT -X- _ B-MethodName
model -X- _ O
. -X- _ O

Ablation -X- _ O
Study -X- _ O
Table -X- _ O
6 -X- _ O
shows -X- _ O
our -X- _ O
ablation -X- _ O
analysis -X- _ O
for -X- _ O
GLM. -X- _ B-MethodName
First -X- _ O
, -X- _ O
to -X- _ O
provide -X- _ O
an -X- _ O
apple-to-apple -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
model -X- _ O
with -X- _ O
our -X- _ O
implementation -X- _ O
, -X- _ O
data -X- _ O
, -X- _ O
and -X- _ O
hyperparameters -X- _ O
( -X- _ O
row -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
performance -X- _ O
is -X- _ O
slightly -X- _ O
worse -X- _ O
than -X- _ O
the -X- _ O
official -X- _ O
BERT -X- _ B-MethodName
Large -X- _ I-MethodName
and -X- _ O
significantly -X- _ O
worse -X- _ O
than -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
It -X- _ O
confirms -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
over -X- _ O
Masked -X- _ B-TaskName
LM -X- _ I-TaskName
pretraining -X- _ I-TaskName
on -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
Second -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
SuperGLUE -X- _ B-DatasetName
performance -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
finetuned -X- _ O
as -X- _ O
sequence -X- _ B-TaskName
classifiers -X- _ I-TaskName
( -X- _ O
row -X- _ O
5 -X- _ O
) -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
clozestyle -X- _ O
finetuning -X- _ O
( -X- _ O
row -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
cloze-style -X- _ O
finetuning -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
benefits -X- _ O
from -X- _ O
the -X- _ O
autoregressive -X- _ O
pretraining. -X- _ O
Especially -X- _ O
on -X- _ O
ReCoRD -X- _ B-DatasetName
and -X- _ O
WSC -X- _ B-DatasetName
, -X- _ O
where -X- _ O
the -X- _ O
verbalizer -X- _ O
consists -X- _ O
of -X- _ O
multiple -X- _ O
tokens -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
consistently -X- _ O
outperforms -X- _ O
BERT. -X- _ B-MethodName
This -X- _ O
demonstrates -X- _ O
GLM -X- _ B-MethodName
's -X- _ O
advantage -X- _ O
in -X- _ O
handling -X- _ O
variable-length -X- _ O
blank. -X- _ O
Another -X- _ O
observation -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
cloze -X- _ O
formulation -X- _ O
is -X- _ O
critical -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
's -X- _ I-MethodName
performance -X- _ O
on -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
For -X- _ O
the -X- _ O
large -X- _ O
model -X- _ O
, -X- _ O
clozestyle -X- _ O
finetuning -X- _ O
can -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
by -X- _ O
7 -X- _ O
points. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
GLM -X- _ B-MethodName
variants -X- _ O
with -X- _ O
different -X- _ O
pretraining -X- _ O
designs -X- _ O
to -X- _ O
understand -X- _ O
their -X- _ O
importance. -X- _ O
Row -X- _ O
6 -X- _ O
shows -X- _ O
that -X- _ O
removing -X- _ O
the -X- _ O
span -X- _ O
shuffling -X- _ O
( -X- _ O
always -X- _ O
predicting -X- _ O
the -X- _ O
masked -X- _ O
spans -X- _ O
from -X- _ O
left -X- _ O
to -X- _ O
right -X- _ O
) -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
severe -X- _ O
performance -X- _ O
drop -X- _ O
on -X- _ O
SuperGLUE. -X- _ B-DatasetName
Row -X- _ O
7 -X- _ O
uses -X- _ O
different -X- _ O
sentinel -X- _ O
tokens -X- _ O
instead -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
token -X- _ O
to -X- _ O
represent -X- _ O
different -X- _ O
masked -X- _ O
spans. -X- _ O
The -X- _ O
model -X- _ O
performs -X- _ O
worse -X- _ O
than -X- _ O
the -X- _ O
standard -X- _ O
GLM. -X- _ B-MethodName
We -X- _ O
hypothesize -X- _ O
that -X- _ O
it -X- _ O
wastes -X- _ O
some -X- _ O
modeling -X- _ O
capacity -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
different -X- _ O
sentinel -X- _ O
tokens -X- _ O
which -X- _ O
are -X- _ O
not -X- _ O
used -X- _ O
in -X- _ O
downstream -X- _ O
tasks -X- _ O
with -X- _ O
only -X- _ O
one -X- _ O
blank. -X- _ O
In -X- _ O
Figure -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
removing -X- _ O
the -X- _ O
second -X- _ O
dimension -X- _ O
of -X- _ O
2D -X- _ O
positional -X- _ O
encoding -X- _ O
hurts -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
long -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O

We -X- _ O
note -X- _ O
that -X- _ O
T5 -X- _ B-MethodName
is -X- _ O
pretrained -X- _ O
with -X- _ O
a -X- _ O
similar -X- _ O
blank -X- _ O
infilling -X- _ O
objective. -X- _ O
GLM -X- _ B-MethodName
differs -X- _ O
in -X- _ O
three -X- _ O
aspects -X- _ O
: -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
GLM -X- _ B-MethodName
consists -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
encoder -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
GLM -X- _ B-MethodName
shuffles -X- _ O
the -X- _ O
masked -X- _ O
spans -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
GLM -X- _ B-MethodName
uses -X- _ O
a -X- _ O
single -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
instead -X- _ O
of -X- _ O
multiple -X- _ O
sentinel -X- _ O
tokens. -X- _ O
While -X- _ O
we -X- _ O
can -X- _ O
not -X- _ O
directly -X- _ O
compare -X- _ O
GLM -X- _ B-MethodName
with -X- _ O
T5 -X- _ B-MethodName
due -X- _ O
to -X- _ O
the -X- _ O
differences -X- _ O
in -X- _ O
training -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Tables -X- _ O
1 -X- _ O
and -X- _ O
6 -X- _ O
have -X- _ O
demonstrated -X- _ O
the -X- _ O
advantage -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
. -X- _ O

Among -X- _ O
encoder-decoder -X- _ O
models -X- _ O
, -X- _ O
BART -X- _ B-MethodName
conducts -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
by -X- _ O
feeding -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
taking -X- _ O
the -X- _ O
final -X- _ O
hidden -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
decoder. -X- _ O
Instead -X- _ O
, -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
formulates -X- _ O
most -X- _ O
language -X- _ O
tasks -X- _ O
in -X- _ O
the -X- _ O
text-to-text -X- _ O
framework. -X- _ O
However -X- _ O
, -X- _ O
both -X- _ O
models -X- _ O
require -X- _ O
more -X- _ O
parameters -X- _ O
to -X- _ O
outperform -X- _ O
autoencoding -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
RoBERTa -X- _ B-MethodName
. -X- _ O
UniLM -X- _ B-MethodName
( -X- _ O
Dong -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Bao -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
unifies -X- _ O
three -X- _ O
pretraining -X- _ O
models -X- _ O
under -X- _ O
the -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
with -X- _ O
different -X- _ O
attention -X- _ O
masks -X- _ O
. -X- _ O

NLU -X- _ B-TaskName
as -X- _ O
Generation. -X- _ O
Previously -X- _ O
, -X- _ O
pretrained -X- _ O
language -X- _ O
models -X- _ O
complete -X- _ O
classification -X- _ O
tasks -X- _ O
for -X- _ O
NLU -X- _ O
with -X- _ O
linear -X- _ O
classifiers -X- _ O
on -X- _ O
the -X- _ O
learned -X- _ O
representations. -X- _ O
GPT-2 -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
) -X- _ O
and -X- _ O
GPT-3 -X- _ B-MethodName
( -X- _ O
Brown -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
show -X- _ O
that -X- _ O
generative -X- _ O
language -X- _ O
models -X- _ O
can -X- _ O
complete -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
question -X- _ O
answering -X- _ O
by -X- _ O
directly -X- _ O
predicting -X- _ O
the -X- _ O
correct -X- _ O
answers -X- _ O
without -X- _ O
finetuning -X- _ O
, -X- _ O
given -X- _ O
task -X- _ O
instructions -X- _ O
or -X- _ O
a -X- _ O
few -X- _ O
labeled -X- _ O
examples. -X- _ O
However -X- _ O
, -X- _ O
generative -X- _ O
models -X- _ O
require -X- _ O
much -X- _ O
more -X- _ O
parameters -X- _ O
to -X- _ O
work -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
limit -X- _ O
of -X- _ O
unidirectional -X- _ O
attention. -X- _ O
Recently -X- _ O
, -X- _ O
PET -X- _ B-MethodName
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2020a -X- _ O
, -X- _ O
b -X- _ O
) -X- _ O
proposes -X- _ O
to -X- _ O
reformulate -X- _ O
input -X- _ O
examples -X- _ O
as -X- _ O
cloze -X- _ O
questions -X- _ O
with -X- _ O
patterns -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
pretraining -X- _ O
corpus -X- _ O
in -X- _ O
the -X- _ O
few-shot -X- _ O
setting. -X- _ O
It -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
that -X- _ O
combined -X- _ O
with -X- _ O
gradient-based -X- _ O
finetuning -X- _ O
, -X- _ O
PET -X- _ B-MethodName
can -X- _ O
achieve -X- _ O
better -X- _ O
performance -X- _ O
in -X- _ O
the -X- _ O
few-shot -X- _ O
setting -X- _ O
than -X- _ O
GPT-3 -X- _ B-MethodName
while -X- _ O
requiring -X- _ O
only -X- _ O
0.1 -X- _ O
% -X- _ O
of -X- _ O
its -X- _ O
parameters. -X- _ O
Similarly -X- _ O
, -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
Paolini -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
convert -X- _ O
structured -X- _ O
prediction -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
sequence -X- _ O
tagging -X- _ O
and -X- _ O
relation -X- _ O
extraction -X- _ O
, -X- _ O
to -X- _ O
sequence -X- _ O
generation -X- _ O
tasks -X- _ O
. -X- _ O

Blank -X- _ O
Language -X- _ O
Modeling. -X- _ O
Donahue -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
also -X- _ O
study -X- _ O
blanking -X- _ O
infilling -X- _ O
models. -X- _ O
Different -X- _ O
from -X- _ O
their -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
pre-train -X- _ O
language -X- _ O
models -X- _ O
with -X- _ O
blank -X- _ O
infilling -X- _ O
objectives -X- _ O
and -X- _ O
evaluate -X- _ O
their -X- _ O
performance -X- _ O
in -X- _ O
downstream -X- _ O
NLU -X- _ B-TaskName
and -X- _ O
generation -X- _ O
tasks -X- _ O
. -X- _ O

Conclusions -X- _ O
GLM -X- _ B-MethodName
is -X- _ O
a -X- _ O
general -X- _ O
pretraining -X- _ O
framework -X- _ O
for -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
and -X- _ O
generation. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
conditional -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
and -X- _ O
therefore -X- _ O
solvable -X- _ O
by -X- _ O
autoregressive -X- _ O
models. -X- _ O
GLM -X- _ B-MethodName
unifies -X- _ O
the -X- _ O
pretraining -X- _ O
objectives -X- _ O
for -X- _ O
different -X- _ O
tasks -X- _ O
as -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling -X- _ O
, -X- _ O
with -X- _ O
mixed -X- _ O
attention -X- _ O
masks -X- _ O
and -X- _ O
the -X- _ O
novel -X- _ O
2D -X- _ O
position -X- _ O
encodings. -X- _ O
Empirically -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
GLM -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
methods -X- _ O
for -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
and -X- _ O
can -X- _ O
effectively -X- _ O
share -X- _ O
parameters -X- _ O
for -X- _ O
different -X- _ O
tasks. -X- _ O
The -X- _ O
hyperparameters -X- _ O
for -X- _ O
all -X- _ O
the -X- _ O
pre-training -X- _ O
settings -X- _ O
are -X- _ O
summarized -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
. -X- _ O

B.1 -X- _ O
SuperGLUE -X- _ B-DatasetName
The -X- _ O
SuperGLUE -X- _ B-DatasetName
benchmark -X- _ O
consists -X- _ O
of -X- _ O
8 -X- _ O
NLU -X- _ B-TaskName
tasks. -X- _ O
We -X- _ O
formulate -X- _ O
them -X- _ O
as -X- _ O
blank -X- _ B-TaskName
infilling -X- _ I-TaskName
tasks -X- _ I-TaskName
, -X- _ O
following -X- _ O
( -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
, -X- _ O
2020b -X- _ O
) -X- _ O
. -X- _ O
Table -X- _ O
8 -X- _ O
shows -X- _ O
the -X- _ O
cloze -X- _ O
questions -X- _ O
and -X- _ O
verbalizers -X- _ O
we -X- _ O
used -X- _ O
in -X- _ O
our -X- _ O
experiments. -X- _ O
For -X- _ O
3 -X- _ O
tasks -X- _ O
( -X- _ O
ReCoRD -X- _ B-DatasetName
, -X- _ O
COPA -X- _ B-DatasetName
, -X- _ O
and -X- _ O
WSC -X- _ B-DatasetName
) -X- _ O
, -X- _ O
the -X- _ O
answer -X- _ O
may -X- _ O
consist -X- _ O
of -X- _ O
multiple -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
for -X- _ O
the -X- _ O
other -X- _ O
5 -X- _ O
tasks -X- _ O
, -X- _ O
the -X- _ O
answer -X- _ O
is -X- _ O
always -X- _ O
a -X- _ O
single -X- _ O
token -X- _ O
. -X- _ O

When -X- _ O
finetuning -X- _ O
GLM -X- _ B-MethodName
on -X- _ O
the -X- _ O
SuperGLUE -X- _ B-DatasetName
tasks -X- _ O
, -X- _ O
we -X- _ O
construct -X- _ O
the -X- _ O
input -X- _ O
using -X- _ O
the -X- _ O
cloze -X- _ O
questions -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
and -X- _ O
replace -X- _ O
the -X- _ O
blank -X- _ O
with -X- _ O
a -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
token. -X- _ O
Then -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
score -X- _ O
of -X- _ O
generating -X- _ O
each -X- _ O
answer -X- _ O
candidate. -X- _ O
For -X- _ O
the -X- _ O
5 -X- _ O
single-token -X- _ O
tasks -X- _ O
, -X- _ O
the -X- _ O
score -X- _ O
is -X- _ O
defined -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
logit -X- _ O
of -X- _ O
the -X- _ O
verbalizer -X- _ O
token. -X- _ O
For -X- _ O
the -X- _ O
3 -X- _ O
multi-token -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
log-probabilities -X- _ O
of -X- _ O
the -X- _ O
verbalizer -X- _ O
tokens. -X- _ O
Thanks -X- _ O
to -X- _ O
the -X- _ O
autoregressive -X- _ O
blank -X- _ O
infilling -X- _ O
mechanism -X- _ O
we -X- _ O
proposed -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
obtain -X- _ O
all -X- _ O
the -X- _ O
log-probabilities -X- _ O
in -X- _ O
one -X- _ O
pass. -X- _ O
Then -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
cross -X- _ O
entropy -X- _ O
loss -X- _ O
using -X- _ O
the -X- _ O
groundtruth -X- _ O
label -X- _ O
and -X- _ O
update -X- _ O
the -X- _ O
model -X- _ O
parameters -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
baseline -X- _ O
classifiers -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
standard -X- _ O
practice -X- _ O
to -X- _ O
concatenate -X- _ O
the -X- _ O
input -X- _ O
parts -X- _ O
of -X- _ O
each -X- _ O
task -X- _ O
( -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
premise -X- _ O
and -X- _ O
hypothesis -X- _ O
for -X- _ O
textual -X- _ O
entailment -X- _ O
, -X- _ O
or -X- _ O
the -X- _ O
passage -X- _ O
, -X- _ O
question -X- _ O
and -X- _ O
answer -X- _ O
for -X- _ O
ReCORD -X- _ B-DatasetName
and -X- _ O
MultiRC -X- _ B-DatasetName
) -X- _ O
and -X- _ O
add -X- _ O
a -X- _ O
classification -X- _ O
layer -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
representation. -X- _ O
We -X- _ O
also -X- _ O
implemented -X- _ O
cloze-style -X- _ O
finetuning -X- _ O
for -X- _ O
the -X- _ O
other -X- _ O
pre-trained -X- _ O
models -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
performance -X- _ O
was -X- _ O
usually -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
standard -X- _ O
classifier -X- _ O
, -X- _ O
as -X- _ O
we -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
ablation -X- _ O
study. -X- _ O
Models -X- _ O
with -X- _ O
blank-infilling -X- _ O
objectives -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
T5 -X- _ B-MethodName
and -X- _ O
our -X- _ O
GLM -X- _ B-MethodName
, -X- _ O
benefits -X- _ O
more -X- _ O
from -X- _ O
converting -X- _ O
the -X- _ O
NLU -X- _ B-TaskName
tasks -X- _ O
into -X- _ O
cloze -X- _ O
questions. -X- _ O
Thus -X- _ O
for -X- _ O
T5 -X- _ B-MethodName
and -X- _ O
GLM -X- _ B-MethodName
, -X- _ O
we -X- _ O
report -X- _ O
the -X- _ O
performance -X- _ O
after -X- _ O
such -X- _ O
conversion -X- _ O
in -X- _ O
our -X- _ O
main -X- _ O
results -X- _ O
. -X- _ O

B.2 -X- _ O
Sequence-to-Sequence -X- _ O
Fot -X- _ O
the -X- _ O
text -X- _ O
summarization -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
dataset -X- _ O
Gigaword -X- _ B-DatasetName
( -X- _ O
Rush -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
for -X- _ O
model -X- _ O
fine-tuning -X- _ O
and -X- _ O
evaluation. -X- _ O
We -X- _ O
finetune -X- _ O
GLM -X- _ B-MethodName
LARGE -X- _ I-MethodName
on -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
for -X- _ O
4 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
with -X- _ O
AdamW -X- _ O
optimizer -X- _ O
. -X- _ O

The -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
has -X- _ O
a -X- _ O
peak -X- _ O
value -X- _ O
of -X- _ O
3e-5 -X- _ B-HyperparameterValue
, -X- _ O
warmup -X- _ O
over -X- _ O
the -X- _ O
6 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
training -X- _ B-HyperparameterName
steps -X- _ I-HyperparameterName
and -X- _ O
a -X- _ O
linear -X- _ O
decay. -X- _ O
We -X- _ O
also -X- _ O
use -X- _ O
label -X- _ B-HyperparameterName
smoothing -X- _ I-HyperparameterName
with -X- _ O
rate -X- _ O
0.1 -X- _ B-HyperparameterValue
( -X- _ O
Pereyra -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
maximum -X- _ B-HyperparameterName
document -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
is -X- _ O
192 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
maximum -X- _ B-HyperparameterName
summary -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
is -X- _ O
32. -X- _ B-HyperparameterValue
During -X- _ O
decoding -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
beam -X- _ O
search -X- _ O
with -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
5 -X- _ B-HyperparameterValue
and -X- _ O
remove -X- _ O
repeated -X- _ O
trigrams. -X- _ O
We -X- _ O
tweak -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
length -X- _ O
penalty -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set. -X- _ O
The -X- _ O
evaluation -X- _ O
metrics -X- _ O
are -X- _ O
the -X- _ O
F1 -X- _ B-MetricName
scores -X- _ O
of -X- _ O
Rouge-1 -X- _ B-MetricName
, -X- _ O
Rouge-2 -X- _ B-MetricName
, -X- _ O
and -X- _ O
Rouge-L -X- _ B-MetricName
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
question -X- _ O
generation -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
SQuAD -X- _ B-DatasetName
1.1 -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
follow -X- _ O
the -X- _ O
dataset -X- _ O
split -X- _ O
of -X- _ O
( -X- _ O
Du -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
optimizer -X- _ O
hyperparameters -X- _ O
are -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
those -X- _ O
of -X- _ O
abstractive -X- _ O
summarization. -X- _ O
The -X- _ O
maximum -X- _ B-HyperparameterName
passage -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
is -X- _ O
464 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
maximum -X- _ B-HyperparameterName
question -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
is -X- _ O
48. -X- _ B-HyperparameterValue
During -X- _ O
decoding -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
beam -X- _ O
search -X- _ O
with -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
5 -X- _ B-HyperparameterValue
and -X- _ O
tweak -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
length -X- _ O
penalty -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set. -X- _ O
The -X- _ O
evaluation -X- _ O
metrics -X- _ O
are -X- _ O
the -X- _ O
scores -X- _ O
of -X- _ O
BLEU-1 -X- _ B-MetricName
, -X- _ O
BLEU-2 -X- _ B-MetricName
, -X- _ O
BLEU-3 -X- _ B-MetricName
, -X- _ O
BLEU-4 -X- _ B-MetricName
( -X- _ O
Papineni -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
, -X- _ O
METEOR -X- _ B-MetricName
( -X- _ O
Denkowski -X- _ O
and -X- _ O
Lavie -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
and -X- _ O
Rouge-L -X- _ B-MetricName
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

Results -X- _ O
of -X- _ O
T5 -X- _ B-DatasetName
Large -X- _ I-DatasetName
on -X- _ O
XSum -X- _ B-DatasetName
are -X- _ O
obtained -X- _ O
by -X- _ O
running -X- _ O
the -X- _ O
summarization -X- _ O
script -X- _ O
provided -X- _ O
by -X- _ O
Huggingface -X- _ O
transformers -X- _ O
6 -X- _ O
. -X- _ O
All -X- _ O
the -X- _ O
other -X- _ O
results -X- _ O
of -X- _ O
well -X- _ O
studied -X- _ O
for -X- _ O
language -X- _ O
modeling. -X- _ O
Perplexity -X- _ O
is -X- _ O
the -X- _ O
exponentiation -X- _ O
of -X- _ O
the -X- _ O
average -X- _ O
cross -X- _ O
entropy -X- _ O
of -X- _ O
a -X- _ O
corpus. -X- _ O
LAMBDA -X- _ B-DatasetName
is -X- _ O
a -X- _ O
cloze-style -X- _ O
dataset -X- _ O
to -X- _ O
test -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
long-range -X- _ O
dependency -X- _ O
modeling. -X- _ O
Each -X- _ O
example -X- _ O
is -X- _ O
a -X- _ O
passage -X- _ O
consisting -X- _ O
of -X- _ O
4-5 -X- _ O
sentences -X- _ O
with -X- _ O
the -X- _ O
last -X- _ O
word -X- _ O
missing -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
required -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
last -X- _ O
word -X- _ O
of -X- _ O
the -X- _ O
passage. -X- _ O
Since -X- _ O
we -X- _ O
use -X- _ O
WordPiece -X- _ O
tokenization -X- _ O
, -X- _ O
a -X- _ O
word -X- _ O
can -X- _ O
be -X- _ O
split -X- _ O
into -X- _ O
several -X- _ O
subword -X- _ O
units. -X- _ O
We -X- _ O
use -X- _ O
teacher -X- _ O
forcing -X- _ O
and -X- _ O
consider -X- _ O
the -X- _ O
prediction -X- _ O
correct -X- _ O
only -X- _ O
when -X- _ O
all -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
are -X- _ O
correct -X- _ O
. -X- _ O

C -X- _ O
Results -X- _ O
on -X- _ O
Other -X- _ O
NLU -X- _ B-TaskName
Benchmarks -X- _ O
GLUE -X- _ B-DatasetName
( -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
is -X- _ O
another -X- _ O
widely-used -X- _ O
NLU -X- _ B-TaskName
benchmark -X- _ O
, -X- _ O
including -X- _ O
single -X- _ O
sentence -X- _ O
tasks -X- _ O
( -X- _ O
e.g. -X- _ O
sentiment -X- _ O
analysis -X- _ O
( -X- _ O
Socher -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
) -X- _ O
and -X- _ O
sentence -X- _ O
pair -X- _ O
tasks -X- _ O
( -X- _ O
e.g. -X- _ O
text -X- _ O
similarity -X- _ O
( -X- _ O
Cer -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
natural -X- _ O
language -X- _ O
inference -X- _ O
( -X- _ O
Williams -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Dagan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2005 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
benchmark -X- _ O
is -X- _ O
usually -X- _ O
considered -X- _ O
as -X- _ O
less -X- _ O
challenging -X- _ O
than -X- _ O
Super-GLUE. -X- _ B-DatasetName
SQuAD -X- _ B-DatasetName
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
is -X- _ O
an -X- _ O
extractive -X- _ O
question -X- _ O
answering -X- _ O
benchmark. -X- _ O
We -X- _ O
further -X- _ O
compare -X- _ O
GLM -X- _ B-MethodName
with -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
the -X- _ O
two -X- _ O
benchmarks -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
and -X- _ O
SQuAD -X- _ B-DatasetName
are -X- _ O
shown -X- _ O
in -X- _ O
Tables -X- _ O
9 -X- _ O
and -X- _ O
10. -X- _ O
On -X- _ O
the -X- _ O
two -X- _ O
benchmarks -X- _ O
, -X- _ O
GLM -X- _ B-MethodName
can -X- _ O
still -X- _ O
outperform -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
the -X- _ O
same -X- _ O
amount -X- _ O
of -X- _ O
parameters -X- _ O
, -X- _ O
but -X- _ O
with -X- _ O
a -X- _ O
smaller -X- _ O
margin -X- _ O
. -X- _ O

D -X- _ O
Text -X- _ O
Generation -X- _ O
Samples -X- _ O
We -X- _ O
show -X- _ O
texts -X- _ O
generated -X- _ O
by -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
given -X- _ O
unseen -X- _ O
contexts -X- _ O
randomly -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set. -X- _ O
We -X- _ O
use -X- _ O
top-k -X- _ O
random -X- _ O
sampling -X- _ O
with -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
40 -X- _ B-HyperparameterValue
for -X- _ O
generation -X- _ O
and -X- _ O
set -X- _ O
maximum -X- _ B-HyperparameterName
sequence -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
to -X- _ O
512. -X- _ B-HyperparameterValue
Some -X- _ O
of -X- _ O
the -X- _ O
texts -X- _ O
are -X- _ O
cut -X- _ O
short. -X- _ O
Simpsons -X- _ O
" -X- _ O
as -X- _ O
the -X- _ O
character -X- _ O
captain -X- _ O
Billy -X- _ O
Higgledypig -X- _ O
, -X- _ O
but -X- _ O
his -X- _ O
character -X- _ O
was -X- _ O
only -X- _ O
a -X- _ O
one-time -X- _ O
recurring -X- _ O
character -X- _ O
in -X- _ O
the -X- _ O
series -X- _ O
' -X- _ O
first -X- _ O
six -X- _ O
seasons. -X- _ O
He -X- _ O
later -X- _ O
appeared -X- _ O
as -X- _ O
a -X- _ O
regular -X- _ O
for -X- _ O
the -X- _ O
show -X- _ O
's -X- _ O
final -X- _ O
six -X- _ O
seasons -X- _ O
, -X- _ O
and -X- _ O
has -X- _ O
been -X- _ O
a -X- _ O
frequent -X- _ O
guest -X- _ O
in -X- _ O
the -X- _ O
show -X- _ O
since -X- _ O
. -X- _ O

Example -X- _ O
D.3. -X- _ O
Corona -X- _ O
was -X- _ O
a -X- _ O
station -X- _ O
along -X- _ O
the -X- _ O
port -X- _ O
Washington -X- _ O
branch -X- _ O
of -X- _ O
the -X- _ O
long -X- _ O
island -X- _ O
rail -X- _ O
road -X- _ O
in -X- _ O
the -X- _ O
Corona -X- _ O
section -X- _ O
of -X- _ O
queens -X- _ O
, -X- _ O
New -X- _ O
York -X- _ O
City. -X- _ O
It -X- _ O
was -X- _ O
one -X- _ O
of -X- _ O
two -X- _ O
stations -X- _ O
built -X- _ O
by -X- _ O
the -X- _ O
flushing -X- _ O
railroad -X- _ O
in -X- _ O
Corona -X- _ O
, -X- _ O
this -X- _ O
one -X- _ O
having -X- _ O
been -X- _ O
at -X- _ O
Grand -X- _ O
Avenue -X- _ O
( -X- _ O
later -X- _ O
called -X- _ O
National -X- _ O
Avenue -X- _ O
, -X- _ O
now -X- _ O
National -X- _ O
Street -X- _ O
) -X- _ O
and -X- _ O
45th -X- _ O
Avenue. -X- _ O
GLM -X- _ B-MethodName
: -X- _ O
It -X- _ O
had -X- _ O
a -X- _ O
brick -X- _ O
entrance -X- _ O
building -X- _ O
next -X- _ O
to -X- _ O
the -X- _ O
tracks -X- _ O
, -X- _ O
and -X- _ O
one -X- _ O
platform -X- _ O
with -X- _ O
2 -X- _ O
side -X- _ O
platforms. -X- _ O
The -X- _ O
station -X- _ O
burned -X- _ O
down -X- _ O
and -X- _ O
was -X- _ O
rebuilt -X- _ O
in -X- _ O
1908. -X- _ O
The -X- _ O
station -X- _ O
closed -X- _ O
on -X- _ O
september -X- _ O
15 -X- _ O
, -X- _ O
1927 -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
train -X- _ O
service -X- _ O
transferred -X- _ O
from -X- _ O
Grand -X- _ O
Avenue -X- _ O
to -X- _ O
45th -X- _ O
Avenue. -X- _ O
There -X- _ O
were -X- _ O
plans -X- _ O
to -X- _ O
build -X- _ O
a -X- _ O
subway -X- _ O
extension -X- _ O
to -X- _ O
Corona -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
was -X- _ O
never -X- _ O
built. -X- _ O
In -X- _ O
the -X- _ O
1980s -X- _ O
, -X- _ O
two -X- _ O
stations -X- _ O
were -X- _ O
constructed -X- _ O
on -X- _ O
the -X- _ O
line -X- _ O
, -X- _ O
Corona -X- _ O
Road -X- _ O
and -X- _ O
Corona -X- _ O
Park. -X- _ O
The -X- _ O
new -X- _ O
Corona -X- _ O
station -X- _ O
opened -X- _ O
in -X- _ O
1988 -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
original -X- _ O
Corona -X- _ O
station -X- _ O
was -X- _ O
demolished -X- _ O
. -X- _ O

GLM -X- _ B-MethodName
: -X- _ O
In -X- _ O
his -X- _ O
four-year -X- _ O
NFL -X- _ O
career -X- _ O
, -X- _ O
he -X- _ O
played -X- _ O
in -X- _ O
33 -X- _ O
games -X- _ O
and -X- _ O
started -X- _ O
14 -X- _ O
, -X- _ O
registering -X- _ O
62 -X- _ O
career -X- _ O
interceptions. -X- _ O
He -X- _ O
completed -X- _ O
his -X- _ O
NFL -X- _ O
career -X- _ O
with -X- _ O
five -X- _ O
interceptions. -X- _ O
Smith -X- _ O
was -X- _ O
born -X- _ O
in -X- _ O
La -X- _ O
Canada -X- _ O
Flintridge -X- _ O
, -X- _ O
Michigan -X- _ O
, -X- _ O
in -X- _ O
1938. -X- _ O
He -X- _ O
attended -X- _ O
Bishop -X- _ O
O -X- _ O
' -X- _ O
Dowd -X- _ O
high -X- _ O
school -X- _ O
in -X- _ O
Flintridge. -X- _ O
Smith -X- _ O
' -X- _ O
s -X- _ O
father -X- _ O
was -X- _ O
a -X- _ O
pharmacist. -X- _ O
He -X- _ O
graduated -X- _ O
from -X- _ O
Michigan -X- _ O
State -X- _ O
University -X- _ O
in -X- _ O
1958 -X- _ O
with -X- _ O
a -X- _ O
degree -X- _ O
in -X- _ O
business -X- _ O
administration. -X- _ O
Smith -X- _ O
played -X- _ O
for -X- _ O
the -X- _ O
Michigan -X- _ O
Wolverines -X- _ O
football -X- _ O
team -X- _ O
from -X- _ O
1959 -X- _ O
to -X- _ O
1963. -X- _ O
In -X- _ O
his -X- _ O
two -X- _ O
years -X- _ O
as -X- _ O
a -X- _ O
reserve -X- _ O
cornerback -X- _ O
, -X- _ O
he -X- _ O
led -X- _ O
the -X- _ O
conference -X- _ O
in -X- _ O
interceptions -X- _ O
with -X- _ O
five. -X- _ O
In -X- _ O
1962 -X- _ O
, -X- _ O
he -X- _ O
set -X- _ O
the -X- _ O
Wolverines -X- _ O
' -X- _ O
all-time -X- _ O
interception -X- _ O
record -X- _ O
with -X- _ O
13 -X- _ O
, -X- _ O
and -X- _ O
was -X- _ O
second -X- _ O
overall -X- _ O
in -X- _ O
the -X- _ O
1962 -X- _ O
season -X- _ O
's -X- _ O
Heisman -X- _ O
Trophy -X- _ O
voting. -X- _ O
Smith -X- _ O
also -X- _ O
won -X- _ O
the -X- _ O
Dick -X- _ O
Butkus -X- _ O
award -X- _ O
as -X- _ O
the -X- _ O
nation -X- _ O
's -X- _ O
outstanding -X- _ O
linebacker. -X- _ O
In -X- _ O
1961 -X- _ O
, -X- _ O
the -X- _ O
" -X- _ O
Los -X- _ O
Angeles -X- _ O
Times -X- _ O
" -X- _ O
wrote -X- _ O
that -X- _ O
Smith -X- _ O
" -X- _ O
is -X- _ O
an -X- _ O
outstanding -X- _ O
pass -X- _ O
rusher -X- _ O
, -X- _ O
with -X- _ O
an -X- _ O
average -X- _ O
of -X- _ O
almost -X- _ O
100 -X- _ O
yards -X- _ O
per -X- _ O
punt -X- _ O
return. -X- _ O
" -X- _ O
Smith -X- _ O
was -X- _ O
inducted -X- _ O
into -X- _ O
the -X- _ O
university -X- _ O
of -X- _ O
Michigan -X- _ O
athletic -X- _ O
hall -X- _ O
of -X- _ O
honor -X- _ O
in -X- _ O
1989 -X- _ O
and -X- _ O
the -X- _ O
national -X- _ O
football -X- _ O
foundation -X- _ O
hall -X- _ O
of -X- _ O
fame -X- _ O
in -X- _ O
1991. -X- _ O
He -X- _ O
was -X- _ O
elected -X- _ O
to -X- _ O
the -X- _ O
Michigan -X- _ O
sports -X- _ O
hall -X- _ O
of -X- _ O
fame -X- _ O
in -X- _ O
1995. -X- _ O
Smith -X- _ O
earned -X- _ O
the -X- _ O
honor -X- _ O
because -X- _ O
of -X- _ O
his -X- _ O
accomplishments -X- _ O
prior -X- _ O
to -X- _ O
his -X- _ O
NFL -X- _ O
career. -X- _ O
He -X- _ O
was -X- _ O
one -X- _ O
of -X- _ O
four -X- _ O
Michigan -X- _ O
players -X- _ O
honored -X- _ O
as -X- _ O
first-overall -X- _ O
selections -X- _ O
in -X- _ O
the -X- _ O
1964 -X- _ O
NFL -X- _ O
draft. -X- _ O
The -X- _ O
others -X- _ O
were -X- _ O
Joe -X- _ O
Namath -X- _ O
, -X- _ O
Bill -X- _ O
Nelsen -X- _ O
, -X- _ O
and -X- _ O
Jerry -X- _ O
Kramer. -X- _ O
In -X- _ O
1966 -X- _ O
, -X- _ O
the -X- _ O
NFL -X- _ O
gave -X- _ O
players -X- _ O
$ -X- _ O
300,000 -X- _ O
a -X- _ O
season -X- _ O
to -X- _ O
play -X- _ O
football. -X- _ O
After -X- _ O
his -X- _ O
rookie -X- _ O
season -X- _ O
, -X- _ O
he -X- _ O
was -X- _ O
not -X- _ O
selected -X- _ O
to -X- _ O
play -X- _ O
in -X- _ O
the -X- _ O
1966 -X- _ O
pro -X- _ O
bowl. -X- _ O
On -X- _ O
January -X- _ O
13 -X- _ O
, -X- _ O
1966 -X- _ O
, -X- _ O
the -X- _ O
Rams -X- _ O
traded -X- _ O
smith -X- _ O
to -X- _ O
the -X- _ O
Detroit -X- _ O
Lions -X- _ O
for -X- _ O
Paul -X- _ O
Hornung -X- _ O
, -X- _ O
and -X- _ O
later -X- _ O
that -X- _ O
year -X- _ O
he -X- _ O
was -X- _ O
traded -X- _ O
to -X- _ O
the -X- _ O
Lions -X- _ O
for -X- _ O
Ray -X- _ O
" -X- _ O
the -X- _ O
Lion -X- _ O
" -X- _ O
Jones -X- _ O
in -X- _ O
exchange -X- _ O
for -X- _ O
Linebacker -X- _ O
Jim -X- _ O
" -X- _ O
the -X- _ O
Hawk -X- _ O
" -X- _ O
Johnson. -X- _ O
On -X- _ O
September -X- _ O
10 -X- _ O
, -X- _ O
1968 -X- _ O
, -X- _ O
he -X- _ O
was -X- _ O
traded -X- _ O
back -X- _ O
to -X- _ O
Los -X- _ O
Angeles -X- _ O
for -X- _ O
a -X- _ O
second -X- _ O
round -X- _ O
pick -X- _ O
in -X- _ O
the -X- _ O
1970 -X- _ O
draft. -X- _ O
He -X- _ O
was -X- _ O
also -X- _ O
traded -X- _ O
to -X- _ O
the -X- _ O
St. -X- _ O
Louis -X- _ O
Cardinals -X- _ O
for -X- _ O
a -X- _ O
second -X- _ O
round -X- _ O
pick -X- _ O
in -X- _ O
the -X- _ O
1970 -X- _ O
draft. -X- _ O
On -X- _ O
June -X- _ O
2 -X- _ O
, -X- _ O
1970 -X- _ O
he -X- _ O
was -X- _ O
cut -X- _ O
by -X- _ O
the -X- _ O
Cardinals. -X- _ O
On -X- _ O
November -X- _ O
15 -X- _ O
, -X- _ O
1970 -X- _ O
, -X- _ O
the -X- _ O
Los -X- _ O
Angeles -X- _ O
Rams -X- _ O
acquired -X- _ O
Smith -X- _ O
from -X- _ O
the -X- _ O
Lions -X- _ O
in -X- _ O
exchange -X- _ O
for -X- _ O
Linebacker -X- _ O
Tony -X- _ O
Harris. -X- _ O
The -X- _ O
Rams -X- _ O
waived -X- _ O
Smith -X- _ O
during -X- _ O
the -X- _ O
September -X- _ O
1 -X- _ O
, -X- _ O
1972 -X- _ O
offseason. -X- _ O
Smith -X- _ O
's -X- _ O
number -X- _ O
at -X- _ O
Michigan -X- _ O
State -X- _ O
was -X- _ O
# -X- _ O
7 -X- _ O
in -X- _ O
1969 -X- _ O
. -X- _ O

A.1 -X- _ O
Datasets -X- _ O
To -X- _ O
train -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
Book-Corpus -X- _ B-DatasetName
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
Wikipedia -X- _ B-DatasetName
used -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
train -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
pretraining -X- _ O
datasets -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
which -X- _ O
consist -X- _ O
of -X- _ O
BookCorups -X- _ B-DatasetName
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
Wikipedia -X- _ B-DatasetName
( -X- _ O
16GB -X- _ O
) -X- _ O
, -X- _ O
CC-News -X- _ B-DatasetName
( -X- _ O
the -X- _ O
English -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
Com-monCrawl -X- _ O
News -X- _ O
dataset -X- _ O
3 -X- _ O
76GB -X- _ O
) -X- _ O
, -X- _ O
OpenWebText -X- _ B-DatasetName
( -X- _ O
web -X- _ O
content -X- _ O
extracted -X- _ O
from -X- _ O
URLs -X- _ O
shared -X- _ O
on -X- _ O
Reddit -X- _ O
with -X- _ O
at -X- _ O
least -X- _ O
three -X- _ O
upvotes -X- _ O
( -X- _ O
Gokaslan -X- _ O
and -X- _ O
Cohen -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
38GB -X- _ O
) -X- _ O
and -X- _ O
Stories -X- _ O
( -X- _ O
subset -X- _ O
of -X- _ O
Common-Crawl -X- _ O
data -X- _ O
filtered -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
story-like -X- _ O
style -X- _ O
of -X- _ O
Winograd -X- _ O
schemas -X- _ O
( -X- _ O
Trinh -X- _ O
and -X- _ O
Le -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
31GB -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
Stories -X- _ O
dataset -X- _ O
is -X- _ O
no -X- _ O
longer -X- _ O
publicly -X- _ O
available -X- _ O
4 -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
Stories -X- _ O
dataset -X- _ O
and -X- _ O
replace -X- _ O
OpenWebText -X- _ B-DatasetName
with -X- _ O
OpenWebText2 -X- _ B-DatasetName
5 -X- _ O
( -X- _ O
66GB -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
CC-News -X- _ O
dataset -X- _ O
is -X- _ O
not -X- _ O
publicly -X- _ O
available -X- _ O
and -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
CC-News-en -X- _ B-DatasetName
published -X- _ O
by -X- _ O
( -X- _ O
Mackenzie -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
All -X- _ O
the -X- _ O
datasets -X- _ O
used -X- _ O
total -X- _ O
158GB -X- _ O
of -X- _ O
uncompressed -X- _ O
texts -X- _ O
, -X- _ O
close -X- _ O
in -X- _ O
size -X- _ O
to -X- _ O
RoBERTa -X- _ B-MethodName
's -X- _ O
160GB -X- _ O
datasets -X- _ O
. -X- _ O

A.2 -X- _ O
Hyperparameters -X- _ O
The -X- _ O
hyperparameters -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
are -X- _ O
similar -X- _ O
to -X- _ O
those -X- _ O
used -X- _ O
by -X- _ O
BERT. -X- _ B-MethodName
For -X- _ O
trade-off -X- _ O
of -X- _ O
training -X- _ O
speed -X- _ O
and -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
batch -X- _ O
size -X- _ O
256 -X- _ O
and -X- _ O
1,000,000 -X- _ O
training -X- _ O
steps -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
batch -X- _ O
size -X- _ O
of -X- _ O
1024 -X- _ O
and -X- _ O
200,000 -X- _ O
training -X- _ O
steps -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
Since -X- _ O
GLM -X- _ B-MethodName
Base -X- _ I-MethodName
is -X- _ O
smaller -X- _ O
, -X- _ O
we -X- _ O
reduce -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
training -X- _ O
steps -X- _ O
to -X- _ O
120,000 -X- _ O
to -X- _ O
speed -X- _ O
up -X- _ O
pre-training. -X- _ O
The -X- _ O
hyperparameters -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
Doc -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
Sent -X- _ I-MethodName
are -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
those -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
The -X- _ O
hyperparameters -X- _ O
except -X- _ O
Transformer -X- _ O
architecture -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
410M -X- _ I-MethodName
and -X- _ O
GLM -X- _ B-MethodName
515M -X- _ I-MethodName
are -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
those -X- _ O
of -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O
The -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
64 -X- _ O
V100 -X- _ O
GPUs -X- _ O
for -X- _ O
200K -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
with -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
1024 -X- _ B-HyperparameterValue
and -X- _ O
maximum -X- _ B-HyperparameterName
sequence -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
takes -X- _ O
about -X- _ O
2.5 -X- _ O
days -X- _ O
for -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
. -X- _ O

To -X- _ O
train -X- _ O
GLM -X- _ B-MethodName
RoBERTa -X- _ I-MethodName
, -X- _ O
we -X- _ O
follow -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
hyperparameters -X- _ O
of -X- _ O
RoBERTa. -X- _ B-MethodName
The -X- _ O
main -X- _ O
difference -X- _ O
baselines -X- _ O
on -X- _ O
seq2seq -X- _ B-TaskName
tasks -X- _ O
are -X- _ O
obtained -X- _ O
from -X- _ O
the -X- _ O
corresponding -X- _ O
papers -X- _ O
. -X- _ O

B.3 -X- _ O
Text -X- _ B-TaskName
Infilling -X- _ I-TaskName
We -X- _ O
follow -X- _ O
( -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
evaluate -X- _ O
text -X- _ B-TaskName
infilling -X- _ I-TaskName
performance -X- _ O
on -X- _ O
the -X- _ O
Yahoo -X- _ B-DatasetName
Answers -X- _ I-DatasetName
dataset -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
contains -X- _ O
100K -X- _ B-HyperparameterValue
/ -X- _ I-HyperparameterValue
10K -X- _ I-HyperparameterValue
/ -X- _ I-HyperparameterValue
10K -X- _ I-HyperparameterValue
documents -X- _ O
for -X- _ O
train -X- _ B-HyperparameterName
/ -X- _ I-HyperparameterName
valid -X- _ I-HyperparameterName
/ -X- _ I-HyperparameterName
test -X- _ I-HyperparameterName
respectively. -X- _ O
The -X- _ O
average -X- _ O
document -X- _ O
length -X- _ O
is -X- _ O
78 -X- _ O
words. -X- _ O
To -X- _ O
construct -X- _ O
the -X- _ O
text -X- _ O
infilling -X- _ O
task -X- _ O
, -X- _ O
we -X- _ O
randomly -X- _ O
mask -X- _ O
a -X- _ O
given -X- _ O
ratio -X- _ O
r -X- _ B-HyperparameterName
∈ -X- _ O
{ -X- _ O
10 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
• -X- _ I-HyperparameterValue
• -X- _ I-HyperparameterValue
• -X- _ I-HyperparameterValue
50 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
} -X- _ O
of -X- _ O
each -X- _ O
document -X- _ O
's -X- _ O
tokens -X- _ O
and -X- _ O
the -X- _ O
contiguous -X- _ O
masked -X- _ O
tokens -X- _ O
are -X- _ O
collapsed -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
blank. -X- _ O
We -X- _ O
finetune -X- _ O
GLM -X- _ B-MethodName
Large -X- _ I-MethodName
on -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
for -X- _ O
5 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
with -X- _ O
dynamic -X- _ O
masking -X- _ O
, -X- _ O
i.e. -X- _ O
the -X- _ O
blanks -X- _ O
are -X- _ O
randomly -X- _ O
generated -X- _ O
at -X- _ O
training -X- _ O
time. -X- _ O
Similar -X- _ O
to -X- _ O
the -X- _ O
sequence-to-sequence -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
an -X- _ O
AdamW -X- _ O
optimizer -X- _ O
with -X- _ O
a -X- _ O
peak -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
1e-5 -X- _ B-HyperparameterValue
and -X- _ O
6 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
warm-up -X- _ B-HyperparameterName
linear -X- _ O
scheduler -X- _ O
. -X- _ O

For -X- _ O
comparison -X- _ O
with -X- _ O
previous -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
test -X- _ O
set -X- _ O
constructed -X- _ O
by -X- _ O
( -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
evaluation -X- _ O
metric -X- _ O
is -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
of -X- _ O
the -X- _ O
infilled -X- _ O
text -X- _ O
against -X- _ O
the -X- _ O
original -X- _ O
document. -X- _ O
We -X- _ O
compare -X- _ O
with -X- _ O
two -X- _ O
baselines -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
which -X- _ O
learns -X- _ O
a -X- _ O
left-to-right -X- _ O
language -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
the -X- _ O
masked -X- _ O
tokens -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
blank -X- _ O
representation -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
BLM -X- _ B-MethodName
proposed -X- _ O
by -X- _ O
( -X- _ O
Shen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
fill -X- _ O
in -X- _ O
the -X- _ O
blank -X- _ O
with -X- _ O
arbitrary -X- _ O
trajectories -X- _ O
. -X- _ O

B.4 -X- _ O
Language -X- _ O
Modeling -X- _ O
We -X- _ O
evaluate -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
ability -X- _ O
of -X- _ O
language -X- _ O
modeling -X- _ O
with -X- _ O
perplexity -X- _ B-MetricName
on -X- _ O
BookWiki -X- _ B-DatasetName
and -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
LAMBDA -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Paperno -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

GLM -X- _ B-MethodName
: -X- _ O
He -X- _ O
was -X- _ O
a -X- _ O
voice -X- _ O
actor -X- _ O
for -X- _ O
the -X- _ O
" -X- _ O
X-Men -X- _ O
" -X- _ O
cartoon -X- _ O
series. -X- _ O
He -X- _ O
was -X- _ O
also -X- _ O
a -X- _ O
voice -X- _ O
actor -X- _ O
for -X- _ O
" -X- _ O
the -X- _ O
Simpsons -X- _ O
" -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
" -X- _ O
the -X- _ O
marvelous -X- _ O
misadventures -X- _ O
of -X- _ O
superman. -X- _ O
He -X- _ O
also -X- _ O
has -X- _ O
voiced -X- _ O
characters -X- _ O
in -X- _ O
" -X- _ O
the -X- _ O
legend -X- _ O
of -X- _ O
korra -X- _ O
" -X- _ O
He -X- _ O
has -X- _ O
appeared -X- _ O
on -X- _ O
several -X- _ O
television -X- _ O
series -X- _ O
, -X- _ O
including -X- _ O
" -X- _ O
the -X- _ O
simpsons -X- _ O
" -X- _ O
, -X- _ O
" -X- _ O
the -X- _ O
x-files -X- _ O
" -X- _ O
, -X- _ O
" -X- _ O
heroes -X- _ O
" -X- _ O
and -X- _ O
" -X- _ O
the -X- _ O
simpsons -X- _ O
movie -X- _ O
" -X- _ O
as -X- _ O
the -X- _ O
character -X- _ O
captain -X- _ O
billy -X- _ O
higgledy-pig -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
web -X- _ O
series -X- _ O
" -X- _ O
krusty -X- _ O
mysteries -X- _ O
" -X- _ O
as -X- _ O
the -X- _ O
character -X- _ O
Colonel -X- _ O
Trungus. -X- _ O
He -X- _ O
wrote -X- _ O
for -X- _ O
" -X- _ O
the -X- _ O
Guardian -X- _ O
" -X- _ O
newspaper. -X- _ O
Jonathan -X- _ O
Terry -X- _ O
was -X- _ O
born -X- _ O
in -X- _ O
London. -X- _ O
Terry -X- _ O
grew -X- _ O
up -X- _ O
in -X- _ O
Surrey -X- _ O
, -X- _ O
England -X- _ O
and -X- _ O
attended -X- _ O
the -X- _ O
University -X- _ O
of -X- _ O
Sussex -X- _ O
in -X- _ O
the -X- _ O
United -X- _ O
Kingdom -X- _ O
, -X- _ O
graduating -X- _ O
with -X- _ O
a -X- _ O
degree -X- _ O
in -X- _ O
english -X- _ O
literature. -X- _ O
He -X- _ O
was -X- _ O
a -X- _ O
guest -X- _ O
lecturer -X- _ O
at -X- _ O
King -X- _ O
's -X- _ O
College -X- _ O
London -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
took -X- _ O
two -X- _ O
years -X- _ O
of -X- _ O
acting -X- _ O
courses -X- _ O
at -X- _ O
the -X- _ O
brit -X- _ O
school -X- _ O
of -X- _ O
acting -X- _ O
to -X- _ O
prepare -X- _ O
for -X- _ O
his -X- _ O
future -X- _ O
career -X- _ O
in -X- _ O
the -X- _ O
entertainment -X- _ O
industry. -X- _ O
Terry -X- _ O
first -X- _ O
appeared -X- _ O
in -X- _ O
the -X- _ O
TV -X- _ O
series -X- _ O
" -X- _ O
the -X- _ O

Flow-Adapter -X- _ B-MethodName
Architecture -X- _ O
for -X- _ O
Unsupervised -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
flow-adapter -X- _ B-MethodName
architecture -X- _ O
for -X- _ O
unsupervised -X- _ O
NMT. -X- _ B-TaskName
It -X- _ O
leverages -X- _ O
normalizing -X- _ O
flows -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
sentence-level -X- _ O
latent -X- _ O
representations -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
subsequently -X- _ O
used -X- _ O
in -X- _ O
conjunction -X- _ O
with -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
for -X- _ O
the -X- _ O
translation -X- _ O
task. -X- _ O
The -X- _ O
primary -X- _ O
novelties -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
are -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
capturing -X- _ O
language-specific -X- _ O
sentence -X- _ O
representations -X- _ O
separately -X- _ O
for -X- _ O
each -X- _ O
language -X- _ O
using -X- _ O
normalizing -X- _ O
flows -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
using -X- _ O
a -X- _ O
simple -X- _ O
transformation -X- _ O
of -X- _ O
these -X- _ O
latent -X- _ O
representations -X- _ O
for -X- _ O
translating -X- _ O
from -X- _ O
one -X- _ O
language -X- _ O
to -X- _ O
another. -X- _ O
This -X- _ O
architecture -X- _ O
allows -X- _ O
for -X- _ O
unsupervised -X- _ O
training -X- _ O
of -X- _ O
each -X- _ O
language -X- _ O
independently. -X- _ O
While -X- _ O
there -X- _ O
is -X- _ O
prior -X- _ O
work -X- _ O
on -X- _ O
latent -X- _ O
variables -X- _ O
for -X- _ O
supervised -X- _ O
MT -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
the -X- _ O
first -X- _ O
work -X- _ O
that -X- _ O
uses -X- _ O
latent -X- _ O
variables -X- _ O
and -X- _ O
normalizing -X- _ O
flows -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
MT. -X- _ I-TaskName
We -X- _ O
obtain -X- _ O
competitive -X- _ O
results -X- _ O
on -X- _ O
several -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
benchmarks -X- _ O
. -X- _ O

Introduction -X- _ O
Recent -X- _ O
advances -X- _ O
in -X- _ O
deep -X- _ O
learning -X- _ O
have -X- _ O
boosted -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
. -X- _ O
Typical -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
leverage -X- _ O
an -X- _ O
encoder-decoder -X- _ O
framework -X- _ O
( -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
datahungry -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parallel -X- _ O
sentences -X- _ O
significantly -X- _ O
influences -X- _ O
the -X- _ O
performance -X- _ O
( -X- _ O
Zoph -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
Unfortunately -X- _ O
, -X- _ O
large-scale -X- _ O
bilingual -X- _ O
corpora -X- _ O
are -X- _ O
limited -X- _ O
to -X- _ O
a -X- _ O
relatively -X- _ O
small -X- _ O
subset -X- _ O
of -X- _ O
languages -X- _ O
( -X- _ O
Al-Onaizan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
bilingual -X- _ O
corpora -X- _ O
, -X- _ O
monolingual -X- _ O
corpora -X- _ O
are -X- _ O
much -X- _ O
easier -X- _ O
to -X- _ O
obtain -X- _ O
. -X- _ O

Unsupervised -X- _ B-TaskName
NMT -X- _ I-TaskName
, -X- _ O
compared -X- _ O
with -X- _ O
supervised -X- _ O
NMT -X- _ B-TaskName
, -X- _ O
aims -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
model -X- _ O
without -X- _ O
parallel -X- _ O
data. -X- _ O
Some -X- _ O
early -X- _ O
works -X- _ O
( -X- _ O
Irvine -X- _ O
and -X- _ O
Callison-Burch -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Sennrich -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016b -X- _ O
; -X- _ O
Cheng -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
used -X- _ O
monolingual -X- _ O
corpora -X- _ O
to -X- _ O
boost -X- _ O
performance -X- _ O
when -X- _ O
parallel -X- _ O
data -X- _ O
is -X- _ O
not -X- _ O
abundant. -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018a -X- _ O
) -X- _ O
and -X- _ O
Artetxe -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
explored -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
training -X- _ O
a -X- _ O
model -X- _ O
relying -X- _ O
only -X- _ O
on -X- _ O
mono- -X- _ O
lingual -X- _ O
corpora. -X- _ O
They -X- _ O
both -X- _ O
leveraged -X- _ O
a -X- _ O
sharedencoder -X- _ O
architecture -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
generate -X- _ O
universal -X- _ O
representations -X- _ O
, -X- _ O
trained -X- _ O
with -X- _ O
techniques -X- _ O
such -X- _ O
as -X- _ O
initial -X- _ O
word-by-word -X- _ O
translation -X- _ O
through -X- _ O
bilingual -X- _ O
dictionaries -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
; -X- _ O
Artetxe -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
denoising -X- _ O
auto-encoding -X- _ O
( -X- _ O
DAE -X- _ O
) -X- _ O
( -X- _ O
Vincent -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2008 -X- _ O
) -X- _ O
and -X- _ O
iterative -X- _ O
back-translation -X- _ O
( -X- _ O
BT -X- _ O
) -X- _ O
( -X- _ O
Hoang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
argued -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
bottleneck -X- _ O
in -X- _ O
such -X- _ O
shared-encoder -X- _ O
models -X- _ O
to -X- _ O
use -X- _ O
a -X- _ O
shared -X- _ O
encoder -X- _ O
that -X- _ O
maps -X- _ O
pairs -X- _ O
of -X- _ O
sentences -X- _ O
of -X- _ O
different -X- _ O
languages -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
shared -X- _ O
latent -X- _ O
space. -X- _ O
They -X- _ O
proposed -X- _ O
to -X- _ O
use -X- _ O
two -X- _ O
independent -X- _ O
encoders -X- _ O
sharing -X- _ O
part -X- _ O
of -X- _ O
their -X- _ O
weights -X- _ O
and -X- _ O
achieved -X- _ O
better -X- _ O
results. -X- _ O
But -X- _ O
all -X- _ O
of -X- _ O
those -X- _ O
aforementioned -X- _ O
approaches -X- _ O
trained -X- _ O
the -X- _ O
translation -X- _ O
models -X- _ O
almost -X- _ O
from -X- _ O
scratch -X- _ O
( -X- _ O
with -X- _ O
only -X- _ O
some -X- _ O
prior -X- _ O
knowledge -X- _ O
in -X- _ O
the -X- _ O
pre-trained -X- _ O
embeddings -X- _ O
) -X- _ O
and -X- _ O
therefore -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
further -X- _ O
advance -X- _ O
their -X- _ O
performance -X- _ O
. -X- _ O

More -X- _ O
recently -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
advance -X- _ O
in -X- _ O
pre-trained -X- _ O
models -X- _ O
( -X- _ O
Peters -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
researchers -X- _ O
have -X- _ O
begun -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
using -X- _ O
pre-trained -X- _ O
models -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
NMT. -X- _ I-TaskName
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
extended -X- _ O
the -X- _ O
pre-training -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
language -X- _ O
to -X- _ O
multiple -X- _ O
languages -X- _ O
, -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
cross-lingual -X- _ O
pre-training. -X- _ O
By -X- _ O
using -X- _ O
pre-trained -X- _ B-MethodName
cross-language -X- _ I-MethodName
models -X- _ I-MethodName
( -X- _ O
XLMs -X- _ B-MethodName
) -X- _ O
to -X- _ O
initialize -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
they -X- _ O
achieved -X- _ O
good -X- _ O
unsupervised -X- _ O
MT -X- _ O
performance -X- _ O
on -X- _ O
multiple -X- _ O
language -X- _ O
pairs. -X- _ O
In -X- _ O
related -X- _ O
work -X- _ O
, -X- _ O
Song -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
proposed -X- _ O
masked -X- _ O
sequence -X- _ O
to -X- _ O
sequence -X- _ O
pre-training -X- _ O
( -X- _ O
MASS -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
directly -X- _ O
pre-trains -X- _ O
a -X- _ O
whole -X- _ O
encoder-decoder -X- _ O
model. -X- _ O
Üstün -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
proposed -X- _ O
a -X- _ O
language-specific -X- _ O
denoising-adapter -X- _ O
architecture -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
multilingual -X- _ O
modeling -X- _ O
capacity -X- _ O
of -X- _ O
the -X- _ O
pre-trained -X- _ O
model -X- _ O
mBART -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
used -X- _ O
these -X- _ O
adapters -X- _ O
for -X- _ O
multilingual -X- _ B-TaskName
unsupervised -X- _ I-TaskName
NMT. -X- _ I-TaskName
Although -X- _ O
these -X- _ O
adapters -X- _ O
are -X- _ O
trained -X- _ O
with -X- _ O
monolingual -X- _ O
data -X- _ O
only -X- _ O
, -X- _ O
the -X- _ O
finetuning -X- _ O
step -X- _ O
relies -X- _ O
on -X- _ O
parallel -X- _ O
data -X- _ O
. -X- _ O

Current -X- _ O
NMT -X- _ B-TaskName
frameworks -X- _ O
rely -X- _ O
heavily -X- _ O
on -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
( -X- _ O
Bahdanau -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
capture -X- _ O
alignments. -X- _ O
However -X- _ O
, -X- _ O
attention-based -X- _ O
context -X- _ O
vectors -X- _ O
can -X- _ O
fail -X- _ O
to -X- _ O
extract -X- _ O
sufficiently -X- _ O
accurate -X- _ O
sentence-level -X- _ O
semantics -X- _ O
and -X- _ O
thus -X- _ O
result -X- _ O
in -X- _ O
incorrect -X- _ O
translations -X- _ O
or -X- _ O
translation -X- _ O
ambiguity -X- _ O
( -X- _ O
Tu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
tackle -X- _ O
this -X- _ O
issue -X- _ O
, -X- _ O
several -X- _ O
variational -X- _ O
frameworks -X- _ O
for -X- _ O
modeling -X- _ O
the -X- _ O
translation -X- _ O
process -X- _ O
have -X- _ O
been -X- _ O
proposed -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Eikema -X- _ O
and -X- _ O
Aziz -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Setiawan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
These -X- _ O
approaches -X- _ O
incorporate -X- _ O
sentence-level -X- _ O
latent -X- _ O
representations -X- _ O
into -X- _ O
NMT. -X- _ B-TaskName
A -X- _ O
latent -X- _ O
representation -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
is -X- _ O
a -X- _ O
fixed-size -X- _ O
continuous -X- _ O
vector -X- _ O
from -X- _ O
an -X- _ O
unknown -X- _ O
distribution -X- _ O
that -X- _ O
captures -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
a -X- _ O
source -X- _ O
sentence. -X- _ O
The -X- _ O
target -X- _ O
sentence -X- _ O
is -X- _ O
then -X- _ O
generated -X- _ O
from -X- _ O
this -X- _ O
latent -X- _ O
representation -X- _ O
using -X- _ O
a -X- _ O
simple -X- _ O
transformation -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
commonly -X- _ O
found -X- _ O
in -X- _ O
transformer -X- _ O
architectures. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
when -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
learns -X- _ O
incorrect -X- _ O
alignments -X- _ O
, -X- _ O
the -X- _ O
latent -X- _ O
representation -X- _ O
plays -X- _ O
a -X- _ O
complementary -X- _ O
role -X- _ O
in -X- _ O
guiding -X- _ O
the -X- _ O
translation -X- _ O
. -X- _ O

Prior -X- _ O
work -X- _ O
in -X- _ O
this -X- _ O
vein -X- _ O
has -X- _ O
only -X- _ O
been -X- _ O
conducted -X- _ O
in -X- _ O
supervised -X- _ O
NMT. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
flow-adapter -X- _ B-MethodName
architecture -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
NMT. -X- _ I-TaskName
Similar -X- _ O
to -X- _ O
variational -X- _ O
methods -X- _ O
, -X- _ O
we -X- _ O
model -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
sentence-level -X- _ O
representations. -X- _ O
However -X- _ O
, -X- _ O
unlike -X- _ O
variational -X- _ O
methods -X- _ O
, -X- _ O
which -X- _ O
model -X- _ O
the -X- _ O
distribution -X- _ O
in -X- _ O
an -X- _ O
implicit -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
normalizing -X- _ O
flows -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages. -X- _ O
Secondly -X- _ O
, -X- _ O
different -X- _ O
from -X- _ O
some -X- _ O
previous -X- _ O
unsupervised -X- _ B-TaskName
NMT -X- _ I-TaskName
models -X- _ O
that -X- _ O
assume -X- _ O
that -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences -X- _ O
share -X- _ O
a -X- _ O
common -X- _ O
semantic -X- _ O
space -X- _ O
, -X- _ O
we -X- _ O
assume -X- _ O
the -X- _ O
representations -X- _ O
are -X- _ O
different -X- _ O
because -X- _ O
of -X- _ O
language-specific -X- _ O
characteristics. -X- _ O
Hence -X- _ O
they -X- _ O
are -X- _ O
modeled -X- _ O
separately -X- _ O
for -X- _ O
each -X- _ O
language. -X- _ O
Subsequently -X- _ O
a -X- _ O
simple -X- _ O
transformation -X- _ O
converts -X- _ O
source -X- _ O
representations -X- _ O
into -X- _ O
target -X- _ O
representations. -X- _ O
This -X- _ O
makes -X- _ O
it -X- _ O
possible -X- _ O
to -X- _ O
better -X- _ O
capture -X- _ O
sentence -X- _ O
semantics -X- _ O
in -X- _ O
a -X- _ O
language-specific -X- _ O
manner. -X- _ O
Lastly -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
minimizing -X- _ O
KL -X- _ O
loss -X- _ O
, -X- _ O
the -X- _ O
flows -X- _ O
are -X- _ O
directly -X- _ O
trained -X- _ O
by -X- _ O
maximum -X- _ O
likelihood -X- _ O
estimation -X- _ O
( -X- _ O
MLE -X- _ O
) -X- _ O
of -X- _ O
sentence-level -X- _ O
latent -X- _ O
representations. -X- _ O
This -X- _ O
gives -X- _ O
the -X- _ O
latent -X- _ O
representations -X- _ O
more -X- _ O
flexibility -X- _ O
. -X- _ O

Our -X- _ O
main -X- _ O
contributions -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
flow-adapter -X- _ B-MethodName
architecture. -X- _ I-MethodName
It -X- _ O
uses -X- _ O
normalizing -X- _ O
flows -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
sentence-level -X- _ O
representations -X- _ O
and -X- _ O
performs -X- _ O
a -X- _ O
latent -X- _ O
representation -X- _ O
transformation -X- _ O
from -X- _ O
source -X- _ O
to -X- _ O
target. -X- _ O
To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
the -X- _ O
first -X- _ O
work -X- _ O
that -X- _ O
uses -X- _ O
latent -X- _ O
variables -X- _ O
and -X- _ O
normalizing -X- _ O
flows -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
NMT -X- _ I-TaskName
. -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
Experiments -X- _ O
show -X- _ O
the -X- _ O
validity -X- _ O
and -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
flow-adapter -X- _ O
architecture. -X- _ O
It -X- _ O
performs -X- _ O
very -X- _ O
well -X- _ O
in -X- _ O
unsupervised -X- _ B-TaskName
NMT -X- _ I-TaskName
on -X- _ O
several -X- _ O
language -X- _ O
pairs -X- _ O
on -X- _ O
the -X- _ O
Multi30K -X- _ B-DatasetName
dataset. -X- _ O
When -X- _ O
additionally -X- _ O
using -X- _ O
pre-trained -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
achieve -X- _ O
results -X- _ O
competitive -X- _ O
with -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
on -X- _ O
WMT -X- _ B-DatasetName
datasets -X- _ O
, -X- _ O
especially -X- _ O
for -X- _ O
en-fr -X- _ B-DatasetName
( -X- _ O
WMT'14 -X- _ B-DatasetName
) -X- _ O
and -X- _ O
en-ro -X- _ B-DatasetName
( -X- _ O
WMT'16 -X- _ B-DatasetName
) -X- _ O
. -X- _ O

Normalizing -X- _ O
Flows -X- _ O
Normalizing -X- _ O
flows -X- _ O
( -X- _ O
NFs -X- _ O
) -X- _ O
are -X- _ O
a -X- _ O
special -X- _ O
type -X- _ O
of -X- _ O
deep -X- _ O
generative -X- _ O
model. -X- _ O
Different -X- _ O
from -X- _ O
generative -X- _ O
adversarial -X- _ O
networks -X- _ O
( -X- _ O
GAN -X- _ O
) -X- _ O
( -X- _ O
Goodfellow -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
and -X- _ O
variational -X- _ O
auto-encoding -X- _ O
( -X- _ O
VAE -X- _ O
) -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Welling -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
NFs -X- _ O
allow -X- _ O
for -X- _ O
not -X- _ O
only -X- _ O
sampling -X- _ O
but -X- _ O
also -X- _ O
exact -X- _ O
density -X- _ O
estimation. -X- _ O
Due -X- _ O
to -X- _ O
such -X- _ O
desirable -X- _ O
properties -X- _ O
, -X- _ O
in -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
they -X- _ O
have -X- _ O
been -X- _ O
successfully -X- _ O
applied -X- _ O
to -X- _ O
fields -X- _ O
such -X- _ O
as -X- _ O
image -X- _ O
( -X- _ O
Ho -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Kingma -X- _ O
and -X- _ O
Dhariwal -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
audio -X- _ O
( -X- _ O
Esling -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
van -X- _ O
den -X- _ O
Oord -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
video -X- _ O
generation -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
to -X- _ O
significant -X- _ O
achievements -X- _ O
in -X- _ O
modeling -X- _ O
continuous -X- _ O
data -X- _ O
, -X- _ O
NFs -X- _ O
have -X- _ O
also -X- _ O
been -X- _ O
used -X- _ O
for -X- _ O
modeling -X- _ O
discrete -X- _ O
data -X- _ O
, -X- _ O
either -X- _ O
by -X- _ O
directly -X- _ O
modeling -X- _ O
the -X- _ O
data -X- _ O
in -X- _ O
discrete -X- _ O
space -X- _ O
( -X- _ O
Tran -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Hesselink -X- _ O
and -X- _ O
Aziz -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
or -X- _ O
by -X- _ O
transforming -X- _ O
the -X- _ O
discrete -X- _ O
data -X- _ O
into -X- _ O
continuous -X- _ O
space -X- _ O
( -X- _ O
Ziegler -X- _ O
and -X- _ O
Rush -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Tang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

NFs -X- _ O
transform -X- _ O
between -X- _ O
two -X- _ O
distributions -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
following -X- _ O
change-of-variables -X- _ O
formula -X- _ O
( -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
( -X- _ O
Dinh -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
( -X- _ O
Dinh -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
, -X- _ O
2017 -X- _ O

where -X- _ O
z -X- _ O
∼ -X- _ O
p -X- _ O
z -X- _ O
( -X- _ O
z -X- _ O
) -X- _ O
and -X- _ O
x -X- _ O
∼ -X- _ O
p -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
denote -X- _ O
two -X- _ O
vectors -X- _ O
from -X- _ O
a -X- _ O
simple -X- _ O
latent -X- _ O
distribution -X- _ O
p -X- _ O
z -X- _ O
( -X- _ O
z -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
complex -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
observed -X- _ O
data -X- _ O
p -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
, -X- _ O
f -X- _ O
θ -X- _ O
is -X- _ O
an -X- _ O
invertible -X- _ O
and -X- _ O
differentiable -X- _ O
function -X- _ O
( -X- _ O
neural -X- _ O
network -X- _ O
with -X- _ O
parameters -X- _ O
θ -X- _ O
) -X- _ O
, -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
) -X- _ O
= -X- _ O
x -X- _ O
and -X- _ O
det -X- _ O
∂f -X- _ O
θ -X- _ O
( -X- _ O
z -X- _ O
) -X- _ O
∂z -X- _ O
denotes -X- _ O
the -X- _ O
determinant -X- _ O
of -X- _ O
the -X- _ O
Jacobian -X- _ O
matrix -X- _ O
of -X- _ O
f -X- _ O
θ -X- _ O
. -X- _ O
The -X- _ O
idea -X- _ O
of -X- _ O
NFs -X- _ O
is -X- _ O
to -X- _ O
learn -X- _ O
an -X- _ O
f -X- _ O
θ -X- _ O
such -X- _ O
that -X- _ O
f -X- _ O
θ -X- _ O
and -X- _ O
f -X- _ O
−1 -X- _ O
θ -X- _ O
transform -X- _ O
between -X- _ O
the -X- _ O
latent -X- _ O
space -X- _ O
p -X- _ O
z -X- _ O
( -X- _ O
z -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
observed -X- _ O
space -X- _ O
p -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
. -X- _ O

Constructing -X- _ O
a -X- _ O
single -X- _ O
arbitrarily -X- _ O
complex -X- _ O
invertible -X- _ O
and -X- _ O
differentiable -X- _ O
function -X- _ O
is -X- _ O
usually -X- _ O
cumbersome. -X- _ O
Therefore -X- _ O
, -X- _ O
a -X- _ O
generally -X- _ O
adopted -X- _ O
approach -X- _ O
is -X- _ O
to -X- _ O
stack -X- _ O
multiple -X- _ O
transformations -X- _ O
f -X- _ O
i -X- _ O
together -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O

, -X- _ O
whose -X- _ O
Jacobian -X- _ O
matrix -X- _ O
is -X- _ O
efficient -X- _ O
to -X- _ O
compute. -X- _ O
Here -X- _ O
K -X- _ B-HyperparameterName
denotes -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
sequential -X- _ O
flows -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
K -X- _ B-HyperparameterName
= -X- _ O
3 -X- _ B-HyperparameterValue
in -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O

Normalizing -X- _ O
flows -X- _ O
are -X- _ O
usually -X- _ O
optimized -X- _ O
by -X- _ O
MLE -X- _ O
of -X- _ O
the -X- _ O
parameters -X- _ O
θ -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
log -X- _ O
p -X- _ O
( -X- _ O
D|θ -X- _ O
) -X- _ O
= -X- _ O
N -X- _ O
n=1 -X- _ O
log -X- _ O
p -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
( -X- _ O
n -X- _ O
) -X- _ O
|θ -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
N -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
data -X- _ O
size. -X- _ O
By -X- _ O
applying -X- _ O
a -X- _ O
variant -X- _ O
of -X- _ O
the -X- _ O
change-of-variable -X- _ O
formula -X- _ O
in -X- _ O
Equation -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
log -X- _ O
p -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O
log -X- _ O
p -X- _ O
z -X- _ O
( -X- _ O
f -X- _ O
−1 -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
) -X- _ O
+ -X- _ O
log -X- _ O
det -X- _ O

Latent-variable -X- _ O
( -X- _ O
variational -X- _ O
) -X- _ O
NMT -X- _ B-TaskName
Compared -X- _ O
with -X- _ O
standard -X- _ O
encoder-decoder -X- _ O
based -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
, -X- _ O
latent-variable -X- _ O
( -X- _ O
variational -X- _ O
) -X- _ O
approaches -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Eikema -X- _ O
and -X- _ O
Aziz -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Ma -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Calixto -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Setiawan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
additionally -X- _ O
leverage -X- _ O
latent -X- _ O
random -X- _ O
variables -X- _ O
. -X- _ O

Let -X- _ O
x -X- _ O
be -X- _ O
a -X- _ O
sentence -X- _ O
from -X- _ O
the -X- _ O
source -X- _ O
language -X- _ O
and -X- _ O
y -X- _ O
be -X- _ O
its -X- _ O
translation -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
Then -X- _ O
, -X- _ O
the -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
framework -X- _ O
introduces -X- _ O
a -X- _ O
continuous -X- _ O
random -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
for -X- _ O
the -X- _ O
translation -X- _ O
modeling -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
p -X- _ O
( -X- _ O
y|z -X- _ O
, -X- _ O
x -X- _ O
) -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
z -X- _ O
, -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
p -X- _ O
( -X- _ O
y|x -X- _ O
) -X- _ O
can -X- _ O
then -X- _ O
be -X- _ O
reformulated -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
z -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
global -X- _ O
semantic -X- _ O
signal -X- _ O
that -X- _ O
is -X- _ O
helpful -X- _ O
to -X- _ O
counteract -X- _ O
incorrect -X- _ O
alignments -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
learned -X- _ O
and -X- _ O
uses -X- _ O
through -X- _ O
attention. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
integration -X- _ O
of -X- _ O
z -X- _ O
poses -X- _ O
challenges -X- _ O
for -X- _ O
inference. -X- _ O
To -X- _ O
address -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
adopts -X- _ O
techniques -X- _ O
from -X- _ O
VAE -X- _ B-TaskName
( -X- _ O
Kingma -X- _ O
and -X- _ O
Welling -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Rezende -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
namely -X- _ O
, -X- _ O
neural -X- _ O
approximation -X- _ O
and -X- _ O
the -X- _ O
reparameterization -X- _ O
trick -X- _ O
. -X- _ O

Neural -X- _ O
approximation -X- _ O
leverages -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
to -X- _ O
approximate -X- _ O
the -X- _ O
posterior -X- _ O
distribution -X- _ O
p -X- _ O
( -X- _ O
z|x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
with -X- _ O
q -X- _ O
ϕ -X- _ O
( -X- _ O
z|x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
ϕ -X- _ O
denotes -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
network. -X- _ O
In -X- _ O
most -X- _ O
works -X- _ O
, -X- _ O
q -X- _ O
ϕ -X- _ O
( -X- _ O
z|x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
is -X- _ O
designed -X- _ O
as -X- _ O
a -X- _ O
diagonal -X- _ O
Gaussian -X- _ O
N -X- _ O
( -X- _ O
µ -X- _ O
, -X- _ O
diag -X- _ O
( -X- _ O
σ -X- _ O
2 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
mean -X- _ O
µ -X- _ O
and -X- _ O
the -X- _ O
variance -X- _ O
σ -X- _ O
2 -X- _ O
are -X- _ O
parameterized -X- _ O
with -X- _ O
neural -X- _ O
networks -X- _ O
. -X- _ O

Reparameterization -X- _ O
means -X- _ O
that -X- _ O
the -X- _ O
latent -X- _ O
random -X- _ O
variable -X- _ O
z -X- _ O
is -X- _ O
parameterized -X- _ O
as -X- _ O
a -X- _ O
function -X- _ O
of -X- _ O
the -X- _ O
mean -X- _ O
µ -X- _ O
and -X- _ O
the -X- _ O
variance -X- _ O
σ -X- _ O
2 -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
the -X- _ O
gradient -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
parameters -X- _ O
µ -X- _ O
and -X- _ O
σ -X- _ O
2 -X- _ O
can -X- _ O
be -X- _ O
computed. -X- _ O
The -X- _ O
reparameterization -X- _ O
of -X- _ O
z -X- _ O
is -X- _ O
often -X- _ O
carried -X- _ O
out -X- _ O
in -X- _ O
a -X- _ O
location-scale -X- _ O
manner -X- _ O
: -X- _ O
z -X- _ O
= -X- _ O
µ -X- _ O
+ -X- _ O
σ -X- _ O
⊙ -X- _ O
ϵ -X- _ O
where -X- _ O
ϵ -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
) -X- _ O
With -X- _ O
these -X- _ O
two -X- _ O
techniques -X- _ O
, -X- _ O
the -X- _ O
learning -X- _ O
objective -X- _ O
of -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
is -X- _ O
the -X- _ O
evidence -X- _ O
lower-bound -X- _ O
or -X- _ O
ELBO -X- _ O
of -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
p -X- _ O
( -X- _ O
y|x -X- _ O
) -X- _ O
: -X- _ O

where -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z|x -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
prior -X- _ O
distribution -X- _ O
modeled -X- _ O
by -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
and -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
y|z -X- _ O
, -X- _ O
x -X- _ O
) -X- _ O
is -X- _ O
modeled -X- _ O
by -X- _ O
the -X- _ O
decoder -X- _ O
given -X- _ O
the -X- _ O
input -X- _ O
source -X- _ O
sentence -X- _ O
x -X- _ O
and -X- _ O
the -X- _ O
latent -X- _ O
variable -X- _ O
z. -X- _ O
The -X- _ O
KL -X- _ O
term -X- _ O
minimizes -X- _ O
the -X- _ O
discrepancy -X- _ O
between -X- _ O
the -X- _ O
prior -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z|x -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
posterior -X- _ O
q -X- _ O
θ -X- _ O
( -X- _ O
z|x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
inference -X- _ O
step -X- _ O
, -X- _ O
z -X- _ O
can -X- _ O
therefore -X- _ O
be -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
prior -X- _ O
, -X- _ O
which -X- _ O
only -X- _ O
requires -X- _ O
x -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
posterior -X- _ O
that -X- _ O
requires -X- _ O
both -X- _ O
x -X- _ O
and -X- _ O
y. -X- _ O
Although -X- _ O
this -X- _ O
variational -X- _ O
framework -X- _ O
leverages -X- _ O
latent -X- _ O
variables -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
helpful -X- _ O
for -X- _ O
translation -X- _ O
, -X- _ O
it -X- _ O
still -X- _ O
has -X- _ O
some -X- _ O
flaws -X- _ O
: -X- _ O
1 -X- _ O
) -X- _ O
training -X- _ O
a -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
framework -X- _ O
requires -X- _ O
parallel -X- _ O
corpora -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
posterior -X- _ O
q -X- _ O
ϕ -X- _ O
( -X- _ O
z|x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
and -X- _ O
such -X- _ O
parallel -X- _ O
corpora -X- _ O
are -X- _ O
not -X- _ O
available -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
; -X- _ O
2 -X- _ O
) -X- _ O
the -X- _ O
distribution -X- _ O
family -X- _ O
of -X- _ O
the -X- _ O
latent -X- _ O
variables -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z|x -X- _ O
) -X- _ O
, -X- _ O
is -X- _ O
pre-defined -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
a -X- _ O
Gaussian -X- _ O
, -X- _ O
which -X- _ O
might -X- _ O
restricts -X- _ O
the -X- _ O
advantage -X- _ O
of -X- _ O
using -X- _ O
a -X- _ O
complex -X- _ O
posterior -X- _ O
; -X- _ O
3 -X- _ O
) -X- _ O
as -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
leverages -X- _ O
z -X- _ O
sampled -X- _ O
from -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
z|x -X- _ O
) -X- _ O
for -X- _ O
inference -X- _ O
, -X- _ O
an -X- _ O
underlying -X- _ O
assumption -X- _ O
is -X- _ O
that -X- _ O
z -X- _ O
should -X- _ O
be -X- _ O
the -X- _ O
same -X- _ O
whether -X- _ O
only -X- _ O
x -X- _ O
is -X- _ O
considered -X- _ O
or -X- _ O
both -X- _ O
x -X- _ O
and -X- _ O
y -X- _ O
are -X- _ O
considered. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
this -X- _ O
framework -X- _ O
assumes -X- _ O
z -X- _ O
is -X- _ O
language-agnostic -X- _ O
, -X- _ O
which -X- _ O
might -X- _ O
not -X- _ O
be -X- _ O
true -X- _ O
since -X- _ O
language-specific -X- _ O
characteristics -X- _ O
can -X- _ O
influence -X- _ O
the -X- _ O
generation -X- _ O
of -X- _ O
z -X- _ O
. -X- _ O

Flow-Adapter -X- _ B-MethodName
Based -X- _ O
Framework -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
reap -X- _ O
the -X- _ O
benefits -X- _ O
of -X- _ O
introducing -X- _ O
latent -X- _ O
variables -X- _ O
into -X- _ O
unsupervised -X- _ O
MT -X- _ B-TaskName
while -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
avoiding -X- _ O
the -X- _ O
flaws -X- _ O
of -X- _ O
variational -X- _ O
NMT -X- _ B-TaskName
we -X- _ O
just -X- _ O
discussed. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
flow-adapter -X- _ O
based -X- _ O
framework -X- _ O
that -X- _ O
uses -X- _ O
two -X- _ O
NFs -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
latent -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
take -X- _ O
account -X- _ O
of -X- _ O
multilinguality -X- _ O
in -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
and -X- _ O
make -X- _ O
use -X- _ O
of -X- _ O
language-specific -X- _ O
sentence-level -X- _ O
representations. -X- _ O
During -X- _ O
the -X- _ O
translation -X- _ O
process -X- _ O
, -X- _ O
a -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
is -X- _ O
performed -X- _ O
to -X- _ O
transform -X- _ O
the -X- _ O
source-language -X- _ O
representation -X- _ O
into -X- _ O
the -X- _ O
targetlanguage -X- _ O
representation -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
decoder -X- _ O
can -X- _ O
leverage -X- _ O
them -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
better -X- _ O
target-language -X- _ O
sentence. -X- _ O
We -X- _ O
will -X- _ O
first -X- _ O
introduce -X- _ O
the -X- _ O
sentence-level -X- _ O
representation -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
in -X- _ O
Section -X- _ O
3.1 -X- _ O
, -X- _ O
followed -X- _ O
by -X- _ O
the -X- _ O
description -X- _ O
of -X- _ O
the -X- _ O
flow-adapter -X- _ O
based -X- _ O
framework -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
in -X- _ O
Section -X- _ O
3.2 -X- _ O
. -X- _ O

Modeling -X- _ O
Representation -X- _ O
by -X- _ O
NFs -X- _ O
& -X- _ O
Latent -X- _ O
Code -X- _ O
Transformation -X- _ O
As -X- _ O
previously -X- _ O
mentioned -X- _ O
, -X- _ O
variational -X- _ O
methods -X- _ O
such -X- _ O
as -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Setiawan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
assume -X- _ O
that -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
x -X- _ O
and -X- _ O
target -X- _ O
sentence -X- _ O
y -X- _ O
are -X- _ O
the -X- _ O
same -X- _ O
and -X- _ O
thus -X- _ O
the -X- _ O
generated -X- _ O
latent -X- _ O
variable -X- _ O
z -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
regardless -X- _ O
of -X- _ O
whether -X- _ O
we -X- _ O
only -X- _ O
consider -X- _ O
x -X- _ O
or -X- _ O
consider -X- _ O
both -X- _ O
x -X- _ O
and -X- _ O
y. -X- _ O
Unsupervised -X- _ B-TaskName
NMT -X- _ I-TaskName
methods -X- _ O
such -X- _ O
as -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
; -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
similarly -X- _ O
assume -X- _ O
that -X- _ O
a -X- _ O
shared -X- _ O
encoder -X- _ O
maps -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences -X- _ O
into -X- _ O
a -X- _ O
shared -X- _ O
latent -X- _ O
space -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
we -X- _ O
diverge -X- _ O
from -X- _ O
this -X- _ O
assumption -X- _ O
and -X- _ O
follow -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
in -X- _ O
adopting -X- _ O
the -X- _ O
desideratum -X- _ O
that -X- _ O
the -X- _ O
unique -X- _ O
and -X- _ O
internal -X- _ O
characteristics -X- _ O
of -X- _ O
each -X- _ O
language -X- _ O
be -X- _ O
respected. -X- _ O
One -X- _ O
could -X- _ O
think -X- _ O
that -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
sentences -X- _ O
should -X- _ O
theoretically -X- _ O
be -X- _ O
the -X- _ O
same -X- _ O
; -X- _ O
but -X- _ O
in -X- _ O
reality -X- _ O
, -X- _ O
because -X- _ O
of -X- _ O
language-specific -X- _ O
characteristics -X- _ O
, -X- _ O
the -X- _ O
latent -X- _ O
representations -X- _ O
z -X- _ O
obtained -X- _ O
by -X- _ O
an -X- _ O
encoder -X- _ O
can -X- _ O
be -X- _ O
different -X- _ O
for -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences. -X- _ O
Differ-ences -X- _ O
in -X- _ O
vocabulary -X- _ O
, -X- _ O
pragmatics -X- _ O
and -X- _ O
other -X- _ O
linguistic -X- _ O
properties -X- _ O
all -X- _ O
influence -X- _ O
the -X- _ O
generation -X- _ O
of -X- _ O
the -X- _ O
latent -X- _ O
representations. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
the -X- _ O
latent -X- _ O
representations -X- _ O
from -X- _ O
a -X- _ O
different -X- _ O
perspective -X- _ O
as -X- _ O
follows. -X- _ O
We -X- _ O
can -X- _ O
view -X- _ O
z -X- _ O
x -X- _ O
and -X- _ O
z -X- _ O
y -X- _ O
as -X- _ O
expressions -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations -X- _ O
in -X- _ O
two -X- _ O
distinct -X- _ O
languages -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
semantics -X- _ O
ϵ -X- _ O
where -X- _ O
ϵ -X- _ O
is -X- _ O
truly -X- _ O
language-agnostic. -X- _ O
z -X- _ O
x -X- _ O
and -X- _ O
z -X- _ O
y -X- _ O
are -X- _ O
obtained -X- _ O
by -X- _ O
applying -X- _ O
parameter-free -X- _ O
techniques -X- _ O
such -X- _ O
as -X- _ O
pooling -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
fed -X- _ O
with -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
3.2 -X- _ O
for -X- _ O
details -X- _ O
) -X- _ O
. -X- _ O

Modeling -X- _ O
by -X- _ O
NFs. -X- _ O
For -X- _ O
our -X- _ O
unsupervised -X- _ O
scenario -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations -X- _ O
of -X- _ O
both -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentences -X- _ O
-i.e. -X- _ O
, -X- _ O
p -X- _ O
zx -X- _ O
( -X- _ O
z -X- _ O
x -X- _ O
) -X- _ O
and -X- _ O
p -X- _ O
zy -X- _ O
( -X- _ O
z -X- _ O
y -X- _ O
) -X- _ O
-using -X- _ O
NFs -X- _ O
with -X- _ O
K -X- _ B-HyperparameterName
sequential -X- _ O
flows -X- _ O
: -X- _ O

Our -X- _ O
transformation -X- _ O
to -X- _ O
the -X- _ O
sentence-level -X- _ O
representations -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
. -X- _ O
They -X- _ O
argued -X- _ O
that -X- _ O
BERT -X- _ O
induces -X- _ O
a -X- _ O
non-smooth -X- _ O
anisotropic -X- _ O
semantic -X- _ O
space -X- _ O
of -X- _ O
sentences -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
harm -X- _ O
its -X- _ O
accurate -X- _ O
representation -X- _ O
of -X- _ O
semantic -X- _ O
similarity. -X- _ O
Therefore -X- _ O
, -X- _ O
they -X- _ O
also -X- _ O
used -X- _ O
NFs -X- _ O
to -X- _ O
transform -X- _ O
the -X- _ O
anisotropic -X- _ O
BERT -X- _ O
sentence-level -X- _ O
distribution -X- _ O
to -X- _ O
a -X- _ O
standard -X- _ O
Gaussian -X- _ O
distribution -X- _ O
that -X- _ O
is -X- _ O
smooth -X- _ O
and -X- _ O
isotropic -X- _ O
and -X- _ O
reported -X- _ O
better -X- _ O
performance -X- _ O
on -X- _ O
some -X- _ O
sentence-level -X- _ O
similarity -X- _ O
tasks. -X- _ O
By -X- _ O
using -X- _ O
this -X- _ O
type -X- _ O
of -X- _ O
sentence-level -X- _ O
representation -X- _ O
, -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
sentences -X- _ O
from -X- _ O
different -X- _ O
languages -X- _ O
can -X- _ O
therefore -X- _ O
be -X- _ O
aligned -X- _ O
in -X- _ O
a -X- _ O
simple -X- _ O
common -X- _ O
space -X- _ O
in -X- _ O
an -X- _ O
unsupervised -X- _ O
way -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
show -X- _ O
is -X- _ O
effective -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
. -X- _ O

For -X- _ O
simplicity -X- _ O
, -X- _ O
we -X- _ O
denote -X- _ O
the -X- _ O
NFs -X- _ O
for -X- _ O
transforming -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
sentencelevel -X- _ O
representations -X- _ O
to -X- _ O
the -X- _ O
base -X- _ O
distribution -X- _ O
as -X- _ O
mappings -X- _ O
G -X- _ O
( -X- _ O
zx→ϵ -X- _ O
) -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
zy→ϵ -X- _ O
) -X- _ O
. -X- _ O
Because -X- _ O
of -X- _ O
the -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
ϵ→zy -X- _ O
) -X- _ O
= -X- _ O
G -X- _ O
−1 -X- _ O
( -X- _ O
zy→ϵ -X- _ O
) -X- _ O
. -X- _ O
Latent -X- _ O
Code -X- _ O
Transformation. -X- _ O
Inspired -X- _ O
by -X- _ O
AlignFlow -X- _ O
( -X- _ O
Grover -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
the -X- _ O
cross-domain -X- _ O
transformation -X- _ O
between -X- _ O
z -X- _ O
x -X- _ O
and -X- _ O
z -X- _ O
y -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
formulate -X- _ O
a -X- _ O
language-specific -X- _ O
latent -X- _ O
code -X- _ O
for -X- _ O
the -X- _ O
decoder. -X- _ O
We -X- _ O
formalize -X- _ O
the -X- _ O
cross-language -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
from -X- _ O
the -X- _ O
source -X- _ O
to -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

The -X- _ O
target-to-source -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
is -X- _ O
then -X- _ O
the -X- _ O
composition -X- _ O
of -X- _ O
G -X- _ O
( -X- _ O
ϵ→zx -X- _ O
) -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
zy→ϵ -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
G -X- _ O
( -X- _ O
ϵ→zy -X- _ O
) -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
ϵ→zx -X- _ O
) -X- _ O
are -X- _ O
the -X- _ O
inverse -X- _ O
mappings -X- _ O
of -X- _ O
G -X- _ O
( -X- _ O
zy→ϵ -X- _ O
) -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
zy→ϵ -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
easily -X- _ O
obtain -X- _ O
them -X- _ O
with -X- _ O
normalizing -X- _ O
flows -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
realNVP -X- _ O
( -X- _ O
Dinh -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
and -X- _ O
Glow -X- _ O
( -X- _ O
Kingma -X- _ O
and -X- _ O
Dhariwal -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
note -X- _ O
that -X- _ O
G -X- _ O
( -X- _ O
zx→zy -X- _ O
) -X- _ O
and -X- _ O
G -X- _ O
( -X- _ O
zy→zx -X- _ O
) -X- _ O
are -X- _ O
both -X- _ O
invertible -X- _ O
since -X- _ O
they -X- _ O
are -X- _ O
compositions -X- _ O
of -X- _ O
two -X- _ O
invertible -X- _ O
mappings. -X- _ O
Moreover -X- _ O
, -X- _ O
G -X- _ O
( -X- _ O
zx→zy -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
inverse -X- _ O
of -X- _ O
G -X- _ O
( -X- _ O
zy→zx -X- _ O
) -X- _ O
and -X- _ O
vice -X- _ O
versa -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.1 -X- _ O
for -X- _ O
details -X- _ O
) -X- _ O
. -X- _ O

Flow-Adapter -X- _ B-MethodName
Based -X- _ O
Unsupervised -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
The -X- _ O
general -X- _ O
architecture -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1. -X- _ O
The -X- _ O
transformer -X- _ O
architecture -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
decoder. -X- _ O
We -X- _ O
use -X- _ O
source -X- _ O
encoder -X- _ O
/ -X- _ O
decoder -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
encoder -X- _ O
/ -X- _ O
decoder -X- _ O
for -X- _ O
encoding -X- _ O
/ -X- _ O
generating -X- _ O
the -X- _ O
source-language -X- _ O
sentence. -X- _ O
Similarly -X- _ O
, -X- _ O
target -X- _ O
encoder -X- _ O
/ -X- _ O
decoder -X- _ O
refer -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
/ -X- _ O
decoder -X- _ O
encoding -X- _ O
/ -X- _ O
generating -X- _ O
the -X- _ O
targetlanguage -X- _ O
sentence. -X- _ O
The -X- _ O
decoders -X- _ O
work -X- _ O
in -X- _ O
an -X- _ O
autoregressive -X- _ O
way. -X- _ O
Source -X- _ O
flow -X- _ O
and -X- _ O
target -X- _ O
flow -X- _ O
are -X- _ O
NFs -X- _ O
for -X- _ O
modeling -X- _ O
the -X- _ O
sentence-level -X- _ O
latent -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
language -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
as -X- _ O
introduced -X- _ O
in -X- _ O
Section -X- _ O
3.1 -X- _ O
. -X- _ O

Encoding. -X- _ O
The -X- _ O
source -X- _ O
encoder -X- _ O
and -X- _ O
the -X- _ O
target -X- _ O
encoder -X- _ O
work -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
way -X- _ O
; -X- _ O
for -X- _ O
brevity -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
describe -X- _ O
the -X- _ O
procedure -X- _ O
of -X- _ O
encoding -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
and -X- _ O
how -X- _ O
z -X- _ O
x -X- _ O
is -X- _ O
generated. -X- _ O
The -X- _ O
source -X- _ O
encoder -X- _ O
takes -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
x -X- _ O
= -X- _ O
{ -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
x -X- _ O
S -X- _ O
} -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
generates -X- _ O
the -X- _ O
hidden -X- _ O
representations -X- _ O
{ -X- _ O
h -X- _ O
0 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
S -X- _ O
} -X- _ O
. -X- _ O
These -X- _ O
hidden -X- _ O
representations -X- _ O
will -X- _ O
be -X- _ O
used -X- _ O
as -X- _ O
encoder-decoder -X- _ O
attentional -X- _ O
inputs. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
hidden -X- _ O
representations -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
sentence-level -X- _ O
representation -X- _ O
for -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
by -X- _ O
applying -X- _ O
max-pooling -X- _ O
and -X- _ O
mean-pooling -X- _ O
to -X- _ O
the -X- _ O
token-level -X- _ O
representations. -X- _ O
After -X- _ O
that -X- _ O
, -X- _ O
we -X- _ O
sum -X- _ O
up -X- _ O
the -X- _ O
results -X- _ O
with -X- _ O
the -X- _ O
CLS -X- _ O
representation -X- _ O
h -X- _ O
0 -X- _ O
, -X- _ O
which -X- _ O
usually -X- _ O
encodes -X- _ O
some -X- _ O
global -X- _ O
information. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
projection -X- _ O
matrix -X- _ O
W -X- _ O
to -X- _ O
project -X- _ O
the -X- _ O
resulting -X- _ O
vector -X- _ O
to -X- _ O
a -X- _ O
latent -X- _ O
space. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
z -X- _ O
x -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
sentence-level -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.2 -X- _ O
for -X- _ O
equation -X- _ O
and -X- _ O
illustration -X- _ O
) -X- _ O
. -X- _ O

Cross-lingual -X- _ O
Translation. -X- _ O
We -X- _ O
hypothesize -X- _ O
that -X- _ O
the -X- _ O
decoder -X- _ O
can -X- _ O
better -X- _ O
leverage -X- _ O
language-specific -X- _ O
latent -X- _ O
representations -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
z -X- _ O
x -X- _ O
for -X- _ O
the -X- _ O
source -X- _ O
decoder -X- _ O
and -X- _ O
z -X- _ O
y -X- _ O
for -X- _ O
the -X- _ O
target -X- _ O
decoder -X- _ O
) -X- _ O
than -X- _ O
indiscriminately -X- _ O
using -X- _ O
the -X- _ O
same -X- _ O
representational -X- _ O
space -X- _ O
for -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
z -X- _ O
x -X- _ O
for -X- _ O
the -X- _ O
target -X- _ O
decoder. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
perform -X- _ O
a -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
for -X- _ O
cross-language -X- _ O
translation -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1. -X- _ O
If -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
performing -X- _ O
the -X- _ O
translation -X- _ O
in -X- _ O
the -X- _ O
source-to-target -X- _ O
direction -X- _ O
, -X- _ O
the -X- _ O
source -X- _ O
flow -X- _ O
first -X- _ O
transforms -X- _ O
the -X- _ O
source -X- _ O
latent -X- _ O
representation -X- _ O
z -X- _ O
x -X- _ O
into -X- _ O
ϵ -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
vector -X- _ O
in -X- _ O
the -X- _ O
semantic -X- _ O
base -X- _ O
space. -X- _ O
Then -X- _ O
the -X- _ O
target -X- _ O
flow -X- _ O
transforms -X- _ O
ϵ -X- _ O
back -X- _ O
into -X- _ O
z -X- _ O
y -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
latent -X- _ O
representation -X- _ O
space. -X- _ O
Then -X- _ O
z -X- _ O
y -X- _ O
is -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
decoder -X- _ O
for -X- _ O
generating -X- _ O
the -X- _ O
target -X- _ O
sentence -X- _ O
. -X- _ O

Denoising -X- _ O
Auto-Encoding -X- _ O
( -X- _ O
DAE -X- _ O
) -X- _ O
and -X- _ O
Back -X- _ O
Translation -X- _ O
( -X- _ O
BT -X- _ O
) -X- _ O
Processes. -X- _ O
The -X- _ O
DAE -X- _ O
reconstructs -X- _ O
a -X- _ O
sentence -X- _ O
from -X- _ O
its -X- _ O
noised -X- _ O
version. -X- _ O
For -X- _ O
inducing -X- _ O
noise -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
strategy -X- _ O
which -X- _ O
is -X- _ O
used -X- _ O
by -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
( -X- _ O
For -X- _ O
more -X- _ O
details -X- _ O
, -X- _ O
please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
A.3 -X- _ O
) -X- _ O
. -X- _ O
Since -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
DAEs -X- _ O
separately -X- _ O
for -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
, -X- _ O
hence -X- _ O
we -X- _ O
do -X- _ O
n't -X- _ O
need -X- _ O
a -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
there. -X- _ O
For -X- _ O
BT -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
such -X- _ O
a -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
is -X- _ O
performed -X- _ O
twice -X- _ O
; -X- _ O
taking -X- _ O
BT -X- _ O
for -X- _ O
the -X- _ O
source -X- _ O
language -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
: -X- _ O
first -X- _ O
in -X- _ O
the -X- _ O
source-to-target -X- _ O
direction -X- _ O
, -X- _ O
then -X- _ O
in -X- _ O
the -X- _ O
target-to-source -X- _ O
direction -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
. -X- _ O

is -X- _ O
the -X- _ O
sigmoid -X- _ O
function -X- _ O
, -X- _ O
⊙ -X- _ O
denotes -X- _ O
Hadamard -X- _ O
product -X- _ O
between -X- _ O
two -X- _ O
vectors -X- _ O
, -X- _ O
and -X- _ O
o -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
logit -X- _ O
vector -X- _ O
used -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
prediction -X- _ O
at -X- _ O
the -X- _ O
i -X- _ O
th -X- _ O
position. -X- _ O
The -X- _ O
values -X- _ O
in -X- _ O
g -X- _ O
i -X- _ O
control -X- _ O
the -X- _ O
contribution -X- _ O
of -X- _ O
z -X- _ O
to -X- _ O
o -X- _ O
i -X- _ O
. -X- _ O
In -X- _ O
case -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
latent -X- _ O
representation -X- _ O
does -X- _ O
not -X- _ O
match -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
output -X- _ O
, -X- _ O
a -X- _ O
linear -X- _ O
projection -X- _ O
maps -X- _ O
z -X- _ O
to -X- _ O
the -X- _ O
desired -X- _ O
dimension -X- _ O
. -X- _ O

Training. -X- _ O
Our -X- _ O
flow-adapter -X- _ B-MethodName
framework -X- _ O
has -X- _ O
three -X- _ O
learning -X- _ O
objectives -X- _ O
: -X- _ O
DAE -X- _ O
, -X- _ O
BT -X- _ O
and -X- _ O
MLE -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations. -X- _ O
The -X- _ O
description -X- _ O
of -X- _ O
DAE -X- _ O
and -X- _ O
BT -X- _ O
is -X- _ O
omitted -X- _ O
here -X- _ O
as -X- _ O
they -X- _ O
are -X- _ O
well -X- _ O
known -X- _ O
from -X- _ O
related -X- _ O
work -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
; -X- _ O
Artetxe -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
single -X- _ O
training -X- _ O
iteration -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
DAE -X- _ O
step -X- _ O
followed -X- _ O
by -X- _ O
a -X- _ O
BT -X- _ O
step -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2. -X- _ O
MLE -X- _ O
computation -X- _ O
is -X- _ O
integrated -X- _ O
into -X- _ O
the -X- _ O
DAE -X- _ O
step -X- _ O
to -X- _ O
calculate -X- _ O
the -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations. -X- _ O
Our -X- _ O
MLE -X- _ O
learning -X- _ O
objective -X- _ O
for -X- _ O
the -X- _ O
source -X- _ O
monolingual -X- _ O
dataset -X- _ O
can -X- _ O
be -X- _ O
formulated -X- _ O
as -X- _ O
follows -X- _ O
( -X- _ O
similar -X- _ O
for -X- _ O
the -X- _ O
target -X- _ O
dataset -X- _ O
, -X- _ O
omitted -X- _ O
) -X- _ O
: -X- _ O

Experiments -X- _ O
4.1 -X- _ O
Datasets -X- _ O
Multi30K -X- _ B-DatasetName
task1 -X- _ O
dataset -X- _ O
( -X- _ O
Elliott -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
( -X- _ O
Elliott -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
1 -X- _ O
This -X- _ O
is -X- _ O
a -X- _ O
multi-modal -X- _ O
dataset -X- _ O
that -X- _ O
has -X- _ O
30,000 -X- _ O
images -X- _ O
annotated -X- _ O
with -X- _ O
captions -X- _ O
in -X- _ O
English -X- _ O
, -X- _ O
German -X- _ O
and -X- _ O
French. -X- _ O
Similar -X- _ O
to -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
use -X- _ O
the -X- _ O
caption -X- _ O
of -X- _ O
each -X- _ O
image. -X- _ O
The -X- _ O
officially -X- _ O
provided -X- _ O
train -X- _ O
, -X- _ O
validation -X- _ O
and -X- _ O
test -X- _ O
sets -X- _ O
are -X- _ O
used. -X- _ O
We -X- _ O
use -X- _ O
this -X- _ O
dataset -X- _ O
as -X- _ O
a -X- _ O
small-scale -X- _ O
test -X- _ O
for -X- _ O
validating -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
methods. -X- _ O
WMT -X- _ B-DatasetName
datasets. -X- _ O
2 -X- _ O
Our -X- _ O
experiments -X- _ O
are -X- _ O
run -X- _ O
with -X- _ O
the -X- _ O
settings -X- _ O
that -X- _ O
were -X- _ O
used -X- _ O
for -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
XLM -X- _ B-TaskName
uses -X- _ O
the -X- _ O
monolingual -X- _ O
data -X- _ O
from -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
News -X- _ I-DatasetName
Crawl -X- _ I-DatasetName
datasets -X- _ O
3 -X- _ O
. -X- _ O
We -X- _ O
report -X- _ O
results -X- _ O
on -X- _ O
newstest2014 -X- _ B-DatasetName
en-fr -X- _ I-DatasetName
, -X- _ O
newstest2016 -X- _ B-DatasetName
en-de -X- _ I-DatasetName
and -X- _ O
newstest2016 -X- _ B-DatasetName
en-ro -X- _ I-DatasetName
. -X- _ O

Setups -X- _ O
Preprocessing. -X- _ O
We -X- _ O
tokenize -X- _ O
the -X- _ O
sentences -X- _ O
with -X- _ O
the -X- _ O
Moses -X- _ O
script -X- _ O
( -X- _ O
Koehn -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2007 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
Multi30K -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
process -X- _ O
it -X- _ O
similar -X- _ O
to -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018a -X- _ O
) -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
the -X- _ O
sentences -X- _ O
are -X- _ O
randomly -X- _ O
divided -X- _ O
into -X- _ O
two -X- _ O
parts. -X- _ O
The -X- _ O
sourcelanguage -X- _ O
monolingual -X- _ O
dataset -X- _ O
is -X- _ O
built -X- _ O
from -X- _ O
the -X- _ O
source-language -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
part -X- _ O
and -X- _ O
the -X- _ O
target-language -X- _ O
dataset -X- _ O
from -X- _ O
the -X- _ O
second -X- _ O
part. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
there -X- _ O
will -X- _ O
be -X- _ O
no -X- _ O
exact -X- _ O
translations -X- _ O
of -X- _ O
any -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
datasets. -X- _ O
For -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
datasets -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
preprocessing -X- _ O
methods -X- _ O
from -X- _ O
( -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
English-Romanian -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
diacritics -X- _ O
as -X- _ O
done -X- _ O
by -X- _ O
Sennrich -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2016a -X- _ O
) -X- _ O
to -X- _ O
avoid -X- _ O
their -X- _ O
inconsistent -X- _ O
usage -X- _ O
in -X- _ O
the -X- _ O
Romanian -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
dataset -X- _ O
. -X- _ O

Metric -X- _ O
& -X- _ O
Performance. -X- _ O
We -X- _ O
use -X- _ O
BLEU -X- _ B-MetricName
as -X- _ O
metric -X- _ O
( -X- _ O
Papineni -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
for -X- _ O
all -X- _ O
our -X- _ O
experiments. -X- _ O
Although -X- _ O
Artetxe -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
1 -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
of -X- _ O
our -X- _ O
flow-adapter -X- _ B-HyperparameterName
model -X- _ O
for -X- _ O
multilingual -X- _ O
translation -X- _ O
on -X- _ O
Multi30K. -X- _ B-DatasetName
Baseline -X- _ O
: -X- _ O
model -X- _ O
without -X- _ O
our -X- _ O
proposed -X- _ O
flow-adapter -X- _ B-HyperparameterName
architecture. -X- _ O
3-scf -X- _ O
or -X- _ O
3-glow -X- _ O
models -X- _ O
: -X- _ O
baseline -X- _ O
models -X- _ O
with -X- _ O
the -X- _ O
flow-adapter -X- _ O
architecture -X- _ O
constructed -X- _ O
by -X- _ O
two -X- _ O
realNVP -X- _ O
NF -X- _ O
models -X- _ O
or -X- _ O
Glow -X- _ O
NF -X- _ O
models -X- _ O
, -X- _ O
each -X- _ O
of -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
3 -X- _ O
sequential -X- _ O
flows -X- _ O
, -X- _ O
for -X- _ O
performing -X- _ O
the -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
in -X- _ O
that -X- _ O
translation -X- _ O
direction. -X- _ O
unsupervised -X- _ O
validation -X- _ O
criteria -X- _ O
for -X- _ O
systematic -X- _ O
tuning -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
setting -X- _ O
of -X- _ O
( -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Song -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
provided -X- _ O
parallel -X- _ O
validation -X- _ O
sets -X- _ O
for -X- _ O
tuning -X- _ O
hyperparameters. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
sets -X- _ O
of -X- _ O
the -X- _ O
models -X- _ O
that -X- _ O
achieve -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
sets -X- _ O
. -X- _ O

Pre-trained -X- _ O
Embeddings -X- _ O
& -X- _ O
Models. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
pre-trained -X- _ O
MUSE -X- _ B-MethodName
4 -X- _ I-MethodName
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
) -X- _ O
embeddings -X- _ O
for -X- _ O
the -X- _ O
multilingual -X- _ O
unsupervised -X- _ B-TaskName
MT -X- _ I-TaskName
experiment -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
leverage -X- _ O
pre-trained -X- _ O
cross-lingual -X- _ O
models -X- _ O
in -X- _ O
the -X- _ O
experiment -X- _ O
of -X- _ O
shared -X- _ O
& -X- _ O
separate -X- _ O
decoder -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
( -X- _ O
Table -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
XLM -X- _ B-TaskName
models -X- _ O
from -X- _ O
HuggingFace -X- _ O
5 -X- _ O
( -X- _ O
Wolf -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
initialize -X- _ O
the -X- _ O
encoder. -X- _ O
Moreover -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
incorporate -X- _ O
our -X- _ O
flow-adapter -X- _ O
architecture -X- _ O
directly -X- _ O
into -X- _ O
the -X- _ O
codebase -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
implementation -X- _ O
of -X- _ O
XLM -X- _ B-TaskName
6 -X- _ I-TaskName
for -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
dataset -X- _ O
experiment -X- _ O
( -X- _ O
Table -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
are -X- _ O
both -X- _ O
initialized -X- _ O
with -X- _ O
pre-trained -X- _ O
models. -X- _ O
Details -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A.3 -X- _ O
. -X- _ O

Results -X- _ O
of -X- _ O
Multilingual -X- _ O
Unsupervised -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
on -X- _ O
Multi30K -X- _ B-DatasetName
As -X- _ O
Multi30K -X- _ B-DatasetName
provides -X- _ O
parallel -X- _ O
test -X- _ O
data -X- _ O
for -X- _ O
English -X- _ O
, -X- _ O
French -X- _ O
and -X- _ O
German -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
conduct -X- _ O
experiments -X- _ O
to -X- _ O
show -X- _ O
the -X- _ O
multilingual -X- _ O
translation -X- _ O
ability -X- _ O
of -X- _ O
our -X- _ O
flow-adapter -X- _ O
models. -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1. -X- _ O
The -X- _ O
baseline -X- _ O
model -X- _ O
( -X- _ O
without -X- _ O
flow-adapter -X- _ O
architecture -X- _ O
) -X- _ O
is -X- _ O
trained -X- _ O
with -X- _ O
only -X- _ O
DAE -X- _ O
loss -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
flow-adapter -X- _ O
based -X- _ O
models -X- _ O
( -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
) -X- _ O
are -X- _ O
additionally -X- _ O
trained -X- _ O
with -X- _ O
MLE -X- _ O
loss -X- _ O
for -X- _ O
the -X- _ O
NFs. -X- _ O
3-scf -X- _ O
( -X- _ O
resp. -X- _ O
3-glow -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
baseline -X- _ O
model -X- _ O
with -X- _ O
two -X- _ O
realNVP -X- _ O
NF -X- _ O
models -X- _ O
( -X- _ O
Dinh -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
( -X- _ O
resp. -X- _ O
Glow -X- _ O
NF -X- _ O
models -X- _ O
( -X- _ O
Kingma -X- _ O
andDhariwal -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
each -X- _ O
of -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
3 -X- _ O
sequential -X- _ O
flows. -X- _ O
Each -X- _ O
NF -X- _ O
model -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
sentence-level -X- _ O
represen-tations -X- _ O
of -X- _ O
one -X- _ O
specific -X- _ O
language -X- _ O
, -X- _ O
and -X- _ O
two -X- _ O
NF -X- _ O
models -X- _ O
then -X- _ O
construct -X- _ O
a -X- _ O
flow-adapter -X- _ B-MethodName
for -X- _ O
that -X- _ O
translation -X- _ O
direction -X- _ O
( -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
flow-adapter -X- _ B-MethodName
based -X- _ O
models -X- _ O
additionally -X- _ O
perform -X- _ O
the -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
language-specific -X- _ O
representation -X- _ O
while -X- _ O
the -X- _ O
baseline -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
perform -X- _ O
such -X- _ O
a -X- _ O
transformation. -X- _ O
For -X- _ O
this -X- _ O
experiment -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
pre-trained -X- _ O
crosslingual -X- _ O
word -X- _ O
embeddings -X- _ O
( -X- _ O
MUSE -X- _ B-MethodName
embeddings -X- _ O
) -X- _ O
and -X- _ O
randomly -X- _ O
initialize -X- _ O
a -X- _ O
shared -X- _ O
encoder -X- _ O
and -X- _ O
a -X- _ O
shared -X- _ O
decoder -X- _ O
for -X- _ O
all -X- _ O
three -X- _ O
languages. -X- _ O
It -X- _ O
is -X- _ O
worth -X- _ O
noting -X- _ O
that -X- _ O
the -X- _ O
training -X- _ O
objective -X- _ O
does -X- _ O
not -X- _ O
contain -X- _ O
the -X- _ O
iterative -X- _ O
back-translation. -X- _ O
For -X- _ O
further -X- _ O
research -X- _ O
where -X- _ O
there -X- _ O
are -X- _ O
far -X- _ O
more -X- _ O
languages -X- _ O
accommodated -X- _ O
, -X- _ O
random -X- _ O
online -X- _ O
back-translation -X- _ O
( -X- _ O
ROBT -X- _ O
) -X- _ O
proposed -X- _ O
by -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
could -X- _ O
be -X- _ O
considered -X- _ O
. -X- _ O

Table -X- _ O
1 -X- _ O
shows -X- _ O
improvements -X- _ O
over -X- _ O
all -X- _ O
six -X- _ O
translation -X- _ O
directions -X- _ O
by -X- _ O
using -X- _ O
the -X- _ O
flow-adapter -X- _ O
architecture. -X- _ O
Notably -X- _ O
, -X- _ O
our -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
models -X- _ O
achieve -X- _ O
19.83 -X- _ B-MetricValue
and -X- _ O
20.14 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
scores -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
on -X- _ O
deen -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
0.52 -X- _ B-MetricValue
and -X- _ O
0.83 -X- _ B-MetricValue
higher -X- _ O
than -X- _ O
the -X- _ O
baseline -X- _ O
model. -X- _ O
Similar -X- _ O
improvements -X- _ O
can -X- _ O
also -X- _ O
be -X- _ O
seen -X- _ O
on -X- _ O
other -X- _ O
translation -X- _ O
directions. -X- _ O
Our -X- _ O
3-scf -X- _ O
model -X- _ O
has -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
that -X- _ O
are -X- _ O
0.46 -X- _ B-MetricValue
to -X- _ O
0.88 -X- _ B-MetricValue
higher -X- _ O
than -X- _ O
the -X- _ O
baselines -X- _ O
while -X- _ O
our -X- _ O
3-glow -X- _ O
model -X- _ O
has -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
that -X- _ O
are -X- _ O
0.04 -X- _ B-MetricValue
to -X- _ O
0.83 -X- _ B-MetricValue
higher -X- _ O
than -X- _ O
the -X- _ O
baselines. -X- _ O
The -X- _ O
overall -X- _ O
improvements -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
flow-adapter -X- _ B-MethodName
can -X- _ O
generate -X- _ O
more -X- _ O
suitable -X- _ O
sentence-level -X- _ O
representations -X- _ O
by -X- _ O
performing -X- _ O
the -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
helpful -X- _ O
for -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
semantics -X- _ O
and -X- _ O
generate -X- _ O
more -X- _ O
suitable -X- _ O
translations -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
translation -X- _ O
performance -X- _ O
is -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
language -X- _ O
pair -X- _ O
and -X- _ O
the -X- _ O
translation -X- _ O
direction -X- _ O
for -X- _ O
both -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
and -X- _ O
flow-adapter -X- _ O
models. -X- _ O
Our -X- _ O
models -X- _ O
obtain -X- _ O
very -X- _ O
good -X- _ O
performance -X- _ O
on -X- _ O
en-fr -X- _ B-DatasetName
, -X- _ I-DatasetName
with -X- _ O
performances -X- _ O
in -X- _ O
both -X- _ O
the -X- _ O
en-fr -X- _ B-DatasetName
or -X- _ O
fr-en -X- _ B-DatasetName
directions -X- _ O
better -X- _ O
by -X- _ O
16 -X- _ B-DatasetName
BLEU -X- _ B-DatasetName
points. -X- _ O
For -X- _ O
other -X- _ O
language -X- _ O
pairs -X- _ O
( -X- _ O
including -X- _ O
enfr -X- _ B-DatasetName
) -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
always -X- _ O
one -X- _ O
direction -X- _ O
showing -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
the -X- _ O
other. -X- _ O
Specifically -X- _ O
, -X- _ O
de-en -X- _ B-DatasetName
achieves -X- _ O
more -X- _ O
than -X- _ O
19 -X- _ B-DatasetName
BLEU -X- _ B-DatasetName
points -X- _ O
compared -X- _ O
with -X- _ O
12 -X- _ B-DatasetName
points -X- _ O
for -X- _ O
en-de -X- _ B-DatasetName
, -X- _ O
and -X- _ O
de-fr -X- _ B-DatasetName
achieves -X- _ O
more -X- _ O
than -X- _ O
11 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
points -X- _ O
compared -X- _ O
with -X- _ O
8.5 -X- _ B-MetricValue
for -X- _ O
fr-de -X- _ B-DatasetName
. -X- _ O

Results -X- _ O
of -X- _ O
Shared-Decoder -X- _ O
& -X- _ O
Separate-Decoder -X- _ O
Models -X- _ O
on -X- _ O
Multi30K -X- _ B-DatasetName
We -X- _ O
present -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
flow-adapter -X- _ O
models -X- _ O
under -X- _ O
the -X- _ O
shared-decoder -X- _ O
and -X- _ O
separatedecoder -X- _ O
settings -X- _ O
on -X- _ O
Multi30K. -X- _ B-DatasetName
For -X- _ O
this -X- _ O
experiment -X- _ O
, -X- _ O
the -X- _ O
encoder -X- _ O
is -X- _ O
initialized -X- _ O
with -X- _ O
the -X- _ O
pre-trained -X- _ O
XLM -X- _ B-HyperparameterName
model -X- _ O
and -X- _ O
fixed -X- _ O
; -X- _ O
the -X- _ O
decoder -X- _ O
parameters -X- _ O
are -X- _ O
ran- -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
BLEU -X- _ B-MetricName
of -X- _ O
the -X- _ O
flow-adapter -X- _ B-MethodName
models -X- _ O
and -X- _ O
unsupervised -X- _ O
SOTA -X- _ O
model -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
UNMT -X- _ B-MethodName
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
, -X- _ O
on -X- _ O
Multi30K. -X- _ B-DatasetName
Baseline -X- _ O
models -X- _ O
use -X- _ O
pre-trained -X- _ O
XLMs -X- _ B-TaskName
from -X- _ O
HuggingFace -X- _ O
as -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
randomly -X- _ O
initialized -X- _ O
decoder -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
without -X- _ O
the -X- _ O
flow-adapter. -X- _ O
( -X- _ O
separate -X- _ O
decoders -X- _ O
) -X- _ O
: -X- _ O
two -X- _ O
independent -X- _ O
and -X- _ O
randomly -X- _ O
initialized -X- _ O
decoders -X- _ O
are -X- _ O
used -X- _ O
, -X- _ O
each -X- _ O
for -X- _ O
decoding -X- _ O
a -X- _ O
specific -X- _ O
language. -X- _ O
( -X- _ O
shared -X- _ O
decoder -X- _ O
) -X- _ O
: -X- _ O
a -X- _ O
single -X- _ O
shared -X- _ O
decoder -X- _ O
for -X- _ O
decoding -X- _ O
both -X- _ O
languages -X- _ O
is -X- _ O
used. -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
( -X- _ O
as -X- _ O
defined -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
and -X- _ O
Section -X- _ O
4.3 -X- _ O
) -X- _ O
denote -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
with -X- _ O
the -X- _ O
flow-adapter -X- _ O
architecture. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
UNMT -X- _ B-MethodName
from -X- _ O
their -X- _ O
original -X- _ O
paper. -X- _ O
domly -X- _ O
initialized -X- _ O
and -X- _ O
then -X- _ O
trained. -X- _ O
We -X- _ O
also -X- _ O
report -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
previous -X- _ O
SOTA -X- _ O
model -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
UNMT -X- _ B-MethodName
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018a -X- _ O
) -X- _ O
. -X- _ O
7 -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
2. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
shareddecoder -X- _ O
baseline -X- _ O
model -X- _ O
obtains -X- _ O
very -X- _ O
low -X- _ O
BLEU -X- _ B-MetricName
scores. -X- _ O
By -X- _ O
checking -X- _ O
the -X- _ O
translation -X- _ O
generated -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
the -X- _ O
model -X- _ O
only -X- _ O
copies -X- _ O
the -X- _ O
input -X- _ O
as -X- _ O
translation. -X- _ O
This -X- _ O
phenomenon -X- _ O
shows -X- _ O
that -X- _ O
this -X- _ O
baseline -X- _ O
, -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
perform -X- _ O
the -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
, -X- _ O
can -X- _ O
not -X- _ O
model -X- _ O
two -X- _ O
languages -X- _ O
simultaneously -X- _ O
well -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
can -X- _ O
not -X- _ O
generate -X- _ O
translations -X- _ O
in -X- _ O
the -X- _ O
desired -X- _ O
language -X- _ O
domains. -X- _ O
However -X- _ O
, -X- _ O
by -X- _ O
incorporating -X- _ O
the -X- _ O
flow-adapter -X- _ O
, -X- _ O
the -X- _ O
models -X- _ O
will -X- _ O
no -X- _ O
longer -X- _ O
have -X- _ O
this -X- _ O
limitation. -X- _ O
Both -X- _ O
shared-decoder -X- _ O
models -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
, -X- _ O
achieve -X- _ O
very -X- _ O
good -X- _ O
performance -X- _ O
on -X- _ O
all -X- _ O
translation -X- _ O
pairs. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
3-scf -X- _ O
model -X- _ O
obtains -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
of -X- _ O
25.80 -X- _ B-MetricValue
, -X- _ O
28.92 -X- _ B-MetricValue
, -X- _ O
39.26 -X- _ B-MetricValue
and -X- _ O
36.84 -X- _ B-MetricValue
on -X- _ O
en-de -X- _ B-DatasetName
, -X- _ O
de-en -X- _ B-DatasetName
, -X- _ O
en-fr -X- _ B-DatasetName
and -X- _ O
fr-en -X- _ B-DatasetName
, -X- _ O
which -X- _ O
are -X- _ O
much -X- _ O
higher -X- _ O
than -X- _ O
the -X- _ O
baseline -X- _ O
. -X- _ O

Compared -X- _ O
with -X- _ O
the -X- _ O
shared-decoder -X- _ O
scenario -X- _ O
, -X- _ O
the -X- _ O
models -X- _ O
under -X- _ O
the -X- _ O
separate-decoder -X- _ O
setting -X- _ O
do -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
copying -X- _ O
problem -X- _ O
, -X- _ O
because -X- _ O
different -X- _ O
decoders -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
specifically -X- _ O
model -X- _ O
and -X- _ O
generate -X- _ O
sentences -X- _ O
in -X- _ O
distinct -X- _ O
language -X- _ O
domains. -X- _ O
The -X- _ O
downside -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
is -X- _ O
that -X- _ O
using -X- _ O
multiple -X- _ O
decoders -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
can -X- _ O
substantially -X- _ O
increase -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
trainable -X- _ O
parameters. -X- _ O
Within -X- _ O
the -X- _ O
separatedecoder -X- _ O
models -X- _ O
, -X- _ O
the -X- _ O
flow-adapter -X- _ O
models -X- _ O
generally -X- _ O
perform -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
baseline -X- _ O
model -X- _ O
, -X- _ O
with -X- _ O
about -X- _ O
1 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
increase -X- _ O
on -X- _ O
en-de -X- _ B-DatasetName
and -X- _ O
de-en -X- _ B-DatasetName
directions -X- _ O
and -X- _ O
relatively -X- _ O
smaller -X- _ O
improvements -X- _ O
on -X- _ O
en-fr -X- _ B-DatasetName
and -X- _ O
fr-en. -X- _ B-DatasetName
Those -X- _ O
improvements -X- _ O
demonstrate -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
benefit -X- _ O
from -X- _ O
the -X- _ O
flow-adapter -X- _ O
architectures -X- _ O
as -X- _ O
language-specific -X- _ O
latent -X- _ O
representations -X- _ O
are -X- _ O
used -X- _ O
, -X- _ O
thus -X- _ O
advancing -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
separate-decoder -X- _ O
mod-els -X- _ O
generally -X- _ O
perform -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
shareddecoder -X- _ O
models. -X- _ O
The -X- _ O
separate-decoder -X- _ O
baseline -X- _ O
is -X- _ O
much -X- _ O
better -X- _ O
than -X- _ O
its -X- _ O
counterpart -X- _ O
as -X- _ O
it -X- _ O
avoids -X- _ O
the -X- _ O
copying -X- _ O
problem. -X- _ O
For -X- _ O
the -X- _ O
3-scf -X- _ O
flow-adapter -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
separate-decoder -X- _ O
model -X- _ O
outperforms -X- _ O
the -X- _ O
shared-decoder -X- _ O
model -X- _ O
by -X- _ O
2.44 -X- _ B-MetricValue
, -X- _ O
1.71 -X- _ B-MetricValue
, -X- _ O
0.38 -X- _ B-MetricValue
on -X- _ O
en-de -X- _ B-DatasetName
, -X- _ O
de-en -X- _ B-DatasetName
and -X- _ O
en-fr. -X- _ B-DatasetName
However -X- _ O
, -X- _ O
on -X- _ O
fr-en -X- _ B-DatasetName
, -X- _ O
the -X- _ O
shared-decoder -X- _ O
model -X- _ O
achieves -X- _ O
a -X- _ O
BLEU -X- _ O
socre -X- _ O
that -X- _ O
is -X- _ O
by -X- _ O
0.39 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
points -X- _ O
better. -X- _ O
A -X- _ O
similar -X- _ O
phenomenon -X- _ O
can -X- _ O
also -X- _ O
be -X- _ O
seen -X- _ O
for -X- _ O
the -X- _ O
3-glow -X- _ O
model. -X- _ O
We -X- _ O
conjecture -X- _ O
this -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
similarity -X- _ O
between -X- _ O
languages. -X- _ O
As -X- _ O
English -X- _ O
and -X- _ O
French -X- _ O
share -X- _ O
common -X- _ O
vocabulary -X- _ O
, -X- _ O
some -X- _ O
common -X- _ O
features -X- _ O
can -X- _ O
therefore -X- _ O
be -X- _ O
captured -X- _ O
by -X- _ O
a -X- _ O
shared -X- _ O
decoder -X- _ O
, -X- _ O
thus -X- _ O
improving -X- _ O
its -X- _ O
generalization -X- _ O
. -X- _ O

Lastly -X- _ O
, -X- _ O
when -X- _ O
compared -X- _ O
with -X- _ O
UNMT -X- _ B-TaskName
, -X- _ O
our -X- _ O
models -X- _ O
show -X- _ O
superiority -X- _ O
, -X- _ O
improving -X- _ O
performance -X- _ O
by -X- _ O
more -X- _ O
than -X- _ O
4 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
points -X- _ O
in -X- _ O
each -X- _ O
direction. -X- _ O
We -X- _ O
attribute -X- _ O
the -X- _ O
improvements -X- _ O
to -X- _ O
the -X- _ O
usage -X- _ O
of -X- _ O
the -X- _ O
pre-trained -X- _ O
model -X- _ O
and -X- _ O
incorporation -X- _ O
of -X- _ O
languagespecific -X- _ O
sentence-level -X- _ O
representations -X- _ O
obtained -X- _ O
by -X- _ O
our -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
. -X- _ O

Results -X- _ O
on -X- _ O
WMT -X- _ B-DatasetName
datasets -X- _ O
We -X- _ O
further -X- _ O
integrate -X- _ O
our -X- _ O
flow-adapter -X- _ O
architecture -X- _ O
into -X- _ O
the -X- _ O
original -X- _ O
implementation -X- _ O
of -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
datasets. -X- _ O
To -X- _ O
fully -X- _ O
leverage -X- _ O
the -X- _ O
pretrained -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
initialize -X- _ O
both -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
with -X- _ O
XLM -X- _ B-TaskName
models -X- _ O
and -X- _ O
set -X- _ O
them -X- _ O
trainable. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
the -X- _ O
experiment -X- _ O
in -X- _ O
Section -X- _ O
4.4 -X- _ O
, -X- _ O
a -X- _ O
single -X- _ O
shared -X- _ O
decoder -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
this -X- _ O
experiment -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
decoder -X- _ O
is -X- _ O
also -X- _ O
initialized -X- _ O
with -X- _ O
the -X- _ O
pre-trained -X- _ O
model -X- _ O
and -X- _ O
has -X- _ O
far -X- _ O
more -X- _ O
parameters -X- _ O
compared -X- _ O
with -X- _ O
the -X- _ O
randomly -X- _ O
initialized -X- _ O
transformer -X- _ O
decoder -X- _ O
we -X- _ O
use -X- _ O
in -X- _ O
Section -X- _ O
4.4. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
flow-adapter -X- _ O
based -X- _ O
models -X- _ O
( -X- _ O
5-scf -X- _ O
and -X- _ O
5-Models -X- _ O
en-de -X- _ B-DatasetName
de-en -X- _ B-DatasetName
en-ro -X- _ B-DatasetName
ro-en -X- _ B-DatasetName
en-fr -X- _ B-DatasetName
fr-en -X- _ B-DatasetName
XLM -X- _ B-TaskName
( -X- _ O
EMD -X- _ B-TaskName
+ -X- _ I-TaskName
EMD -X- _ I-TaskName
) -X- _ O
( -X- _ O
Conneau -X- _ O
and -X- _ O
Lample -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
21 -X- _ O
glow -X- _ O
8 -X- _ O
) -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
SOTA -X- _ O
models -X- _ O
, -X- _ O
namely -X- _ O
XLM -X- _ B-TaskName
, -X- _ O
MASS -X- _ B-TaskName
and -X- _ O
CSP. -X- _ B-TaskName
9 -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3. -X- _ O
Noticeably -X- _ O
, -X- _ O
both -X- _ O
of -X- _ O
our -X- _ O
flow-adapter -X- _ B-MethodName
models -X- _ O
achieve -X- _ O
remarkable -X- _ O
performance -X- _ O
on -X- _ O
all -X- _ O
language -X- _ O
pairs. -X- _ O
Compared -X- _ O
with -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
EMD -X- _ B-TaskName
+ -X- _ I-TaskName
EMD -X- _ I-TaskName
) -X- _ O
, -X- _ O
which -X- _ O
uses -X- _ O
the -X- _ O
pre-trained -X- _ O
cross-lingual -X- _ O
embeddings -X- _ O
instead -X- _ O
of -X- _ O
pre-trained -X- _ O
models -X- _ O
, -X- _ O
both -X- _ O
5-scf -X- _ O
and -X- _ O
5-glow -X- _ O
achieve -X- _ O
overall -X- _ O
better -X- _ O
performance. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
3-scf -X- _ O
achieves -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
higher -X- _ O
by -X- _ O
5. -X- _ B-MetricValue
20 -X- _ I-MetricValue
, -X- _ O
5.33 -X- _ B-MetricValue
, -X- _ O
6.61 -X- _ B-MetricValue
, -X- _ O
5.09 -X- _ B-MetricValue
, -X- _ O
6.37 -X- _ B-MetricValue
and -X- _ O
4.32 -X- _ B-MetricValue
on -X- _ O
en-de -X- _ B-DatasetName
, -X- _ O
de-en -X- _ B-DatasetName
, -X- _ O
en-ro -X- _ B-DatasetName
, -X- _ O
ro-en -X- _ B-DatasetName
, -X- _ O
en-fr -X- _ B-DatasetName
and -X- _ O
fr-en -X- _ B-DatasetName
, -X- _ O
respectively. -X- _ O
While -X- _ O
not -X- _ O
being -X- _ O
as -X- _ O
good -X- _ O
as -X- _ O
5-scf -X- _ O
, -X- _ O
5-glow -X- _ O
still -X- _ O
shows -X- _ O
superiority -X- _ O
over -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
EMD -X- _ B-TaskName
+ -X- _ I-TaskName
EMD -X- _ I-TaskName
) -X- _ O
. -X- _ O
These -X- _ O
improvements -X- _ O
can -X- _ O
be -X- _ O
contributed -X- _ O
to -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
the -X- _ O
usage -X- _ O
of -X- _ O
pre-trained -X- _ O
models -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
the -X- _ O
flow-adapter. -X- _ O
We -X- _ O
further -X- _ O
compare -X- _ O
our -X- _ O
flow-adapter -X- _ O
based -X- _ O
models -X- _ O
with -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
MLM -X- _ B-TaskName
+ -X- _ I-TaskName
MLM -X- _ I-TaskName
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
also -X- _ O
initialized -X- _ O
with -X- _ O
pre-trained -X- _ O
models. -X- _ O
We -X- _ O
find -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
x-en -X- _ O
directions -X- _ O
is -X- _ O
consistently -X- _ O
lower -X- _ O
than -X- _ O
en-x -X- _ O
directions -X- _ O
for -X- _ O
our -X- _ O
models -X- _ O
except -X- _ O
for -X- _ O
ende. -X- _ O
This -X- _ O
pattern -X- _ O
is -X- _ O
not -X- _ O
limited -X- _ O
to -X- _ O
our -X- _ O
architecture -X- _ O
but -X- _ O
is -X- _ O
consistently -X- _ O
present -X- _ O
in -X- _ O
prior -X- _ O
work. -X- _ O
We -X- _ O
, -X- _ O
again -X- _ O
, -X- _ O
speculate -X- _ O
this -X- _ O
is -X- _ O
relating -X- _ O
to -X- _ O
the -X- _ O
complexity -X- _ O
of -X- _ O
languages -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
similarity -X- _ O
between -X- _ O
languages. -X- _ O
We -X- _ O
leave -X- _ O
this -X- _ O
finding -X- _ O
for -X- _ O
future -X- _ O
investigation. -X- _ O
Our -X- _ O
flow-adapter -X- _ O
based -X- _ O
models -X- _ O
, -X- _ O
though -X- _ O
achieving -X- _ O
similar -X- _ O
or -X- _ O
relatively -X- _ O
worse -X- _ O
BLEU -X- _ O
scores -X- _ O
on -X- _ O
de-en -X- _ O
and -X- _ O
ro-en -X- _ O
compared -X- _ O
with -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
MLM -X- _ B-TaskName
+ -X- _ I-TaskName
MLM -X- _ I-TaskName
) -X- _ O
, -X- _ O
obtain -X- _ O
higher -X- _ O
scores -X- _ O
on -X- _ O
other -X- _ O
directions -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
en-de -X- _ O
and -X- _ O
en-ro -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
our -X- _ O
models -X- _ O
might -X- _ O
be -X- _ O
more -X- _ O
helpful -X- _ O
on -X- _ O
specific -X- _ O
translation -X- _ O
directions -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
flow-adapter -X- _ O
generates -X- _ O
language-specific -X- _ O
rep- -X- _ O
8 -X- _ O
Preliminary -X- _ O
experiments -X- _ O
showed -X- _ O
that -X- _ O
using -X- _ O
5 -X- _ O
flows -X- _ O
provides -X- _ O
slightly -X- _ O
better -X- _ O
results -X- _ O
than -X- _ O
3 -X- _ O
flows -X- _ O
for -X- _ O
WMT -X- _ B-DatasetName
as -X- _ O
WMT -X- _ B-DatasetName
has -X- _ O
many -X- _ O
more -X- _ O
sentences -X- _ O
than -X- _ O
Multi30K -X- _ B-DatasetName
and -X- _ O
therefore -X- _ O
more -X- _ O
powerful -X- _ O
generative -X- _ O
models -X- _ O
( -X- _ O
by -X- _ O
adding -X- _ O
more -X- _ O
intermediate -X- _ O
flows -X- _ O
) -X- _ O
are -X- _ O
needed -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
sentence-level -X- _ O
representations. -X- _ O
9 -X- _ O
We -X- _ O
follow -X- _ O
prior -X- _ O
convention -X- _ O
and -X- _ O
compare -X- _ O
directly -X- _ O
with -X- _ O
MASS -X- _ B-DatasetName
and -X- _ O
CSP -X- _ B-DatasetName
even -X- _ O
though -X- _ O
dataset -X- _ O
processing -X- _ O
for -X- _ O
MASS -X- _ B-DatasetName
and -X- _ O
CSP -X- _ B-DatasetName
( -X- _ O
e.g. -X- _ O
, -X- _ O
filtering -X- _ O
, -X- _ O
sampling -X- _ O
) -X- _ O
are -X- _ O
not -X- _ O
strictly -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
for -X- _ O
XLM. -X- _ B-DatasetName
But -X- _ O
the -X- _ O
difference -X- _ O
is -X- _ O
small -X- _ O
and -X- _ O
results -X- _ O
would -X- _ O
not -X- _ O
be -X- _ O
much -X- _ O
different -X- _ O
as -X- _ O
mentions. -X- _ O
resentations. -X- _ O
Lastly -X- _ O
, -X- _ O
5-scf -X- _ O
achieves -X- _ O
scores -X- _ O
by -X- _ O
2.37 -X- _ B-MetricValue
and -X- _ O
0.42 -X- _ B-MetricValue
better -X- _ O
than -X- _ O
XLM -X- _ B-TaskName
( -X- _ O
MLM -X- _ B-TaskName
+ -X- _ I-TaskName
MLM -X- _ I-TaskName
) -X- _ O
on -X- _ O
en-fr -X- _ B-DatasetName
and -X- _ O
fr-en. -X- _ B-DatasetName
As -X- _ O
in -X- _ O
the -X- _ O
other -X- _ O
experiments -X- _ O
, -X- _ O
the -X- _ O
improvement -X- _ O
due -X- _ O
to -X- _ O
flow -X- _ O
adapters -X- _ O
seems -X- _ O
to -X- _ O
be -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
languages -X- _ O
involved -X- _ O
in -X- _ O
that -X- _ O
language -X- _ O
pair -X- _ O
and -X- _ O
the -X- _ O
translation -X- _ O
directions. -X- _ O
We -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
investigate -X- _ O
this -X- _ O
phenomenon -X- _ O
in -X- _ O
future -X- _ O
research -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
out -X- _ O
models -X- _ O
are -X- _ O
competitve -X- _ O
with -X- _ O
MASS -X- _ B-TaskName
and -X- _ O
CSP -X- _ B-TaskName
, -X- _ O
with -X- _ O
only -X- _ O
small -X- _ O
differences -X- _ O
in -X- _ O
BLEU. -X- _ B-MetricName
In -X- _ O
general -X- _ O
, -X- _ O
the -X- _ O
experiments -X- _ O
shows -X- _ O
the -X- _ O
validity -X- _ O
and -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
flow-adapter -X- _ B-MethodName
architecture -X- _ O
integrated -X- _ O
into -X- _ O
pre-trained -X- _ O
models -X- _ O
. -X- _ O

Conclusion -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
flow-adapter -X- _ B-MethodName
architecture -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
NMT. -X- _ I-TaskName
The -X- _ O
flow-adapter -X- _ B-MethodName
employs -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
NFs -X- _ O
to -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations. -X- _ O
A -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
is -X- _ O
performed -X- _ O
in -X- _ O
translation -X- _ O
, -X- _ O
which -X- _ O
enables -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
better -X- _ O
capture -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
sentences -X- _ O
in -X- _ O
certain -X- _ O
language -X- _ O
domains. -X- _ O
Through -X- _ O
extensive -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
the -X- _ O
flow-adapter -X- _ O
can -X- _ O
improve -X- _ O
multilingual -X- _ O
translation -X- _ O
ability. -X- _ O
Moreover -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
alleviate -X- _ O
the -X- _ O
copying -X- _ O
problem. -X- _ O
By -X- _ O
integrating -X- _ O
the -X- _ O
flow-adapter -X- _ O
into -X- _ O
pretrained -X- _ O
XLM -X- _ B-TaskName
models -X- _ O
, -X- _ O
we -X- _ O
achieve -X- _ O
results -X- _ O
competitive -X- _ O
to -X- _ O
state-of-the-art -X- _ O
models -X- _ O
on -X- _ O
WMT -X- _ B-DatasetName
datasets -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
pre-training -X- _ O
the -X- _ O
flow-adapter -X- _ B-MethodName
simultaneously -X- _ O
when -X- _ O
pre-training -X- _ O
the -X- _ O
language -X- _ O
models -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
flows -X- _ O
can -X- _ O
learn -X- _ O
more -X- _ O
information. -X- _ O
Moreover -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
extend -X- _ O
normalizing -X- _ O
flows -X- _ O
to -X- _ O
language -X- _ O
generation. -X- _ O
By -X- _ O
using -X- _ O
different -X- _ O
flows -X- _ O
for -X- _ O
different -X- _ O
languages -X- _ O
, -X- _ O
multilingual -X- _ O
language -X- _ O
generation -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
semantics -X- _ O
can -X- _ O
be -X- _ O
performed -X- _ O
. -X- _ O

A.1 -X- _ O
Proof -X- _ O
of -X- _ O
the -X- _ O
Invertibility -X- _ O
The -X- _ O
following -X- _ O
proof -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
proof -X- _ O
by -X- _ O
Grover -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
shows -X- _ O
the -X- _ O
source-to-target -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
is -X- _ O
the -X- _ O
inverse -X- _ O
of -X- _ O
the -X- _ O
target-tosource -X- _ O
latent -X- _ O
code -X- _ O
transformation -X- _ O
, -X- _ O
and -X- _ O
vice -X- _ O
versa -X- _ O
: -X- _ O

A.2 -X- _ O
Generation -X- _ O
of -X- _ O
Sentence-level -X- _ O
Representation -X- _ O
The -X- _ O
following -X- _ O
formula -X- _ O
shows -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
how -X- _ O
the -X- _ O
sentence-level -X- _ O
representation -X- _ O
is -X- _ O
generated -X- _ O
: -X- _ O

where -X- _ O
the -X- _ O
pooling -X- _ O
operation -X- _ O
generates -X- _ O
a -X- _ O
vector -X- _ O
that -X- _ O
has -X- _ O
the -X- _ O
same -X- _ O
dimension -X- _ O
as -X- _ O
h -X- _ O
0 -X- _ O
, -X- _ O
so -X- _ O
the -X- _ O
three -X- _ O
vectors -X- _ O
have -X- _ O
the -X- _ O
same -X- _ O
shape -X- _ O
and -X- _ O
therefore -X- _ O
are -X- _ O
additive. -X- _ O
An -X- _ O
illustration -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O

A.3.1 -X- _ O
Multi30K -X- _ B-DatasetName
Experiment -X- _ O
For -X- _ O
the -X- _ O
multilingual -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
cross-lingual -X- _ O
MUSE -X- _ O
( -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018b -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
embeddings -X- _ O
were -X- _ O
learned -X- _ O
using -X- _ O
fastText -X- _ B-TaskName
10 -X- _ O
( -X- _ O
Bojanowski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
on -X- _ O
Wikipedia -X- _ O
data -X- _ O
and -X- _ O
then -X- _ O
aligned -X- _ O
in -X- _ O
a -X- _ O
common -X- _ O
space -X- _ O
by -X- _ O
the -X- _ O
method -X- _ O
proposed -X- _ O
by -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018b -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
is -X- _ O
the -X- _ O
average -X- _ O
over -X- _ O
10 -X- _ O
runs -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
sets. -X- _ O
Denosing -X- _ O
auto-encoding -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
baseline -X- _ O
model. -X- _ O
The -X- _ O
flow-adapter -X- _ O
based -X- _ O
( -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
) -X- _ O
models -X- _ O
are -X- _ O
additionally -X- _ O
trained -X- _ O
with -X- _ O
MLE -X- _ O
loss. -X- _ O
We -X- _ O
follow -X- _ O
the -X- _ O
denoising -X- _ O
auto-encoding -X- _ O
hyperparameter -X- _ O
settings -X- _ O
used -X- _ O
by -X- _ O
Lample -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018a -X- _ O
) -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
word -X- _ O
drop -X- _ O
and -X- _ O
word -X- _ O
shuffling -X- _ O
are -X- _ O
used. -X- _ O
For -X- _ O
word -X- _ O
drop -X- _ O
, -X- _ O
every -X- _ O
word -X- _ O
in -X- _ O
a -X- _ O
sentence -X- _ O
( -X- _ O
except -X- _ O
< -X- _ O
bos -X- _ O
> -X- _ O
and -X- _ O
< -X- _ O
eos -X- _ O
> -X- _ O
) -X- _ O
can -X- _ O
be -X- _ O
dropped -X- _ O
with -X- _ O
a -X- _ O
probability -X- _ O
p -X- _ B-HyperparameterName
wd -X- _ I-HyperparameterName
, -X- _ O
which -X- _ O
we -X- _ O
set -X- _ O
0.1 -X- _ B-HyperparameterValue
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

For -X- _ O
word -X- _ O
shuffling -X- _ O
, -X- _ O
a -X- _ O
random -X- _ O
permutation -X- _ O
σ -X- _ B-HyperparameterName
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
, -X- _ O
which -X- _ O
satisfy -X- _ O
the -X- _ O
condition -X- _ O
: -X- _ O
∀i -X- _ O
∈ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
n -X- _ O
} -X- _ O
, -X- _ O
|σ -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
− -X- _ O
i| -X- _ O
≤ -X- _ O
k -X- _ O
, -X- _ O
where -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
index -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
n -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
and -X- _ O
k -X- _ B-HyperparameterName
is -X- _ O
a -X- _ O
hyperparameter -X- _ O
that -X- _ O
controls -X- _ O
the -X- _ O
degree -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ O
which -X- _ O
we -X- _ O
set -X- _ O
3 -X- _ O
in -X- _ O
our -X- _ O
experiments. -X- _ O
The -X- _ O
dimension -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
pre-trained -X- _ O
embedding -X- _ O
is -X- _ O
300. -X- _ B-HyperparameterValue
The -X- _ O
randomly -X- _ O
initialized -X- _ O
shared -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
use -X- _ O
transformer -X- _ O
architecture -X- _ O
with -X- _ O
512 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units -X- _ I-HyperparameterName
, -X- _ O
4 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
and -X- _ O
3 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
by -X- _ O
default. -X- _ O
We -X- _ O
use -X- _ O
separate -X- _ O
embedding -X- _ O
layers -X- _ O
for -X- _ O
each -X- _ O
language -X- _ O
and -X- _ O
tie -X- _ O
their -X- _ O
weights -X- _ O
with -X- _ O
the -X- _ O
output -X- _ O
layers -X- _ O
for -X- _ O
each -X- _ O
language. -X- _ O
The -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ B-HyperparameterName
latent -X- _ I-HyperparameterName
representation -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
100. -X- _ B-HyperparameterValue
And -X- _ O
the -X- _ O
weight -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
MLE -X- _ O
loss -X- _ O
for -X- _ O
the -X- _ O
flows -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
0.01. -X- _ B-HyperparameterValue
We -X- _ O
use -X- _ O
dropout -X- _ B-HyperparameterName
( -X- _ O
Srivastava -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
probability -X- _ O
of -X- _ O
0.2 -X- _ B-HyperparameterValue
for -X- _ O
the -X- _ O
transformers -X- _ O
and -X- _ O
0 -X- _ B-HyperparameterValue
for -X- _ O
the -X- _ O
flows. -X- _ O
The -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
32. -X- _ B-HyperparameterValue
The -X- _ O
whole -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
in -X- _ O
an -X- _ O
end-to-end -X- _ O
manner -X- _ O
with -X- _ O
Adam -X- _ B-HyperparameterValue
optimizer -X- _ B-HyperparameterName
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
with -X- _ O
an -X- _ O
initial -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
0.0001. -X- _ B-HyperparameterValue
For -X- _ O
the -X- _ O
shared-decoder -X- _ O
& -X- _ O
separate-decoder -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
pre-trained -X- _ O
language -X- _ O
models -X- _ O
xlm-mlm-enfr-1024 -X- _ O
, -X- _ O
xlm-mlm-ende-1024 -X- _ O
, -X- _ O
xlm-mlmenro-1024 -X- _ O
from -X- _ O
HuggingFace -X- _ O
11 -X- _ O
( -X- _ O
Wolf -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
Figure -X- _ O
3 -X- _ O
: -X- _ O
The -X- _ O
illustration -X- _ O
of -X- _ O
generation -X- _ O
of -X- _ O
the -X- _ O
sentence-level -X- _ O
representations. -X- _ O
CLS -X- _ O
embedding -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
first -X- _ O
vector -X- _ O
output -X- _ O
by -X- _ O
the -X- _ O
transformer -X- _ O
encoder -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
h -X- _ O
0 -X- _ O
. -X- _ O

to -X- _ O
initialize -X- _ O
a -X- _ O
shared -X- _ O
encoder -X- _ O
and -X- _ O
randomly -X- _ O
initialize -X- _ O
the -X- _ O
decoder -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
.using -X- _ O
pre-trained -X- _ O
models. -X- _ O
Denosing -X- _ O
auto-encoding -X- _ O
and -X- _ O
iterative -X- _ O
back-translation -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
baseline -X- _ O
model. -X- _ O
The -X- _ O
flowadapter -X- _ O
based -X- _ O
( -X- _ O
3-scf -X- _ O
and -X- _ O
3-glow -X- _ O
) -X- _ O
models -X- _ O
are -X- _ O
additionally -X- _ O
trained -X- _ O
with -X- _ O
MLE -X- _ O
loss. -X- _ O
The -X- _ O
same -X- _ O
denoising -X- _ O
auto-encoding -X- _ O
hyperparameters -X- _ O
as -X- _ O
above -X- _ O
are -X- _ O
used. -X- _ O
For -X- _ O
iterative -X- _ O
back-translation -X- _ O
, -X- _ O
greedy -X- _ O
decoding -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
generate -X- _ O
synthetic -X- _ O
parallel -X- _ O
sentences -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
reconstructions. -X- _ O
A -X- _ O
single -X- _ O
embedding -X- _ O
layer -X- _ O
( -X- _ O
from -X- _ O
the -X- _ O
pre-trained -X- _ O
model -X- _ O
) -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
both -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
and -X- _ O
its -X- _ O
weight -X- _ O
is -X- _ O
tied -X- _ O
with -X- _ O
the -X- _ O
output -X- _ O
layer. -X- _ O
The -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
are -X- _ O
fixed -X- _ O
except -X- _ O
for -X- _ O
its -X- _ O
embedding -X- _ O
layer -X- _ O
which -X- _ O
is -X- _ O
also -X- _ O
used -X- _ O
by -X- _ O
the -X- _ O
decoder -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
size -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
sentence-level -X- _ O
latent -X- _ O
representation -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
256. -X- _ B-HyperparameterValue
The -X- _ O
pre-trained -X- _ O
encoder -X- _ O
uses -X- _ O
1024 -X- _ B-HyperparameterValue
as -X- _ O
the -X- _ O
embedding -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
and -X- _ O
GELU -X- _ B-HyperparameterValue
activations -X- _ B-HyperparameterName
( -X- _ O
Hendrycks -X- _ O
and -X- _ O
Gimpel -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
has -X- _ O
4096 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units -X- _ I-HyperparameterName
, -X- _ O
8 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
and -X- _ O
6 -X- _ O
layers. -X- _ B-HyperparameterName
The -X- _ O
randomly -X- _ O
initialized -X- _ O
decoder -X- _ O
has -X- _ O
512 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units -X- _ I-HyperparameterName
, -X- _ O
8 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
and -X- _ O
3 -X- _ B-HyperparameterValue
layers. -X- _ B-HyperparameterName
The -X- _ O
models -X- _ O
are -X- _ O
firstly -X- _ O
trained -X- _ O
with -X- _ O
DAE -X- _ O
loss -X- _ O
( -X- _ O
and -X- _ O
MLE -X- _ O
loss -X- _ O
for -X- _ O
flow-adapter -X- _ O
models -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
first -X- _ O
3 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
, -X- _ O
then -X- _ O
trained -X- _ O
with -X- _ O
all -X- _ O
losses -X- _ O
( -X- _ O
including -X- _ O
the -X- _ O
iterative -X- _ O
back-translation -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
rest -X- _ O
epochs. -X- _ O
The -X- _ O
rest -X- _ O
hyperparameters -X- _ O
are -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
above -X- _ O
. -X- _ O

A.3.2 -X- _ O
WMT -X- _ B-DatasetName
Experiment -X- _ O
We -X- _ O
insert -X- _ O
our -X- _ O
implementation -X- _ O
of -X- _ O
flow-adapter -X- _ O
architecture -X- _ O
into -X- _ O
the -X- _ O
codebase -X- _ O
of -X- _ O
XLM -X- _ B-TaskName
12 -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
pretrained -X- _ O
model -X- _ O
of -X- _ O
en-fr -X- _ B-DatasetName
, -X- _ O
en-de -X- _ B-DatasetName
and -X- _ O
en-ro -X- _ B-DatasetName
from -X- _ O
them. -X- _ O
We -X- _ O
also -X- _ O
follow -X- _ O
their -X- _ O
recommended -X- _ O
unsupervised -X- _ O
training -X- _ O
settings. -X- _ O
For -X- _ O
the -X- _ O
flow-related -X- _ O
hyperparameters -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
256 -X- _ B-HyperparameterValue
as -X- _ O
the -X- _ O
size -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
sentence-level -X- _ O
latent -X- _ O
representation. -X- _ O
The -X- _ O
weight -X- _ B-HyperparameterName
of -X- _ O
the -X- _ O
MLE -X- _ O
loss -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
0.01 -X- _ B-HyperparameterValue
. -X- _ O

WeCheck -X- _ B-MethodName
: -X- _ O
Strong -X- _ B-MethodName
Factual -X- _ I-MethodName
Consistency -X- _ I-MethodName
Checker -X- _ I-MethodName
via -X- _ I-MethodName
Weakly -X- _ I-MethodName
Supervised -X- _ I-MethodName
Learning -X- _ I-MethodName

A -X- _ O
crucial -X- _ O
issue -X- _ O
of -X- _ O
current -X- _ O
text -X- _ O
generation -X- _ O
models -X- _ O
is -X- _ O
that -X- _ O
they -X- _ O
often -X- _ O
uncontrollably -X- _ O
generate -X- _ O
text -X- _ O
that -X- _ O
is -X- _ O
factually -X- _ O
inconsistent -X- _ O
with -X- _ O
inputs. -X- _ O
Due -X- _ O
to -X- _ O
lack -X- _ O
of -X- _ O
annotated -X- _ O
data -X- _ O
, -X- _ O
existing -X- _ O
factual -X- _ O
consistency -X- _ O
metrics -X- _ O
usually -X- _ O
train -X- _ O
evaluation -X- _ O
models -X- _ O
on -X- _ O
synthetic -X- _ O
texts -X- _ O
or -X- _ O
directly -X- _ O
transfer -X- _ O
from -X- _ O
other -X- _ O
related -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
( -X- _ O
QA -X- _ B-TaskName
) -X- _ O
and -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
( -X- _ O
NLI -X- _ B-TaskName
) -X- _ O
. -X- _ O
Bias -X- _ O
in -X- _ O
synthetic -X- _ O
text -X- _ O
or -X- _ O
upstream -X- _ O
tasks -X- _ O
makes -X- _ O
them -X- _ O
perform -X- _ O
poorly -X- _ O
on -X- _ O
text -X- _ O
actually -X- _ O
generated -X- _ O
by -X- _ O
language -X- _ O
models -X- _ O
, -X- _ O
especially -X- _ O
for -X- _ O
general -X- _ O
evaluation -X- _ O
for -X- _ O
various -X- _ O
tasks. -X- _ O
To -X- _ O
alleviate -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
weakly -X- _ O
supervised -X- _ O
framework -X- _ O
named -X- _ O
WeCheck -X- _ B-MethodName
that -X- _ O
is -X- _ O
directly -X- _ O
trained -X- _ O
on -X- _ O
actual -X- _ O
generated -X- _ O
samples -X- _ O
from -X- _ O
language -X- _ O
models -X- _ O
with -X- _ O
weakly -X- _ O
annotated -X- _ O
labels. -X- _ O
WeCheck -X- _ B-MethodName
first -X- _ O
utilizes -X- _ O
a -X- _ O
generative -X- _ O
model -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
factual -X- _ O
labels -X- _ O
of -X- _ O
generated -X- _ O
samples -X- _ O
by -X- _ O
aggregating -X- _ O
weak -X- _ O
labels -X- _ O
from -X- _ O
multiple -X- _ O
resources. -X- _ O
Next -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
simple -X- _ O
noise-aware -X- _ O
classification -X- _ O
model -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
metric -X- _ O
using -X- _ O
the -X- _ O
inferred -X- _ O
weakly -X- _ O
supervised -X- _ O
information. -X- _ O
Comprehensive -X- _ O
experiments -X- _ O
on -X- _ O
various -X- _ O
tasks -X- _ O
demonstrate -X- _ O
the -X- _ O
strong -X- _ O
performance -X- _ O
of -X- _ O
WeCheck -X- _ B-MethodName
, -X- _ O
achieving -X- _ O
an -X- _ O
average -X- _ B-MetricName
absolute -X- _ I-MetricName
improvement -X- _ I-MetricName
of -X- _ O
3.3 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
the -X- _ O
TRUE -X- _ B-TaskName
benchmark -X- _ O
over -X- _ O
11B -X- _ O
state-of-the-art -X- _ O
methods -X- _ O
using -X- _ O
only -X- _ O
435M -X- _ O
parameters. -X- _ O
Furthermore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
up -X- _ O
to -X- _ O
30× -X- _ O
faster -X- _ O
than -X- _ O
previous -X- _ O
evaluation -X- _ O
methods -X- _ O
, -X- _ O
greatly -X- _ O
improving -X- _ O
the -X- _ O
accuracy -X- _ O
and -X- _ O
efficiency -X- _ O
of -X- _ O
factual -X- _ O
consistency -X- _ O
evaluation. -X- _ O
1 -X- _ O
* -X- _ O
Work -X- _ O
is -X- _ O
done -X- _ O
during -X- _ O
an -X- _ O
internship -X- _ O
at -X- _ O
Baidu -X- _ O
Inc -X- _ O
. -X- _ O

Introduction -X- _ O
The -X- _ O
research -X- _ O
of -X- _ O
text -X- _ O
generation -X- _ O
has -X- _ O
achieved -X- _ O
significant -X- _ O
progress -X- _ O
in -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
still -X- _ O
suffers -X- _ O
the -X- _ O
main -X- _ O
issue -X- _ O
of -X- _ O
generating -X- _ O
output -X- _ O
which -X- _ O
is -X- _ O
factually -X- _ O
inconsistent -X- _ O
with -X- _ O
the -X- _ O
given -X- _ O
inputs -X- _ O
( -X- _ O
Maynez -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
tackle -X- _ O
this -X- _ O
issue -X- _ O
, -X- _ O
various -X- _ O
metrics -X- _ O
have -X- _ O
been -X- _ O
designed -X- _ O
to -X- _ O
check -X- _ O
the -X- _ O
consistency -X- _ O
between -X- _ O
generated -X- _ O
text -X- _ O
and -X- _ O
the -X- _ O
given -X- _ O
inputs -X- _ O
( -X- _ O
Kryscinski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Scialom -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
we -X- _ O
know -X- _ O
, -X- _ O
how -X- _ O
to -X- _ O
construct -X- _ O
such -X- _ O
a -X- _ O
metric -X- _ O
has -X- _ O
attracted -X- _ O
increasing -X- _ O
attention -X- _ O
in -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
fields -X- _ O
( -X- _ O
Wu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
, -X- _ O
including -X- _ O
text -X- _ O
summarization -X- _ O
( -X- _ O
Kryscinski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Wu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
, -X- _ O
dialogue -X- _ O
generation -X- _ O
( -X- _ O
Welleck -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
text -X- _ O
simplification -X- _ O
( -X- _ O
Devaraj -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

Existing -X- _ O
factual -X- _ O
metrics -X- _ O
can -X- _ O
be -X- _ O
classified -X- _ O
into -X- _ O
two -X- _ O
types -X- _ O
: -X- _ O
one -X- _ O
based -X- _ O
on -X- _ O
synthetic -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
based -X- _ O
on -X- _ O
task -X- _ O
transfer. -X- _ O
Synthetic-data -X- _ O
based -X- _ O
metrics -X- _ O
( -X- _ O
Kryscinski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Mishra -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
apply -X- _ O
data -X- _ O
augmentation -X- _ O
techniques -X- _ O
to -X- _ O
construct -X- _ O
factual -X- _ O
and -X- _ O
non-factual -X- _ O
texts -X- _ O
as -X- _ O
positive -X- _ O
and -X- _ O
negative -X- _ O
samples -X- _ O
, -X- _ O
respectively. -X- _ O
Metrics -X- _ O
trained -X- _ O
from -X- _ O
these -X- _ O
synthetic -X- _ O
samples -X- _ O
often -X- _ O
perform -X- _ O
poorly -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
significant -X- _ O
mismatch -X- _ O
between -X- _ O
features -X- _ O
of -X- _ O
actual -X- _ O
generated -X- _ O
and -X- _ O
synthetic -X- _ O
text -X- _ O
( -X- _ O
e.g. -X- _ O
distribution -X- _ O
of -X- _ O
factual -X- _ O
errors -X- _ O
) -X- _ O
( -X- _ O
Goyal -X- _ O
and -X- _ O
Durrett -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Task-transfer -X- _ O
based -X- _ O
metrics -X- _ O
utilize -X- _ O
the -X- _ O
reasoning -X- _ O
ability -X- _ O
of -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
relevant -X- _ O
upstream -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
( -X- _ O
NLI -X- _ B-TaskName
) -X- _ O
( -X- _ O
Falke -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Laban -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
and -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
( -X- _ O
QA -X- _ B-TaskName
) -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
and -X- _ O
directly -X- _ O
apply -X- _ O
them -X- _ O
to -X- _ O
evaluate -X- _ O
factual -X- _ O
consistency -X- _ O
without -X- _ O
any -X- _ O
adaption -X- _ O
. -X- _ O

As -X- _ O
described -X- _ O
above -X- _ O
, -X- _ O
previous -X- _ O
metrics -X- _ O
are -X- _ O
learned -X- _ O
indirectly -X- _ O
from -X- _ O
other -X- _ O
related -X- _ O
resources -X- _ O
but -X- _ O
without -X- _ O
seeing -X- _ O
the -X- _ O
actual -X- _ O
generated -X- _ O
text. -X- _ O
In -X- _ O
such -X- _ O
cases -X- _ O
, -X- _ O
they -X- _ O
may -X- _ O
overfit -X- _ O
to -X- _ O
their -X- _ O
upstream -X- _ O
tasks -X- _ O
and -X- _ O
fail -X- _ O
to -X- _ O
generalize -X- _ O
to -X- _ O
actual -X- _ O
generated -X- _ O
samples -X- _ O
that -X- _ O
have -X- _ O
significantly -X- _ O
different -X- _ O
data -X- _ O
features. -X- _ O
Figure -X- _ O
1 -X- _ O
illustrates -X- _ O
the -X- _ O
probability -X- _ O
density -X- _ O
of -X- _ O
three -X- _ O
metrics -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
horizontal -X- _ O
axis -X- _ O
is -X- _ O
metric -X- _ O
scores -X- _ O
and -X- _ O
the -X- _ O
vertical -X- _ O
axis -X- _ O
is -X- _ O
the -X- _ O
score -X- _ O
density. -X- _ O
Though -X- _ O
these -X- _ O
metrics -X- _ O
are -X- _ O
comparable -X- _ O
in -X- _ O
performance -X- _ O
, -X- _ O
they -X- _ O
vary -X- _ O
significantly -X- _ O
in -X- _ O
probability -X- _ O
distributions -X- _ O
, -X- _ O
especially -X- _ O
in -X- _ O
the -X- _ O
XSUM -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
where -X- _ O
sample -X- _ O
features -X- _ O
are -X- _ O
greatly -X- _ O
different -X- _ O
from -X- _ O
upstream -X- _ O
tasks -X- _ O
of -X- _ O
these -X- _ O
metrics -X- _ O
2 -X- _ O
, -X- _ O
NLI-warmup -X- _ B-MethodName
is -X- _ O
extremely -X- _ O
confident -X- _ O
in -X- _ O
predicting -X- _ O
both -X- _ O
very -X- _ O
high -X- _ O
and -X- _ O
low -X- _ O
scores -X- _ O
while -X- _ O
SUMMAC -X- _ B-MethodName
and -X- _ O
QAFact -X- _ B-MethodName
are -X- _ O
only -X- _ O
confident -X- _ O
in -X- _ O
predicting -X- _ O
low -X- _ O
scores -X- _ O
3 -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
during -X- _ O
testing -X- _ O
, -X- _ O
ensembling -X- _ O
different -X- _ O
metric -X- _ O
scores -X- _ O
by -X- _ O
simply -X- _ O
averaging -X- _ O
will -X- _ O
further -X- _ O
improve -X- _ O
their -X- _ O
performance -X- _ O
( -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
also -X- _ O
implies -X- _ O
that -X- _ O
the -X- _ O
evaluation -X- _ O
metrics -X- _ O
learned -X- _ O
from -X- _ O
different -X- _ O
resources -X- _ O
are -X- _ O
also -X- _ O
complementary -X- _ O
. -X- _ O

To -X- _ O
bridge -X- _ O
the -X- _ O
gap -X- _ O
between -X- _ O
training -X- _ O
and -X- _ O
testing -X- _ O
and -X- _ O
mitigate -X- _ O
the -X- _ O
scarcity -X- _ O
of -X- _ O
labeled -X- _ O
data -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
WeCheck -X- _ B-MethodName
, -X- _ O
a -X- _ O
factual -X- _ O
consistency -X- _ O
Checking -X- _ O
framework -X- _ O
based -X- _ O
on -X- _ O
Weakly -X- _ O
supervised -X- _ O
learning. -X- _ O
Specifically -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
learning -X- _ O
paradigm -X- _ O
that -X- _ O
provides -X- _ O
weak -X- _ O
supervision -X- _ O
via -X- _ O
modeling -X- _ O
multiple -X- _ O
label -X- _ O
sources -X- _ O
without -X- _ O
access -X- _ O
to -X- _ O
ground -X- _ O
truth. -X- _ O
Different -X- _ O
from -X- _ O
previous -X- _ O
metrics -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
directly -X- _ O
utilizes -X- _ O
the -X- _ O
abundant -X- _ O
actual -X- _ O
generated -X- _ O
samples -X- _ O
bootstrapped -X- _ O
from -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
target -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
BART -X- _ O
on -X- _ O
text -X- _ O
summarization. -X- _ O
Then -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
follows -X- _ O
a -X- _ O
two-step -X- _ O
pipeline -X- _ O
consisting -X- _ O
of -X- _ O
weak -X- _ O
annotation -X- _ O
and -X- _ O
noiseaware -X- _ O
fine-tuning -X- _ O
to -X- _ O
get -X- _ O
the -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
weak -X- _ O
annotation -X- _ O
step -X- _ O
, -X- _ O
by -X- _ O
aggregating -X- _ O
multiple -X- _ O
weak -X- _ O
supervision -X- _ O
resources -X- _ O
, -X- _ O
we -X- _ O
infer -X- _ O
the -X- _ O
unknown -X- _ O
ground -X- _ O
truth -X- _ O
label -X- _ O
of -X- _ O
a -X- _ O
sample. -X- _ O
To -X- _ O
reach -X- _ O
this -X- _ O
goal -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
provide -X- _ O
each -X- _ O
sample -X- _ O
with -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
calculated -X- _ O
from -X- _ O
various -X- _ O
other -X- _ O
metrics. -X- _ O
These -X- _ O
metrics -X- _ O
are -X- _ O
learned -X- _ O
from -X- _ O
various -X- _ O
resources -X- _ O
or -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
QA-based -X- _ B-TaskName
metrics -X- _ O
and -X- _ O
NLI-based -X- _ B-TaskName
metrics. -X- _ O
After -X- _ O
unifying -X- _ O
and -X- _ O
filtering -X- _ O
these -X- _ O
signals -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
generative -X- _ O
labeling -X- _ O
model -X- _ O
that -X- _ O
models -X- _ O
agreements -X- _ O
and -X- _ O
disagreements -X- _ O
between -X- _ O
them -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
likelihood -X- _ O
of -X- _ O
their -X- _ O
latent -X- _ O
ground -X- _ O
truth -X- _ O
label. -X- _ O
The -X- _ O
inferred -X- _ O
ground -X- _ O
truth -X- _ O
likelihood -X- _ O
is -X- _ O
then -X- _ O
treated -X- _ O
as -X- _ O
a -X- _ O
probabilistic -X- _ O
label -X- _ O
to -X- _ O
provide -X- _ O
weak -X- _ O
supervision. -X- _ O
In -X- _ O
the -X- _ O
second -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
noise-aware -X- _ O
fine-tuning -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
target -X- _ O
metric -X- _ O
model. -X- _ O
It -X- _ O
is -X- _ O
noted -X- _ O
here -X- _ O
, -X- _ O
the -X- _ O
weak -X- _ O
annotation -X- _ O
also -X- _ O
brings -X- _ O
noises -X- _ O
to -X- _ O
the -X- _ O
supervision -X- _ O
signal -X- _ O
and -X- _ O
brings -X- _ O
new -X- _ O
challenges -X- _ O
to -X- _ O
the -X- _ O
model -X- _ O
optimization -X- _ O
process. -X- _ O
As -X- _ O
a -X- _ O
solution -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
warmup -X- _ O
our -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
with -X- _ O
NLI -X- _ B-TaskName
data -X- _ O
for -X- _ O
a -X- _ O
better -X- _ O
initialization -X- _ O
before -X- _ O
weakly -X- _ O
supervised -X- _ O
training. -X- _ O
Then -X- _ O
, -X- _ O
after -X- _ O
filtering -X- _ O
out -X- _ O
samples -X- _ O
that -X- _ O
are -X- _ O
likely -X- _ O
to -X- _ O
be -X- _ O
noisy -X- _ O
, -X- _ O
we -X- _ O
finetune -X- _ O
our -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
with -X- _ O
weak -X- _ O
annotations. -X- _ O
In -X- _ O
summary -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
could -X- _ O
learn -X- _ O
how -X- _ O
to -X- _ O
utilize -X- _ O
multiple -X- _ O
resources -X- _ O
for -X- _ O
weak -X- _ O
annotation -X- _ O
while -X- _ O
recognizing -X- _ O
and -X- _ O
filtering -X- _ O
the -X- _ O
potential -X- _ O
noises -X- _ O
accompanied -X- _ O
by -X- _ O
weak -X- _ O
supervision -X- _ O
. -X- _ O

Experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
WeCheck -X- _ B-MethodName
not -X- _ O
only -X- _ O
achieves -X- _ O
state-of-the-art -X- _ O
performance -X- _ O
but -X- _ O
also -X- _ O
is -X- _ O
computationally -X- _ O
efficient. -X- _ O
On -X- _ O
the -X- _ O
TRUE -X- _ O
benchmark -X- _ O
( -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
current -X- _ O
most -X- _ O
comprehensive -X- _ O
benchmark -X- _ O
for -X- _ O
factual -X- _ O
consistency -X- _ O
evaluation -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
obtains -X- _ O
an -X- _ O
average -X- _ B-MetricName
ROC -X- _ I-MetricName
AUC -X- _ I-MetricName
of -X- _ O
84.8 -X- _ B-MetricValue
, -X- _ O
3.3 -X- _ B-MetricValue
% -X- _ I-MetricValue
absolute -X- _ O
improvement -X- _ O
over -X- _ O
previous -X- _ O
11B -X- _ O
pre-trained -X- _ O
task -X- _ O
transferred -X- _ O
metrics -X- _ O
with -X- _ O
only -X- _ O
a -X- _ O
size -X- _ O
of -X- _ O
435M -X- _ O
parameters. -X- _ O
Moreover -X- _ O
, -X- _ O
it -X- _ O
's -X- _ O
much -X- _ O
more -X- _ O
stable -X- _ O
for -X- _ O
various -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
with -X- _ O
much -X- _ O
lower -X- _ O
variance -X- _ O
on -X- _ O
different -X- _ O
tasks. -X- _ O
Thus -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
a -X- _ O
simple -X- _ O
but -X- _ O
more -X- _ O
effective -X- _ O
and -X- _ O
efficient -X- _ O
metric -X- _ O
for -X- _ O
factual -X- _ O
consistency -X- _ O
evaluation -X- _ O
. -X- _ O

• -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
factual -X- _ O
consistency -X- _ O
evaluation -X- _ O
metric -X- _ O
based -X- _ O
on -X- _ O
weakly -X- _ O
supervised -X- _ O
learning -X- _ O
, -X- _ O
namely -X- _ O
WeCheck -X- _ B-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
directly -X- _ O
trained -X- _ O
on -X- _ O
actual -X- _ O
generated -X- _ O
samples -X- _ O
from -X- _ O
language -X- _ O
models -X- _ O
with -X- _ O
weakly -X- _ O
annotated -X- _ O
labels -X- _ O
. -X- _ O

• -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
both -X- _ O
effective -X- _ O
and -X- _ O
efficient -X- _ O
achieving -X- _ O
3.3 -X- _ O
% -X- _ O
absolute -X- _ O
improvement -X- _ O
and -X- _ O
up -X- _ O
to -X- _ O
30 -X- _ O
times -X- _ O
faster -X- _ O
comparing -X- _ O
with -X- _ O
previous -X- _ O
state-ofart -X- _ O
metrics -X- _ O
. -X- _ O

• -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
a -X- _ O
general -X- _ O
metric -X- _ O
which -X- _ O
is -X- _ O
also -X- _ O
more -X- _ O
stable -X- _ O
on -X- _ O
various -X- _ O
generation -X- _ O
tasks -X- _ O
and -X- _ O
datasets -X- _ O
than -X- _ O
previous -X- _ O
methods -X- _ O
. -X- _ O

WeCheck -X- _ B-MethodName
Framework -X- _ O
Figure -X- _ O
2 -X- _ O
illustrates -X- _ O
the -X- _ O
two-step -X- _ O
pipeline -X- _ O
of -X- _ O
WeCheck -X- _ B-MethodName
framework. -X- _ O
In -X- _ O
the -X- _ O
upper -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
figure -X- _ O
, -X- _ O
during -X- _ O
the -X- _ O
weak -X- _ O
annotation -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
calculate -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
for -X- _ O
each -X- _ O
< -X- _ O
l -X- _ O
a -X- _ O
t -X- _ O
e -X- _ O
x -X- _ O
i -X- _ O
t -X- _ O
s -X- _ O
h -X- _ O
a -X- _ O
1 -X- _ O
_ -X- _ O
b -X- _ O
a -X- _ O
s -X- _ O
e -X- _ O
6 -X- _ O
4 -X- _ O
= -X- _ O
" -X- _ O
p -X- _ O
c -X- _ O
m -X- _ O
T -X- _ O
Y -X- _ O
4 -X- _ O
n -X- _ O
3 -X- _ O
q -X- _ O
a -X- _ O
J -X- _ O
V -X- _ O
+ -X- _ O
S -X- _ O
I -X- _ O
l -X- _ O
z -X- _ O
W -X- _ O
R -X- _ O
D -X- _ O
o -X- _ O
c -X- _ O
H -X- _ O
p -X- _ O
i -X- _ O
s -X- _ O
0 -X- _ O
= -X- _ O
" -X- _ O
> -X- _ O
A -X- _ O

Noise-aware -X- _ O
fine-tuning -X- _ O
first -X- _ O
warmup -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
with -X- _ O
NLI -X- _ B-TaskName
data -X- _ O
and -X- _ O
training -X- _ O
it -X- _ O
with -X- _ O
filtered -X- _ O
probabilistic -X- _ O
labels. -X- _ O
In -X- _ O
the -X- _ O
following -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
our -X- _ O
problem -X- _ O
definition -X- _ O
and -X- _ O
detailed -X- _ O
method -X- _ O
. -X- _ O

Problem -X- _ O
Definition -X- _ O
Factual -X- _ O
Consistency -X- _ O
Evaluation -X- _ O
Given -X- _ O
a -X- _ O
textual -X- _ O
sequence -X- _ O
as -X- _ O
a -X- _ O
premise -X- _ O
, -X- _ O
and -X- _ O
another -X- _ O
textual -X- _ O
sequence -X- _ O
as -X- _ O
a -X- _ O
hypothesis -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
be -X- _ O
a -X- _ O
generated -X- _ O
summary -X- _ O
or -X- _ O
dialogue -X- _ O
, -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
a -X- _ O
factual -X- _ O
consistency -X- _ O
metric -X- _ O
f -X- _ O
θ -X- _ O
is -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
the -X- _ O
hypothesis -X- _ O
is -X- _ O
factual -X- _ O
consistent -X- _ O
given -X- _ O
the -X- _ O
premise. -X- _ O
For -X- _ O
simplicity -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
previous -X- _ O
textual -X- _ O
entailment -X- _ O
based -X- _ O
framework -X- _ O
( -X- _ O
Kryscinski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
takes -X- _ O
x -X- _ O
, -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
hypothesis -X- _ O
and -X- _ O
premise -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
format -X- _ O
and -X- _ O
unifies -X- _ O
the -X- _ O
evaluation -X- _ O
as -X- _ O
a -X- _ O
binary -X- _ O
classification -X- _ O
problem -X- _ O
: -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
∈ -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
] -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
predicted -X- _ O
logit -X- _ O
indicates -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
x -X- _ O
being -X- _ O
factually -X- _ O
consistent. -X- _ O
Another -X- _ O
advantage -X- _ O
of -X- _ O
using -X- _ O
the -X- _ O
entailment-based -X- _ O
framework -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
effective -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
time -X- _ O
complexity -X- _ O
compared -X- _ O
with -X- _ O
other -X- _ O
methods -X- _ O
( -X- _ O
Laban -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
Taking -X- _ O
f -X- _ O
θ -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
to -X- _ O
train -X- _ O
f -X- _ O
θ -X- _ O
into -X- _ O
an -X- _ O
efficient -X- _ O
factual -X- _ O
consistency -X- _ O
metric -X- _ O
. -X- _ O

Weakly -X- _ O
Supervised -X- _ O
Training -X- _ O
In -X- _ O
our -X- _ O
weakly -X- _ O
supervised -X- _ O
settings -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
bootstrap -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
sam-ples -X- _ O
from -X- _ O
the -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
text -X- _ O
summarization -X- _ O
, -X- _ O
and -X- _ O
dialogue -X- _ O
generation. -X- _ O
Using -X- _ O
various -X- _ O
factual -X- _ O
metrics -X- _ O
trained -X- _ O
from -X- _ O
multiple -X- _ O
resources -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
each -X- _ O
sample -X- _ O
x -X- _ O
with -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
weak -X- _ O
signals -X- _ O
λ -X- _ O
= -X- _ O
( -X- _ O
λ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
λ -X- _ O
k -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
each -X- _ O
λ -X- _ O
i -X- _ O
is -X- _ O
a -X- _ O
logit -X- _ O
separately -X- _ O
calculated -X- _ O
by -X- _ O
a -X- _ O
metric. -X- _ O
We -X- _ O
treat -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
label -X- _ O
y -X- _ O
of -X- _ O
x -X- _ O
as -X- _ O
a -X- _ O
hidden -X- _ O
variable -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
estimated -X- _ O
by -X- _ O
aggregating -X- _ O
λ. -X- _ O
To -X- _ O
reach -X- _ O
this -X- _ O
goal -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
labeling -X- _ O
model -X- _ O
p -X- _ O
ϕ -X- _ O
to -X- _ O
model -X- _ O
agreements -X- _ O
and -X- _ O
disagreements -X- _ O
relations -X- _ O
between -X- _ O
weak -X- _ O
signals -X- _ O
in -X- _ O
λ -X- _ O
and -X- _ O
estimate -X- _ O
the -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
truth -X- _ O
label -X- _ O
, -X- _ O
p -X- _ O
ϕ -X- _ O
( -X- _ O
y|λ -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
p -X- _ O
ϕ -X- _ O
( -X- _ O
y|λ -X- _ O
) -X- _ O
to -X- _ O
supervise -X- _ O
the -X- _ O
metric -X- _ O
model -X- _ O
f -X- _ O
θ -X- _ O
. -X- _ O

Weak -X- _ O
Annotation -X- _ O
To -X- _ O
provide -X- _ O
weak -X- _ O
supervision -X- _ O
for -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
data -X- _ O
programming -X- _ O
, -X- _ O
a -X- _ O
weakly -X- _ O
supervised -X- _ O
learning -X- _ O
paradigm -X- _ O
based -X- _ O
on -X- _ O
modeling -X- _ O
multiple -X- _ O
label -X- _ O
sources. -X- _ O
However -X- _ O
, -X- _ O
in -X- _ O
data -X- _ O
programming -X- _ O
, -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
are -X- _ O
often -X- _ O
produced -X- _ O
by -X- _ O
various -X- _ O
checking -X- _ O
clauses -X- _ O
, -X- _ O
e.g. -X- _ O
whether -X- _ O
word -X- _ O
" -X- _ O
causes -X- _ O
" -X- _ O
appears -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
? -X- _ O
and -X- _ O
produce -X- _ O
a -X- _ O
discrete -X- _ O
weak -X- _ O
signal -X- _ O
λ -X- _ O
i -X- _ O
∈ -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
−1 -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
0 -X- _ O
/ -X- _ O
1 -X- _ O
stands -X- _ O
for -X- _ O
a -X- _ O
vote -X- _ O
for -X- _ O
positive -X- _ O
/ -X- _ O
negative -X- _ O
label -X- _ O
and -X- _ O
−1 -X- _ O
stands -X- _ O
for -X- _ O
a -X- _ O
abstain -X- _ O
vote. -X- _ O
However -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
scenario -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
diversity -X- _ O
of -X- _ O
metric -X- _ O
frameworks -X- _ O
, -X- _ O
outputs -X- _ O
of -X- _ O
different -X- _ O
metrics -X- _ O
often -X- _ O
do -X- _ O
not -X- _ O
share -X- _ O
a -X- _ O
unified -X- _ O
output -X- _ O
format -X- _ O
and -X- _ O
are -X- _ O
usually -X- _ O
continuous. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
QA-based -X- _ B-TaskName
metrics -X- _ O
often -X- _ O
produce -X- _ O
continuous -X- _ O
logits -X- _ O
in -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
NLI-based -X- _ B-TaskName
metrics -X- _ O
often -X- _ O
produce -X- _ O
discrete -X- _ O
labels -X- _ O
of -X- _ O
entailment -X- _ O
or -X- _ O
contradiction. -X- _ O
Thus -X- _ O
, -X- _ O
the -X- _ O
first -X- _ O
thing -X- _ O
before -X- _ O
training -X- _ O
the -X- _ O
labeling -X- _ O
model -X- _ O
is -X- _ O
to -X- _ O
unify -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
by -X- _ O
a -X- _ O
mapping -X- _ O
function -X- _ O
, -X- _ O
m -X- _ O
( -X- _ O
λ -X- _ O
i -X- _ O
) -X- _ O
→ -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
−1 -X- _ O
} -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
model -X- _ O
the -X- _ O
transformed -X- _ O
λ -X- _ O
by -X- _ O
a -X- _ O
data -X- _ O
programming -X- _ O
based -X- _ O
labeling -X- _ O
model -X- _ O
. -X- _ O

Weak -X- _ O
Signal -X- _ O
Unification -X- _ O
We -X- _ O
first -X- _ O
unify -X- _ O
all -X- _ O
the -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
from -X- _ O
different -X- _ O
metrics -X- _ O
into -X- _ O
the -X- _ O
same -X- _ O
format -X- _ O
, -X- _ O
a -X- _ O
logit -X- _ O
λ -X- _ O
i -X- _ O
∈ -X- _ O
[ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
] -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
metric -X- _ O
with -X- _ O
single -X- _ O
logit -X- _ O
output -X- _ O
, -X- _ O
we -X- _ O
directly -X- _ O
use -X- _ O
its -X- _ O
output -X- _ O
as -X- _ O
λ -X- _ O
i -X- _ O
. -X- _ O
For -X- _ O
multi-label -X- _ O
classification -X- _ O
output -X- _ O
, -X- _ O
we -X- _ O
select -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
predicting -X- _ O
entailment. -X- _ O
Notice -X- _ O
that -X- _ O
all -X- _ O
the -X- _ O
signals -X- _ O
predicted -X- _ O
by -X- _ O
imperfect -X- _ O
metrics -X- _ O
will -X- _ O
introduce -X- _ O
a -X- _ O
portion -X- _ O
of -X- _ O
noises. -X- _ O
For -X- _ O
a -X- _ O
more -X- _ O
reliable -X- _ O
signal -X- _ O
, -X- _ O
the -X- _ O
core -X- _ O
idea -X- _ O
for -X- _ O
designing -X- _ O
a -X- _ O
mapping -X- _ O
function -X- _ O
m -X- _ O
is -X- _ O
to -X- _ O
map -X- _ O
signals -X- _ O
that -X- _ O
the -X- _ O
metric -X- _ O
has -X- _ O
high -X- _ O
confidence -X- _ O
into -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
} -X- _ O
and -X- _ O
abstain -X- _ O
low-confidence -X- _ O
signals -X- _ O
by -X- _ O
mapping -X- _ O
them -X- _ O
to -X- _ O
−1 -X- _ O
. -X- _ O

Generally -X- _ O
, -X- _ O
this -X- _ O
can -X- _ O
be -X- _ O
achieved -X- _ O
by -X- _ O
setting -X- _ O
thresholds -X- _ O
on -X- _ O
signals. -X- _ O
But -X- _ O
another -X- _ O
important -X- _ O
issue -X- _ O
to -X- _ O
be -X- _ O
noticed -X- _ O
is -X- _ O
that -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
signal -X- _ O
distributions -X- _ O
vary -X- _ O
significantly -X- _ O
across -X- _ O
metrics -X- _ O
and -X- _ O
datasets -X- _ O
, -X- _ O
which -X- _ O
makes -X- _ O
threshold -X- _ O
selection -X- _ O
difficult. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
instead -X- _ O
dynamically -X- _ O
determine -X- _ O
thresholds -X- _ O
by -X- _ O
setting -X- _ O
constant -X- _ O
probability -X- _ O
mass -X- _ O
that -X- _ O
contains -X- _ O
the -X- _ O
highest -X- _ O
confidence. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
to -X- _ O
map -X- _ O
the -X- _ O
lowest -X- _ O
p -X- _ B-HyperparameterName
− -X- _ I-HyperparameterName
percent -X- _ O
and -X- _ O
the -X- _ O
highest -X- _ O
p -X- _ B-HyperparameterName
+ -X- _ I-HyperparameterName
percent -X- _ O
of -X- _ O
signal -X- _ O
scores -X- _ O
into -X- _ O
label -X- _ O
0 -X- _ O
and -X- _ O
1 -X- _ O
, -X- _ O
separately -X- _ O
, -X- _ O
and -X- _ O
map -X- _ O
the -X- _ O
rest -X- _ O
interval -X- _ O
of -X- _ O
low-confident -X- _ O
scores -X- _ O
into -X- _ O
-1. -X- _ O
Given -X- _ O
the -X- _ O
inverse -X- _ O
cumulative -X- _ O
distribution -X- _ O
function -X- _ O
of -X- _ O
the -X- _ O
i-th -X- _ O
signal -X- _ O
F -X- _ O
i -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
calculate -X- _ O
its -X- _ O
positive -X- _ O
and -X- _ O
negative -X- _ O
threshold -X- _ O
γ -X- _ O
+ -X- _ O
i -X- _ O
and -X- _ O
γ -X- _ O
− -X- _ O
i -X- _ O
by -X- _ O
: -X- _ O

For -X- _ O
simplicity -X- _ O
, -X- _ O
we -X- _ O
share -X- _ O
p -X- _ B-HyperparameterName
− -X- _ I-HyperparameterName
and -X- _ O
p -X- _ B-HyperparameterName
+ -X- _ I-HyperparameterName
across -X- _ O
different -X- _ O
resources -X- _ O
and -X- _ O
datasets. -X- _ O
By -X- _ O
applying -X- _ O
the -X- _ O
mapping -X- _ O
function -X- _ O
, -X- _ O
we -X- _ O
unify -X- _ O
each -X- _ O
λ -X- _ O
i -X- _ O
into -X- _ O
a -X- _ O
discrete -X- _ O
label -X- _ O
in -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
−1 -X- _ O
} -X- _ O
. -X- _ O

where -X- _ O
α -X- _ B-HyperparameterName
i -X- _ I-HyperparameterName
, -X- _ O
β -X- _ B-HyperparameterName
i -X- _ I-HyperparameterName
are -X- _ O
learnable -X- _ O
hyper-parameters. -X- _ O
Given -X- _ O
all -X- _ O
samples -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
labeling -X- _ O
model -X- _ O
by -X- _ O
optimizing -X- _ O
: -X- _ O

2.3 -X- _ O
Noise -X- _ O
Aware -X- _ O
Fine-tuning -X- _ O
NLI -X- _ O
Warmup -X- _ O
After -X- _ O
we -X- _ O
get -X- _ O
the -X- _ O
labeling -X- _ O
model -X- _ O
p -X- _ O
ϕ -X- _ O
, -X- _ O
the -X- _ O
next -X- _ O
step -X- _ O
is -X- _ O
to -X- _ O
train -X- _ O
our -X- _ O
metric -X- _ O
model -X- _ O
f -X- _ O
θ -X- _ O
with -X- _ O
the -X- _ O
weak -X- _ O
supervision -X- _ O
inferred -X- _ O
by -X- _ O
it. -X- _ O
But -X- _ O
in -X- _ O
practice -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
direct -X- _ O
training -X- _ O
with -X- _ O
weak -X- _ O
supervision -X- _ O
will -X- _ O
cause -X- _ O
the -X- _ O
model -X- _ O
easily -X- _ O
converges -X- _ O
to -X- _ O
the -X- _ O
local -X- _ O
minima. -X- _ O
This -X- _ O
may -X- _ O
because -X- _ O
reasoning -X- _ O
over -X- _ O
a -X- _ O
long -X- _ O
range -X- _ O
of -X- _ O
context -X- _ O
is -X- _ O
challenging -X- _ O
and -X- _ O
weak -X- _ O
supervisions -X- _ O
are -X- _ O
also -X- _ O
potential -X- _ O
to -X- _ O
be -X- _ O
noisy. -X- _ O
These -X- _ O
problems -X- _ O
cause -X- _ O
great -X- _ O
difficulties -X- _ O
in -X- _ O
optimization. -X- _ O
Inspired -X- _ O
by -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
curriculum -X- _ O
learning -X- _ O
( -X- _ O
Bengio -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
warmup -X- _ O
our -X- _ O
metric -X- _ O
model -X- _ O
on -X- _ O
NLI -X- _ B-TaskName
, -X- _ O
an -X- _ O
easier -X- _ O
and -X- _ O
closely -X- _ O
related -X- _ O
task. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
mixture -X- _ O
of -X- _ O
four -X- _ O
NLI -X- _ B-TaskName
datasets -X- _ O
, -X- _ O
MultiNLI -X- _ B-DatasetName
( -X- _ O
Williams -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
Fever-NLI -X- _ B-DatasetName
( -X- _ O
Thorne -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
LingNLI -X- _ B-DatasetName
( -X- _ O
Parrish -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
Adversarial-NLI -X- _ B-DatasetName
( -X- _ O
Nie -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
warmed-up -X- _ O
checkpoint -X- _ O
, -X- _ O
our -X- _ O
metric -X- _ O
model -X- _ O
achieves -X- _ O
much -X- _ O
better -X- _ O
results -X- _ O
under -X- _ O
weak -X- _ O
supervision -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
will -X- _ O
later -X- _ O
show -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

Experimental -X- _ O
Settings -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
the -X- _ O
experimental -X- _ O
settings -X- _ O
of -X- _ O
WeCheck -X- _ B-MethodName
including -X- _ O
the -X- _ O
evaluation -X- _ O
benchmark -X- _ O
, -X- _ O
baseline -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
implementation -X- _ O
details -X- _ O
. -X- _ O

TRUE -X- _ B-MetricName
Benchmark -X- _ O
Recent -X- _ O
works -X- _ O
point -X- _ O
out -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
metric -X- _ O
should -X- _ O
be -X- _ O
evaluated -X- _ O
comprehensively -X- _ O
across -X- _ O
multiple -X- _ O
tasks -X- _ O
and -X- _ O
datasets -X- _ O
to -X- _ O
reduce -X- _ O
variance. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
WeCheck -X- _ B-MethodName
on -X- _ O
TRUE -X- _ B-MetricName
( -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
benchmark -X- _ O
consisting -X- _ O
of -X- _ O
11 -X- _ O
datasets -X- _ O
of -X- _ O
4 -X- _ O
tasks -X- _ O
including -X- _ O
text -X- _ O
summarization -X- _ O
, -X- _ O
dialogue -X- _ O
generation -X- _ O
, -X- _ O
paraphrasing -X- _ O
, -X- _ O
and -X- _ O
fact -X- _ O
checking -X- _ O
, -X- _ O
where -X- _ O
each -X- _ O
sample -X- _ O
in -X- _ O
datasets -X- _ O
is -X- _ O
annotated -X- _ O
with -X- _ O
a -X- _ O
binary -X- _ O
label -X- _ O
manually. -X- _ O
We -X- _ O
only -X- _ O
test -X- _ O
on -X- _ O
the -X- _ O
first -X- _ O
three -X- _ O
tasks -X- _ O
as -X- _ O
fact -X- _ O
checking -X- _ O
is -X- _ O
beyond -X- _ O
our -X- _ O
scope. -X- _ O
Following -X- _ O
TRUE -X- _ B-MetricName
, -X- _ O
we -X- _ O
normalize -X- _ O
each -X- _ O
metric -X- _ O
score -X- _ O
into -X- _ O
a -X- _ O
logit -X- _ O
and -X- _ O
report -X- _ O
their -X- _ O
Characteristic -X- _ B-MetricName
Area -X- _ I-MetricName
Under -X- _ I-MetricName
the -X- _ I-MetricName
Curve -X- _ I-MetricName
( -X- _ O
ROC -X- _ B-MetricName
AUC -X- _ I-MetricName
) -X- _ O
w.r.t -X- _ O
binary -X- _ O
logits. -X- _ O
Evaluation -X- _ O
with -X- _ O
ROC -X- _ B-MetricName
AUC -X- _ I-MetricName
does -X- _ O
not -X- _ O
require -X- _ O
metrics -X- _ O
to -X- _ O
set -X- _ O
specific -X- _ O
decision -X- _ O
thresholds. -X- _ O
Details -X- _ O
of -X- _ O
tasks -X- _ O
and -X- _ O
datasets -X- _ O
of -X- _ O
TRUE -X- _ B-MetricName
are -X- _ O
introduce -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

Baseline -X- _ O
We -X- _ O
evaluate -X- _ O
WeCheck -X- _ B-MethodName
by -X- _ O
comparing -X- _ O
with -X- _ O
recently -X- _ O
proposed -X- _ O
metrics. -X- _ O
We -X- _ O
categorize -X- _ O
these -X- _ O
baselines -X- _ O
by -X- _ O
types -X- _ O
of -X- _ O
their -X- _ O
methods. -X- _ O
( -X- _ O
Kryscinski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
BERT-based -X- _ B-MetricName
metric -X- _ O
with -X- _ O
synthetic -X- _ O
training -X- _ O
samples -X- _ O
constructed -X- _ O
from -X- _ O
rule-based -X- _ O
data -X- _ O
augmentation. -X- _ O
SUMMAC -X- _ B-MethodName
( -X- _ O
SCZS -X- _ B-MethodName
) -X- _ O
( -X- _ O
Laban -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
aggregates -X- _ O
sentence-level -X- _ O
entailment -X- _ O
scores -X- _ O
for -X- _ O
the -X- _ O
final -X- _ O
factual -X- _ O
consistency -X- _ O
score. -X- _ O
We -X- _ O
only -X- _ O
report -X- _ O
the -X- _ O
zero-shot -X- _ O
version -X- _ O
SCZS -X- _ B-MethodName
instead -X- _ O
of -X- _ O
the -X- _ O
supervised -X- _ O
version -X- _ O
SCCONV -X- _ B-MethodName
because -X- _ O
it -X- _ O
is -X- _ O
more -X- _ O
effective -X- _ O
on -X- _ O
the -X- _ O
TRUE -X- _ B-MetricName
benchmark. -X- _ O
ANLI -X- _ O
( -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
directly -X- _ O
apply -X- _ O
a -X- _ O
large -X- _ O
11B -X- _ O
T5 -X- _ O
trained -X- _ O
on -X- _ O
Adversarial-NLI -X- _ B-DatasetName
( -X- _ O
Nie -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
dataset -X- _ O
for -X- _ O
fact -X- _ O
checking -X- _ O
and -X- _ O
achieve -X- _ O
SOTA -X- _ O
performance -X- _ O
on -X- _ O
TRUE -X- _ B-MetricName
. -X- _ O

NLI-based -X- _ O
Metrics -X- _ O
FactCC -X- _ O
QA-QG -X- _ O
based -X- _ O
Metrics -X- _ O
QuestEval -X- _ B-MetricName
( -X- _ O
Scialom -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
QA-QG -X- _ B-MetricName
based -X- _ O
metric -X- _ O
that -X- _ O
jointly -X- _ O
measures -X- _ O
factual -X- _ O
consistency -X- _ O
and -X- _ O
semantic -X- _ O
relevance -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
generated -X- _ O
questions -X- _ O
are -X- _ O
weighted -X- _ O
by -X- _ O
a -X- _ O
trained -X- _ O
model. -X- _ O
QAFactEval -X- _ B-MetricName
( -X- _ O
QAFact -X- _ B-MetricName
) -X- _ O
( -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
metric -X- _ O
designed -X- _ O
by -X- _ O
carefully -X- _ O
optimizing -X- _ O
each -X- _ O
component -X- _ O
of -X- _ O
the -X- _ O
QG-QA -X- _ B-MetricName
framework. -X- _ O
Q -X- _ O
2 -X- _ O
, -X- _ O
from -X- _ O
the -X- _ O
version -X- _ O
of -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
replace -X- _ O
all -X- _ O
the -X- _ O
component -X- _ O
of -X- _ O
QA-QG -X- _ B-MetricName
framework -X- _ O
into -X- _ O
T5 -X- _ O
11B -X- _ O
large -X- _ O
models -X- _ O
. -X- _ O

Other -X- _ O
Types -X- _ O
BERTScore -X- _ B-MetricName
( -X- _ O
BERTS -X- _ O
) -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019a -X- _ O
) -X- _ O
measure -X- _ O
the -X- _ O
similarity -X- _ O
of -X- _ O
a -X- _ O
generated -X- _ O
text -X- _ O
and -X- _ O
its -X- _ O
reference -X- _ O
by -X- _ O
aggregating -X- _ O
tokenlevel -X- _ O
similarities -X- _ O
of -X- _ O
their -X- _ O
contextual -X- _ O
representations. -X- _ O
BARTScore -X- _ B-MetricName
( -X- _ O
BARTS -X- _ O
) -X- _ O
( -X- _ O
Yuan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
evaluate -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
generated -X- _ O
text -X- _ O
by -X- _ O
its -X- _ O
modeling -X- _ O
perplexity -X- _ O
of -X- _ O
a -X- _ O
fine-tuned -X- _ O
BART -X- _ O
. -X- _ O

Implementation -X- _ O
Details -X- _ O
All -X- _ O
the -X- _ O
baseline -X- _ O
metrics -X- _ O
are -X- _ O
tested -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
open-sourced -X- _ O
codes. -X- _ O
The -X- _ O
metric -X- _ O
model -X- _ O
of -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
powerful -X- _ O
pre-trained -X- _ O
language -X- _ O
model -X- _ O
DeBERTaV3 -X- _ B-MethodName
( -X- _ O
He -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Following -X- _ O
the -X- _ O
description -X- _ O
in -X- _ O
§ -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
warm -X- _ O
up -X- _ O
DeBERTaV3 -X- _ B-MethodName
on -X- _ O
NLI -X- _ B-TaskName
datasets -X- _ O
and -X- _ O
apply -X- _ O
it -X- _ O
for -X- _ O
weak -X- _ O
supervised -X- _ O
training. -X- _ O
As -X- _ O
regards -X- _ O
to -X- _ O
training -X- _ O
data -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
text -X- _ O
summarization -X- _ O
examples -X- _ O
from -X- _ O
BART -X- _ B-MethodName
fine-tuned -X- _ O
on -X- _ O
CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DM -X- _ I-DatasetName
and -X- _ O
XSum -X- _ B-DatasetName
datasets. -X- _ O
We -X- _ O
sample -X- _ O
dialogue -X- _ O
generation -X- _ O
examples -X- _ O
from -X- _ O
Mem-Net -X- _ B-DatasetName
and -X- _ O
dodecaDialogue -X- _ B-DatasetName
( -X- _ O
Shuster -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
trained -X- _ O
on -X- _ O
WoW -X- _ B-DatasetName
dataset -X- _ O
following -X- _ O
Honovich -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
paraphrase -X- _ O
, -X- _ O
we -X- _ O
directly -X- _ O
use -X- _ O
samples -X- _ O
in -X- _ O
PAWS -X- _ B-DatasetName
since -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
regard -X- _ O
as -X- _ O
a -X- _ O
consistency -X- _ O
checking -X- _ O
dataset -X- _ O
itself. -X- _ O
For -X- _ O
weak -X- _ O
signals -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
QAFact -X- _ O
( -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
SUMMAC -X- _ O
( -X- _ O
Laban -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
NLI -X- _ O
warmed -X- _ O
up -X- _ O
DeBERTaV3 -X- _ B-MethodName
( -X- _ O
NLI-warmup -X- _ B-TaskName
) -X- _ O
as -X- _ O
to -X- _ O
provide -X- _ O
weak -X- _ O
signals -X- _ O
for -X- _ O
each -X- _ O
sample -X- _ O
as -X- _ O
default. -X- _ O
For -X- _ O
weak -X- _ O
signal -X- _ O
unification -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
p -X- _ O
+ -X- _ O
and -X- _ O
p -X- _ O
− -X- _ O
in -X- _ O
mapping -X- _ O
function -X- _ O
m -X- _ O
to -X- _ O
0.75 -X- _ O
and -X- _ O
0.25 -X- _ O
based -X- _ O
on -X- _ O
validation. -X- _ O
For -X- _ O
labeling -X- _ O
model -X- _ O
p -X- _ O
ϕ -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
the -X- _ O
implementation -X- _ O
of -X- _ O
Snorkel -X- _ O
for -X- _ O
efficiency -X- _ O
and -X- _ O
train -X- _ O
it -X- _ O
on -X- _ O
CPUs -X- _ O
with -X- _ O
Adam -X- _ O
optimizer. -X- _ O
For -X- _ O
noise-aware -X- _ O
fine-tuning -X- _ O
, -X- _ O
we -X- _ O
finetune -X- _ O
the -X- _ O
warmed -X- _ O
up -X- _ O
checkpoint -X- _ O
with -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
1e -X- _ B-HyperparameterValue
−6 -X- _ I-HyperparameterValue
, -X- _ O
warmup -X- _ B-HyperparameterName
steps -X- _ I-HyperparameterName
of -X- _ O
500 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
the -X- _ O
total -X- _ O
training -X- _ O
steps -X- _ B-HyperparameterName
of -X- _ O
3 -X- _ B-MetricName
epoch. -X- _ O
We -X- _ O
train -X- _ O
on -X- _ O
4 -X- _ O
NVIDIA -X- _ O
Tesla -X- _ O
V100 -X- _ O
GPUs -X- _ O
, -X- _ O
and -X- _ O
it -X- _ O
takes -X- _ O
around -X- _ O
only -X- _ O
5000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
to -X- _ O
reach -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
. -X- _ O

Results -X- _ O
The -X- _ O
experimental -X- _ O
results -X- _ O
on -X- _ O
TRUE -X- _ B-MetricName
are -X- _ O
reported -X- _ O
in -X- _ O
and -X- _ O
closely -X- _ O
related -X- _ O
task -X- _ O
, -X- _ O
provides -X- _ O
a -X- _ O
much -X- _ O
better -X- _ O
initialization -X- _ O
for -X- _ O
training -X- _ O
with -X- _ O
weak -X- _ O
supervision. -X- _ O
For -X- _ O
noise-aware -X- _ O
finetuning -X- _ O
, -X- _ O
we -X- _ O
study -X- _ O
how -X- _ O
filtering -X- _ O
potential -X- _ O
noisy -X- _ O
samples -X- _ O
( -X- _ O
Eq. -X- _ O
7 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
probabilistic -X- _ O
label -X- _ O
( -X- _ O
Eq. -X- _ O
6 -X- _ O
) -X- _ O
affect -X- _ O
the -X- _ O
overall -X- _ O
performance. -X- _ O
After -X- _ O
removing -X- _ O
noise -X- _ O
filtering -X- _ O
( -X- _ O
w -X- _ O
/ -X- _ O
o -X- _ O
Noise -X- _ O
Filter -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
drops -X- _ O
around -X- _ O
1-2 -X- _ O
points -X- _ O
in -X- _ O
each -X- _ O
task -X- _ O
and -X- _ O
dataset -X- _ O
in -X- _ O
average. -X- _ O
By -X- _ O
replacing -X- _ O
the -X- _ O
probabilistic -X- _ O
labels -X- _ O
into -X- _ O
hard -X- _ O
labels -X- _ O
( -X- _ O
w -X- _ O
/ -X- _ O
Hard -X- _ O
Label -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
around -X- _ O
0.1-0.2 -X- _ O
drops -X- _ O
in -X- _ O
performance. -X- _ O
This -X- _ O
implies -X- _ O
how -X- _ O
to -X- _ O
filter -X- _ O
potential -X- _ O
noisy -X- _ O
samples -X- _ O
is -X- _ O
crucial -X- _ O
in -X- _ O
noise -X- _ O
aware -X- _ O
fine-tuning -X- _ O
, -X- _ O
and -X- _ O
probabilistic -X- _ O
labels -X- _ O
also -X- _ O
slightly -X- _ O
help -X- _ O
. -X- _ O

Effects -X- _ O
of -X- _ O
Task -X- _ O
We -X- _ O
also -X- _ O
analyse -X- _ O
how -X- _ O
each -X- _ O
bootstrapped -X- _ O
task -X- _ O
affect -X- _ O
WeCheck. -X- _ B-MethodName
In -X- _ O
1 -X- _ O
) -X- _ O
is -X- _ O
much -X- _ O
worse -X- _ O
than -X- _ O
others. -X- _ O
Among -X- _ O
the -X- _ O
rest -X- _ O
two -X- _ O
methods -X- _ O
with -X- _ O
comparable -X- _ O
performance -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
2.9 -X- _ O
times -X- _ O
faster -X- _ O
than -X- _ O
SCZS -X- _ B-MethodName
and -X- _ O
30 -X- _ O
times -X- _ O
faster -X- _ O
than -X- _ O
QAFact -X- _ B-MethodName
. -X- _ O

Abstractiveness -X- _ O
As -X- _ O
mentioned -X- _ O
above -X- _ O
, -X- _ O
abstractive -X- _ O
hypotheses -X- _ O
are -X- _ O
challenging -X- _ O
for -X- _ O
current -X- _ O
metrics -X- _ O
, -X- _ O
e.g. -X- _ O
XSUM -X- _ B-DatasetName
summaries -X- _ O
from -X- _ O
MNBM. -X- _ B-DatasetName
We -X- _ O
give -X- _ O
an -X- _ O
in-depth -X- _ O
analysis -X- _ O
of -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
hypothesis -X- _ O
abstractiveness -X- _ O
on -X- _ O
the -X- _ O
metrics -X- _ O
performance -X- _ O
. -X- _ O

Following -X- _ O
See -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
percentage -X- _ O
of -X- _ O
unique -X- _ O
unigrams -X- _ O
in -X- _ O
a -X- _ O
hypothesis -X- _ O
w.r.t -X- _ O
its -X- _ O
premise -X- _ O
to -X- _ O
measure -X- _ O
abstractivenss. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
spilt -X- _ O
all -X- _ O
the -X- _ O
examples -X- _ O
in -X- _ O
TRUE -X- _ O
into -X- _ O
10 -X- _ O
bins -X- _ O
according -X- _ O
to -X- _ O
their -X- _ O
abstractiveness. -X- _ O
For -X- _ O
each -X- _ O
bin -X- _ O
, -X- _ O
we -X- _ O
measure -X- _ O
the -X- _ O
ROC -X- _ B-DatasetName
AUC -X- _ I-DatasetName
of -X- _ O
WeCheck -X- _ B-MethodName
and -X- _ O
the -X- _ O
other -X- _ O
three -X- _ O
representative -X- _ O
baselines -X- _ O
: -X- _ O
QAFact -X- _ B-MethodName
, -X- _ O
Summac -X- _ B-MethodName
, -X- _ O
and -X- _ O
NLI-warmup. -X- _ B-MethodName
From -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
a -X- _ O
significant -X- _ O
drop -X- _ O
in -X- _ O
the -X- _ O
performance -X- _ O
for -X- _ O
all -X- _ O
baselines -X- _ O
as -X- _ O
the -X- _ O
hypothesis -X- _ O
becomes -X- _ O
more -X- _ O
abstractive -X- _ O
, -X- _ O
while -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
keeps -X- _ O
its -X- _ O
performance -X- _ O
( -X- _ O
around -X- _ O
0.85 -X- _ O
) -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
consistently -X- _ O
outperforms -X- _ O
baseline -X- _ O
metrics -X- _ O
in -X- _ O
every -X- _ O
bin -X- _ O
of -X- _ O
ab- -X- _ O
stractiveness. -X- _ O
This -X- _ O
further -X- _ O
verifies -X- _ O
the -X- _ O
superiority -X- _ O
of -X- _ O
directly -X- _ O
training -X- _ O
with -X- _ O
real -X- _ O
task -X- _ O
data -X- _ O
. -X- _ O

Labeling -X- _ O
Model -X- _ O
We -X- _ O
compare -X- _ O
how -X- _ O
different -X- _ O
data -X- _ O
programming -X- _ O
based -X- _ O
labeling -X- _ O
models -X- _ O
affect -X- _ O
the -X- _ O
final -X- _ O
metric -X- _ O
performance -X- _ O
. -X- _ O

In -X- _ O
WeCheck -X- _ O
, -X- _ O
labeling -X- _ O
model -X- _ O
p -X- _ O
ϕ -X- _ O
learns -X- _ O
to -X- _ O
aggregate -X- _ O
multi-resource -X- _ O
labels -X- _ O
to -X- _ O
infer -X- _ O
the -X- _ O
hidden -X- _ O
true -X- _ O
label. -X- _ O
Comparing -X- _ O
concretely -X- _ O
, -X- _ O
our -X- _ O
method -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
Snorkel -X- _ O
. -X- _ O
Because -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
scenario -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
is -X- _ O
small -X- _ O
and -X- _ O
their -X- _ O
relationships -X- _ O
are -X- _ O
relatively -X- _ O
simple -X- _ O
as -X- _ O
they -X- _ O
are -X- _ O
trained -X- _ O
from -X- _ O
different -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
prefer -X- _ O
this -X- _ O
method -X- _ O
over -X- _ O
other -X- _ O
recent -X- _ O
more -X- _ O
advanced -X- _ O
ones -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
labeling -X- _ O
model -X- _ O
by -X- _ O
replacing -X- _ O
it -X- _ O
with -X- _ O
other -X- _ O
methods. -X- _ O
In -X- _ O
these -X- _ O
baselines -X- _ O
, -X- _ O
simpler -X- _ O
methods -X- _ O
include -X- _ O
: -X- _ O
Average -X- _ O
Signals -X- _ O
, -X- _ O
which -X- _ O
simply -X- _ O
averages -X- _ O
all -X- _ O
the -X- _ O
weak -X- _ O
signals -X- _ O
as -X- _ O
the -X- _ O
probabilistic -X- _ O
label -X- _ O
p -X- _ O
( -X- _ O
y -X- _ O
+ -X- _ O
) -X- _ O
; -X- _ O
Major -X- _ O
Vote -X- _ O
, -X- _ O
which -X- _ O
select -X- _ O
the -X- _ O
most -X- _ O
frequently -X- _ O
appeared -X- _ O
label -X- _ O
in -X- _ O
a -X- _ O
unified -X- _ O
weak -X- _ O
signal -X- _ O
set -X- _ O
as -X- _ O
the -X- _ O
true -X- _ O
label. -X- _ O
More -X- _ O
advanced -X- _ O
methods -X- _ O
include -X- _ O
: -X- _ O
Flying -X- _ O
Squid -X- _ O
( -X- _ O
Fu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
applies -X- _ O
an -X- _ O
Ising -X- _ O
model -X- _ O
( -X- _ O
Parsons -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
method -X- _ O
, -X- _ O
which -X- _ O
uses -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
as -X- _ O
the -X- _ O
labeling -X- _ O
method -X- _ O
and -X- _ O
trains -X- _ O
it -X- _ O
end-to-end -X- _ O
with -X- _ O
the -X- _ O
target -X- _ O
tasks -X- _ O
model -X- _ O
; -X- _ O
DWS -X- _ O
( -X- _ O
Parker -X- _ O
and -X- _ O
Yu -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
treats -X- _ O
the -X- _ O
true -X- _ O
label -X- _ O
of -X- _ O
a -X- _ O
sample -X- _ O
as -X- _ O
the -X- _ O
hidden -X- _ O
variable -X- _ O
and -X- _ O
applies -X- _ O
Estimation-Maximization -X- _ O
( -X- _ O
EM -X- _ O
) -X- _ O
for -X- _ O
inference -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O

From -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
our -X- _ O
default -X- _ O
labeling -X- _ O
model -X- _ O
outperforms -X- _ O
all -X- _ O
others. -X- _ O
Furthermore -X- _ O
, -X- _ O
more -X- _ O
complex -X- _ O
methods -X- _ O
( -X- _ O
Flying -X- _ O
Squid -X- _ O
, -X- _ O
Weasel -X- _ O
, -X- _ O
and -X- _ O
EM -X- _ O
) -X- _ O
perform -X- _ O
worse -X- _ O
than -X- _ O
simpler -X- _ O
methods -X- _ O
( -X- _ O
Ours -X- _ O
, -X- _ O
Average -X- _ O
Signal -X- _ O
, -X- _ O
and -X- _ O
Major -X- _ O
Vote -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
further -X- _ O
verifies -X- _ O
that -X- _ O
the -X- _ O
relations -X- _ O
between -X- _ O
weak -X- _ O
signals -X- _ O
are -X- _ O
simple -X- _ O
, -X- _ O
and -X- _ O
complex -X- _ O
modeling -X- _ O
will -X- _ O
not -X- _ O
bring -X- _ O
further -X- _ O
improvements. -X- _ O
From -X- _ O
another -X- _ O
perspective -X- _ O
, -X- _ O
overly -X- _ O
simplistic -X- _ O
approaches -X- _ O
without -X- _ O
any -X- _ O
statistical -X- _ O
modeling -X- _ O
( -X- _ O
Average -X- _ O
Signal -X- _ O
and -X- _ O
Major -X- _ O
Vote -X- _ O
) -X- _ O
also -X- _ O
perform -X- _ O
worse -X- _ O
than -X- _ O
our -X- _ O
methods -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
Factual -X- _ O
Consistency -X- _ O
Evaluation -X- _ O
Recently -X- _ O
, -X- _ O
automatically -X- _ O
checking -X- _ O
factual -X- _ O
consistency -X- _ O
has -X- _ O
become -X- _ O
an -X- _ O
increasingly -X- _ O
popular -X- _ O
topic -X- _ O
. -X- _ O
Reasoning -X- _ O
over -X- _ O
a -X- _ O
long -X- _ O
range -X- _ O
of -X- _ O
context -X- _ O
for -X- _ O
factual -X- _ O
evaluation -X- _ O
is -X- _ O
a -X- _ O
challenging -X- _ O
task -X- _ O
that -X- _ O
even -X- _ O
human -X- _ O
annotators -X- _ O
may -X- _ O
frequently -X- _ O
disagree -X- _ O
with -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
collect -X- _ O
a -X- _ O
large-scale -X- _ O
high-quality -X- _ O
dataset -X- _ O
for -X- _ O
training -X- _ O
a -X- _ O
fully -X- _ O
supervised -X- _ O
model -X- _ O
, -X- _ O
and -X- _ O
previous -X- _ O
works -X- _ O
search -X- _ O
for -X- _ O
indirect -X- _ O
methods. -X- _ O
One -X- _ O
branch -X- _ O
of -X- _ O
them -X- _ O
leverage -X- _ O
the -X- _ O
reasoning -X- _ O
ability -X- _ O
of -X- _ O
NLI. -X- _ B-TaskName
Based -X- _ O
on -X- _ O
the -X- _ O
model -X- _ O
trained -X- _ O
on -X- _ O
NLI -X- _ B-TaskName
datasets -X- _ O
, -X- _ O
e.g. -X- _ O
MNLI -X- _ B-DatasetName
( -X- _ O
Williams -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
ANLI -X- _ B-DatasetName
( -X- _ O
Nie -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
some -X- _ O
works -X- _ O
aggregate -X- _ O
sentence-level -X- _ O
entailment -X- _ O
score -X- _ O
for -X- _ O
checking -X- _ O
( -X- _ O
Falke -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Laban -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
while -X- _ O
others -X- _ O
adopt -X- _ O
document-level -X- _ O
NLI -X- _ B-MetricName
which -X- _ O
directly -X- _ O
reasoning -X- _ O
over -X- _ O
the -X- _ O
full -X- _ O
context -X- _ O
( -X- _ O
Maynez -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Gehrmann -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Another -X- _ O
branch -X- _ O
of -X- _ O
methods -X- _ O
apply -X- _ O
QA-QG -X- _ B-MetricName
based -X- _ O
pipeline -X- _ O
for -X- _ O
a -X- _ O
more -X- _ O
fine-grained -X- _ O
checking. -X- _ O
QAGS -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
FEQA -X- _ O
( -X- _ O
Durmus -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
are -X- _ O
the -X- _ O
earliest -X- _ O
attempt -X- _ O
on -X- _ O
this -X- _ O
method -X- _ O
, -X- _ O
and -X- _ O
QuestEval -X- _ O
( -X- _ O
Scialom -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
QAFactEval -X- _ O
( -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
further -X- _ O
improve -X- _ O
this -X- _ O
type -X- _ O
of -X- _ O
methods -X- _ O
by -X- _ O
applying -X- _ O
NLI -X- _ B-TaskName
for -X- _ O
answer -X- _ O
matching. -X- _ O
Data -X- _ O
Programming -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
mainly -X- _ O
focus -X- _ O
on -X- _ O
data -X- _ O
programming -X- _ O
( -X- _ O
Ratner -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
( -X- _ O
DP -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
weak -X- _ O
supervision -X- _ O
paradigm -X- _ O
proposed -X- _ O
to -X- _ O
infer -X- _ O
correct -X- _ O
labels -X- _ O
based -X- _ O
on -X- _ O
noisy -X- _ O
labels -X- _ O
from -X- _ O
labeling -X- _ O
functions -X- _ O
( -X- _ O
LFs -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
rule-based -X- _ O
decision-making -X- _ O
processes -X- _ O
that -X- _ O
generate -X- _ O
discrete -X- _ O
labels. -X- _ O
Following -X- _ O
the -X- _ O
DP -X- _ O
paradigm -X- _ O
, -X- _ O
Snorkel -X- _ O
is -X- _ O
proposed -X- _ O
to -X- _ O
for -X- _ O
rapid -X- _ O
training -X- _ O
, -X- _ O
more -X- _ O
recent -X- _ O
works -X- _ O
study -X- _ O
how -X- _ O
to -X- _ O
adapt -X- _ O
label -X- _ O
model -X- _ O
in -X- _ O
DP -X- _ O
( -X- _ O
Ratner -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Awasthi -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
or -X- _ O
modeling -X- _ O
more -X- _ O
complex -X- _ O
structure -X- _ O
between -X- _ O
LFs -X- _ O
( -X- _ O
Fu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
DP -X- _ O
is -X- _ O
also -X- _ O
applied -X- _ O
to -X- _ O
several -X- _ O
NLP -X- _ O
tasks. -X- _ O
DWS -X- _ O
( -X- _ O
Parker -X- _ O
and -X- _ O
Yu -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
combine -X- _ O
DP -X- _ O
and -X- _ O
CRF -X- _ O
for -X- _ O
weakly -X- _ O
supervised -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
, -X- _ O
Min -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
apply -X- _ O
DP -X- _ O
for -X- _ O
QA. -X- _ B-TaskName
Different -X- _ O
from -X- _ O
all -X- _ O
previous -X- _ O
tasks -X- _ O
, -X- _ O
our -X- _ O
weak -X- _ O
supervision -X- _ O
signals -X- _ O
are -X- _ O
logits -X- _ O
from -X- _ O
other -X- _ O
models -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
discrete -X- _ O
labels -X- _ O
generated -X- _ O
from -X- _ O
rules -X- _ O
. -X- _ O

Conclusion -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
weakly -X- _ O
supervised -X- _ O
framework -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
, -X- _ O
which -X- _ O
aggregates -X- _ O
weakly -X- _ O
supervised -X- _ O
signals -X- _ O
from -X- _ O
multiple -X- _ O
resources -X- _ O
and -X- _ O
trains -X- _ O
a -X- _ O
target -X- _ O
metric -X- _ O
model -X- _ O
in -X- _ O
a -X- _ O
noise-aware -X- _ O
manner. -X- _ O
Different -X- _ O
from -X- _ O
previous -X- _ O
metrics -X- _ O
that -X- _ O
trains -X- _ O
from -X- _ O
synthetic -X- _ O
data -X- _ O
or -X- _ O
transferred -X- _ O
from -X- _ O
other -X- _ O
tasks -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
directly -X- _ O
trains -X- _ O
with -X- _ O
the -X- _ O
real -X- _ O
generated -X- _ O
text. -X- _ O
WeCheck -X- _ B-MethodName
first -X- _ O
annotates -X- _ O
each -X- _ O
sample -X- _ O
with -X- _ O
a -X- _ O
probabilistic -X- _ O
label -X- _ O
via -X- _ O
a -X- _ O
labeling -X- _ O
function -X- _ O
that -X- _ O
aggregates -X- _ O
multiple -X- _ O
resources. -X- _ O
Then -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
noise-aware -X- _ O
finetuning -X- _ O
stage -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
applies -X- _ O
probabilistic -X- _ O
labels -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
target -X- _ O
metric -X- _ O
model. -X- _ O
Experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
not -X- _ O
only -X- _ O
surpass -X- _ O
previous -X- _ O
methods -X- _ O
in -X- _ O
performance -X- _ O
but -X- _ O
also -X- _ O
time -X- _ O
efficient. -X- _ O
Moreover -X- _ O
, -X- _ O
WeCheck -X- _ B-MethodName
is -X- _ O
potential -X- _ O
to -X- _ O
be -X- _ O
compatible -X- _ O
with -X- _ O
future -X- _ O
more -X- _ O
stronger -X- _ O
metrics -X- _ O
, -X- _ O
bring -X- _ O
further -X- _ O
improvements -X- _ O
to -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
. -X- _ O

Limitations -X- _ O
Hyper-parameters -X- _ O
Selection -X- _ O
Some -X- _ O
hyperparameters -X- _ O
still -X- _ O
acquire -X- _ O
careful -X- _ O
selection -X- _ O
for -X- _ O
WeCheck -X- _ B-MethodName
, -X- _ O
e.g. -X- _ O
p -X- _ B-HyperparameterName
+ -X- _ I-HyperparameterName
, -X- _ O
p -X- _ B-HyperparameterName
− -X- _ I-HyperparameterName
. -X- _ O
Also -X- _ O
, -X- _ O
using -X- _ O
different -X- _ O
set -X- _ O
of -X- _ O
hyper-parameters -X- _ O
for -X- _ O
different -X- _ O
tasks -X- _ O
and -X- _ O
datasets -X- _ O
will -X- _ O
further -X- _ O
boost -X- _ O
performance. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
several -X- _ O
time -X- _ O
and -X- _ O
select -X- _ O
the -X- _ O
best -X- _ O
performing -X- _ O
parameters -X- _ O
based -X- _ O
on -X- _ O
validation -X- _ O
. -X- _ O

End-to-End -X- _ O
Training -X- _ O
WeCheck -X- _ B-MethodName
applies -X- _ O
the -X- _ O
weak -X- _ O
annotation -X- _ O
and -X- _ O
noise-aware -X- _ O
fine-tuning -X- _ O
twostep -X- _ O
pipeline -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
noises -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
step -X- _ O
will -X- _ O
greatly -X- _ O
affect -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
second -X- _ O
step. -X- _ O
By -X- _ O
modifying -X- _ O
the -X- _ O
overall -X- _ O
framework -X- _ O
into -X- _ O
end-toend -X- _ O
training -X- _ O
will -X- _ O
solve -X- _ O
this -X- _ O
problem -X- _ O
. -X- _ O

Why -X- _ O
Are -X- _ O
n't -X- _ O
We -X- _ O
NER -X- _ B-TaskName
Yet -X- _ O
? -X- _ O
Artifacts -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
Errors -X- _ O
in -X- _ O
Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName
in -X- _ O
Spontaneous -X- _ O
Speech -X- _ O
Transcripts -X- _ O

Transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
human -X- _ O
speech -X- _ O
present -X- _ O
a -X- _ O
significant -X- _ O
obstacle -X- _ O
for -X- _ O
traditional -X- _ O
NER -X- _ B-TaskName
models. -X- _ O
The -X- _ O
lack -X- _ O
of -X- _ O
grammatical -X- _ O
structure -X- _ O
of -X- _ O
spoken -X- _ O
utterances -X- _ O
and -X- _ O
word -X- _ O
errors -X- _ O
introduced -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
make -X- _ O
downstream -X- _ O
NLP -X- _ O
tasks -X- _ O
challenging. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
in -X- _ O
detail -X- _ O
the -X- _ O
complex -X- _ O
relationship -X- _ O
between -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
which -X- _ O
limit -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
NER -X- _ B-TaskName
models -X- _ O
to -X- _ O
recover -X- _ O
entity -X- _ O
mentions -X- _ O
from -X- _ O
spontaneous -X- _ O
speech -X- _ O
transcripts. -X- _ O
Using -X- _ O
publicly -X- _ O
available -X- _ O
benchmark -X- _ O
datasets -X- _ O
( -X- _ O
SWNE -X- _ B-DatasetName
, -X- _ O
Earnings-21 -X- _ B-DatasetName
, -X- _ O
OntoNotes -X- _ B-DatasetName
) -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
full -X- _ O
taxonomy -X- _ O
of -X- _ O
ASR-NER -X- _ B-TaskName
errors -X- _ O
and -X- _ O
measure -X- _ O
their -X- _ O
true -X- _ O
impact -X- _ O
on -X- _ O
entity -X- _ B-TaskName
recognition. -X- _ I-TaskName
We -X- _ O
find -X- _ O
that -X- _ O
NER -X- _ B-TaskName
models -X- _ O
fail -X- _ O
to -X- _ O
recognize -X- _ O
entity -X- _ O
spans -X- _ O
even -X- _ O
if -X- _ O
no -X- _ O
word -X- _ O
errors -X- _ O
are -X- _ O
introduced -X- _ O
by -X- _ O
the -X- _ O
ASR. -X- _ B-TaskName
We -X- _ O
also -X- _ O
show -X- _ O
why -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
is -X- _ O
inadequate -X- _ O
to -X- _ O
evaluate -X- _ O
NER -X- _ B-TaskName
models -X- _ O
on -X- _ O
conversational -X- _ O
transcripts -X- _ O
1 -X- _ O
. -X- _ O

Introduction -X- _ O
The -X- _ O
performance -X- _ O
of -X- _ O
NLP -X- _ O
models -X- _ O
tends -X- _ O
to -X- _ O
deteriorate -X- _ O
significantly -X- _ O
when -X- _ O
the -X- _ O
models -X- _ O
are -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
raw -X- _ O
outputs -X- _ O
of -X- _ O
the -X- _ O
Automatic -X- _ B-TaskName
Speech -X- _ I-TaskName
Recognition -X- _ I-TaskName
( -X- _ O
ASR -X- _ B-TaskName
) -X- _ O
system. -X- _ O
We -X- _ O
coin -X- _ O
the -X- _ O
term -X- _ O
ASR-NLP -X- _ B-TaskName
gap -X- _ O
to -X- _ O
describe -X- _ O
this -X- _ O
phenomenon. -X- _ O
Despite -X- _ O
unprecedented -X- _ O
advances -X- _ O
in -X- _ O
modern -X- _ O
language -X- _ O
models -X- _ O
, -X- _ O
the -X- _ O
transcript -X- _ O
of -X- _ O
a -X- _ O
spontaneous -X- _ O
human-human -X- _ O
conversation -X- _ O
remains -X- _ O
an -X- _ O
insurmountable -X- _ O
challenge -X- _ O
for -X- _ O
most -X- _ O
models. -X- _ O
This -X- _ O
is -X- _ O
particularly -X- _ O
true -X- _ O
for -X- _ O
Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName
( -X- _ O
NER -X- _ B-TaskName
) -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
struggle -X- _ O
to -X- _ O
retrieve -X- _ O
even -X- _ O
the -X- _ O
most -X- _ O
basic -X- _ O
entity -X- _ O
mentions -X- _ O
from -X- _ O
spontaneous -X- _ O
speech. -X- _ O
1 -X- _ O
All -X- _ O
code -X- _ O
necessary -X- _ O
to -X- _ O
reproduce -X- _ O
our -X- _ O
results -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
niedakh -X- _ O
/ -X- _ O
asr-ner-eval-repository -X- _ O
Three -X- _ O
primary -X- _ O
factors -X- _ O
contribute -X- _ O
to -X- _ O
the -X- _ O
existence -X- _ O
of -X- _ O
the -X- _ O
ASR-NLP -X- _ B-TaskName
gap. -X- _ O
Firstly -X- _ O
, -X- _ O
the -X- _ O
structure -X- _ O
of -X- _ O
spontaneous -X- _ O
human -X- _ O
conversations -X- _ O
is -X- _ O
diametrically -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
prescriptive -X- _ O
written -X- _ O
language -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
language -X- _ O
models. -X- _ O
These -X- _ O
models -X- _ O
can -X- _ O
use -X- _ O
the -X- _ O
grammatical -X- _ O
structure -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
corpora -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
part-of-speech -X- _ O
sequences -X- _ O
, -X- _ O
dependency -X- _ O
trees -X- _ O
, -X- _ O
and -X- _ O
dialog -X- _ O
acts. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
spontaneous -X- _ O
conversations -X- _ O
lack -X- _ O
sentence -X- _ O
structure. -X- _ O
They -X- _ O
contain -X- _ O
repetitions -X- _ O
, -X- _ O
back-channeling -X- _ O
, -X- _ O
phatic -X- _ O
expressions -X- _ O
, -X- _ O
and -X- _ O
other -X- _ O
artifacts -X- _ O
of -X- _ O
turn-taking. -X- _ O
The -X- _ O
second -X- _ O
challenge -X- _ O
comes -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
containing -X- _ O
neither -X- _ O
punctuation -X- _ O
nor -X- _ O
sentence -X- _ O
segmentation. -X- _ O
These -X- _ O
have -X- _ O
to -X- _ O
be -X- _ O
restored -X- _ O
by -X- _ O
an -X- _ O
auxiliary -X- _ O
downstream -X- _ O
model. -X- _ O
Thus -X- _ O
, -X- _ O
NLP -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
prescriptive -X- _ O
written -X- _ O
text -X- _ O
or -X- _ O
scripted -X- _ O
conversations -X- _ O
already -X- _ O
have -X- _ O
to -X- _ O
process -X- _ O
the -X- _ O
out-ofdomain -X- _ O
input. -X- _ O
The -X- _ O
third -X- _ O
problem -X- _ O
stems -X- _ O
from -X- _ O
ASR -X- _ B-TaskName
systems -X- _ O
injecting -X- _ O
word -X- _ O
errors -X- _ O
into -X- _ O
the -X- _ O
transcript. -X- _ O
Due -X- _ O
to -X- _ O
efficiency -X- _ O
requirements -X- _ O
, -X- _ O
most -X- _ O
ASR -X- _ B-TaskName
systems -X- _ O
use -X- _ O
unsophisticated -X- _ O
language -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
ngram -X- _ O
models -X- _ O
with -X- _ O
limited -X- _ O
vocabulary. -X- _ O
Thus -X- _ O
, -X- _ O
many -X- _ O
utterances -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
audio -X- _ O
may -X- _ O
be -X- _ O
unrecognized -X- _ O
and -X- _ O
deleted -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
, -X- _ O
while -X- _ O
other -X- _ O
utterances -X- _ O
may -X- _ O
cause -X- _ O
substitutions -X- _ O
or -X- _ O
insertions -X- _ O
of -X- _ O
erroneous -X- _ O
tokens -X- _ O
into -X- _ O
the -X- _ O
output -X- _ O
. -X- _ O

Consider -X- _ O
the -X- _ O
following -X- _ O
sentence -X- _ O
: -X- _ O
" -X- _ O
I -X- _ O
am -X- _ O
to -X- _ O
see -X- _ O
[ -X- _ O
Dr -X- _ O
Smith -X- _ O
] -X- _ O
PERSON -X- _ O
at -X- _ O
[ -X- _ O
9 -X- _ O
am -X- _ O
] -X- _ O
TIME -X- _ O
on -X- _ O
[ -X- _ O
Monday -X- _ O
, -X- _ O
May -X- _ O
14th -X- _ O
] -X- _ O
DATE -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
NER -X- _ B-TaskName
model -X- _ O
2 -X- _ O
correctly -X- _ O
recognizes -X- _ O
three -X- _ O
entity -X- _ O
spans -X- _ O
in -X- _ O
the -X- _ O
sentence. -X- _ O
Compare -X- _ O
this -X- _ O
to -X- _ O
the -X- _ O
NER -X- _ O
spans -X- _ O
recognized -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
far -X- _ O
more -X- _ O
likely -X- _ O
to -X- _ O
be -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
: -X- _ O
" -X- _ O
I -X- _ O
am -X- _ O
to -X- _ O
see -X- _ O
doctor -X- _ O
[ -X- _ O
Smith -X- _ O
] -X- _ O
PERSON -X- _ O
at -X- _ O
nine -X- _ O
I -X- _ O
am -X- _ O
on -X- _ O
[ -X- _ O
monday -X- _ O
] -X- _ O
DATE -X- _ O
[ -X- _ O
uhm -X- _ O
] -X- _ O
ORG -X- _ O
yeah -X- _ O
[ -X- _ O
monday -X- _ O
] -X- _ O
DATE -X- _ O
may -X- _ O
for -X- _ O
teen. -X- _ O
" -X- _ O
Two -X- _ O
entity -X- _ O
spans -X- _ O
have -X- _ O
been -X- _ O
cut -X- _ O
short -X- _ O
, -X- _ O
an -X- _ O
incorrect -X- _ O
label -X- _ O
has -X- _ O
replaced -X- _ O
one -X- _ O
span -X- _ O
's -X- _ O
label -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
recognized -X- _ O
a -X- _ O
filler -X- _ O
uhm -X- _ O
as -X- _ O
the -X- _ O
entity -X- _ O
ORG -X- _ O
! -X- _ O
With -X- _ O
a -X- _ O
few -X- _ O
more -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
and -X- _ O
lowercase -X- _ O
output -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
recognize -X- _ O
a -X- _ O
single -X- _ O
entity -X- _ O
in -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
: -X- _ O
" -X- _ O
I -X- _ O
am -X- _ O
to -X- _ O
see -X- _ O
doctor -X- _ O
uhm -X- _ O
doctor -X- _ O
smith -X- _ O
at -X- _ O
nine -X- _ O
I -X- _ O
am -X- _ O
on -X- _ O
man -X- _ O
day -X- _ O
may -X- _ O
for -X- _ O
teen -X- _ O
. -X- _ O
" -X- _ O

The -X- _ O
main -X- _ O
problem -X- _ O
is -X- _ O
that -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
are -X- _ O
very -X- _ O
" -X- _ O
unnatural -X- _ O
" -X- _ O
from -X- _ O
the -X- _ O
point -X- _ O
of -X- _ O
view -X- _ O
of -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
because -X- _ O
they -X- _ O
tend -X- _ O
to -X- _ O
break -X- _ O
the -X- _ O
grammar -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
on -X- _ O
which -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
depends. -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
consequential -X- _ O
errors -X- _ O
made -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
is -X- _ O
the -X- _ O
confusion -X- _ O
about -X- _ O
the -X- _ O
part-of-speech -X- _ O
tag. -X- _ O
Consider -X- _ O
possible -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
" -X- _ O
My -X- _ O
[ -X- _ O
second -X- _ O
] -X- _ O
ORDINAL -X- _ O
visit -X- _ O
is -X- _ O
[ -X- _ O
Wednesday -X- _ O
] -X- _ O
DATE -X- _ O
at -X- _ O
[ -X- _ O
half -X- _ O
past -X- _ O
one -X- _ O
] -X- _ O
TIME -X- _ O
. -X- _ O
" -X- _ O
Changing -X- _ O
the -X- _ O
personal -X- _ O
pronoun -X- _ O
" -X- _ O
My -X- _ O
" -X- _ O
to -X- _ O
the -X- _ O
noun -X- _ O
" -X- _ O
May -X- _ O
" -X- _ O
forces -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
to -X- _ O
recognize -X- _ O
a -X- _ O
DATE -X- _ O
span -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
reasonable. -X- _ O
But -X- _ O
if -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
changes -X- _ O
the -X- _ O
preposition -X- _ O
" -X- _ O
at -X- _ O
" -X- _ O
into -X- _ O
a -X- _ O
verb -X- _ O
" -X- _ O
add -X- _ O
, -X- _ O
" -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
loses -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
recognize -X- _ O
the -X- _ O
utterance -X- _ O
" -X- _ O
half -X- _ O
past -X- _ O
one -X- _ O
" -X- _ O
as -X- _ O
TIME -X- _ O
because -X- _ O
of -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
the -X- _ O
preceding -X- _ O
preposition. -X- _ O
Similarly -X- _ O
, -X- _ O
changing -X- _ O
" -X- _ O
half -X- _ O
past -X- _ O
one -X- _ O
" -X- _ O
to -X- _ O
" -X- _ O
[ -X- _ O
one -X- _ O
thirty -X- _ O
] -X- _ O
TIME -X- _ O
" -X- _ O
retrieves -X- _ O
the -X- _ O
TIME -X- _ O
span -X- _ O
, -X- _ O
but -X- _ O
an -X- _ O
ASR -X- _ B-TaskName
error -X- _ O
confusing -X- _ O
the -X- _ O
numeral -X- _ O
" -X- _ O
one -X- _ O
" -X- _ O
with -X- _ O
the -X- _ O
conjunction -X- _ O
" -X- _ O
when -X- _ O
" -X- _ O
produces -X- _ O
" -X- _ O
[ -X- _ O
Wednesday -X- _ O
] -X- _ O
DATE -X- _ O
at -X- _ O
when -X- _ O
[ -X- _ O
thirty -X- _ O
] -X- _ O
DATE -X- _ O
. -X- _ O
" -X- _ O
If -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
word -X- _ O
is -X- _ O
mistakenly -X- _ O
recognized -X- _ O
as -X- _ O
the -X- _ O
verb -X- _ O
" -X- _ O
want -X- _ O
, -X- _ O
" -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
produces -X- _ O
" -X- _ O
[ -X- _ O
Wednesday -X- _ O
] -X- _ O
DATE -X- _ O
at -X- _ O
want -X- _ O
[ -X- _ O
thirty -X- _ O
] -X- _ O
CARDINAL -X- _ O
" -X- _ O
. -X- _ O

Unfortunately -X- _ O
, -X- _ O
the -X- _ O
problems -X- _ O
mentioned -X- _ O
above -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
easily -X- _ O
solved. -X- _ O
Word -X- _ B-MetricName
error -X- _ I-MetricName
rates -X- _ I-MetricName
( -X- _ O
WER -X- _ B-MetricName
) -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
systems -X- _ O
remain -X- _ O
high -X- _ O
for -X- _ O
spontaneous -X- _ O
human -X- _ O
conversations -X- _ O
( -X- _ O
Del -X- _ O
Rio -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Recently -X- _ O
announced -X- _ O
results -X- _ O
claiming -X- _ O
WERs -X- _ B-MetricName
at -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
5 -X- _ O
% -X- _ O
apply -X- _ O
to -X- _ O
conversations -X- _ O
with -X- _ O
digital -X- _ O
assistants -X- _ O
, -X- _ O
where -X- _ O
spoken -X- _ O
utterances -X- _ O
are -X- _ O
imperative -X- _ O
phrases -X- _ O
with -X- _ O
limited -X- _ O
vocabulary. -X- _ O
These -X- _ O
results -X- _ O
are -X- _ O
not -X- _ O
representative -X- _ O
of -X- _ O
spontaneous -X- _ O
human -X- _ O
open -X- _ O
dialogues -X- _ O
, -X- _ O
which -X- _ O
lack -X- _ O
the -X- _ O
rigid -X- _ O
grammatical -X- _ O
phrase -X- _ O
structure -X- _ O
and -X- _ O
contain -X- _ O
fillers -X- _ O
, -X- _ O
back-channeling -X- _ O
, -X- _ O
repetitions -X- _ O
, -X- _ O
hesitation -X- _ O
markers -X- _ O
, -X- _ O
and -X- _ O
other -X- _ O
elements -X- _ O
which -X- _ O
are -X- _ O
a -X- _ O
part -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
. -X- _ O

The -X- _ O
interplay -X- _ O
of -X- _ O
two -X- _ O
phenomena -X- _ O
makes -X- _ O
the -X- _ O
processing -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
transcripts -X- _ O
with -X- _ O
NLP -X- _ O
models -X- _ O
so -X- _ O
challenging. -X- _ O
On -X- _ O
the -X- _ O
one -X- _ O
hand -X- _ O
, -X- _ O
every -X- _ O
NLP -X- _ O
model -X- _ O
is -X- _ O
inherently -X- _ O
flawed -X- _ O
and -X- _ O
produces -X- _ O
errors -X- _ O
( -X- _ O
such -X- _ O
as -X- _ O
not -X- _ O
recognizing -X- _ O
an -X- _ O
instance -X- _ O
of -X- _ O
an -X- _ O
entity -X- _ O
) -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
system -X- _ O
injects -X- _ O
errors -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
insertions -X- _ O
, -X- _ O
deletions -X- _ O
, -X- _ O
and -X- _ O
substitutions. -X- _ O
This -X- _ O
changes -X- _ O
the -X- _ O
structure -X- _ O
and -X- _ O
semantics -X- _ O
of -X- _ O
transcribed -X- _ O
speech -X- _ O
and -X- _ O
introduces -X- _ O
yet -X- _ O
another -X- _ O
source -X- _ O
of -X- _ O
errors -X- _ O
: -X- _ O
alignment. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
on -X- _ O
the -X- _ O
transcript -X- _ O
, -X- _ O
one -X- _ O
has -X- _ O
to -X- _ O
align -X- _ O
tokens -X- _ O
between -X- _ O
gold -X- _ O
transcripts -X- _ O
and -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
to -X- _ O
match -X- _ O
entity -X- _ O
spans. -X- _ O
This -X- _ O
process -X- _ O
may -X- _ O
produce -X- _ O
artifacts -X- _ O
that -X- _ O
significantly -X- _ O
skew -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
the -X- _ O
evaluation -X- _ O
. -X- _ O

The -X- _ O
evaluation -X- _ O
of -X- _ O
the -X- _ O
NER -X- _ B-TaskName
task -X- _ O
is -X- _ O
usually -X- _ O
performed -X- _ O
using -X- _ O
precision -X- _ O
, -X- _ O
recall -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score. -X- _ O
Unfortunately -X- _ O
, -X- _ O
these -X- _ O
measures -X- _ O
are -X- _ O
of -X- _ O
limited -X- _ O
use -X- _ O
for -X- _ O
processing -X- _ O
spontaneous -X- _ O
conversation -X- _ O
transcripts -X- _ O
because -X- _ O
they -X- _ O
confound -X- _ O
two -X- _ O
independent -X- _ O
factors -X- _ O
contributing -X- _ O
to -X- _ O
the -X- _ O
errors -X- _ O
mentioned -X- _ O
above -X- _ O
: -X- _ O
the -X- _ O
inability -X- _ O
of -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
to -X- _ O
recognize -X- _ O
a -X- _ O
span -X- _ O
as -X- _ O
an -X- _ O
entity -X- _ O
and -X- _ O
the -X- _ O
word -X- _ O
error -X- _ O
introduced -X- _ O
by -X- _ O
the -X- _ O
wrong -X- _ O
transcription -X- _ O
of -X- _ O
a -X- _ O
token -X- _ O
. -X- _ O

Our -X- _ O
paper -X- _ O
is -X- _ O
a -X- _ O
reality -X- _ O
check -X- _ O
on -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
in -X- _ O
spontaneous -X- _ O
speech -X- _ O
transcripts. -X- _ O
Using -X- _ O
popular -X- _ O
benchmark -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
how -X- _ O
state-of-the-art -X- _ O
language -X- _ O
models -X- _ O
fail -X- _ O
to -X- _ O
discover -X- _ O
entity -X- _ O
spans -X- _ O
in -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
speech. -X- _ O
We -X- _ O
identify -X- _ O
several -X- _ O
artifacts -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
entity -X- _ O
recognition. -X- _ O
We -X- _ O
measure -X- _ O
the -X- _ O
propensity -X- _ O
of -X- _ O
each -X- _ O
type -X- _ O
of -X- _ O
artifact -X- _ O
to -X- _ O
influence -X- _ O
the -X- _ O
recognition -X- _ O
of -X- _ O
named -X- _ O
entities. -X- _ O
This -X- _ O
approach -X- _ O
brings -X- _ O
us -X- _ O
closer -X- _ O
to -X- _ O
understanding -X- _ O
the -X- _ O
true -X- _ O
reasons -X- _ O
for -X- _ O
NER -X- _ B-TaskName
model -X- _ O
failures -X- _ O
on -X- _ O
spontaneous -X- _ O
speech -X- _ O
transcripts. -X- _ O
We -X- _ O
argue -X- _ O
that -X- _ O
misalignment -X- _ O
artifacts -X- _ O
are -X- _ O
essential -X- _ O
characteristics -X- _ O
of -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
NLP -X- _ O
models -X- _ O
and -X- _ O
should -X- _ O
be -X- _ O
considered -X- _ O
when -X- _ O
evaluating -X- _ O
downstream -X- _ O
NLP -X- _ O
models -X- _ O
on -X- _ O
spontaneous -X- _ O
speech -X- _ O
transcripts -X- _ O
. -X- _ O

Entity -X- _ O
span -X- _ O
alignment -X- _ O
We -X- _ O
measure -X- _ O
the -X- _ O
loss -X- _ O
of -X- _ O
entity -X- _ O
spans -X- _ O
recognized -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
compared -X- _ O
to -X- _ O
those -X- _ O
recognized -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
must -X- _ O
perform -X- _ O
token -X- _ O
alignment -X- _ O
between -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
and -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O
as -X- _ O
they -X- _ O
may -X- _ O
differ -X- _ O
in -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens. -X- _ O
Alignment -X- _ O
is -X- _ O
performed -X- _ O
after -X- _ O
diarisation -X- _ O
( -X- _ O
separating -X- _ O
speakers -X- _ O
' -X- _ O
utterances -X- _ O
into -X- _ O
separate -X- _ O
channels -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
channel -X- _ O
independently. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
greedy -X- _ O
alignment -X- _ O
procedure. -X- _ O
We -X- _ O
begin -X- _ O
by -X- _ O
running -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
on -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
and -X- _ O
tagging -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
transcript -X- _ O
using -X- _ O
the -X- _ O
IOB -X- _ O
scheme -X- _ O
( -X- _ O
B -X- _ O
-beginning -X- _ O
of -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
, -X- _ O
I -X- _ O
-inside -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
, -X- _ O
O -X- _ O
-outside -X- _ O
of -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
) -X- _ O
. -X- _ O
Next -X- _ O
, -X- _ O
we -X- _ O
collapse -X- _ O
all -X- _ O
adjacent -X- _ O
I-tags -X- _ O
so -X- _ O
that -X- _ O
each -X- _ O
channel -X- _ O
is -X- _ O
represented -X- _ O
by -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
B-tags -X- _ O
and -X- _ O
O-tags. -X- _ O
We -X- _ O
repeat -X- _ O
the -X- _ O
same -X- _ O
procedure -X- _ O
for -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
and -X- _ O
then -X- _ O
align -X- _ O
both -X- _ O
transcripts. -X- _ O
The -X- _ O
alignment -X- _ O
of -X- _ O
gold -X- _ O
transcripts -X- _ O
, -X- _ O
normalized -X- _ O
gold -X- _ O
transcripts -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
is -X- _ O
performed -X- _ O
by -X- _ O
the -X- _ O
fstalign -X- _ O
( -X- _ O
McNamara -X- _ O
and -X- _ O
Kokotov -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
kaldialign -X- _ O
( -X- _ O
Żelasko -X- _ O
and -X- _ O
Guo -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
libraries -X- _ O
, -X- _ O
with -X- _ O
minor -X- _ O
additional -X- _ O
corrections. -X- _ O
All -X- _ O
transcripts -X- _ O
are -X- _ O
matched -X- _ O
at -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
tokens -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
remainder -X- _ O
of -X- _ O
the -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
use -X- _ O
the -X- _ O
following -X- _ O
terminology -X- _ O
( -X- _ O
Pallett -X- _ O
, -X- _ O
1985 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
distinguish -X- _ O
the -X- _ O
following -X- _ O
types -X- _ O
of -X- _ O
errors -X- _ O
: -X- _ O

• -X- _ O
insertion -X- _ O
: -X- _ O
a -X- _ O
token -X- _ O
has -X- _ O
been -X- _ O
inserted -X- _ O
into -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
appear -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O

• -X- _ O
substitution -X- _ O
: -X- _ O
a -X- _ O
token -X- _ O
has -X- _ O
been -X- _ O
wrongly -X- _ O
transcribed -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
both -X- _ O
transcripts -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
values -X- _ O
of -X- _ O
tokens -X- _ O
differ -X- _ O
, -X- _ O

• -X- _ O
deletion -X- _ O
: -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
has -X- _ O
not -X- _ O
recognized -X- _ O
a -X- _ O
token -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
of -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
is -X- _ O
shorter -X- _ O
than -X- _ O
the -X- _ O
original -X- _ O
gold -X- _ O
transcript -X- _ O
. -X- _ O

In -X- _ O
parallel -X- _ O
, -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
can -X- _ O
introduce -X- _ O
the -X- _ O
following -X- _ O
errors -X- _ O
: -X- _ O

• -X- _ O
hallucination -X- _ O
: -X- _ O
an -X- _ O
entity -X- _ O
tag -X- _ O
has -X- _ O
been -X- _ O
produced -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
appear -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O

• -X- _ O
replacement -X- _ O
: -X- _ O
an -X- _ O
entity -X- _ O
tag -X- _ O
has -X- _ O
been -X- _ O
added -X- _ O
to -X- _ O
the -X- _ O
token -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
label -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
class -X- _ O
is -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O

• -X- _ O
omission -X- _ O
: -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
does -X- _ O
not -X- _ O
produce -X- _ O
an -X- _ O
entity -X- _ O
tag -X- _ O
for -X- _ O
a -X- _ O
token -X- _ O
tagged -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
. -X- _ O

Let -X- _ O
us -X- _ O
now -X- _ O
describe -X- _ O
in -X- _ O
detail -X- _ O
all -X- _ O
possible -X- _ O
combinations -X- _ O
of -X- _ O
the -X- _ O
above -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NLP -X- _ O
errors -X- _ O
and -X- _ O
their -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
recognition -X- _ O
of -X- _ O
named -X- _ O
entities. -X- _ O
For -X- _ O
the -X- _ O
sake -X- _ O
of -X- _ O
clarity -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
only -X- _ O
consider -X- _ O
artifacts -X- _ O
of -X- _ O
the -X- _ O
ASR-NLP -X- _ B-TaskName
gap -X- _ O
within -X- _ O
a -X- _ O
single -X- _ O
entity -X- _ O
span. -X- _ O
Detailed -X- _ O
examples -X- _ O
of -X- _ O
every -X- _ O
combination -X- _ O
of -X- _ O
ASR-NLP -X- _ B-TaskName
errors -X- _ O
discovered -X- _ O
in -X- _ O
the -X- _ O
Earnings-21 -X- _ B-DatasetName
dataset -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
let -X- _ O
us -X- _ O
consider -X- _ O
a -X- _ O
scenario -X- _ O
where -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
and -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
are -X- _ O
perfectly -X- _ O
aligned -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
all -X- _ O
tokens -X- _ O
are -X- _ O
correctly -X- _ O
recognized. -X- _ O
The -X- _ O
gold -X- _ O
transcript -X- _ O
contains -X- _ O
the -X- _ O
utterance -X- _ O
" -X- _ O
second -X- _ O
B-DATE -X- _ O
quarter -X- _ O
B-DATE -X- _ O
twenty -X- _ O
B-DATE -X- _ O
twenty -X- _ O
B-DATE -X- _ O
. -X- _ O
" -X- _ O
The -X- _ O
following -X- _ O
entity -X- _ O
span -X- _ O
errors -X- _ O
are -X- _ O
possible -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
: -X- _ O
• -X- _ O
full -X- _ O
match -X- _ O
: -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
receives -X- _ O
the -X- _ O
same -X- _ O
entity -X- _ O
tag -X- _ O
as -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
( -X- _ O
row -X- _ O
B -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
full -X- _ O
omission -X- _ O
: -X- _ O
no -X- _ O
entity -X- _ O
tags -X- _ O
are -X- _ O
produced -X- _ O
for -X- _ O
tokens -X- _ O
inside -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
entity -X- _ O
span -X- _ O
( -X- _ O
row -X- _ O
C -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
full -X- _ O
replacement -X- _ O
: -X- _ O
each -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
has -X- _ O
a -X- _ O
different -X- _ O
entity -X- _ O
tag -X- _ O
from -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
( -X- _ O
row -X- _ O
D -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
match -X- _ O
with -X- _ O
replacement -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
have -X- _ O
different -X- _ O
entity -X- _ O
tags -X- _ O
from -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
( -X- _ O
row -X- _ O
E -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
match -X- _ O
with -X- _ O
omission -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
entity -X- _ O
tags -X- _ O
( -X- _ O
row -X- _ O
F -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
match -X- _ O
with -X- _ O
omission -X- _ O
and -X- _ O
replacement -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-MethodName
output -X- _ O
have -X- _ O
a -X- _ O
different -X- _ O
entity -X- _ O
class -X- _ O
tag -X- _ O
, -X- _ O
and -X- _ O
some -X- _ O
tokens -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
entity -X- _ O
tags -X- _ O
. -X- _ O

Consider -X- _ O
a -X- _ O
situation -X- _ O
where -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
inserts -X- _ O
a -X- _ O
token -X- _ O
into -X- _ O
the -X- _ O
gold -X- _ O
transcript. -X- _ O
Obviously -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
mismatch -X- _ O
in -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
and -X- _ O
the -X- _ O
transcription. -X- _ O
Let -X- _ O
us -X- _ O
assume -X- _ O
that -X- _ O
the -X- _ O
utterance -X- _ O
" -X- _ O
nextstart -X- _ O
B−ORG -X- _ O
group -X- _ O
I−ORG -X- _ O
" -X- _ O
has -X- _ O
been -X- _ O
mistakenly -X- _ O
transcribed -X- _ O
as -X- _ O
" -X- _ O
next -X- _ O
door -X- _ O
group. -X- _ O
" -X- _ O
Table -X- _ O
2 -X- _ O
summarizes -X- _ O
possible -X- _ O
combinations -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
. -X- _ O

• -X- _ O
full -X- _ O
match -X- _ O
: -X- _ O
tokens -X- _ O
are -X- _ O
tagged -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
entity -X- _ O
class -X- _ O
labels -X- _ O
( -X- _ O
row -X- _ O
B -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
full -X- _ O
omission -X- _ O
: -X- _ O
the -X- _ O
introduction -X- _ O
of -X- _ O
a -X- _ O
token -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
prevents -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
from -X- _ O
finding -X- _ O
any -X- _ O
entity -X- _ O
tags -X- _ O
( -X- _ O
row -X- _ O
C -X- _ O
) -X- _ O
, -X- _ O
• -X- _ O
full -X- _ O
substitution -X- _ O
: -X- _ O
tag -X- _ O
introduced -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
forces -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
to -X- _ O
generate -X- _ O
different -X- _ O
entity -X- _ O
labels -X- _ O
( -X- _ O
row -X- _ O
D -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
substitution -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
are -X- _ O
tagged -X- _ O
with -X- _ O
different -X- _ O
entity -X- _ O
class -X- _ O
labels -X- _ O
( -X- _ O
row -X- _ O
E -X- _ O
) -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
omission -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
an -X- _ O
entity -X- _ O
tag -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
result -X- _ O
in -X- _ O
the -X- _ O
multiplication -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
span -X- _ O
( -X- _ O
row -X- _ O
F -X- _ O
) -X- _ O
or -X- _ O
shortening -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
span -X- _ O
( -X- _ O
row -X- _ O
G -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
ASR -X- _ B-TaskName
can -X- _ O
delete -X- _ O
a -X- _ O
token -X- _ O
from -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
possible -X- _ O
misalignment. -X- _ O
In -X- _ O
this -X- _ O
scenario -X- _ O
, -X- _ O
full -X- _ O
matching -X- _ O
is -X- _ O
impossible -X- _ O
because -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
will -X- _ O
contain -X- _ O
an -X- _ O
unmatched -X- _ O
token. -X- _ O
Similarly -X- _ O
, -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
hallucinated -X- _ O
or -X- _ O
fully -X- _ O
substituted. -X- _ O
Let -X- _ O
us -X- _ O
assume -X- _ O
that -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
utterance -X- _ O
" -X- _ O
next -X- _ O
B-ORG -X- _ O
door -X- _ O
I-ORG -X- _ O
group -X- _ O
I-ORG -X- _ O
" -X- _ O
has -X- _ O
been -X- _ O
mistakenly -X- _ O
transcribed -X- _ O
as -X- _ O
" -X- _ O
next -X- _ O
< -X- _ O
del -X- _ O
> -X- _ O
group -X- _ O
" -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
ASR -X- _ O
failed -X- _ O
to -X- _ O
recognize -X- _ O
the -X- _ O
" -X- _ O
door -X- _ O
" -X- _ O
token -X- _ O
) -X- _ O
. -X- _ O
Table -X- _ O
3 -X- _ O
presents -X- _ O
possible -X- _ O
combinations -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
. -X- _ O

• -X- _ O
partial -X- _ O
match -X- _ O
: -X- _ O
tokens -X- _ O
not -X- _ O
deleted -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
have -X- _ O
correct -X- _ O
entity -X- _ O
tags -X- _ O
, -X- _ O

• -X- _ O
full -X- _ O
omission -X- _ O
: -X- _ O
the -X- _ O
deletion -X- _ O
of -X- _ O
a -X- _ O
token -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
prevents -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
from -X- _ O
producing -X- _ O
any -X- _ O
entity -X- _ O
tags -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
replacement -X- _ O
: -X- _ O
some -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
have -X- _ O
the -X- _ O
wrong -X- _ O
entity -X- _ O
tag -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
omission -X- _ O
: -X- _ O
the -X- _ O
loss -X- _ O
of -X- _ O
token -X- _ O
results -X- _ O
in -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
tokens -X- _ O
not -X- _ O
being -X- _ O
tagged -X- _ O
with -X- _ O
an -X- _ O
entity -X- _ O
tag -X- _ O
, -X- _ O

• -X- _ O
partial -X- _ O
replacement -X- _ O
and -X- _ O
omission -X- _ O
: -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
tokens -X- _ O
receive -X- _ O
correct -X- _ O
entity -X- _ O
tags -X- _ O
, -X- _ O
some -X- _ O
receive -X- _ O
wrong -X- _ O
entity -X- _ O
tags -X- _ O
, -X- _ O
and -X- _ O
some -X- _ O
do -X- _ O
not -X- _ O
receive -X- _ O
any -X- _ O
entity -X- _ O
tags -X- _ O
at -X- _ O
all -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
can -X- _ O
hallucinate -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
where -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
has -X- _ O
no -X- _ O
entities -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
possible -X- _ O
mistakes -X- _ O
is -X- _ O
large -X- _ O
, -X- _ O
and -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
obvious -X- _ O
which -X- _ O
scenarios -X- _ O
are -X- _ O
common -X- _ O
or -X- _ O
rare. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
are -X- _ O
to -X- _ O
develop -X- _ O
more -X- _ O
robust -X- _ O
models -X- _ O
for -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
in -X- _ O
the -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
, -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
understand -X- _ O
which -X- _ O
scenarios -X- _ O
are -X- _ O
the -X- _ O
most -X- _ O
impactful -X- _ O
for -X- _ O
the -X- _ O
NER -X- _ B-TaskName
task. -X- _ O
In -X- _ O
the -X- _ O
next -X- _ O
sections -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
experiments -X- _ O
that -X- _ O
try -X- _ O
to -X- _ O
present -X- _ O
a -X- _ O
much -X- _ O
more -X- _ O
detailed -X- _ O
and -X- _ O
nuanced -X- _ O
view -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
. -X- _ O

Datasets -X- _ O
We -X- _ O
use -X- _ O
three -X- _ O
datasets -X- _ O
in -X- _ O
our -X- _ O
experiments -X- _ O
. -X- _ O

• -X- _ O
OntoNotes -X- _ B-DatasetName
: -X- _ O
the -X- _ O
LDC-released -X- _ O
OntoNotes -X- _ B-DatasetName
v5 -X- _ I-DatasetName
( -X- _ O
Weischedel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
with -X- _ O
texts -X- _ O
from -X- _ O
news -X- _ O
, -X- _ O
broadcast -X- _ O
/ -X- _ O
telephone -X- _ O
conversations -X- _ O
, -X- _ O
and -X- _ O
web -X- _ O
data -X- _ O
annotated -X- _ O
with -X- _ O
18 -X- _ O
entity -X- _ O
types -X- _ O
. -X- _ O

• -X- _ O
SWNE -X- _ B-DatasetName
: -X- _ O
data -X- _ O
from -X- _ O
Switchboard -X- _ B-DatasetName
Dialog -X- _ I-DatasetName
Acts -X- _ I-DatasetName
Corpus -X- _ I-DatasetName
annotated -X- _ O
with -X- _ O
entity -X- _ O
tags -X- _ O
following -X- _ O
the -X- _ O
OntoNotes -X- _ B-DatasetName
v5 -X- _ I-DatasetName
annotation -X- _ O
scheme -X- _ O
( -X- _ O
Choi -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O

• -X- _ O
Earnings-21 -X- _ B-DatasetName
: -X- _ O
audio -X- _ O
and -X- _ O
transcriptions -X- _ O
of -X- _ O
44 -X- _ O
public -X- _ O
phone -X- _ O
calls -X- _ O
which -X- _ O
span -X- _ O
almost -X- _ O
40 -X- _ O
hours -X- _ O
of -X- _ O
recordings -X- _ O
of -X- _ O
human -X- _ O
conversations -X- _ O
, -X- _ O
with -X- _ O
25 -X- _ O
different -X- _ O
entity -X- _ O
classes -X- _ O
annotated -X- _ O
in -X- _ O
transcripts -X- _ O
( -X- _ O
Del -X- _ O
Rio -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
decided -X- _ O
to -X- _ O
omit -X- _ O
the -X- _ O
CoNLL-2003 -X- _ O
/ -X- _ O
CoNLL++ -X- _ O
( -X- _ O
Tjong -X- _ O
Kim -X- _ O
Sang -X- _ O
and -X- _ O
De -X- _ O
Meulder -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
dataset -X- _ O
because -X- _ O
it -X- _ O
is -X- _ O
annotated -X- _ O
with -X- _ O
only -X- _ O
four -X- _ O
classes -X- _ O
of -X- _ O
entities. -X- _ O
Unfortunately -X- _ O
, -X- _ O
the -X- _ O
three -X- _ O
listed -X- _ O
datasets -X- _ O
are -X- _ O
the -X- _ O
only -X- _ O
publicly -X- _ O
available -X- _ O
datasets -X- _ O
that -X- _ O
contain -X- _ O
audio -X- _ O
segments -X- _ O
and -X- _ O
transcripts -X- _ O
annotated -X- _ O
with -X- _ O
entity -X- _ O
types. -X- _ O
One -X- _ O
may -X- _ O
argue -X- _ O
that -X- _ O
these -X- _ O
datasets -X- _ O
are -X- _ O
not -X- _ O
representative -X- _ O
of -X- _ O
spontaneous -X- _ O
conversations. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
Earnings-21 -X- _ B-DatasetName
transcripts -X- _ O
sound -X- _ O
heavily -X- _ O
scripted -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
interlocutors -X- _ O
present -X- _ O
speeches -X- _ O
rather -X- _ O
than -X- _ O
a -X- _ O
free -X- _ O
exchange -X- _ O
of -X- _ O
utterances. -X- _ O
While -X- _ O
this -X- _ O
is -X- _ O
true -X- _ O
, -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
, -X- _ O
these -X- _ O
three -X- _ O
datasets -X- _ O
present -X- _ O
the -X- _ O
closest -X- _ O
that -X- _ O
researchers -X- _ O
can -X- _ O
get -X- _ O
to -X- _ O
conversational -X- _ O
audio -X- _ O
transcripts -X- _ O
with -X- _ O
annotated -X- _ O
entity -X- _ O
spans -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
datasets -X- _ O
with -X- _ O
audio -X- _ O
recordings -X- _ O
annotated -X- _ O
with -X- _ O
entity -X- _ O
spans -X- _ O
, -X- _ O
but -X- _ O
these -X- _ O
datasets -X- _ O
are -X- _ O
not -X- _ O
in -X- _ O
the -X- _ O
domain -X- _ O
of -X- _ O
spontaneous -X- _ O
speech. -X- _ O
In -X- _ O
recent -X- _ O
years -X- _ O
we -X- _ O
are -X- _ O
observing -X- _ O
significant -X- _ O
progress -X- _ O
in -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
in -X- _ O
transcripts -X- _ O
of -X- _ O
scripted -X- _ O
speech. -X- _ O
This -X- _ O
progress -X- _ O
is -X- _ O
made -X- _ O
possible -X- _ O
mostly -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
publication -X- _ O
of -X- _ O
annotated -X- _ O
datasets. -X- _ O
Yadav -X- _ O
et -X- _ O
al. -X- _ O
present -X- _ O
a -X- _ O
dataset -X- _ O
consisting -X- _ O
of -X- _ O
TED -X- _ O
talks -X- _ O
, -X- _ O
Mozilla -X- _ O
Common -X- _ O
Voice -X- _ O
recordings -X- _ O
, -X- _ O
LibriSpeech -X- _ O
audiobook -X- _ O
recordings -X- _ O
, -X- _ O
and -X- _ O
VoxForge -X- _ O
recordings. -X- _ O
As -X- _ O
the -X- _ O
authors -X- _ O
observe -X- _ O
, -X- _ O
NER -X- _ B-TaskName
models -X- _ O
achieve -X- _ O
promising -X- _ O
results -X- _ O
on -X- _ O
these -X- _ O
transcripts -X- _ O
( -X- _ O
probably -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
input -X- _ O
transcript -X- _ O
is -X- _ O
semantically -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
typical -X- _ O
training -X- _ O
data -X- _ O
for -X- _ O
NER -X- _ B-TaskName
models -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
same -X- _ O
dataset -X- _ O
is -X- _ O
used -X- _ O
by -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
to -X- _ O
illustrate -X- _ O
the -X- _ O
error -X- _ O
correction -X- _ O
model. -X- _ O
Recently -X- _ O
, -X- _ O
annotated -X- _ O
transcripts -X- _ O
of -X- _ O
speech -X- _ O
( -X- _ O
albeit -X- _ O
non-conversional -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
released -X- _ O
for -X- _ O
Scandinavian -X- _ O
languages -X- _ O
( -X- _ O
Porjazovski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
French -X- _ O
( -X- _ O
Millour -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
for -X- _ O
Chinese -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
worth -X- _ O
mentioning -X- _ O
that -X- _ O
NER -X- _ B-TaskName
task -X- _ O
has -X- _ O
been -X- _ O
added -X- _ O
to -X- _ O
the -X- _ O
recent -X- _ O
Spoken -X- _ O
Language -X- _ O
Understanding -X- _ O
Evaluation -X- _ O
( -X- _ O
SLUE -X- _ O
) -X- _ O
benchmark -X- _ O
( -X- _ O
Shon -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
Unfortunately -X- _ O
, -X- _ O
the -X- _ O
annotation -X- _ O
covers -X- _ O
a -X- _ O
small -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
VoxPopuli -X- _ O
dataset -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
representative -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
, -X- _ O
the -X- _ O
VoxPopuli -X- _ O
is -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
recorded -X- _ O
speeches -X- _ O
in -X- _ O
the -X- _ O
European -X- _ O
Parliament -X- _ O
. -X- _ O

Entity -X- _ O
classes -X- _ O
annotated -X- _ O
in -X- _ O
the -X- _ O
above -X- _ O
datasets -X- _ O
can -X- _ O
be -X- _ O
broadly -X- _ O
divided -X- _ O
into -X- _ O
closed-domain -X- _ O
and -X- _ O
opendomain -X- _ O
types. -X- _ O
Closed-domain -X- _ O
entity -X- _ O
classes -X- _ O
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
almost -X- _ O
gazetteers -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
these -X- _ O
are -X- _ O
classes -X- _ O
for -X- _ O
which -X- _ O
a -X- _ O
vast -X- _ O
majority -X- _ O
of -X- _ O
entities -X- _ O
can -X- _ O
be -X- _ O
listed. -X- _ O
Examples -X- _ O
of -X- _ O
closed-domain -X- _ O
entity -X- _ O
classes -X- _ O
include -X- _ O
geographical -X- _ O
locations -X- _ O
or -X- _ O
first -X- _ O
names -X- _ O
( -X- _ O
since -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
US -X- _ O
first -X- _ O
names -X- _ O
follows -X- _ O
a -X- _ O
power -X- _ O
law -X- _ O
distribution -X- _ O
( -X- _ O
Hahn -X- _ O
and -X- _ O
Bentley -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
relatively -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
first -X- _ O
names -X- _ O
represents -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
first -X- _ O
names -X- _ O
encountered -X- _ O
in -X- _ O
the -X- _ O
dataset -X- _ O
) -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
open-domain -X- _ O
entity -X- _ O
classes -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
summarized -X- _ O
using -X- _ O
a -X- _ O
gazetteer. -X- _ O
This -X- _ O
is -X- _ O
the -X- _ O
case -X- _ O
with -X- _ O
numbers -X- _ O
, -X- _ O
product -X- _ O
names -X- _ O
, -X- _ O
money -X- _ O
, -X- _ O
or -X- _ O
organizations. -X- _ O
Unfortunately -X- _ O
, -X- _ O
gazetteers -X- _ O
are -X- _ O
not -X- _ O
a -X- _ O
viable -X- _ O
solution -X- _ O
even -X- _ O
for -X- _ O
closed-domain -X- _ O
entity -X- _ O
classes -X- _ O
because -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
may -X- _ O
produce -X- _ O
tokens -X- _ O
outside -X- _ O
the -X- _ O
gazetteer -X- _ O
. -X- _ O

One -X- _ O
possible -X- _ O
solution -X- _ O
would -X- _ O
be -X- _ O
to -X- _ O
try -X- _ O
to -X- _ O
overcome -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
by -X- _ O
retrofitting -X- _ O
token -X- _ O
representations -X- _ O
using -X- _ O
domain -X- _ O
datasets. -X- _ O
This -X- _ O
technique -X- _ O
has -X- _ O
been -X- _ O
successfully -X- _ O
applied -X- _ O
to -X- _ O
static -X- _ O
word -X- _ O
embeddings -X- _ O
to -X- _ O
mitigate -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
by -X- _ O
. -X- _ O

It -X- _ O
would -X- _ O
be -X- _ O
interesting -X- _ O
to -X- _ O
see -X- _ O
the -X- _ O
same -X- _ O
technique -X- _ O
applied -X- _ O
to -X- _ O
transformer-based -X- _ O
embeddings -X- _ O
. -X- _ O

Experiments -X- _ O
One -X- _ O
might -X- _ O
argue -X- _ O
that -X- _ O
the -X- _ O
most -X- _ O
important -X- _ O
variable -X- _ O
influencing -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
downstream -X- _ O
NLP -X- _ O
tasks -X- _ O
on -X- _ O
a -X- _ O
transcript -X- _ O
is -X- _ O
the -X- _ O
choice -X- _ O
of -X- _ O
a -X- _ O
particular -X- _ O
ASR -X- _ B-TaskName
system. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
find -X- _ O
this -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
case. -X- _ O
The -X- _ O
ASR-NLP -X- _ B-TaskName
gap -X- _ O
is -X- _ O
equally -X- _ O
pronounced -X- _ O
for -X- _ O
all -X- _ O
major -X- _ O
commercial -X- _ O
ASR -X- _ B-TaskName
systems. -X- _ O
In -X- _ O
our -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
offered -X- _ O
by -X- _ O
Microsoft -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
lowest -X- _ O
reported -X- _ O
WER -X- _ B-MetricName
on -X- _ O
the -X- _ O
Earnings-21 -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Del -X- _ O
Rio -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

Performance -X- _ O
on -X- _ O
gold -X- _ O
transcripts -X- _ O
In -X- _ O
our -X- _ O
first -X- _ O
experiment -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
state-of-theart -X- _ O
NER -X- _ B-TaskName
model -X- _ O
on -X- _ O
gold -X- _ O
transcripts. -X- _ O
We -X- _ O
train -X- _ O
a -X- _ O
transformer -X- _ O
using -X- _ O
the -X- _ O
Roberta-Large -X- _ B-MethodName
architecture -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
on -X- _ O
the -X- _ O
train -X- _ O
split -X- _ O
of -X- _ O
the -X- _ O
OntoNotes -X- _ B-DatasetName
dataset -X- _ O
3 -X- _ O
. -X- _ O
The -X- _ O
evaluation -X- _ O
is -X- _ O
performed -X- _ O
on -X- _ O
Earnings-21 -X- _ B-DatasetName
, -X- _ O
SWNE -X- _ B-DatasetName
, -X- _ O
and -X- _ O
the -X- _ O
test -X- _ O
split -X- _ O
of -X- _ O
the -X- _ O
OntoNotes -X- _ B-DatasetName
datasets. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
comparison -X- _ O
as -X- _ O
fair -X- _ O
as -X- _ O
possible -X- _ O
, -X- _ O
we -X- _ O
normalize -X- _ O
gold -X- _ O
transcripts -X- _ O
using -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
heuristics. -X- _ O
Normalization -X- _ O
changes -X- _ O
all -X- _ O
numbers -X- _ O
into -X- _ O
respective -X- _ O
words. -X- _ O
We -X- _ O
unify -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
the -X- _ O
currency -X- _ O
indicator -X- _ O
when -X- _ O
spelling -X- _ O
monetary -X- _ O
values -X- _ O
and -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
the -X- _ O
percent -X- _ O
sign. -X- _ O
All -X- _ O
gold -X- _ O
transcripts -X- _ O
are -X- _ O
properly -X- _ O
cased -X- _ O
and -X- _ O
punctuated. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
as -X- _ O
measured -X- _ O
by -X- _ O
the -X- _ O
micro -X- _ B-MetricName
F -X- _ I-MetricName
1 -X- _ I-MetricName
score -X- _ O
because -X- _ O
the -X- _ O
dataset -X- _ O
is -X- _ O
highly -X- _ O
imbalanced -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
are -X- _ O
interested -X- _ O
in -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model. -X- _ O
We -X- _ O
must -X- _ O
point -X- _ O
out -X- _ O
that -X- _ O
the -X- _ O
experimental -X- _ O
setting -X- _ O
is -X- _ O
very -X- _ O
favorable -X- _ O
for -X- _ O
the -X- _ O
ASR. -X- _ B-TaskName
Not -X- _ O
only -X- _ O
is -X- _ O
the -X- _ O
transcript -X- _ O
fully -X- _ O
normalized -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
alignment -X- _ O
procedure -X- _ O
is -X- _ O
fine-tuned -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
misalignments -X- _ O
as -X- _ O
much -X- _ O
as -X- _ O
possible. -X- _ O
Furthermore -X- _ O
, -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
text -X- _ O
fragments -X- _ O
chunked -X- _ O
according -X- _ O
to -X- _ O
punctuation -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcripts -X- _ O
and -X- _ O
not -X- _ O
to -X- _ O
fixed-width -X- _ O
sliding -X- _ O
windows. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
of -X- _ O
much -X- _ O
higher -X- _ O
quality -X- _ O
than -X- _ O
should -X- _ O
be -X- _ O
expected -X- _ O
from -X- _ O
the -X- _ O
commercial -X- _ O
ASR -X- _ B-TaskName
. -X- _ O

Despite -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
OntoNotes -X- _ B-DatasetName
contains -X- _ O
a -X- _ O
significant -X- _ O
amount -X- _ O
of -X- _ O
transcripts -X- _ O
of -X- _ O
unscripted -X- _ O
human -X- _ O
conversations -X- _ O
, -X- _ O
the -X- _ O
accuracy -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
deteriorates -X- _ O
dramatically -X- _ O
on -X- _ O
SWNE -X- _ B-DatasetName
and -X- _ O
Earnings-21 -X- _ B-DatasetName
datasets. -X- _ O
For -X- _ O
all -X- _ O
entity -X- _ O
classes -X- _ O
, -X- _ O
the -X- _ O
recognition -X- _ O
in -X- _ O
SWNE -X- _ B-DatasetName
and -X- _ O
Earnings-21 -X- _ B-DatasetName
is -X- _ O
much -X- _ O
lower -X- _ O
than -X- _ O
for -X- _ O
the -X- _ O
OntoNotes. -X- _ O
The -X- _ O
NER -X- _ B-TaskName
model -X- _ O
struggles -X- _ O
particularly -X- _ O
with -X- _ O
open-domain -X- _ O
entity -X- _ O
classes. -X- _ O
The -X- _ O
complete -X- _ O
failure -X- _ O
to -X- _ O
recognize -X- _ O
MONEY -X- _ O
, -X- _ O
PRODUCT -X- _ O
or -X- _ O
TIME -X- _ O
entities -X- _ O
makes -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
practically -X- _ O
unusable -X- _ O
in -X- _ O
real-world -X- _ O
scenarios. -X- _ O
Leaving -X- _ O
aside -X- _ O
more -X- _ O
exotic -X- _ O
classes -X- _ O
represented -X- _ O
in -X- _ O
the -X- _ O
data -X- _ O
by -X- _ O
a -X- _ O
few -X- _ O
examples -X- _ O
( -X- _ O
LANGUAGE -X- _ O
, -X- _ O
LAW -X- _ O
, -X- _ O
WORK_OF_ART -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
performs -X- _ O
better -X- _ O
( -X- _ O
albeit -X- _ O
not -X- _ O
satisfactorily -X- _ O
) -X- _ O
for -X- _ O
closed-domain -X- _ O
classes -X- _ O
, -X- _ O
where -X- _ O
it -X- _ O
can -X- _ O
to -X- _ O
a -X- _ O
certain -X- _ O
degree -X- _ O
memorize -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
instances -X- _ O
of -X- _ O
a -X- _ O
class. -X- _ O
For -X- _ O
open-domain -X- _ O
entity -X- _ O
classes -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
disappointingly -X- _ O
bad. -X- _ O
Please -X- _ O
note -X- _ O
that -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
properly -X- _ O
cased -X- _ O
and -X- _ O
punctuated -X- _ O
transcripts -X- _ O
of -X- _ O
conversations -X- _ O
and -X- _ O
not -X- _ O
to -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
, -X- _ O
yet -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
scores -X- _ O
are -X- _ O
significantly -X- _ O
lower -X- _ O
than -X- _ O
the -X- _ O
scores -X- _ O
obtained -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
split -X- _ O
of -X- _ O
the -X- _ O
OntoNotes -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Performance -X- _ O
on -X- _ O
ASR -X- _ B-TaskName
transcripts -X- _ O
In -X- _ O
the -X- _ O
second -X- _ O
experiment -X- _ O
, -X- _ O
we -X- _ O
run -X- _ O
our -X- _ O
NER -X- _ B-TaskName
model -X- _ O
on -X- _ O
the -X- _ O
Earnings-21 -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
measure -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
occurrences -X- _ O
of -X- _ O
every -X- _ O
error -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
2. -X- _ O
Transcripts -X- _ O
of -X- _ O
Earnings-21 -X- _ B-DatasetName
recordings -X- _ O
are -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
Microsoft -X- _ B-MethodName
ASR. -X- _ I-MethodName
The -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
5. -X- _ O
The -X- _ O
first -X- _ O
column -X- _ O
reports -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
occurrences -X- _ O
of -X- _ O
NER -X- _ B-TaskName
model -X- _ O
errors -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
is -X- _ O
fully -X- _ O
matched -X- _ O
with -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
( -X- _ O
no -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
in -X- _ O
the -X- _ O
transcript -X- _ O
) -X- _ O
. -X- _ O
Subsequent -X- _ O
columns -X- _ O
report -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
occurrences -X- _ O
of -X- _ O
NER -X- _ B-TaskName
model -X- _ O
errors -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
is -X- _ O
misaligned -X- _ O
with -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
due -X- _ O
to -X- _ O
token -X- _ O
insertion -X- _ O
, -X- _ O
substitution -X- _ O
, -X- _ O
or -X- _ O
deletion -X- _ O
by -X- _ O
the -X- _ O
ASR. -X- _ B-TaskName
Please -X- _ O
note -X- _ O
that -X- _ O
ASR -X- _ B-TaskName
insertion -X- _ O
, -X- _ O
substitution -X- _ O
, -X- _ O
and -X- _ O
deletion -X- _ O
errors -X- _ O
often -X- _ O
co-occur -X- _ O
within -X- _ O
a -X- _ O
single -X- _ O
entity -X- _ O
span -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O
so -X- _ O
a -X- _ O
single -X- _ O
entity -X- _ O
span -X- _ O
may -X- _ O
contribute -X- _ O
to -X- _ O
multiple -X- _ O
cells -X- _ O
in -X- _ O
the -X- _ O
table. -X- _ O
Our -X- _ O
intention -X- _ O
is -X- _ O
to -X- _ O
show -X- _ O
the -X- _ O
real -X- _ O
impact -X- _ O
of -X- _ O
each -X- _ O
type -X- _ O
of -X- _ O
ASR-NLP -X- _ B-TaskName
error -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
clearly -X- _ O
show -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
the -X- _ O
joint -X- _ O
ASR-NLP -X- _ B-TaskName
model -X- _ O
evaluation -X- _ O
, -X- _ O
as -X- _ O
reflected -X- _ O
by -X- _ O
the -X- _ O
breakdown -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
error -X- _ O
sources -X- _ O
4 -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
makes -X- _ O
mistakes -X- _ O
on -X- _ O
fully -X- _ O
matched -X- _ O
transcripts -X- _ O
of -X- _ O
spoken -X- _ O
conversations -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
manages -X- _ O
to -X- _ O
retrieve -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
in -X- _ O
the -X- _ O
entity -X- _ O
span -X- _ O
without -X- _ O
errors. -X- _ O
These -X- _ O
errors -X- _ O
are -X- _ O
responsible -X- _ O
for -X- _ O
approximately -X- _ O
half -X- _ O
of -X- _ O
all -X- _ O
recorded -X- _ O
errors. -X- _ O
Let -X- _ O
us -X- _ O
stress -X- _ O
this -X- _ O
result -X- _ O
again -X- _ O
: -X- _ O
NER -X- _ B-TaskName
models -X- _ O
are -X- _ O
inherently -X- _ O
incapable -X- _ O
of -X- _ O
processing -X- _ O
the -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
; -X- _ O
even -X- _ O
if -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
introduces -X- _ O
no -X- _ O
errors -X- _ O
, -X- _ O
37 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
entity -X- _ O
spans -X- _ O
are -X- _ O
partially -X- _ O
or -X- _ O
fully -X- _ O
wrong -X- _ O
( -X- _ O
first -X- _ O
column -X- _ O
in -X- _ O
Tab. -X- _ O
5 -X- _ O
) -X- _ O

We -X- _ O
also -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
is -X- _ O
very -X- _ O
sensitive -X- _ O
to -X- _ O
errors -X- _ O
introduced -X- _ O
by -X- _ O
the -X- _ O
ASR. -X- _ B-TaskName
It -X- _ O
can -X- _ O
correctly -X- _ O
recognize -X- _ O
only -X- _ O
18 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
entities -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
substitutes -X- _ O
a -X- _ O
token -X- _ O
inside -X- _ O
the -X- _ O
entity -X- _ O
span -X- _ O
, -X- _ O
6.8 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
entities -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
inserts -X- _ O
a -X- _ O
token -X- _ O
inside -X- _ O
the -X- _ O
entity -X- _ O
span -X- _ O
, -X- _ O
and -X- _ O
it -X- _ O
fails -X- _ O
to -X- _ O
correctly -X- _ O
recognize -X- _ O
an -X- _ O
entity -X- _ O
when -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
deletes -X- _ O
a -X- _ O
token -X- _ O
inside -X- _ O
the -X- _ O
entity -X- _ O
span. -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
are -X- _ O
responsible -X- _ O
for -X- _ O
many -X- _ O
hallucinated -X- _ O
entities -X- _ O
and -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
omissions. -X- _ O
In -X- _ O
practice -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entity -X- _ O
errors -X- _ O
doubles -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
errors -X- _ O
made -X- _ O
on -X- _ O
fully -X- _ O
matched -X- _ O
transcript -X- _ O
: -X- _ O
ca. -X- _ O
6200 -X- _ O
omitted -X- _ O
entities -X- _ O
in -X- _ O
total -X- _ O
vs. -X- _ O
3600 -X- _ O
with -X- _ O
perfect -X- _ O
transcript -X- _ O
and -X- _ O
ca. -X- _ O
2000 -X- _ O
hallucinated -X- _ O
ones -X- _ O
versus -X- _ O
1000 -X- _ O
with -X- _ O
the -X- _ O
perfect -X- _ O
transcript. -X- _ O
Again -X- _ O
, -X- _ O
let -X- _ O
us -X- _ O
reiterate -X- _ O
this -X- _ O
finding -X- _ O
: -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
is -X- _ O
helpless -X- _ O
when -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
are -X- _ O
introduced -X- _ O
inside -X- _ O
entity -X- _ O
spans -X- _ O
and -X- _ O
can -X- _ O
not -X- _ O
retrieve -X- _ O
an -X- _ O
entity -X- _ O
when -X- _ O
tokens -X- _ O
are -X- _ O
inserted -X- _ O
, -X- _ O
substituted -X- _ O
, -X- _ O
or -X- _ O
deleted -X- _ O
from -X- _ O
entity -X- _ O
spans. -X- _ O
WER -X- _ B-MetricName
of -X- _ O
20.0 -X- _ B-MetricValue
reported -X- _ O
by -X- _ O
( -X- _ O
Del -X- _ O
Rio -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
! -X- _ O
Finally -X- _ O
, -X- _ O
the -X- _ O
case -X- _ O
for -X- _ O
partial -X- _ O
matches -X- _ O
, -X- _ O
while -X- _ O
smaller -X- _ O
than -X- _ O
hallucinated -X- _ O
, -X- _ O
replacement -X- _ O
, -X- _ O
and -X- _ O
omissions -X- _ O
, -X- _ O
is -X- _ O
of -X- _ O
great -X- _ O
importance. -X- _ O
The -X- _ O
true -X- _ O
effect -X- _ O
of -X- _ O
entity -X- _ O
hallucinations -X- _ O
and -X- _ O
omissions -X- _ O
in -X- _ O
a -X- _ O
joint -X- _ O
ASR-NLP -X- _ B-MethodName
system -X- _ O
can -X- _ O
only -X- _ O
be -X- _ O
measured -X- _ O
on -X- _ O
a -X- _ O
downstream -X- _ O
task. -X- _ O
Usually -X- _ O
, -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
is -X- _ O
a -X- _ O
single -X- _ O
step -X- _ O
in -X- _ O
a -X- _ O
wider -X- _ O
NLP -X- _ O
task. -X- _ O
This -X- _ O
task -X- _ O
may -X- _ O
have -X- _ O
a -X- _ O
separate -X- _ O
evaluation -X- _ O
scheme -X- _ O
with -X- _ O
different -X- _ O
metrics -X- _ O
and -X- _ O
business -X- _ O
objectives. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
intent -X- _ O
retrieval -X- _ O
and -X- _ O
slot -X- _ O
filling -X- _ O
, -X- _ O
hallucinating -X- _ O
or -X- _ O
omitting -X- _ O
an -X- _ O
entity -X- _ O
span -X- _ O
can -X- _ O
lead -X- _ O
to -X- _ O
a -X- _ O
situation -X- _ O
where -X- _ O
the -X- _ O
intent -X- _ O
is -X- _ O
either -X- _ O
not -X- _ O
matched -X- _ O
or -X- _ O
matched -X- _ O
in -X- _ O
the -X- _ O
wrong -X- _ O
place. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
effect -X- _ O
of -X- _ O
partial -X- _ O
matches -X- _ O
is -X- _ O
more -X- _ O
difficult -X- _ O
to -X- _ O
evaluate. -X- _ O
With -X- _ O
partial -X- _ O
matching -X- _ O
, -X- _ O
the -X- _ O
intent -X- _ O
is -X- _ O
caught -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
slot -X- _ O
is -X- _ O
filled -X- _ O
, -X- _ O
but -X- _ O
most -X- _ O
probably -X- _ O
, -X- _ O
the -X- _ O
slot -X- _ O
is -X- _ O
filled -X- _ O
with -X- _ O
incorrect -X- _ O
values. -X- _ O
The -X- _ O
scale -X- _ O
of -X- _ O
failures -X- _ O
and -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
upstream -X- _ O
model -X- _ O
improvements -X- _ O
can -X- _ O
only -X- _ O
be -X- _ O
measured -X- _ O
by -X- _ O
evaluating -X- _ O
the -X- _ O
entire -X- _ O
NLP -X- _ O
pipeline -X- _ O
on -X- _ O
a -X- _ O
reference -X- _ O
dataset -X- _ O
with -X- _ O
annotations -X- _ O
of -X- _ O
intents -X- _ O
and -X- _ O
slots. -X- _ O
This -X- _ O
observation -X- _ O
strengthens -X- _ O
our -X- _ O
belief -X- _ O
that -X- _ O
measuring -X- _ O
the -X- _ O
increase -X- _ O
in -X- _ O
the -X- _ O
scale -X- _ O
of -X- _ O
errors -X- _ O
in -X- _ O
a -X- _ O
joint -X- _ O
ASR-NLP -X- _ B-MethodName
system -X- _ O
is -X- _ O
more -X- _ O
important -X- _ O
than -X- _ O
focusing -X- _ O
on -X- _ O
technical -X- _ O
details -X- _ O
of -X- _ O
measures -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
, -X- _ O
WER -X- _ B-MetricName
, -X- _ O
or -X- _ O
entity -X- _ B-MetricName
WER -X- _ I-MetricName
. -X- _ O

Related -X- _ O
Work -X- _ O
In -X- _ O
our -X- _ O
opinion -X- _ O
, -X- _ O
the -X- _ O
NLP -X- _ O
research -X- _ O
community -X- _ O
has -X- _ O
an -X- _ O
overly -X- _ O
optimistic -X- _ O
view -X- _ O
of -X- _ O
the -X- _ O
WERs -X- _ B-MetricName
introduced -X- _ O
by -X- _ O
ASR -X- _ B-TaskName
systems. -X- _ O
Recent -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
WERs -X- _ B-MetricName
in -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
human -X- _ O
speech -X- _ O
is -X- _ O
much -X- _ O
higher -X- _ O
than -X- _ O
expected. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
Szymański -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
showed -X- _ O
that -X- _ O
a -X- _ O
transcript -X- _ O
of -X- _ O
a -X- _ O
standard -X- _ O
GSM -X- _ O
phone -X- _ O
call -X- _ O
conversation -X- _ O
is -X- _ O
subject -X- _ O
to -X- _ O
a -X- _ O
16 -X- _ O
% -X- _ O
-20 -X- _ O
% -X- _ O
error -X- _ O
rate. -X- _ O
Del -X- _ O
Rio -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
confirm -X- _ O
this -X- _ O
result -X- _ O
and -X- _ O
report -X- _ O
how -X- _ O
WERs -X- _ B-MetricName
differ -X- _ O
between -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
entity -X- _ O
spans. -X- _ O
Spans -X- _ O
related -X- _ O
to -X- _ O
date -X- _ O
, -X- _ O
time -X- _ O
, -X- _ O
and -X- _ O
ordinal -X- _ O
numbers -X- _ O
were -X- _ O
observed -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
lower -X- _ O
WER -X- _ B-MetricName
than -X- _ O
entities -X- _ O
related -X- _ O
to -X- _ O
proper -X- _ O
names. -X- _ O
Facility -X- _ O
names -X- _ O
, -X- _ O
organizations -X- _ O
, -X- _ O
and -X- _ O
personal -X- _ O
names -X- _ O
demonstrate -X- _ O
a -X- _ O
very -X- _ O
high -X- _ O
WER -X- _ B-MetricName
of -X- _ O
30 -X- _ B-MetricValue
% -X- _ I-MetricValue
-50 -X- _ I-MetricValue
% -X- _ I-MetricValue
. -X- _ O
McNamara -X- _ O
and -X- _ O
Kokotov -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
also -X- _ O
released -X- _ O
a -X- _ O
library -X- _ O
for -X- _ O
using -X- _ O
Finite -X- _ O
State -X- _ O
Transducers -X- _ O
( -X- _ O
FSTs -X- _ O
) -X- _ O
to -X- _ O
account -X- _ O
for -X- _ O
different -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
entity -X- _ O
( -X- _ O
2020 -X- _ O
vs. -X- _ O
twenty -X- _ O
twenty -X- _ O
) -X- _ O
among -X- _ O
ASRs -X- _ B-TaskName
. -X- _ O

These -X- _ O
findings -X- _ O
are -X- _ O
in -X- _ O
stark -X- _ O
contrast -X- _ O
to -X- _ O
initial -X- _ O
reports. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
Surdeanu -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2005 -X- _ O
) -X- _ O
reported -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
in -X- _ O
Switchboard -X- _ O
corpus -X- _ O
to -X- _ O
be -X- _ O
within -X- _ O
5 -X- _ O
% -X- _ O
from -X- _ O
a -X- _ O
system -X- _ O
evaluated -X- _ O
on -X- _ O
clean -X- _ O
textual -X- _ O
data. -X- _ O
Similarly -X- _ O
, -X- _ O
Béchet -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2002 -X- _ O
) -X- _ O
claims -X- _ O
to -X- _ O
have -X- _ O
achieved -X- _ O
approximately -X- _ O
0.90 -X- _ B-MetricValue
F -X- _ B-MetricName
1 -X- _ I-MetricName
for -X- _ O
recognizing -X- _ O
phone -X- _ O
numbers -X- _ O
and -X- _ O
0.70 -X- _ B-MetricValue
F -X- _ B-MetricName
1 -X- _ I-MetricName
for -X- _ O
recognizing -X- _ O
money -X- _ O
mentions -X- _ O
in -X- _ O
the -X- _ O
transcripts -X- _ O
from -X- _ O
the -X- _ O
AT -X- _ O
& -X- _ O
T -X- _ O
How -X- _ O
may -X- _ O
I -X- _ O
help -X- _ O
you -X- _ O
? -X- _ O
system -X- _ O
under -X- _ O
27.4 -X- _ B-MetricValue
% -X- _ I-MetricValue
WER -X- _ B-MetricName
ratio. -X- _ O
Favre -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2005 -X- _ O
) -X- _ O
apply -X- _ O
NER -X- _ O
models -X- _ O
to -X- _ O
French -X- _ O
corpora -X- _ O
and -X- _ O
achieve -X- _ O
0.74 -X- _ B-MetricValue
F -X- _ B-MetricName
1 -X- _ I-MetricName
for -X- _ O
a -X- _ O
relatively -X- _ O
broad -X- _ O
set -X- _ O
of -X- _ O
named -X- _ O
entities. -X- _ O
Precision -X- _ O
, -X- _ O
recall -X- _ O
, -X- _ O
and -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
scores -X- _ O
are -X- _ O
standard -X- _ O
metrics -X- _ O
for -X- _ O
reporting -X- _ O
NER -X- _ B-TaskName
model -X- _ O
performance -X- _ O
in -X- _ O
NLP. -X- _ O
However -X- _ O
, -X- _ O
these -X- _ O
metrics -X- _ O
can -X- _ O
produce -X- _ O
unreliable -X- _ O
scores -X- _ O
where -X- _ O
entity -X- _ O
spans -X- _ O
are -X- _ O
marked -X- _ O
on -X- _ O
spontaneous -X- _ O
human -X- _ O
conversation -X- _ O
transcripts -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
conversational -X- _ O
artifacts -X- _ O
( -X- _ O
repetitions -X- _ O
mentioned -X- _ O
above -X- _ O
, -X- _ O
backchanneling -X- _ O
, -X- _ O
phatic -X- _ O
expressions -X- _ O
) -X- _ O
. -X- _ O
An -X- _ O
example -X- _ O
of -X- _ O
entity -X- _ O
span -X- _ O
tagging -X- _ O
where -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
metric -X- _ O
produces -X- _ O
highly -X- _ O
misleading -X- _ O
scores -X- _ O
is -X- _ O
presented -X- _ O
in -X- _ O
Section -X- _ O
6 -X- _ O
. -X- _ O

To -X- _ O
account -X- _ O
for -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
these -X- _ O
artifacts -X- _ O
, -X- _ O
Message -X- _ O
Understanding -X- _ O
Conference -X- _ O
( -X- _ O
MUC -X- _ O
) -X- _ O
( -X- _ O
Grishman -X- _ O
and -X- _ O
Sundheim -X- _ O
( -X- _ O
1996 -X- _ O
) -X- _ O
; -X- _ O
Nadeau -X- _ O
and -X- _ O
Sekine -X- _ O
( -X- _ O
2007 -X- _ O
) -X- _ O
) -X- _ O
introduced -X- _ O
metrics -X- _ O
that -X- _ O
allow -X- _ O
for -X- _ O
partial -X- _ O
matching -X- _ O
of -X- _ O
an -X- _ O
entity -X- _ O
span. -X- _ O
MUC -X- _ O
defines -X- _ O
six -X- _ O
categories -X- _ O
of -X- _ O
partial -X- _ O
matching -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
degree -X- _ O
of -X- _ O
span -X- _ O
overlap -X- _ O
, -X- _ O
the -X- _ O
type -X- _ O
of -X- _ O
the -X- _ O
matched -X- _ O
entity -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
strictness -X- _ O
of -X- _ O
expectations -X- _ O
, -X- _ O
as -X- _ O
outlined -X- _ O
by -X- _ O
Batista -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
this -X- _ O
problem -X- _ O
has -X- _ O
been -X- _ O
addressed -X- _ O
by -X- _ O
Caubrière -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
who -X- _ O
argues -X- _ O
for -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
slot -X- _ O
error -X- _ O
rates -X- _ O
. -X- _ O

To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
Hatmi -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
was -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
attempt -X- _ O
to -X- _ O
incorporate -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
into -X- _ O
the -X- _ O
automatic -X- _ O
speech -X- _ O
transcription -X- _ O
process. -X- _ O
The -X- _ O
authors -X- _ O
tagged -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
dictionary -X- _ O
with -X- _ O
named -X- _ O
entity -X- _ O
tags -X- _ O
( -X- _ O
since -X- _ O
ASR -X- _ O
can -X- _ O
not -X- _ O
produce -X- _ O
any -X- _ O
words -X- _ O
not -X- _ O
present -X- _ O
in -X- _ O
its -X- _ O
dictionary -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
initial -X- _ O
approach -X- _ O
has -X- _ O
been -X- _ O
superseded -X- _ O
by -X- _ O
methods -X- _ O
aiming -X- _ O
at -X- _ O
training -X- _ O
end-to-end -X- _ O
joint -X- _ O
models -X- _ O
for -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
, -X- _ O
as -X- _ O
proposed -X- _ O
by -X- _ O
Ghannay -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
Serdyuk -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Stiefel -X- _ O
and -X- _ O
Vu -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
authors -X- _ O
train -X- _ O
ASR -X- _ B-TaskName
systems -X- _ O
to -X- _ O
predict -X- _ O
transcription -X- _ O
tokens -X- _ O
and -X- _ O
their -X- _ O
part-of-speech -X- _ O
or -X- _ O
named -X- _ O
entity -X- _ O
tags -X- _ O
in -X- _ O
these -X- _ O
works -X- _ O
. -X- _ O

Limitations -X- _ O
Obviously -X- _ O
, -X- _ O
the -X- _ O
work -X- _ O
presented -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
limited -X- _ O
to -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
conversations -X- _ O
in -X- _ O
English. -X- _ O
Since -X- _ O
we -X- _ O
are -X- _ O
investigating -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
, -X- _ O
we -X- _ O
have -X- _ O
to -X- _ O
point -X- _ O
out -X- _ O
that -X- _ O
there -X- _ O
are -X- _ O
practically -X- _ O
no -X- _ O
datasets -X- _ O
of -X- _ O
human -X- _ O
conversations -X- _ O
( -X- _ O
both -X- _ O
audio -X- _ O
and -X- _ O
transcripts -X- _ O
) -X- _ O
annotated -X- _ O
with -X- _ O
entity -X- _ O
spans -X- _ O
apart -X- _ O
from -X- _ O
SWNE -X- _ B-DatasetName
, -X- _ O
OntoNotes -X- _ B-DatasetName
and -X- _ O
Earnings-21 -X- _ B-DatasetName
, -X- _ O
the -X- _ O
three -X- _ O
datasets -X- _ O
used -X- _ O
in -X- _ O
our -X- _ O
paper. -X- _ O
These -X- _ O
datasets -X- _ O
are -X- _ O
relatively -X- _ O
small -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
frequency -X- _ O
of -X- _ O
appearance -X- _ O
of -X- _ O
entity -X- _ O
classes -X- _ O
is -X- _ O
extremely -X- _ O
skewed -X- _ O
, -X- _ O
with -X- _ O
several -X- _ O
entity -X- _ O
classes -X- _ O
represented -X- _ O
by -X- _ O
a -X- _ O
handful -X- _ O
of -X- _ O
examples -X- _ O
. -X- _ O

Another -X- _ O
significant -X- _ O
limitation -X- _ O
of -X- _ O
the -X- _ O
results -X- _ O
reported -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
the -X- _ O
choice -X- _ O
of -X- _ O
metric. -X- _ O
Following -X- _ O
the -X- _ O
common -X- _ O
practice -X- _ O
in -X- _ O
the -X- _ O
NLP -X- _ O
community -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
chosen -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
as -X- _ O
the -X- _ O
primary -X- _ O
metric -X- _ O
of -X- _ O
entity -X- _ O
recognition. -X- _ O
However -X- _ O
, -X- _ O
this -X- _ O
metric -X- _ O
is -X- _ O
questionable -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
NER -X- _ B-TaskName
recognition -X- _ O
in -X- _ O
ASR -X- _ B-TaskName
transcripts -X- _ O
because -X- _ O
it -X- _ O
is -X- _ O
highly -X- _ O
dependent -X- _ O
on -X- _ O
two -X- _ O
factors -X- _ O
: -X- _ O
the -X- _ O
WER -X- _ B-MetricName
produced -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
the -X- _ O
definition -X- _ O
of -X- _ O
span -X- _ O
alignment. -X- _ O
Consider -X- _ O
a -X- _ O
gold -X- _ O
transcript -X- _ O
annotation -X- _ O
" -X- _ O
John -X- _ O
B-PERSON -X- _ O
F. -X- _ O
I-PERSON -X- _ O
Kennedy -X- _ O
I-PERSON -X- _ O
" -X- _ O
and -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
with -X- _ O
" -X- _ O
F. -X- _ O
" -X- _ O
transcribed -X- _ O
as -X- _ O
" -X- _ O
eh -X- _ O
" -X- _ O
annotated -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
" -X- _ O
John -X- _ O
B-PERSON -X- _ O
eh -X- _ O
Kennedy -X- _ O
B-PERSON -X- _ O
. -X- _ O
" -X- _ O
Should -X- _ O
this -X- _ O
annotation -X- _ O
be -X- _ O
considered -X- _ O
correct -X- _ O
? -X- _ O
The -X- _ O
original -X- _ O
person -X- _ O
entity -X- _ O
starting -X- _ O
at -X- _ O
" -X- _ O
John -X- _ O
" -X- _ O
is -X- _ O
only -X- _ O
partially -X- _ O
matched -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
new -X- _ O
person -X- _ O
entity -X- _ O
starting -X- _ O
at -X- _ O
" -X- _ O
Kennedy -X- _ O
" -X- _ O
is -X- _ O
introduced -X- _ O
in -X- _ O
the -X- _ O
ASR -X- _ O
output. -X- _ O
Consider -X- _ O
another -X- _ O
gold -X- _ O
annotation -X- _ O
of -X- _ O
the -X- _ O
following -X- _ O
transcript -X- _ O
: -X- _ O
" -X- _ O
second -X- _ O
B-DATE -X- _ O
quarter -X- _ O
I-DATE -X- _ O
twenty -X- _ O
I-DATE -X- _ O
twenty -X- _ O
I-DATE -X- _ O
, -X- _ O
" -X- _ O
which -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model -X- _ O
tags -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

" -X- _ O
second -X- _ O
B-DATE -X- _ O
quarter -X- _ O
I-DATE -X- _ O
twenty -X- _ O
B-CARDINAL -X- _ O
twenty -X- _ O
I-CARDINAL -X- _ O
" -X- _ O
( -X- _ O
NER -X- _ B-TaskName
model -X- _ O
trained -X- _ O
on -X- _ O
written -X- _ O
language -X- _ O
does -X- _ O
not -X- _ O
recognize -X- _ O
" -X- _ O
twenty -X- _ O
twenty -X- _ O
" -X- _ O
as -X- _ O
a -X- _ O
valid -X- _ O
date -X- _ O
) -X- _ O
. -X- _ O
Again -X- _ O
, -X- _ O
how -X- _ O
should -X- _ O
this -X- _ O
scenario -X- _ O
be -X- _ O
scored -X- _ O
by -X- _ O
an -X- _ O
accuracy -X- _ O
metric -X- _ O
? -X- _ O
Unfortunately -X- _ O
, -X- _ O
the -X- _ O
traditional -X- _ O
definition -X- _ O
of -X- _ O
the -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
is -X- _ O
too -X- _ O
restrictive -X- _ O
to -X- _ O
produce -X- _ O
a -X- _ O
robust -X- _ O
score -X- _ O
that -X- _ O
could -X- _ O
paint -X- _ O
a -X- _ O
reliable -X- _ O
picture -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
performance. -X- _ O
The -X- _ O
design -X- _ O
and -X- _ O
implementation -X- _ O
of -X- _ O
a -X- _ O
metric -X- _ O
that -X- _ O
could -X- _ O
compute -X- _ O
the -X- _ O
alignment -X- _ O
of -X- _ O
entity -X- _ O
spans -X- _ O
in -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
would -X- _ O
be -X- _ O
a -X- _ O
significant -X- _ O
step -X- _ O
in -X- _ O
the -X- _ O
direction -X- _ O
of -X- _ O
producing -X- _ O
more -X- _ O
robust -X- _ O
NER -X- _ B-TaskName
models -X- _ O
for -X- _ O
spoken -X- _ O
conversations -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
with -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
on -X- _ O
audio -X- _ O
files -X- _ O
from -X- _ O
the -X- _ O
Earnings-21 -X- _ B-DatasetName
dataset. -X- _ O
These -X- _ O
files -X- _ O
are -X- _ O
recorded -X- _ O
at -X- _ O
11 -X- _ O
kHz-44 -X- _ O
kHz -X- _ O
, -X- _ O
while -X- _ O
typical -X- _ O
call -X- _ O
center -X- _ O
conversations -X- _ O
are -X- _ O
recorded -X- _ O
at -X- _ O
8 -X- _ O
kHz-16 -X- _ O
kHz. -X- _ O
Unfortunately -X- _ O
, -X- _ O
training -X- _ O
datasets -X- _ O
with -X- _ O
recording -X- _ O
characteristics -X- _ O
resembling -X- _ O
real-world -X- _ O
usage -X- _ O
scenarios -X- _ O
are -X- _ O
unavailable. -X- _ O
We -X- _ O
also -X- _ O
do -X- _ O
not -X- _ O
address -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
racial -X- _ O
, -X- _ O
gender -X- _ O
, -X- _ O
and -X- _ O
age -X- _ O
disparity -X- _ O
( -X- _ O
Koenecke -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
availability -X- _ O
of -X- _ O
sufficiently -X- _ O
representative -X- _ O
and -X- _ O
inclusive -X- _ O
datasets. -X- _ O
It -X- _ O
is -X- _ O
, -X- _ O
however -X- _ O
, -X- _ O
to -X- _ O
be -X- _ O
expected -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
deteriorates -X- _ O
for -X- _ O
the -X- _ O
recordings -X- _ O
of -X- _ O
speakers -X- _ O
other -X- _ O
than -X- _ O
male -X- _ O
speakers -X- _ O
of -X- _ O
General -X- _ O
American -X- _ O
. -X- _ O

Conclusions -X- _ O
Our -X- _ O
work -X- _ O
provides -X- _ O
a -X- _ O
thorough -X- _ O
, -X- _ O
albeit -X- _ O
pessimistic -X- _ O
, -X- _ O
reality -X- _ O
check -X- _ O
on -X- _ O
the -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
in -X- _ O
conversational -X- _ O
transcripts. -X- _ O
Our -X- _ O
first -X- _ O
conclusion -X- _ O
is -X- _ O
straightforward -X- _ O
: -X- _ O
currently -X- _ O
available -X- _ O
NER -X- _ B-TaskName
models -X- _ O
are -X- _ O
not -X- _ O
trained -X- _ O
on -X- _ O
representative -X- _ O
data -X- _ O
( -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
annotated -X- _ O
datasets -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
their -X- _ O
performance -X- _ O
on -X- _ O
transcripts -X- _ O
of -X- _ O
spontaneous -X- _ O
conversations -X- _ O
is -X- _ O
much -X- _ O
worse -X- _ O
than -X- _ O
their -X- _ O
performance -X- _ O
on -X- _ O
written -X- _ O
language. -X- _ O
Importantly -X- _ O
, -X- _ O
this -X- _ O
failure -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
attributed -X- _ O
solely -X- _ O
to -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
ASR -X- _ B-TaskName
word -X- _ O
errors. -X- _ O
As -X- _ O
we -X- _ O
show -X- _ O
, -X- _ O
NER -X- _ B-TaskName
models -X- _ O
exhibit -X- _ O
very -X- _ O
high -X- _ O
entity -X- _ O
WERs -X- _ B-MetricName
even -X- _ O
on -X- _ O
gold -X- _ O
transcripts -X- _ O
, -X- _ O
where -X- _ O
no -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
are -X- _ O
present. -X- _ O
When -X- _ O
the -X- _ O
transcript -X- _ O
contains -X- _ O
ASR -X- _ B-TaskName
insertions -X- _ O
, -X- _ O
substitutions -X- _ O
, -X- _ O
or -X- _ O
deletions -X- _ O
, -X- _ O
the -X- _ O
entity -X- _ O
recognition -X- _ O
rates -X- _ O
fall -X- _ O
to -X- _ O
the -X- _ O
level -X- _ O
where -X- _ O
NER -X- _ B-TaskName
models -X- _ O
become -X- _ O
unusable -X- _ O
in -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

Secondly -X- _ O
, -X- _ O
we -X- _ O
conclude -X- _ O
that -X- _ O
a -X- _ O
completely -X- _ O
new -X- _ O
approach -X- _ O
is -X- _ O
required -X- _ O
to -X- _ O
meaningfully -X- _ O
measure -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
NER -X- _ O
models -X- _ O
on -X- _ O
conversational -X- _ O
transcripts. -X- _ O
Traditional -X- _ O
metrics -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
or -X- _ O
entity -X- _ O
WER -X- _ B-MetricName
do -X- _ O
not -X- _ O
account -X- _ O
for -X- _ O
the -X- _ O
intricate -X- _ O
interplay -X- _ O
of -X- _ O
factors -X- _ O
( -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
, -X- _ O
ASR -X- _ B-TaskName
errors -X- _ O
, -X- _ O
artifacts -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
) -X- _ O
and -X- _ O
do -X- _ O
not -X- _ O
provide -X- _ O
a -X- _ O
useful -X- _ O
insight -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
performance. -X- _ O
We -X- _ O
need -X- _ O
to -X- _ O
design -X- _ O
a -X- _ O
more -X- _ O
complex -X- _ O
evaluation -X- _ O
scheme -X- _ O
that -X- _ O
would -X- _ O
take -X- _ O
into -X- _ O
account -X- _ O
the -X- _ O
token -X- _ O
alignment -X- _ O
errors -X- _ O
, -X- _ O
partial -X- _ O
entity -X- _ O
span -X- _ O
matchings -X- _ O
, -X- _ O
ASR -X- _ B-TaskName
word -X- _ O
errors -X- _ O
, -X- _ O
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
. -X- _ O

Ethics -X- _ O
statement -X- _ O
Following -X- _ O
the -X- _ O
ACM -X- _ O
Code -X- _ O
of -X- _ O
Ethics -X- _ O
and -X- _ O
Professional -X- _ O
Conduct -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
ethical -X- _ O
impact -X- _ O
of -X- _ O
the -X- _ O
work -X- _ O
presented -X- _ O
in -X- _ O
this -X- _ O
paper. -X- _ O
Our -X- _ O
work -X- _ O
aims -X- _ O
at -X- _ O
broadening -X- _ O
the -X- _ O
accessibility -X- _ O
of -X- _ O
communication -X- _ O
technology. -X- _ O
Spontaneous -X- _ O
spoken -X- _ O
language -X- _ O
is -X- _ O
the -X- _ O
least -X- _ O
limiting -X- _ O
and -X- _ O
exclusive -X- _ O
mode -X- _ O
of -X- _ O
interacting -X- _ O
with -X- _ O
an -X- _ O
information -X- _ O
system. -X- _ O
This -X- _ O
mode -X- _ O
does -X- _ O
not -X- _ O
require -X- _ O
any -X- _ O
digital -X- _ O
competencies -X- _ O
or -X- _ O
expensive -X- _ O
resources. -X- _ O
The -X- _ O
ability -X- _ O
to -X- _ O
correctly -X- _ O
process -X- _ O
spontaneous -X- _ O
human -X- _ O
conversations -X- _ O
opens -X- _ O
access -X- _ O
to -X- _ O
technology -X- _ O
to -X- _ O
stakeholders -X- _ O
who -X- _ O
might -X- _ O
have -X- _ O
been -X- _ O
previously -X- _ O
excluded. -X- _ O
We -X- _ O
strive -X- _ O
to -X- _ O
diminish -X- _ O
discrimination -X- _ O
resulting -X- _ O
from -X- _ O
biased -X- _ O
training -X- _ O
datasets -X- _ O
, -X- _ O
which -X- _ O
may -X- _ O
cause -X- _ O
specific -X- _ O
individuals -X- _ O
to -X- _ O
be -X- _ O
disproportionally -X- _ O
mistranscribed -X- _ O
due -X- _ O
to -X- _ O
their -X- _ O
accent -X- _ O
, -X- _ O
dialect -X- _ O
, -X- _ O
or -X- _ O
speech -X- _ O
impediments. -X- _ O
As -X- _ O
digital -X- _ O
voice -X- _ O
applications -X- _ O
become -X- _ O
increasingly -X- _ O
integrated -X- _ O
into -X- _ O
society -X- _ O
's -X- _ O
infrastructure -X- _ O
, -X- _ O
we -X- _ O
feel -X- _ O
the -X- _ O
need -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
statistical -X- _ O
models -X- _ O
processing -X- _ O
spoken -X- _ O
communications -X- _ O
continuously -X- _ O
. -X- _ O

The -X- _ O
ability -X- _ O
to -X- _ O
better -X- _ O
process -X- _ O
and -X- _ O
understand -X- _ O
spoken -X- _ O
human -X- _ O
conversations -X- _ O
carries -X- _ O
the -X- _ O
significant -X- _ O
ethical -X- _ O
risk -X- _ O
associated -X- _ O
with -X- _ O
clandestine -X- _ O
eavesdropping -X- _ O
by -X- _ O
adversarial -X- _ O
agents. -X- _ O
Correct -X- _ O
recognition -X- _ O
of -X- _ O
spoken -X- _ O
names -X- _ O
of -X- _ O
people -X- _ O
, -X- _ O
places -X- _ O
, -X- _ O
organizations -X- _ O
, -X- _ O
or -X- _ O
events -X- _ O
, -X- _ O
can -X- _ O
be -X- _ O
malevolently -X- _ O
used -X- _ O
by -X- _ O
authoritarian -X- _ O
government -X- _ O
agencies -X- _ O
trying -X- _ O
to -X- _ O
suppress -X- _ O
free -X- _ O
speech. -X- _ O
Recognition -X- _ O
of -X- _ O
names -X- _ O
of -X- _ O
products -X- _ O
or -X- _ O
services -X- _ O
may -X- _ O
be -X- _ O
utilized -X- _ O
by -X- _ O
marketers -X- _ O
for -X- _ O
non-consensual -X- _ O
profiling. -X- _ O
Thus -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
in -X- _ O
the -X- _ O
best -X- _ O
interest -X- _ O
to -X- _ O
foster -X- _ O
public -X- _ O
awareness -X- _ O
and -X- _ O
understanding -X- _ O
of -X- _ O
computing -X- _ O
, -X- _ O
the -X- _ O
automatic -X- _ O
processing -X- _ O
of -X- _ O
spontaneous -X- _ O
speech -X- _ O
, -X- _ O
and -X- _ O
its -X- _ O
consequences -X- _ O
. -X- _ O

A -X- _ O
Examples -X- _ O
of -X- _ O
ASR-NLP -X- _ B-TaskName
errors -X- _ O
from -X- _ O
the -X- _ O
Earnings-21 -X- _ B-DatasetName
dataset -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
several -X- _ O
examples -X- _ O
of -X- _ O
alignments -X- _ O
of -X- _ O
the -X- _ O
ASR -X- _ B-TaskName
output -X- _ O
with -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
with -X- _ O
entity -X- _ O
tags. -X- _ O
In -X- _ O
each -X- _ O
table -X- _ O
, -X- _ O
the -X- _ O
upper -X- _ O
two -X- _ O
rows -X- _ O
present -X- _ O
entity -X- _ O
tags -X- _ O
and -X- _ O
word -X- _ O
tokens -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
gold -X- _ O
transcript -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
bottom -X- _ O
two -X- _ O
rows -X- _ O
present -X- _ O
word -X- _ O
tokens -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
ASR -X- _ O
and -X- _ O
entity -X- _ O
tags -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
NER -X- _ B-TaskName
model. -X- _ O
A -X- _ O
detailed -X- _ O
description -X- _ O
of -X- _ O
each -X- _ O
case -X- _ O
is -X- _ O
presented -X- _ O
in -X- _ O
the -X- _ O
caption -X- _ O
of -X- _ O
each -X- _ O

Our -X- _ O
work -X- _ O
does -X- _ O
not -X- _ O
introduce -X- _ O
new -X- _ O
models -X- _ O
or -X- _ O
methods -X- _ O
but -X- _ O
provides -X- _ O
a -X- _ O
negative -X- _ O
reality -X- _ O
check -X- _ O
on -X- _ O
the -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
in -X- _ O
NER -X- _ B-TaskName
recognition -X- _ O
from -X- _ O
spoken -X- _ O
transcripts. -X- _ O
We -X- _ O
address -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
potential -X- _ O
risks -X- _ O
of -X- _ O
NER -X- _ B-TaskName
in -X- _ O
conversational -X- _ O
transcripts -X- _ O
in -X- _ O
Section -X- _ O
8 -X- _ O
Ethics -X- _ O
statement -X- _ O
. -X- _ O

C1. -X- _ O
Did -X- _ O
you -X- _ O
report -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
models -X- _ O
used -X- _ O
, -X- _ O
the -X- _ O
total -X- _ O
computational -X- _ O
budget -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
GPU -X- _ O
hours -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
computing -X- _ O
infrastructure -X- _ O
used -X- _ O
? -X- _ O
Although -X- _ O
we -X- _ O
have -X- _ O
experimented -X- _ O
with -X- _ O
several -X- _ O
NER -X- _ B-TaskName
model -X- _ O
architectures -X- _ O
, -X- _ O
our -X- _ O
contribution -X- _ O
is -X- _ O
not -X- _ O
in -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
SOTA -X- _ O
models. -X- _ O
Quite -X- _ O
the -X- _ O
contrary -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
negative -X- _ O
results -X- _ O
and -X- _ O
we -X- _ O
have -X- _ O
decided -X- _ O
to -X- _ O
omit -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
benchmark -X- _ O
model -X- _ O
training -X- _ O
to -X- _ O
focus -X- _ O
the -X- _ O
paper -X- _ O
on -X- _ O
the -X- _ O
presentation -X- _ O
of -X- _ O
a -X- _ O
much -X- _ O
more -X- _ O
important -X- _ O
aspect -X- _ O
, -X- _ O
namely -X- _ O
, -X- _ O
the -X- _ O
deep -X- _ O
dive -X- _ O
into -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
ASR -X- _ B-TaskName
and -X- _ O
NER -X- _ B-TaskName
errors -X- _ O
. -X- _ O

C2. -X- _ O
Did -X- _ O
you -X- _ O
discuss -X- _ O
the -X- _ O
experimental -X- _ O
setup -X- _ O
, -X- _ O
including -X- _ O
hyperparameter -X- _ O
search -X- _ O
and -X- _ O
best-found -X- _ O
hyperparameter -X- _ O
values -X- _ O
? -X- _ O
As -X- _ O
above -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
experiments -X- _ O
only -X- _ O
serve -X- _ O
to -X- _ O
illustrate -X- _ O
a -X- _ O
much -X- _ O
more -X- _ O
important -X- _ O
and -X- _ O
overlooked -X- _ O
issue. -X- _ O
We -X- _ O
do -X- _ O
not -X- _ O
find -X- _ O
the -X- _ O
particular -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
trained -X- _ O
NER -X- _ B-TaskName
model -X- _ O
important. -X- _ O
We -X- _ O
provide -X- _ O
the -X- _ O
architecture -X- _ O
and -X- _ O
the -X- _ O
training -X- _ O
dataset. -X- _ O
The -X- _ O
training -X- _ O
uses -X- _ O
default -X- _ O
values -X- _ O
of -X- _ O
hyper-parameters -X- _ O
. -X- _ O

C3. -X- _ O
Did -X- _ O
you -X- _ O
report -X- _ O
descriptive -X- _ O
statistics -X- _ O
about -X- _ O
your -X- _ O
results -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
error -X- _ O
bars -X- _ O
around -X- _ O
results -X- _ O
, -X- _ O
summary -X- _ O
statistics -X- _ O
from -X- _ O
sets -X- _ O
of -X- _ O
experiments -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
it -X- _ O
transparent -X- _ O
whether -X- _ O
you -X- _ O
are -X- _ O
reporting -X- _ O
the -X- _ O
max -X- _ O
, -X- _ O
mean -X- _ O
, -X- _ O
etc. -X- _ O
or -X- _ O
just -X- _ O
a -X- _ O
single -X- _ O
run -X- _ O
? -X- _ O
Our -X- _ O
experiments -X- _ O
involve -X- _ O
the -X- _ O
description -X- _ O
of -X- _ O
particularities -X- _ O
of -X- _ O
ASR-NER -X- _ B-TaskName
errors -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
on -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
occurrences -X- _ O
of -X- _ O
each -X- _ O
error -X- _ O
combination -X- _ O
. -X- _ O

CrossSum -X- _ B-DatasetName
: -X- _ O
Beyond -X- _ B-MethodName
English-Centric -X- _ I-MethodName
Cross-Lingual -X- _ I-MethodName
Summarization -X- _ I-MethodName
for -X- _ O
1,500+ -X- _ O
Language -X- _ O
Pairs -X- _ O

We -X- _ O
present -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
a -X- _ O
large-scale -X- _ O
crosslingual -X- _ O
summarization -X- _ O
dataset -X- _ O
comprising -X- _ O
1.68 -X- _ O
million -X- _ O
article-summary -X- _ O
samples -X- _ O
in -X- _ O
1,500+ -X- _ O
language -X- _ O
pairs. -X- _ O
We -X- _ O
create -X- _ O
CrossSum -X- _ B-DatasetName
by -X- _ O
aligning -X- _ O
parallel -X- _ O
articles -X- _ O
written -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
via -X- _ O
cross-lingual -X- _ O
retrieval -X- _ O
from -X- _ O
a -X- _ O
multilingual -X- _ O
abstractive -X- _ O
summarization -X- _ O
dataset -X- _ O
and -X- _ O
perform -X- _ O
a -X- _ O
controlled -X- _ O
human -X- _ O
evaluation -X- _ O
to -X- _ O
validate -X- _ O
its -X- _ O
quality. -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
multistage -X- _ O
data -X- _ O
sampling -X- _ O
algorithm -X- _ O
to -X- _ O
effectively -X- _ O
train -X- _ O
a -X- _ O
cross-lingual -X- _ B-TaskName
summarization -X- _ I-TaskName
model -X- _ O
capable -X- _ O
of -X- _ O
summarizing -X- _ O
an -X- _ O
article -X- _ O
in -X- _ O
any -X- _ O
target -X- _ O
language. -X- _ O
We -X- _ O
also -X- _ O
introduce -X- _ O
LaSE -X- _ B-MetricName
, -X- _ O
an -X- _ O
embedding-based -X- _ O
metric -X- _ O
for -X- _ O
automatically -X- _ O
evaluating -X- _ O
model-generated -X- _ O
summaries. -X- _ O
LaSE -X- _ B-MetricName
is -X- _ O
strongly -X- _ O
correlated -X- _ O
with -X- _ O
ROUGE -X- _ B-MetricName
and -X- _ O
, -X- _ O
unlike -X- _ O
ROUGE -X- _ B-MetricName
, -X- _ O
can -X- _ O
be -X- _ O
reliably -X- _ O
measured -X- _ O
even -X- _ O
in -X- _ O
the -X- _ O
absence -X- _ O
of -X- _ O
references -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
Performance -X- _ O
on -X- _ O
ROUGE -X- _ B-MetricName
and -X- _ O
LaSE -X- _ B-MetricName
indicate -X- _ O
that -X- _ O
our -X- _ O
proposed -X- _ O
model -X- _ O
consistently -X- _ O
outperforms -X- _ O
baseline -X- _ O
models. -X- _ O
To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
CrossSum -X- _ B-DatasetName
is -X- _ O
the -X- _ O
largest -X- _ O
cross-lingual -X- _ B-TaskName
summarization -X- _ I-TaskName
dataset -X- _ O
and -X- _ O
the -X- _ O
first -X- _ O
ever -X- _ O
that -X- _ O
is -X- _ O
not -X- _ O
centered -X- _ O
around -X- _ O
English. -X- _ O
We -X- _ O
are -X- _ O
releasing -X- _ O
the -X- _ O
dataset -X- _ O
, -X- _ O
training -X- _ O
and -X- _ O
evaluation -X- _ O
scripts -X- _ O
, -X- _ O
and -X- _ O
models -X- _ O
to -X- _ O
spur -X- _ O
future -X- _ O
research -X- _ O
on -X- _ O
cross-lingual -X- _ B-TaskName
summarization. -X- _ I-TaskName
The -X- _ O
resources -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
at -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
csebuetnlp -X- _ O
/ -X- _ O
CrossSum -X- _ O
. -X- _ O

Introduction -X- _ O
Cross-lingual -X- _ B-TaskName
summarization -X- _ I-TaskName
( -X- _ O
hereinafter -X- _ O
XLS -X- _ B-TaskName
) -X- _ O
is -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
generating -X- _ O
a -X- _ O
summary -X- _ O
in -X- _ O
a -X- _ O
target -X- _ O
language -X- _ O
given -X- _ O
a -X- _ O
source -X- _ O
text -X- _ O
in -X- _ O
another -X- _ O
language. -X- _ O
The -X- _ O
task -X- _ O
is -X- _ O
challenging -X- _ O
as -X- _ O
it -X- _ O
combines -X- _ O
summarization -X- _ O
and -X- _ O
translation -X- _ O
in -X- _ O
one -X- _ O
task -X- _ O
, -X- _ O
both -X- _ O
challenging -X- _ O
tasks -X- _ O
in -X- _ O
their -X- _ O
own -X- _ O
right. -X- _ O
Earlier -X- _ O
approaches -X- _ O
to -X- _ O
XLS -X- _ B-TaskName
thus -X- _ O
employed -X- _ O
pipeline -X- _ O
methods -X- _ O
such -X- _ O
as -X- _ O
translate-thensummarize -X- _ O
( -X- _ O
Leuski -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
and -X- _ O
summarizethen-translate -X- _ O
( -X- _ O
Wan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2010 -X- _ O
) -X- _ O
. -X- _ O
Not -X- _ O
only -X- _ O
are -X- _ O
they -X- _ O
computationally -X- _ O
expensive -X- _ O
, -X- _ O
having -X- _ O
to -X- _ O
use -X- _ O
multiple -X- _ O
* -X- _ O
These -X- _ O
authors -X- _ O
contributed -X- _ O
equally -X- _ O
to -X- _ O
this -X- _ O
work -X- _ O
. -X- _ O

Figure -X- _ O
1 -X- _ O
: -X- _ O
A -X- _ O
sample -X- _ O
article-summary -X- _ O
pair -X- _ O
from -X- _ O
Cross-Sum -X- _ B-DatasetName
, -X- _ O
the -X- _ O
article -X- _ O
is -X- _ O
written -X- _ O
in -X- _ O
Japanese -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
summary -X- _ O
is -X- _ O
in -X- _ O
Bengali. -X- _ O
We -X- _ O
translate -X- _ O
the -X- _ O
texts -X- _ O
to -X- _ O
English -X- _ O
inside -X- _ O
parentheses -X- _ O
for -X- _ O
better -X- _ O
understanding. -X- _ O
Words -X- _ O
and -X- _ O
phrases -X- _ O
of -X- _ O
the -X- _ O
article -X- _ O
relevant -X- _ O
to -X- _ O
the -X- _ O
summary -X- _ O
are -X- _ O
color-coded. -X- _ O
models -X- _ O
, -X- _ O
but -X- _ O
these -X- _ O
approaches -X- _ O
also -X- _ O
suffer -X- _ O
from -X- _ O
errorpropagation -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
from -X- _ O
one -X- _ O
model -X- _ O
to -X- _ O
another -X- _ O
, -X- _ O
degrading -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
. -X- _ O

The -X- _ O
success -X- _ O
of -X- _ O
sequence-to-sequence -X- _ O
( -X- _ O
seq2seq -X- _ O
) -X- _ O
models -X- _ O
( -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
advances -X- _ O
in -X- _ O
Transformer-based -X- _ O
models -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
have -X- _ O
aided -X- _ O
in -X- _ O
the -X- _ O
emergence -X- _ O
of -X- _ O
end-to-end -X- _ O
methods -X- _ O
that -X- _ O
can -X- _ O
perform -X- _ O
XLS -X- _ B-TaskName
with -X- _ O
one -X- _ O
single -X- _ O
model -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Cao -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020b -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
availability -X- _ O
of -X- _ O
XLS -X- _ B-TaskName
datasets -X- _ O
( -X- _ O
Ladhak -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Perez-Beltrachini -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
has -X- _ O
also -X- _ O
helped -X- _ O
this -X- _ O
task -X- _ O
gain -X- _ O
popularity -X- _ O
in -X- _ O
recent -X- _ O
times. -X- _ O
However -X- _ O
, -X- _ O
they -X- _ O
cover -X- _ O
only -X- _ O
a -X- _ O
few -X- _ O
languages -X- _ O
, -X- _ O
contain -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
samples -X- _ O
for -X- _ O
training -X- _ O
and -X- _ O
evaluation -X- _ O
, -X- _ O
or -X- _ O
use -X- _ O
English -X- _ O
as -X- _ O
the -X- _ O
pivot -X- _ O
language -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
always -X- _ O
remains -X- _ O
English -X- _ O
) -X- _ O
, -X- _ O
thereby -X- _ O
limiting -X- _ O
their -X- _ O
applicability -X- _ O
to -X- _ O
a -X- _ O
great -X- _ O
extent -X- _ O
. -X- _ O

To -X- _ O
democratize -X- _ O
XLS -X- _ B-TaskName
beyond -X- _ O
high-resource -X- _ O
languages -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
a -X- _ O
large-scale -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
containing -X- _ O
1.68 -X- _ O
million -X- _ O
article-summary -X- _ O
samples -X- _ O
in -X- _ O
1,500+ -X- _ O
language -X- _ O
pairs. -X- _ O
We -X- _ O
align -X- _ O
parallel -X- _ O
articles -X- _ O
1 -X- _ O
written -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
via -X- _ O
cross-lingual -X- _ O
retrieval -X- _ O
from -X- _ O
the -X- _ O
multilingual -X- _ O
XL-Sum -X- _ B-TaskName
( -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
dataset. -X- _ O
We -X- _ O
introduce -X- _ O
and -X- _ O
rigorously -X- _ O
study -X- _ O
the -X- _ O
notions -X- _ O
'induced -X- _ O
pairs -X- _ O
' -X- _ O
and -X- _ O
'implicit -X- _ O
leakage -X- _ O
' -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
coverage -X- _ O
of -X- _ O
the -X- _ O
dataset -X- _ O
while -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
ensuring -X- _ O
maximum -X- _ O
quality. -X- _ O
We -X- _ O
also -X- _ O
perform -X- _ O
a -X- _ O
controlled -X- _ O
human -X- _ O
evaluation -X- _ O
of -X- _ O
CrossSum -X- _ B-DatasetName
spanning -X- _ O
nine -X- _ O
languages -X- _ O
from -X- _ O
high-to -X- _ O
low-resource -X- _ O
and -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
alignments -X- _ O
are -X- _ O
highly -X- _ O
accurate -X- _ O
. -X- _ O

We -X- _ O
design -X- _ O
MLS -X- _ B-MethodName
, -X- _ O
a -X- _ O
multistage -X- _ O
language -X- _ O
sampling -X- _ O
algorithm -X- _ O
, -X- _ O
for -X- _ O
successfully -X- _ O
training -X- _ O
models -X- _ O
that -X- _ O
can -X- _ O
generate -X- _ O
a -X- _ O
summary -X- _ O
in -X- _ O
any -X- _ O
target -X- _ O
language -X- _ O
for -X- _ O
an -X- _ O
input -X- _ O
article -X- _ O
in -X- _ O
any -X- _ O
source -X- _ O
language -X- _ O
, -X- _ O
both -X- _ O
from -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
languages -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
dataset. -X- _ O
For -X- _ O
the -X- _ O
first -X- _ O
time -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
XLS -X- _ B-TaskName
with -X- _ O
CrossSum -X- _ B-DatasetName
on -X- _ O
a -X- _ O
broad -X- _ O
and -X- _ O
diverse -X- _ O
set -X- _ O
of -X- _ O
languages -X- _ O
without -X- _ O
relying -X- _ O
on -X- _ O
English -X- _ O
as -X- _ O
the -X- _ O
standalone -X- _ O
pivot -X- _ O
, -X- _ O
consistently -X- _ O
outperforming -X- _ O
many-to-one -X- _ O
and -X- _ O
one-to-many -X- _ O
models -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
summarize-then-translate -X- _ O
baselines -X- _ O
. -X- _ O

We -X- _ O
propose -X- _ O
LaSE -X- _ B-MetricName
, -X- _ O
an -X- _ O
embedding-based -X- _ O
metric -X- _ O
for -X- _ O
evaluating -X- _ O
summaries -X- _ O
when -X- _ O
reference -X- _ O
summaries -X- _ O
may -X- _ O
not -X- _ O
be -X- _ O
available -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
but -X- _ O
may -X- _ O
be -X- _ O
available -X- _ O
in -X- _ O
another -X- _ O
language -X- _ O
, -X- _ O
potentially -X- _ O
opening -X- _ O
new -X- _ O
doors -X- _ O
for -X- _ O
evaluating -X- _ O
lowresource -X- _ O
languages. -X- _ O
Furthermore -X- _ O
, -X- _ O
we -X- _ O
demonstrate -X- _ O
the -X- _ O
reliability -X- _ O
of -X- _ O
LaSE -X- _ B-MetricName
by -X- _ O
its -X- _ O
high -X- _ O
correlation -X- _ O
with -X- _ O
ROUGE -X- _ B-MetricName
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
de-facto -X- _ O
metric -X- _ O
for -X- _ O
evaluating -X- _ O
text -X- _ O
summarization -X- _ O
systems -X- _ O
. -X- _ O

To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
CrossSum -X- _ B-DatasetName
is -X- _ O
the -X- _ O
largest -X- _ O
publicly -X- _ O
available -X- _ O
abdtractive -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
, -X- _ O
both -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
samples -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
language -X- _ O
pairs. -X- _ O
We -X- _ O
are -X- _ O
releasing -X- _ O
the -X- _ O
dataset -X- _ O
, -X- _ O
training -X- _ O
and -X- _ O
evaluation -X- _ O
scripts -X- _ O
, -X- _ O
and -X- _ O
models -X- _ O
hoping -X- _ O
that -X- _ O
these -X- _ O
resources -X- _ O
will -X- _ O
encourage -X- _ O
the -X- _ O
community -X- _ O
to -X- _ O
push -X- _ O
the -X- _ O
boundaries -X- _ O
of -X- _ O
XLS -X- _ B-TaskName
beyond -X- _ O
English -X- _ O
and -X- _ O
other -X- _ O
high-resource -X- _ O
languages -X- _ O
. -X- _ O

The -X- _ O
CrossSum -X- _ B-DatasetName
Dataset -X- _ O
The -X- _ O
most -X- _ O
straightforward -X- _ O
way -X- _ O
of -X- _ O
curating -X- _ O
a -X- _ O
highquality -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
is -X- _ O
via -X- _ O
crowd-sourcing -X- _ O
( -X- _ O
Nguyen -X- _ O
and -X- _ O
Daumé -X- _ O
III -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
it -X- _ O
may -X- _ O
be -X- _ O
difficult -X- _ O
to -X- _ O
find -X- _ O
crowd -X- _ O
workers -X- _ O
having -X- _ O
professional -X- _ O
command -X- _ O
over -X- _ O
low-resource -X- _ O
languages -X- _ O
or -X- _ O
distant -X- _ O
language -X- _ O
pairs. -X- _ O
Moreover -X- _ O
, -X- _ O
scalability -X- _ O
issues -X- _ O
might -X- _ O
arise -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
time -X- _ O
and -X- _ O
budget -X- _ O
constraints -X- _ O
for -X- _ O
1 -X- _ O
We -X- _ O
re-purpose -X- _ O
the -X- _ O
terminology -X- _ O
of -X- _ O
parallel -X- _ O
corpus -X- _ O
here. -X- _ O
crowd-sourcing. -X- _ O
Therefore -X- _ O
, -X- _ O
synthetic -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
automatic -X- _ O
methods -X- _ O
( -X- _ O
Ladhak -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Perez-Beltrachini -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
have -X- _ O
gained -X- _ O
traction -X- _ O
over -X- _ O
crowd-sourcing -X- _ O
. -X- _ O

Automatic -X- _ O
curation -X- _ O
of -X- _ O
an -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
is -X- _ O
simply -X- _ O
to -X- _ O
pair -X- _ O
an -X- _ O
article -X- _ O
A -X- _ O
in -X- _ O
a -X- _ O
source -X- _ O
language -X- _ O
with -X- _ O
the -X- _ O
summary -X- _ O
of -X- _ O
a -X- _ O
parallel -X- _ O
article -X- _ O
B -X- _ O
written -X- _ O
in -X- _ O
a -X- _ O
different -X- _ O
target -X- _ O
language -X- _ O
( -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
assuming -X- _ O
the -X- _ O
availability -X- _ O
of -X- _ O
a -X- _ O
multilingual -X- _ O
dataset -X- _ O
having -X- _ O
identical -X- _ O
contents -X- _ O
in -X- _ O
different -X- _ O
languages. -X- _ O
Two -X- _ O
contemporary -X- _ O
works -X- _ O
have -X- _ O
compiled -X- _ O
large-scale -X- _ O
multilingual -X- _ O
summarization -X- _ O
datasets -X- _ O
, -X- _ O
namely -X- _ O
XL-Sum -X- _ B-DatasetName
( -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
( -X- _ O
1.35M -X- _ O
samples -X- _ O
in -X- _ O
45 -X- _ O
languages -X- _ O
) -X- _ O
and -X- _ O
MassiveSumm -X- _ O
( -X- _ O
Varab -X- _ O
and -X- _ O
Schluter -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
( -X- _ O
28.8M -X- _ O
samples -X- _ O
in -X- _ O
92 -X- _ O
languages -X- _ O
) -X- _ O
. -X- _ O
Though -X- _ O
substantially -X- _ O
larger -X- _ O
than -X- _ O
the -X- _ O
other -X- _ O
, -X- _ O
MassiveSumm -X- _ B-DatasetName
is -X- _ O
not -X- _ O
publicly -X- _ O
available. -X- _ O
Since -X- _ O
public -X- _ O
availability -X- _ O
is -X- _ O
crucial -X- _ O
for -X- _ O
promoting -X- _ O
open -X- _ O
research -X- _ O
, -X- _ O
we -X- _ O
opted -X- _ O
for -X- _ O
XL-Sum -X- _ B-DatasetName
, -X- _ O
distributed -X- _ O
under -X- _ O
a -X- _ O
non-commercial -X- _ O
license. -X- _ O
Additionally -X- _ O
, -X- _ O
all -X- _ O
articles -X- _ O
of -X- _ O
XL-Sum -X- _ B-DatasetName
are -X- _ O
crawled -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
source -X- _ O
, -X- _ O
BBC -X- _ O
News. -X- _ O
We -X- _ O
observed -X- _ O
that -X- _ O
BBC -X- _ O
publishes -X- _ O
similar -X- _ O
news -X- _ O
content -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
and -X- _ O
follow -X- _ O
similar -X- _ O
summarization -X- _ O
strategies. -X- _ O
Hence -X- _ O
adopting -X- _ O
XL-Sum -X- _ B-DatasetName
would -X- _ O
increase -X- _ O
the -X- _ O
quality -X- _ O
and -X- _ O
quantity -X- _ O
of -X- _ O
the -X- _ O
article-summary -X- _ O
pairs -X- _ O
. -X- _ O

Unlike -X- _ O
previous -X- _ O
automatic -X- _ O
methods -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
no -X- _ O
explicit -X- _ O
links -X- _ O
between -X- _ O
parallel -X- _ O
articles -X- _ O
in -X- _ O
XL-Sum. -X- _ B-DatasetName
Fortunately -X- _ O
, -X- _ O
language-agnostic -X- _ O
sentence -X- _ O
representations -X- _ O
( -X- _ O
Artetxe -X- _ O
and -X- _ O
Schwenk -X- _ O
, -X- _ O
2019a -X- _ O
; -X- _ O
Feng -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
have -X- _ O
achieved -X- _ O
state-of-the-art -X- _ O
results -X- _ O
in -X- _ O
crosslingual -X- _ O
text -X- _ O
mining -X- _ O
( -X- _ O
Artetxe -X- _ O
and -X- _ O
Schwenk -X- _ O
, -X- _ O
2019b -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
hence -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
them -X- _ O
to -X- _ O
search -X- _ O
identical -X- _ O
contents -X- _ O
across -X- _ O
languages. -X- _ O
For -X- _ O
simplicity -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
the -X- _ O
search -X- _ O
over -X- _ O
summaries -X- _ O
only. -X- _ O
To -X- _ O
ensure -X- _ O
maximum -X- _ O
quality -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
two -X- _ O
conditions -X- _ O
for -X- _ O
a -X- _ O
summary -X- _ O
S -X- _ O
A -X- _ O
in -X- _ O
language -X- _ O
A -X- _ O
to -X- _ O
be -X- _ O
aligned -X- _ O
with -X- _ O
another -X- _ O
summary -X- _ O
S -X- _ O
B -X- _ O
in -X- _ O
language -X- _ O
B -X- _ O
: -X- _ O

1. -X- _ O
S -X- _ O
B -X- _ O
must -X- _ O
be -X- _ O
the -X- _ O
nearest -X- _ O
neighbor -X- _ O
of -X- _ O
S -X- _ O
A -X- _ O
among -X- _ O
all -X- _ O
summaries -X- _ O
in -X- _ O
B -X- _ O
, -X- _ O
and -X- _ O
vice-versa. -X- _ O
2. -X- _ O
The -X- _ O
similarity -X- _ O
between -X- _ O
S -X- _ O
A -X- _ O
and -X- _ O
S -X- _ O
B -X- _ O
must -X- _ O
be -X- _ O
above -X- _ O
the -X- _ O
threshold -X- _ O
, -X- _ O
τ -X- _ B-HyperparameterName
. -X- _ O
The -X- _ O
similarity -X- _ O
of -X- _ O
a -X- _ O
summary -X- _ O
pair -X- _ O
is -X- _ O
measured -X- _ O
by -X- _ O
the -X- _ O
inner -X- _ O
product -X- _ O
of -X- _ O
their -X- _ O
Language-agnostic -X- _ O
BERT -X- _ O
Sentence -X- _ O
Embeddings -X- _ O
( -X- _ O
LaBSE -X- _ O
) -X- _ O
( -X- _ O
Feng -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
( -X- _ O
a -X- _ O
unit -X- _ O
vector -X- _ O
for -X- _ O
an -X- _ O
input -X- _ O
text -X- _ O
sequence -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
empirically -X- _ O
set -X- _ O
the -X- _ O
similarity -X- _ O
threshold -X- _ O
as -X- _ O
the -X- _ O
average -X- _ O
over -X- _ O
all -X- _ O
languages -X- _ O
that -X- _ O
maximized -X- _ O
their -X- _ O
respective -X- _ O
F -X- _ B-MetricName
1 -X- _ I-MetricName
score -X- _ O
( -X- _ O
τ -X- _ B-HyperparameterName
= -X- _ O
0.7437 -X- _ B-HyperparameterValue
) -X- _ O
in -X- _ O
the -X- _ O
BUCC -X- _ O
mining -X- _ O
tasks -X- _ O
( -X- _ O
Zweigenbaum -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
3 -X- _ O
Training -X- _ O
on -X- _ O
the -X- _ O
dataset -X- _ O
respecting -X- _ O
the -X- _ O
original -X- _ O
XL-Sum -X- _ B-DatasetName
splits -X- _ O
causes -X- _ O
unusually -X- _ O
high -X- _ O
ROUGE -X- _ B-HyperparameterName
scores -X- _ O
( -X- _ O
marked -X- _ O
red -X- _ O
) -X- _ O
in -X- _ O
many-to-one -X- _ O
models -X- _ O
due -X- _ O
to -X- _ O
implicit -X- _ O
data -X- _ O
leakage. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
redid -X- _ O
the -X- _ O
splits -X- _ O
taking -X- _ O
the -X- _ O
issue -X- _ O
into -X- _ O
account -X- _ O
, -X- _ O
and -X- _ O
consequently -X- _ O
, -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
new -X- _ O
set -X- _ O
( -X- _ O
marked -X- _ O
blue -X- _ O
) -X- _ O
do -X- _ O
not -X- _ O
exhibit -X- _ O
any -X- _ O
unusual -X- _ O
spike -X- _ O
. -X- _ O

Induced -X- _ O
Pairs -X- _ O
We -X- _ O
observed -X- _ O
that -X- _ O
many -X- _ O
summary -X- _ O
pairs -X- _ O
, -X- _ O
despite -X- _ O
being -X- _ O
nearest -X- _ O
neighbors -X- _ O
in -X- _ O
their -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
were -X- _ O
filtered -X- _ O
out -X- _ O
because -X- _ O
of -X- _ O
the -X- _ O
threshold -X- _ O
τ -X- _ B-HyperparameterName
. -X- _ O
Although -X- _ O
interestingly -X- _ O
, -X- _ O
both -X- _ O
were -X- _ O
aligned -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
summary -X- _ O
in -X- _ O
a -X- _ O
different -X- _ O
language. -X- _ O
Moreover -X- _ O
, -X- _ O
these -X- _ O
pairs -X- _ O
are -X- _ O
prevalent -X- _ O
if -X- _ O
their -X- _ O
languages -X- _ O
are -X- _ O
distant -X- _ O
or -X- _ O
low-resource. -X- _ O
LaBSE -X- _ B-MethodName
uses -X- _ O
contrastive -X- _ O
learning -X- _ O
( -X- _ O
Guo -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
to -X- _ O
rank -X- _ O
parallel -X- _ O
sentences -X- _ O
over -X- _ O
non-parallels -X- _ O
. -X- _ O

Since -X- _ O
parallel -X- _ O
pairs -X- _ O
are -X- _ O
mostly -X- _ O
found -X- _ O
for -X- _ O
highresource -X- _ O
and -X- _ O
linguistically -X- _ O
close -X- _ O
languages -X- _ O
, -X- _ O
we -X- _ O
hypothesize -X- _ O
that -X- _ O
LaBSE -X- _ B-MethodName
fails -X- _ O
to -X- _ O
assign -X- _ O
high -X- _ O
similarity -X- _ O
to -X- _ O
sentences -X- _ O
from -X- _ O
languages -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
. -X- _ O

To -X- _ O
include -X- _ O
these -X- _ O
pairs -X- _ O
into -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
we -X- _ O
introduce -X- _ O
the -X- _ O
notion -X- _ O
'induced -X- _ O
pairs. -X- _ O
' -X- _ O
Formally -X- _ O
, -X- _ O
two -X- _ O
summaries -X- _ O
S -X- _ O
A -X- _ O
, -X- _ O
S -X- _ O
B -X- _ O
in -X- _ O
languages -X- _ O
A -X- _ O
, -X- _ O
B -X- _ O
are -X- _ O
induced -X- _ O
pairs -X- _ O
if -X- _ O
they -X- _ O
are -X- _ O
nearest -X- _ O
neighbors -X- _ O
of -X- _ O
each -X- _ O
other -X- _ O
in -X- _ O
A -X- _ O
, -X- _ O
B -X- _ O
, -X- _ O
their -X- _ O
similarity -X- _ O
score -X- _ O
is -X- _ O
below -X- _ O
τ -X- _ B-HyperparameterName
, -X- _ O
and -X- _ O
both -X- _ O
are -X- _ O
aligned -X- _ O
with -X- _ O
S -X- _ O
C -X- _ O
in -X- _ O
language -X- _ O
C -X- _ O
, -X- _ O
or -X- _ O
through -X- _ O
a -X- _ O
chain -X- _ O
of -X- _ O
aligned -X- _ O
pairs -X- _ O

We -X- _ O
thus -X- _ O
incorporate -X- _ O
the -X- _ O
induced -X- _ O
pairs -X- _ O
into -X- _ O
Cross-Sum -X- _ B-DatasetName
through -X- _ O
a -X- _ O
simple -X- _ O
graph-based -X- _ O
algorithm. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
represent -X- _ O
all -X- _ O
summaries -X- _ O
as -X- _ O
vertices -X- _ O
in -X- _ O
a -X- _ O
graph -X- _ O
and -X- _ O
draw -X- _ O
an -X- _ O
edge -X- _ O
between -X- _ O
two -X- _ O
vertices -X- _ O
if -X- _ O
the -X- _ O
summaries -X- _ O
are -X- _ O
aligned. -X- _ O
Then -X- _ O
we -X- _ O
find -X- _ O
the -X- _ O
connected -X- _ O
components -X- _ O
in -X- _ O
the -X- _ O
graph -X- _ O
and -X- _ O
draw -X- _ O
edges -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
induced -X- _ O
pairs -X- _ O
) -X- _ O
between -X- _ O
all -X- _ O
vertices -X- _ O
in -X- _ O
a -X- _ O
component. -X- _ O
Again -X- _ O
to -X- _ O
ensure -X- _ O
quality -X- _ O
, -X- _ O
before -X- _ O
computing -X- _ O
the -X- _ O
induced -X- _ O
pairs -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
max-flow -X- _ O
min-cut -X- _ O
theorem -X- _ O
( -X- _ O
Dantzig -X- _ O
and -X- _ O
Fulkerson -X- _ O
, -X- _ O
1955 -X- _ O
) -X- _ O
considering -X- _ O
the -X- _ O
similarity -X- _ O
scores -X- _ O
as -X- _ O
edge -X- _ O
weights -X- _ O
to -X- _ O
limit -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
each -X- _ O
component -X- _ O
to -X- _ O
50 -X- _ B-HyperparameterValue
vertices -X- _ O
( -X- _ O
since -X- _ O
ideally -X- _ O
, -X- _ O
a -X- _ O
component -X- _ O
should -X- _ O
have -X- _ O
at -X- _ O
most -X- _ O
45 -X- _ B-HyperparameterValue
vertices -X- _ O
, -X- _ O
one -X- _ O
summary -X- _ O
from -X- _ O
each -X- _ O
language -X- _ O
) -X- _ O
and -X- _ O
set -X- _ O
their -X- _ O
minimum -X- _ O
acceptance -X- _ O
threshold -X- _ O
to -X- _ O
τ -X- _ B-HyperparameterName
′ -X- _ O
← -X- _ O
τ -X- _ B-HyperparameterName
− -X- _ O
0.10. -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
assess -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
alignments -X- _ O
using -X- _ O
human -X- _ O
evaluation -X- _ O
. -X- _ O

We -X- _ O
finally -X- _ O
assembled -X- _ O
the -X- _ O
originally -X- _ O
aligned -X- _ O
pairs -X- _ O
and -X- _ O
induced -X- _ O
pairs -X- _ O
to -X- _ O
create -X- _ O
the -X- _ O
CrossSum -X- _ B-DatasetName
dataset. -X- _ O
Figure -X- _ O
6 -X- _ O
( -X- _ O
Appendix -X- _ O
) -X- _ O
shows -X- _ O
the -X- _ O
article-summary -X- _ O
statistics -X- _ O
for -X- _ O
all -X- _ O
language -X- _ O
pairs -X- _ O
in -X- _ O
CrossSum. -X- _ B-DatasetName
As -X- _ O
evident -X- _ O
from -X- _ O
the -X- _ O
figure -X- _ O
, -X- _ O
CrossSum -X- _ B-DatasetName
is -X- _ O
not -X- _ O
centered -X- _ O
only -X- _ O
around -X- _ O
the -X- _ O
English -X- _ O
language -X- _ O
but -X- _ O
rather -X- _ O
distributed -X- _ O
across -X- _ O
multiple -X- _ O
languages -X- _ O
. -X- _ O

Implicit -X- _ O
Leakage -X- _ O
We -X- _ O
initially -X- _ O
made -X- _ O
the -X- _ O
traindev-test -X- _ O
splits -X- _ O
respecting -X- _ O
the -X- _ O
original -X- _ O
XL-Sum -X- _ B-DatasetName
splits -X- _ O
and -X- _ O
performed -X- _ O
an -X- _ O
initial -X- _ O
assessment -X- _ O
of -X- _ O
Cross-Sum -X- _ B-DatasetName
by -X- _ O
training -X- _ O
a -X- _ O
many-to-one -X- _ O
model -X- _ O
( -X- _ O
articles -X- _ O
written -X- _ O
in -X- _ O
any -X- _ O
source -X- _ O
language -X- _ O
being -X- _ O
summarized -X- _ O
into -X- _ O
one -X- _ O
target -X- _ O
language -X- _ O
) -X- _ O
. -X- _ O
Upon -X- _ O
evaluation -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
very -X- _ O
high -X- _ O
ROUGE-2 -X- _ B-MetricName
scores -X- _ O
( -X- _ O
around -X- _ O
40 -X- _ O
) -X- _ O
for -X- _ O
many -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
even -X- _ O
reaching -X- _ O
as -X- _ O
high -X- _ O
as -X- _ O
60 -X- _ O
for -X- _ O
some -X- _ O
( -X- _ O
Figure -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
reported -X- _ O
ROUGE-2 -X- _ B-MetricName
in -X- _ O
the -X- _ O
10-20 -X- _ O
range -X- _ O
for -X- _ O
the -X- _ O
multilingual -X- _ O
summarization -X- _ O
task -X- _ O
. -X- _ O

We -X- _ O
inspected -X- _ O
the -X- _ O
model -X- _ O
outputs -X- _ O
and -X- _ O
found -X- _ O
that -X- _ O
many -X- _ O
summaries -X- _ O
were -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
references. -X- _ O
Through -X- _ O
closer -X- _ O
inspection -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
that -X- _ O
their -X- _ O
corresponding -X- _ O
articles -X- _ O
had -X- _ O
a -X- _ O
parallel -X- _ O
counterpart -X- _ O
occurring -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
in -X- _ O
some -X- _ O
other -X- _ O
language -X- _ O
. -X- _ O

During -X- _ O
training -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
was -X- _ O
able -X- _ O
to -X- _ O
align -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
parallel -X- _ O
articles -X- _ O
( -X- _ O
albeit -X- _ O
written -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
) -X- _ O
and -X- _ O
generate -X- _ O
the -X- _ O
same -X- _ O
output -X- _ O
by -X- _ O
memorizing -X- _ O
from -X- _ O
the -X- _ O
training -X- _ O
sample. -X- _ O
While -X- _ O
models -X- _ O
should -X- _ O
undoubtedly -X- _ O
be -X- _ O
credited -X- _ O
for -X- _ O
being -X- _ O
able -X- _ O
to -X- _ O
make -X- _ O
these -X- _ O
cross-lingual -X- _ O
mappings -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
not -X- _ O
ideal -X- _ O
for -X- _ O
benchmarking -X- _ O
purposes -X- _ O
as -X- _ O
this -X- _ O
creates -X- _ O
unusually -X- _ O
high -X- _ O
ROUGE -X- _ B-MetricName
scores. -X- _ O
We -X- _ O
denote -X- _ O
this -X- _ O
phenomenon -X- _ O
as -X- _ O
'implicit -X- _ O
leakage -X- _ O
' -X- _ O
and -X- _ O
make -X- _ O
a -X- _ O
new -X- _ O
dataset -X- _ O
split -X- _ O
to -X- _ O
avoid -X- _ O
this. -X- _ O
Before -X- _ O
proceeding -X- _ O
, -X- _ O
we -X- _ O
deduplicate -X- _ O
the -X- _ O
XL-Sum -X- _ B-DatasetName
dataset -X- _ O
4 -X- _ O
using -X- _ O
semantic -X- _ O
similarity -X- _ O
, -X- _ O
considering -X- _ O
two -X- _ O
summaries -X- _ O
S -X- _ O
A -X- _ O
, -X- _ O
S -X- _ O
′ -X- _ O
A -X- _ O
in -X- _ O
language -X- _ O
A -X- _ O
to -X- _ O
be -X- _ O
duplicates -X- _ O
of -X- _ O
one -X- _ O
another -X- _ O
if -X- _ O
their -X- _ O
LaBSE -X- _ B-MethodName
representations -X- _ O
have -X- _ O
similarity -X- _ O
above -X- _ O
0.95. -X- _ O
We -X- _ O
take -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
component -X- _ O
graph -X- _ O
mentioned -X- _ O
previously -X- _ O
to -X- _ O
address -X- _ O
the -X- _ O
leakage -X- _ O
and -X- _ O
assign -X- _ O
all -X- _ O
article-summary -X- _ O
pairs -X- _ O
originating -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
component -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
( -X- _ O
dev -X- _ O
/ -X- _ O
test -X- _ O
) -X- _ O
set -X- _ O
of -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
creating -X- _ O
an -X- _ O
80 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
-10 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
-10 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
split -X- _ B-HyperparameterName
for -X- _ O
all -X- _ O
language -X- _ O
pairs. -X- _ O
Since -X- _ O
parallel -X- _ O
articles -X- _ O
no -X- _ O
longer -X- _ O
appear -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
of -X- _ O
one -X- _ O
and -X- _ O
the -X- _ O
dev -X- _ O
/ -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
another -X- _ O
, -X- _ O
the -X- _ O
leakage -X- _ O
is -X- _ O
not -X- _ O
observed -X- _ O
anymore -X- _ O
( -X- _ O
Figure -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
further -X- _ O
validated -X- _ O
this -X- _ O
by -X- _ O
inspecting -X- _ O
the -X- _ O
model -X- _ O
outputs -X- _ O
and -X- _ O
found -X- _ O
no -X- _ O
exact -X- _ O
copies -X- _ O
. -X- _ O

Human -X- _ O
Evaluation -X- _ O
of -X- _ O
CrossSum -X- _ B-DatasetName
To -X- _ O
establish -X- _ O
the -X- _ O
validity -X- _ O
of -X- _ O
our -X- _ O
automatic -X- _ O
alignment -X- _ O
pipeline -X- _ O
, -X- _ O
we -X- _ O
conducted -X- _ O
a -X- _ O
human -X- _ O
evaluation -X- _ O
to -X- _ O
study -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
cross-lingual -X- _ O
alignments -X- _ O
. -X- _ O

We -X- _ O
selected -X- _ O
all -X- _ O
possible -X- _ O
combinations -X- _ O
of -X- _ O
language -X- _ O
pairs -X- _ O
from -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
nine -X- _ O
languages -X- _ O
ranging -X- _ O
from -X- _ O
high-resource -X- _ O
to -X- _ O
low-resource -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
alignment -X- _ O
quality -X- _ O
in -X- _ O
different -X- _ O
pair -X- _ O
configurations -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
high-high -X- _ O
, -X- _ O
low-high -X- _ O
, -X- _ O
low-low -X- _ O
) -X- _ O
as -X- _ O
per -X- _ O
the -X- _ O
language -X- _ O
diversity -X- _ O
categorization -X- _ O
by -X- _ O
Joshi -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
chose -X- _ O
three -X- _ O
high-resource -X- _ O
languages -X- _ O
, -X- _ O
English -X- _ O
, -X- _ O
Arabic -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
simplified -X- _ O
) -X- _ O
Chinese -X- _ O
( -X- _ O
categories -X- _ O
4 -X- _ O
and -X- _ O
5 -X- _ O
) -X- _ O
; -X- _ O
three -X- _ O
mid-resource -X- _ O
languages -X- _ O
, -X- _ O
Indonesian -X- _ O
, -X- _ O
Bengali -X- _ O
, -X- _ O
and -X- _ O
Urdu -X- _ O
( -X- _ O
category -X- _ O
3 -X- _ O
) -X- _ O
; -X- _ O
and -X- _ O
three -X- _ O
low-resource -X- _ O
languages -X- _ O
, -X- _ O
Punjabi -X- _ O
, -X- _ O
Swahili -X- _ O
, -X- _ O
and -X- _ O
Pashto -X- _ O
( -X- _ O
categories -X- _ O
1 -X- _ O
and -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
representative -X- _ O
languages -X- _ O
and -X- _ O
randomly -X- _ O
sampled -X- _ O
fifty -X- _ O
cross-lingual -X- _ O
summary -X- _ O
alignments -X- _ O
from -X- _ O
each -X- _ O
language -X- _ O
pair -X- _ O
for -X- _ O
annotation. -X- _ O
As -X- _ O
a -X- _ O
direct -X- _ O
evaluation -X- _ O
of -X- _ O
these -X- _ O
pairs -X- _ O
would -X- _ O
require -X- _ O
bilinguallyproficient -X- _ O
annotators -X- _ O
for -X- _ O
both -X- _ O
languages -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
practically -X- _ O
intractable -X- _ O
for -X- _ O
distantly -X- _ O
related -X- _ O
languages -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Bengali-Swahili -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
resorted -X- _ O
to -X- _ O
a -X- _ O
pivoting -X- _ O
approach -X- _ O
during -X- _ O
annotation -X- _ O
for -X- _ O
language -X- _ O
pairs -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
contain -X- _ O
English. -X- _ O
For -X- _ O
a -X- _ O
language -X- _ O
pair -X- _ O
( -X- _ O
l -X- _ O
1 -X- _ O
− -X- _ O
l -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
l -X- _ O
1 -X- _ O
̸ -X- _ O
= -X- _ O
en -X- _ O
and -X- _ O
l -X- _ O
2 -X- _ O
̸ -X- _ O
= -X- _ O
en -X- _ O
, -X- _ O
we -X- _ O
sampled -X- _ O
alignments -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
such -X- _ O
that -X- _ O
∃ -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
e -X- _ O
) -X- _ O
∈ -X- _ O
( -X- _ O
l -X- _ O
1 -X- _ O
−en -X- _ O
) -X- _ O
and -X- _ O
∃ -X- _ O
( -X- _ O
y -X- _ O
, -X- _ O
e -X- _ O
) -X- _ O
∈ -X- _ O
( -X- _ O
l -X- _ O
2 -X- _ O
− -X- _ O
en -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
an -X- _ O
English -X- _ O
article -X- _ O
e. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
we -X- _ O
ensure -X- _ O
that -X- _ O
both -X- _ O
the -X- _ O
articles -X- _ O
of -X- _ O
the -X- _ O
sampled -X- _ O
cross-lingual -X- _ O
pair -X- _ O
have -X- _ O
a -X- _ O
corresponding -X- _ O
cross-lingual -X- _ O
pair -X- _ O
with -X- _ O
an -X- _ O
English -X- _ O
article. -X- _ O
An -X- _ O
alignment -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
y -X- _ O
) -X- _ O
would -X- _ O
be -X- _ O
deemed -X- _ O
correct -X- _ O
if -X- _ O
both -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
e -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
y -X- _ O
, -X- _ O
e -X- _ O
) -X- _ O
are -X- _ O
correct. -X- _ O
This -X- _ O
formulation -X- _ O
thus -X- _ O
reduced -X- _ O
the -X- _ O
original -X- _ O
problem -X- _ O
to -X- _ O
annotating -X- _ O
samples -X- _ O
from -X- _ O
language -X- _ O
pairs -X- _ O
( -X- _ O
l -X- _ O
1 -X- _ O
− -X- _ O
en -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
l -X- _ O
2 -X- _ O
− -X- _ O
en -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
l -X- _ O
1 -X- _ O
and -X- _ O
l -X- _ O
2 -X- _ O
are -X- _ O
from -X- _ O
the -X- _ O
previously -X- _ O
selected -X- _ O
languages -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
English -X- _ O
. -X- _ O

We -X- _ O
hired -X- _ O
bilingually -X- _ O
proficient -X- _ O
expert -X- _ O
annotators -X- _ O
adept -X- _ O
in -X- _ O
the -X- _ O
language -X- _ O
of -X- _ O
interest -X- _ O
and -X- _ O
English. -X- _ O
Two -X- _ O
annotators -X- _ O
labeled -X- _ O
each -X- _ O
language -X- _ O
pair -X- _ O
where -X- _ O
one -X- _ O
language -X- _ O
is -X- _ O
English. -X- _ O
We -X- _ O
presented -X- _ O
them -X- _ O
with -X- _ O
corresponding -X- _ O
summaries -X- _ O
of -X- _ O
the -X- _ O
cross-lingual -X- _ O
pairs -X- _ O
( -X- _ O
and -X- _ O
optionally -X- _ O
the -X- _ O
articles -X- _ O
themselves -X- _ O
) -X- _ O
and -X- _ O
elicited -X- _ O
yes -X- _ O
/ -X- _ O
no -X- _ O
answers -X- _ O
to -X- _ O
the -X- _ O
question -X- _ O
: -X- _ O
" -X- _ O
Can -X- _ O
the -X- _ O
provided -X- _ O
sequences -X- _ O
be -X- _ O
considered -X- _ O
summaries -X- _ O
for -X- _ O
the -X- _ O
same -X- _ O
article -X- _ O
? -X- _ O
" -X- _ O
5 -X- _ O
We -X- _ O
deem -X- _ O
a -X- _ O
sequence -X- _ O
pair -X- _ O
accurate -X- _ O
if -X- _ O
both -X- _ O
annotators -X- _ O
judge -X- _ O
it -X- _ O
as -X- _ O
valid. -X- _ O
We -X- _ O
show -X- _ O
the -X- _ O
alignment -X- _ O
accuracies -X- _ O
of -X- _ O
the -X- _ O
language -X- _ O
pairs -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O

As -X- _ O
evident -X- _ O
from -X- _ O
the -X- _ O
figure -X- _ O
, -X- _ O
the -X- _ O
annotators -X- _ O
judge -X- _ O
the -X- _ O
aligned -X- _ O
summaries -X- _ O
to -X- _ O
be -X- _ O
highly -X- _ O
accurate -X- _ O
, -X- _ O
with -X- _ O
an -X- _ O
average -X- _ B-MetricName
accuracy -X- _ I-MetricName
of -X- _ O
95.67 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O
We -X- _ O
used -X- _ O
Cohen -X- _ B-MetricName
's -X- _ I-MetricName
Kappa -X- _ I-MetricName
( -X- _ O
Cohen -X- _ O
, -X- _ O
1960 -X- _ O
) -X- _ O
to -X- _ O
establish -X- _ O
the -X- _ O
interannotator -X- _ O
agreement -X- _ O
and -X- _ O
show -X- _ O
the -X- _ O
corresponding -X- _ O
statistics -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

Multistage -X- _ B-HyperparameterName
Language -X- _ I-HyperparameterName
Sampling -X- _ I-HyperparameterName
( -X- _ O
MLS -X- _ B-HyperparameterName
) -X- _ O
From -X- _ O
Figure -X- _ O
6 -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
observed -X- _ O
that -X- _ O
CrossSum -X- _ B-DatasetName
is -X- _ O
heavily -X- _ O
imbalanced. -X- _ O
Thus -X- _ O
, -X- _ O
training -X- _ O
directly -X- _ O
without -X- _ O
upsampling -X- _ O
low-resource -X- _ O
languages -X- _ O
may -X- _ O
result -X- _ O
in -X- _ O
their -X- _ O
degraded -X- _ O
performance. -X- _ O
Conneau -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
used -X- _ O
probability -X- _ O
smoothing -X- _ O
for -X- _ O
upsampling -X- _ O
in -X- _ O
multilingual -X- _ O
pretraining -X- _ O
and -X- _ O
sampled -X- _ O
all -X- _ O
examples -X- _ O
of -X- _ O
a -X- _ O
batch -X- _ O
from -X- _ O
one -X- _ O
language. -X- _ O
However -X- _ O
, -X- _ O
extending -X- _ O
this -X- _ O
technique -X- _ O
to -X- _ O
the -X- _ O
language -X- _ O
pairs -X- _ O
in -X- _ O
CrossSum -X- _ B-DatasetName
would -X- _ O
result -X- _ O
in -X- _ O
many -X- _ O
batches -X- _ O
having -X- _ O
repeated -X- _ O
samples -X- _ O
as -X- _ O
many -X- _ O
language -X- _ O
pairs -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
enough -X- _ O
training -X- _ O
samples -X- _ O
in -X- _ O
total -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
batch -X- _ O
sizes -X- _ O
used -X- _ O
in -X- _ O
practice -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Conneau -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
used -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
256 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
exceeds -X- _ O
the -X- _ O
training -X- _ B-HyperparameterName
set -X- _ I-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
nearly -X- _ O
1,000 -X- _ B-HyperparameterValue
language -X- _ O
pairs -X- _ O
in -X- _ O
CrossSum -X- _ B-DatasetName
) -X- _ O
. -X- _ O
At -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
, -X- _ O
many -X- _ O
language -X- _ O
pairs -X- _ O
would -X- _ O
not -X- _ O
be -X- _ O
sampled -X- _ O
during -X- _ O
training -X- _ O
for -X- _ O
lack -X- _ O
of -X- _ O
enough -X- _ O
training -X- _ O
steps -X- _ O
( -X- _ O
due -X- _ O
to -X- _ O
our -X- _ O
constraints -X- _ O
on -X- _ O
computational -X- _ O
resources -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
address -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
adapt -X- _ O
their -X- _ O
method -X- _ O
to -X- _ O
introduce -X- _ O
a -X- _ O
Multistage -X- _ B-HyperparameterName
Language -X- _ I-HyperparameterName
Sampling -X- _ I-HyperparameterName
algorithm -X- _ O
( -X- _ O
MLS -X- _ B-HyperparameterName
) -X- _ O
to -X- _ O
ensure -X- _ O
that -X- _ O
the -X- _ O
target -X- _ O
summaries -X- _ O
of -X- _ O
a -X- _ O
batch -X- _ O
are -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
language -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
use -X- _ O
an -X- _ O
exponent -X- _ O
smoothing -X- _ O
factor -X- _ O
α -X- _ B-HyperparameterName
and -X- _ O
normalize -X- _ O
the -X- _ O
probabilities -X- _ O

We -X- _ O
again -X- _ O
smooth -X- _ O
p -X- _ O
j|i -X- _ O
by -X- _ O
a -X- _ O
factor -X- _ O
β -X- _ B-HyperparameterName
and -X- _ O
obtain -X- _ O
the -X- _ O
normalized -X- _ O
probabilities -X- _ O

Using -X- _ O
the -X- _ O
probabilities -X- _ O
, -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
training -X- _ O
process -X- _ O
with -X- _ O
the -X- _ O
MLS -X- _ B-MethodName
algorithm -X- _ O
in -X- _ O
Algorithm -X- _ O
1 -X- _ O
. -X- _ O

Evaluating -X- _ O
Summaries -X- _ O
Across -X- _ O
Languages -X- _ O
A -X- _ O
sufficient -X- _ O
number -X- _ O
of -X- _ O
reference -X- _ O
samples -X- _ O
are -X- _ O
essential -X- _ O
for -X- _ O
the -X- _ O
reliable -X- _ O
evaluation -X- _ O
of -X- _ O
model-generated -X- _ O
summaries. -X- _ O
However -X- _ O
, -X- _ O
for -X- _ O
many -X- _ O
CrossSum -X- _ B-DatasetName
language -X- _ O
pairs -X- _ O
, -X- _ O
even -X- _ O
the -X- _ O
training -X- _ O
sets -X- _ O
are -X- _ O
small -X- _ O
, -X- _ O
let -X- _ O

Update -X- _ O
model -X- _ O
parameters -X- _ O
using -X- _ O
batch -X- _ O
alone -X- _ O
the -X- _ O
test -X- _ O
sets -X- _ O
( -X- _ O
the -X- _ O
median -X- _ O
size -X- _ O
is -X- _ O
only -X- _ O
33 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
Japanese-Bengali -X- _ O
language -X- _ O
pair -X- _ O
has -X- _ O
34 -X- _ O
test -X- _ O
samples -X- _ O
only -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
too -X- _ O
few -X- _ O
for -X- _ O
reliable -X- _ O
evaluation. -X- _ O
But -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
in-language -X- _ O
6 -X- _ O
test -X- _ O
sets -X- _ O
of -X- _ O
Japanese -X- _ O
and -X- _ O
Bengali -X- _ O
are -X- _ O
nearly -X- _ O
1,000. -X- _ O
Being -X- _ O
able -X- _ O
to -X- _ O
evaluate -X- _ O
against -X- _ O
reference -X- _ O
summaries -X- _ O
written -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
language -X- _ O
would -X- _ O
thus -X- _ O
alleviate -X- _ O
this -X- _ O
insufficiency -X- _ O
problem -X- _ O
by -X- _ O
leveraging -X- _ O
the -X- _ O
in-language -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
language. -X- _ O
For -X- _ O
this -X- _ O
purpose -X- _ O
, -X- _ O
cross-lingual -X- _ O
similarity -X- _ O
metrics -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
lexical -X- _ O
overlap -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
unlike -X- _ O
ROUGE -X- _ B-MetricName
) -X- _ O
are -X- _ O
required. -X- _ O
Embedding-based -X- _ O
similarity -X- _ O
metrics -X- _ O
( -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Zhao -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
have -X- _ O
recently -X- _ O
gained -X- _ O
popularity. -X- _ O
We -X- _ O
draw -X- _ O
inspiration -X- _ O
from -X- _ O
them -X- _ O
and -X- _ O
design -X- _ O
a -X- _ O
similarity -X- _ O
metric -X- _ O
that -X- _ O
can -X- _ O
effectively -X- _ O
measure -X- _ O
similarity -X- _ O
across -X- _ O
languages -X- _ O
in -X- _ O
a -X- _ O
language-independent -X- _ O
manner. -X- _ O
We -X- _ O
consider -X- _ O
three -X- _ O
essential -X- _ O
factors -X- _ O
: -X- _ O
1. -X- _ O
Meaning -X- _ O
Similarity -X- _ O
: -X- _ O
The -X- _ O
generated -X- _ O
and -X- _ O
reference -X- _ O
summaries -X- _ O
should -X- _ O
convey -X- _ O
the -X- _ O
same -X- _ O
meaning -X- _ O
irrespective -X- _ O
of -X- _ O
their -X- _ O
languages. -X- _ O
Just -X- _ O
like -X- _ O
our -X- _ O
alignment -X- _ O
procedure -X- _ O
from -X- _ O
Section -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
LaBSE -X- _ B-MethodName
to -X- _ O
compute -X- _ O
the -X- _ O
meaning -X- _ O
similarity -X- _ O
between -X- _ O
the -X- _ O
generated -X- _ O
( -X- _ O
s -X- _ O
gen -X- _ O
) -X- _ O
and -X- _ O
reference -X- _ O
summary -X- _ O
( -X- _ O
s -X- _ O
where -X- _ O
emb -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
embedding -X- _ O
vector -X- _ O
output -X- _ O
of -X- _ O
LaBSE -X- _ B-MethodName
for -X- _ O
input -X- _ O
text -X- _ O
s -X- _ O
. -X- _ O

Language -X- _ O
Confidence -X- _ O
: -X- _ O
The -X- _ O
metric -X- _ O
should -X- _ O
identify -X- _ O
, -X- _ O
with -X- _ O
high -X- _ O
confidence -X- _ O
, -X- _ O
that -X- _ O
the -X- _ O
summary -X- _ O
is -X- _ O
indeed -X- _ O
being -X- _ O
generated -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
As -X- _ O
such -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
fastText -X- _ B-MethodName
language-ID -X- _ O
classifier -X- _ O
( -X- _ O
Joulin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
language -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
summary -X- _ O
and -X- _ O
define -X- _ O
the -X- _ O
Language -X- _ O
Confidence -X- _ O
( -X- _ O
LC -X- _ O
) -X- _ O
as -X- _ O
: -X- _ O
LC -X- _ O
( -X- _ O
s -X- _ O
gen -X- _ O
, -X- _ O
s -X- _ O
ref -X- _ O
) -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
if -X- _ O
L -X- _ O
ref -X- _ O
= -X- _ O
argmax -X- _ O
P -X- _ O
( -X- _ O
L -X- _ O
gen -X- _ O
) -X- _ O
P -X- _ O
( -X- _ O
L -X- _ O
gen -X- _ O
= -X- _ O
L -X- _ O
ref -X- _ O
) -X- _ O
, -X- _ O
otherwise -X- _ O
3. -X- _ O
Length -X- _ O
Penalty -X- _ O
: -X- _ O
Generated -X- _ O
summaries -X- _ O
should -X- _ O
not -X- _ O
be -X- _ O
unnecessarily -X- _ O
long -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
metric -X- _ O
should -X- _ O
penalize -X- _ O
long -X- _ O
summaries. -X- _ O
While -X- _ O
model-based -X- _ O
metrics -X- _ O
may -X- _ O
indicate -X- _ O
how -X- _ O
similar -X- _ O
a -X- _ O
generated -X- _ O
summary -X- _ O
is -X- _ O
to -X- _ O
its -X- _ O
reference -X- _ O
and -X- _ O
language -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
unclear -X- _ O
how -X- _ O
they -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
determine -X- _ O
its -X- _ O
brevity. -X- _ O
As -X- _ O
such -X- _ O
, -X- _ O
we -X- _ O
adapt -X- _ O
the -X- _ O
BLEU -X- _ B-MetricName
( -X- _ O
Papineni -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
brevity -X- _ O
penalty -X- _ O
to -X- _ O
measure -X- _ O
the -X- _ O
length -X- _ O
penalty -X- _ O
: -X- _ O

We -X- _ O
finally -X- _ O
define -X- _ O
our -X- _ O
metric -X- _ O
, -X- _ O
Language-agnostic -X- _ B-MetricName
Summary -X- _ I-MetricName
Evaluation -X- _ I-MetricName
( -X- _ O
LaSE -X- _ B-MetricName
) -X- _ O
score -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O

Experiments -X- _ O
& -X- _ O
Discussions -X- _ O
One -X- _ O
model -X- _ O
capable -X- _ O
of -X- _ O
generating -X- _ O
summaries -X- _ O
in -X- _ O
any -X- _ O
target -X- _ O
language -X- _ O
for -X- _ O
an -X- _ O
input -X- _ O
article -X- _ O
from -X- _ O
any -X- _ O
source -X- _ O
language -X- _ O
is -X- _ O
highly -X- _ O
desirable. -X- _ O
However -X- _ O
, -X- _ O
it -X- _ O
may -X- _ O
not -X- _ O
be -X- _ O
the -X- _ O
case -X- _ O
that -X- _ O
such -X- _ O
a -X- _ O
'many-to-many -X- _ O
' -X- _ O
model -X- _ O
( -X- _ O
m2m -X- _ O
in -X- _ O
brief -X- _ O
) -X- _ O
would -X- _ O
outperform -X- _ O
many-toone -X- _ O
( -X- _ O
m2o -X- _ O
) -X- _ O
or -X- _ O
one-to-many -X- _ O
( -X- _ O
o2m -X- _ O
) -X- _ O
models -X- _ O
7 -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
widely-used -X- _ O
practices -X- _ O
for -X- _ O
XLS -X- _ B-TaskName
( -X- _ O
Ladhak -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Perez-Beltrachini -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
establish -X- _ O
that -X- _ O
the -X- _ O
m2m -X- _ O
model -X- _ O
, -X- _ O
trained -X- _ O
in -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
samples -X- _ O
from -X- _ O
all -X- _ O
possible -X- _ O
language -X- _ O
pairs -X- _ O
using -X- _ O
the -X- _ O
MLS -X- _ B-MethodName
algorithm -X- _ O
from -X- _ O
Section -X- _ O
4 -X- _ O
, -X- _ O
consistently -X- _ O
outperforms -X- _ O
m2o -X- _ O
, -X- _ O
o2m -X- _ O
, -X- _ O
and -X- _ O
summarize-then-translate -X- _ O
( -X- _ O
s.+t. -X- _ O
) -X- _ O
baselines -X- _ O
given -X- _ O
equal -X- _ O
training -X- _ O
steps -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
proposed -X- _ O
m2m -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
five -X- _ O
different -X- _ O
m2o -X- _ B-MethodName
and -X- _ O
o2m -X- _ B-MethodName
models -X- _ O
using -X- _ O
five -X- _ O
highly -X- _ O
spoken -X- _ O
8 -X- _ O
and -X- _ O
typologically -X- _ O
diverse -X- _ O
pivot -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
'one -X- _ O
' -X- _ O
in -X- _ O
m2o -X- _ B-MethodName
and -X- _ O
o2m -X- _ B-MethodName
) -X- _ O
languages -X- _ O
: -X- _ O
English -X- _ O
, -X- _ O
Chinese -X- _ O
( -X- _ O
simplified -X- _ O
) -X- _ O
, -X- _ O
Hindi -X- _ O
, -X- _ O
Arabic -X- _ O
, -X- _ O
and -X- _ O
Russian. -X- _ O
As -X- _ O
another -X- _ O
baseline -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
summarizethen-translate -X- _ O
pipeline. -X- _ O
As -X- _ O
fine-tuning -X- _ O
pretrained -X- _ O
language -X- _ O
models -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Xue -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
have -X- _ O
shown -X- _ O
state-of-the-art -X- _ O
results -X- _ O
on -X- _ O
monolingual -X- _ O
and -X- _ O
multilingual -X- _ O
text -X- _ O
summarization -X- _ O
( -X- _ O
Rothe -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
fine-tune -X- _ O
each -X- _ O
model -X- _ O
using -X- _ O
a -X- _ O
pretrained -X- _ O
mT5 -X- _ O
( -X- _ O
Xue -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
by -X- _ O
providing -X- _ O
explicit -X- _ O
cross-lingual -X- _ O
supervision. -X- _ O
We -X- _ O
show -X- _ O
the -X- _ O
results -X- _ O
on -X- _ O
ROUGE-2 -X- _ B-MetricName
F1 -X- _ B-MetricName
and -X- _ O
LaSE -X- _ B-MetricName
in -X- _ O
Figures -X- _ O
4 -X- _ O
and -X- _ O
5 -X- _ O
9 -X- _ O
. -X- _ O
We -X- _ O
limit -X- _ O
our -X- _ O
evaluation -X- _ O
only -X- _ O
to -X- _ O
the -X- _ O
languages -X- _ O
supported -X- _ O
by -X- _ O
mT5 -X- _ B-MethodName
, -X- _ O
fastText -X- _ B-MethodName
, -X- _ O
and -X- _ O
M2M-100 -X- _ B-MethodName
( -X- _ O
the -X- _ O
translation -X- _ O
model -X- _ O
used -X- _ O
in -X- _ O
s.+t. -X- _ O
) -X- _ O
. -X- _ O

Results -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
m2m -X- _ B-MethodName
model -X- _ O
consistently -X- _ O
outperforms -X- _ O
m2o -X- _ B-MethodName
, -X- _ O
o2m -X- _ B-MethodName
, -X- _ O
and -X- _ O
s.+t. -X- _ B-MethodName
, -X- _ O
with -X- _ O
an -X- _ O
average -X- _ O
ROUGE-2 -X- _ B-MetricName
( -X- _ O
LaSE -X- _ B-MetricName
) -X- _ O
score -X- _ O
of -X- _ O
8.15 -X- _ B-MetricValue
( -X- _ O
57.15 -X- _ B-MetricValue
) -X- _ O
over -X- _ O
all -X- _ O
languages -X- _ O
tested -X- _ O
, -X- _ O
3.12 -X- _ B-MetricValue
( -X- _ O
9.02 -X- _ B-MetricValue
) -X- _ O
above -X- _ O
s.+t. -X- _ B-MethodName
Moreover -X- _ O
, -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
o2m -X- _ B-MethodName
models -X- _ O
on -X- _ O
language -X- _ O
pairs -X- _ O
where -X- _ O
the -X- _ O
pivots -X- _ O
are -X- _ O
the -X- _ O
targets -X- _ O
, -X- _ O
the -X- _ O
m2m -X- _ O
model -X- _ O
scores -X- _ O
1.80 -X- _ B-MetricValue
( -X- _ O
5.84 -X- _ B-MetricValue
) -X- _ O
over -X- _ O
m2os -X- _ B-MethodName
, -X- _ O
and -X- _ O
on -X- _ O
those -X- _ O
where -X- _ O
the -X- _ O
pivots -X- _ O
are -X- _ O
the -X- _ O
sources -X- _ O
, -X- _ O
6.52 -X- _ B-MetricValue
( -X- _ O
51.80 -X- _ B-MetricValue
) -X- _ O
over -X- _ O
o2ms -X- _ B-MethodName
. -X- _ O

Upon -X- _ O
inspection -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
outputs -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
the -X- _ O
m2o -X- _ O
models -X- _ O
to -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
generate -X- _ O
non-trivial -X- _ O
summaries. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
the -X- _ O
o2m -X- _ O
models -X- _ O
completely -X- _ O
failed -X- _ O
to -X- _ O
produce -X- _ O
cross-lingual -X- _ O
summaries -X- _ O
, -X- _ O
performing -X- _ O
in-language -X- _ O
summarization -X- _ O
( -X- _ O
the -X- _ O
language -X- _ O
of -X- _ O
the -X- _ O
summary -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
that -X- _ O
of -X- _ O
its -X- _ O
input -X- _ O
article -X- _ O
) -X- _ O
for -X- _ O
all -X- _ O
targets. -X- _ O
We -X- _ O
hypothesize -X- _ O
that -X- _ O
varying -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
in -X- _ O
a -X- _ O
batch -X- _ O
hampers -X- _ O
the -X- _ O
decoder -X- _ O
's -X- _ O
ability -X- _ O
to -X- _ O
generate -X- _ O
from -X- _ O
a -X- _ O
specific -X- _ O
language -X- _ O
, -X- _ O
possibly -X- _ O
because -X- _ O
of -X- _ O
the -X- _ O
vast -X- _ O
diversity -X- _ O
of -X- _ O
target -X- _ O
languages -X- _ O
in -X- _ O
the -X- _ O
batch -X- _ O
( -X- _ O
discussed -X- _ O
further -X- _ O
in -X- _ O
Appendix -X- _ O
E -X- _ O
) -X- _ O
. -X- _ O
s.+t. -X- _ O
performed -X- _ O
well -X- _ O
on -X- _ O
high-resource -X- _ O
languages -X- _ O
but -X- _ O
poorly -X- _ O
on -X- _ O
lowresource -X- _ O
ones. -X- _ O
This -X- _ O
was -X- _ O
revealed -X- _ O
to -X- _ O
be -X- _ O
a -X- _ O
limitation -X- _ O
of -X- _ O
the -X- _ O
translation -X- _ O
model -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
pipeline -X- _ O
. -X- _ O

Zero-shot -X- _ O
Cross-lingual -X- _ O
Transfer -X- _ O
The -X- _ O
previous -X- _ O
experiments -X- _ O
were -X- _ O
done -X- _ O
in -X- _ O
a -X- _ O
fully -X- _ O
supervised -X- _ O
fashion. -X- _ O
However -X- _ O
, -X- _ O
for -X- _ O
many -X- _ O
low-resource -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
samples -X- _ O
are -X- _ O
not -X- _ O
abundantly -X- _ O
available. -X- _ O
Hence -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
attractive -X- _ O
to -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
perform -X- _ O
zero-shot -X- _ O
cross-lingual -X- _ O
generation -X- _ O
( -X- _ O
Duan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
without -X- _ O
relying -X- _ O
on -X- _ O
any -X- _ O
labeled -X- _ O
examples -X- _ O
. -X- _ O

To -X- _ O
this -X- _ O
end -X- _ O
, -X- _ O
we -X- _ O
fine-tuned -X- _ O
mT5 -X- _ B-MethodName
with -X- _ O
only -X- _ O
the -X- _ O
inlanguage -X- _ O
samples -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
both -X- _ O
have -X- _ O
the -X- _ O
same -X- _ O
language -X- _ O
) -X- _ O
in -X- _ O
a -X- _ O
multilingual -X- _ O
fashion -X- _ O
and -X- _ O
, -X- _ O
during -X- _ O
inference -X- _ O
, -X- _ O
varied -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
Unfortunately -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
totally -X- _ O
fails -X- _ O
at -X- _ O
generating -X- _ O
cross-lingual -X- _ O
summaries -X- _ O
and -X- _ O
performs -X- _ O
in-language -X- _ O
summarization -X- _ O
instead. -X- _ O
We -X- _ O
also -X- _ O
fine-tuned -X- _ O
m2o -X- _ B-MethodName
models -X- _ O
( -X- _ O
with -X- _ O
only -X- _ O
the -X- _ O
in-language -X- _ O
samples -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
) -X- _ O
in -X- _ O
a -X- _ O
monolingual -X- _ O
fashion -X- _ O
and -X- _ O
ran -X- _ O
inference -X- _ O
in -X- _ O
a -X- _ O
zeroshot -X- _ O
setting -X- _ O
with -X- _ O
samples -X- _ O
from -X- _ O
other -X- _ O
languages -X- _ O
as -X- _ O
input. -X- _ O
Here -X- _ O
, -X- _ O
the -X- _ O
models -X- _ O
are -X- _ O
able -X- _ O
to -X- _ O
generate -X- _ O
nontrivial -X- _ O
summaries -X- _ O
for -X- _ O
some -X- _ O
language -X- _ O
pairs -X- _ O
but -X- _ O
still -X- _ O
lag -X- _ O
behind -X- _ O
fully -X- _ O
supervised -X- _ O
models -X- _ O
by -X- _ O
a -X- _ O
significant -X- _ O
margin. -X- _ O
We -X- _ O
have -X- _ O
included -X- _ O
Figures -X- _ O
10 -X- _ O
and -X- _ O
11 -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
to -X- _ O
illustrate -X- _ O
this -X- _ O
. -X- _ O

Furthermore -X- _ O
, -X- _ O
we -X- _ O
ran -X- _ O
inference -X- _ O
with -X- _ O
the -X- _ O
m2m -X- _ B-MethodName
model -X- _ O
on -X- _ O
distant -X- _ O
low-resource -X- _ O
language -X- _ O
pairs -X- _ O
that -X- _ O
were -X- _ O
absent -X- _ O
in -X- _ O
training. -X- _ O
Their -X- _ O
LaSE -X- _ B-MetricName
scores -X- _ O
were -X- _ O
substantially -X- _ O
below -X- _ O
supervised -X- _ O
pairs -X- _ O
, -X- _ O
meaning -X- _ O
zeroshot -X- _ O
transfer -X- _ O
in -X- _ O
supervised -X- _ O
multilingual -X- _ O
models -X- _ O
( -X- _ O
Johnson -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
shows -X- _ O
weak -X- _ O
performance -X- _ O
. -X- _ O

Analysis -X- _ O
of -X- _ O
Results -X- _ O
Statistical -X- _ O
significance -X- _ O
While -X- _ O
the -X- _ O
scores -X- _ O
obtained -X- _ O
from -X- _ O
the -X- _ O
experiments -X- _ O
in -X- _ O
Section -X- _ O
5 -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
m2m -X- _ O
model -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
others -X- _ O
, -X- _ O
the -X- _ O
differences -X- _ O
are -X- _ O
very -X- _ O
close -X- _ O
in -X- _ O
many -X- _ O
language -X- _ O
pairs. -X- _ O
Therefore -X- _ O
, -X- _ O
a -X- _ O
statistical -X- _ O
significance -X- _ O
test -X- _ O
is -X- _ O
still -X- _ O
warranted -X- _ O
to -X- _ O
support -X- _ O
our -X- _ O
claim -X- _ O
further. -X- _ O
As -X- _ O
such -X- _ O
, -X- _ O
for -X- _ O
each -X- _ O
language -X- _ O
pair -X- _ O
experimented -X- _ O
on -X- _ O
, -X- _ O
we -X- _ O
performed -X- _ O
the -X- _ O
Bootstrap -X- _ O
resampling -X- _ O
test -X- _ O
( -X- _ O
Koehn -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
with -X- _ O
the -X- _ O
m2m -X- _ O
model -X- _ O
against -X- _ O
the -X- _ O
best-performing -X- _ O
model -X- _ O
among -X- _ O
the -X- _ O
others -X- _ O
in -X- _ O
a -X- _ O
one -X- _ O
vs. -X- _ O
all -X- _ O
manner -X- _ O
: -X- _ O
if -X- _ O
m2m -X- _ O
has -X- _ O
the -X- _ O
best -X- _ O
( -X- _ O
ROUGE-2 -X- _ B-MetricName
/ -X- _ O
LaSE -X- _ B-MetricName
) -X- _ O
score -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
it -X- _ O
with -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
second-best -X- _ O
score -X- _ O
, -X- _ O
and -X- _ O
if -X- _ O
m2m -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
best -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
it -X- _ O
with -X- _ O
the -X- _ O
best. -X- _ O
Results -X- _ O
( -X- _ O
p -X- _ O
< -X- _ O
0.05 -X- _ O
) -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
reveal -X- _ O
that -X- _ O
in -X- _ O
more -X- _ O
than -X- _ O
42 -X- _ O
% -X- _ O
language -X- _ O
pairs -X- _ O
tested -X- _ O
, -X- _ O
m2m -X- _ O
is -X- _ O
significantly -X- _ O
better -X- _ O
, -X- _ O
and -X- _ O
in -X- _ O
less -X- _ O
than -X- _ O
10 -X- _ O
% -X- _ O
pairs -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
considerably -X- _ O
worse. -X- _ O
10 -X- _ O
This -X- _ O
provides -X- _ O
additional -X- _ O
evidence -X- _ O
in -X- _ O
support -X- _ O
of -X- _ O
our -X- _ O
claim -X- _ O
that -X- _ O
the -X- _ O
m2m -X- _ O
model -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
others -X- _ O
. -X- _ O

How -X- _ O
reliable -X- _ O
is -X- _ O
LaSE -X- _ B-MetricName
? -X- _ O
At -X- _ O
first -X- _ O
, -X- _ O
we -X- _ O
validated -X- _ O
the -X- _ O
reliability -X- _ O
of -X- _ O
LaSE -X- _ B-MetricName
by -X- _ O
showing -X- _ O
its -X- _ O
correlation -X- _ O
with -X- _ O
ROUGE-2. -X- _ B-MetricName
We -X- _ O
took -X- _ O
different -X- _ O
checkpoints -X- _ O
of -X- _ O
the -X- _ O
in-language -X- _ O
summarization -X- _ O
model -X- _ O
used -X- _ O
in -X- _ O
s.+t. -X- _ O
and -X- _ O
computed -X- _ O
ROUGE-2 -X- _ B-MetricName
and -X- _ O
LaSE -X- _ B-MetricName
for -X- _ O
the -X- _ O
nine -X- _ O
languages -X- _ O
in -X- _ O
Section -X- _ O
3 -X- _ O
for -X- _ O
each -X- _ O
checkpoint. -X- _ O
The -X- _ O
correlation -X- _ O
coefficients -X- _ O
of -X- _ O
the -X- _ O
calculated -X- _ O
scores -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
column -X- _ O
of -X- _ O
Table -X- _ O
2. -X- _ O
For -X- _ O
all -X- _ O
languages -X- _ O
( -X- _ O
from -X- _ O
high-to -X- _ O
low-resource -X- _ O
) -X- _ O
, -X- _ O
LaSE -X- _ B-MetricName
has -X- _ O
a -X- _ O
near-perfect -X- _ O
correlation -X- _ O
with -X- _ O
ROUGE-2 -X- _ B-MetricName
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
purpose -X- _ O
of -X- _ O
LaSE -X- _ B-MetricName
is -X- _ O
to -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
language-agnostic -X- _ O
and -X- _ O
can -X- _ O
even -X- _ O
be -X- _ O
computed -X- _ O
in -X- _ O
the -X- _ O
absence -X- _ O
of -X- _ O
references -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
summaries -X- _ O
with -X- _ O
references -X- _ O
in -X- _ O
a -X- _ O
different -X- _ O
language -X- _ O
from -X- _ O
the -X- _ O
target -X- _ O
using -X- _ O
the -X- _ O
m2m -X- _ B-MethodName
model. -X- _ O
For -X- _ O
each -X- _ O
target -X- _ O
language -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
compute -X- _ O
the -X- _ O
standard -X- _ O
LaSE -X- _ B-MetricName
for -X- _ O
different -X- _ O
source -X- _ O
languages -X- _ O
( -X- _ O
denoted -X- _ O
as -X- _ O
LaSE-in-lang -X- _ B-MetricName
) -X- _ O
. -X- _ O
We -X- _ O
again -X- _ O
compute -X- _ O
LaSE -X- _ B-MetricName
after -X- _ O
swapping -X- _ O
the -X- _ O
reference -X- _ O
texts -X- _ O
with -X- _ O
the -X- _ O
references -X- _ O
in -X- _ O
the -X- _ O
language -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
11 -X- _ O
( -X- _ O
denoted -X- _ O
as -X- _ O
LaSE-out-lang -X- _ B-MetricName
) -X- _ O
. -X- _ O
We -X- _ O
then -X- _ O
show -X- _ O
the -X- _ O
correlation -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
variants -X- _ O
of -X- _ O
LaSE -X- _ B-MetricName
in -X- _ O
the -X- _ O
third -X- _ O
column -X- _ O
of -X- _ O
Table -X- _ O
2 -X- _ O
12 -X- _ O
for -X- _ O
each -X- _ O
target -X- _ O
language. -X- _ O
Results -X- _ O
show -X- _ O
a -X- _ O
substantial -X- _ O
correlation -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
variants -X- _ O
of -X- _ O
LaSE -X- _ B-MetricName
for -X- _ O
all -X- _ O
languages -X- _ O
. -X- _ O

From -X- _ O
these -X- _ O
two -X- _ O
experiments -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
conclude -X- _ O
that -X- _ O
LaSE -X- _ B-MetricName
is -X- _ O
an -X- _ O
ideal -X- _ O
metric -X- _ O
for -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
summarization -X- _ O
systems -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
in -X- _ O
a -X- _ O
language-independent -X- _ O
manner. -X- _ O
2021 -X- _ O
) -X- _ O
introduced -X- _ O
multiple -X- _ O
pretraining -X- _ O
objectives -X- _ O
specifically -X- _ O
tailored -X- _ O
to -X- _ O
cross-lingual -X- _ O
tasks -X- _ O
that -X- _ O
showed -X- _ O
improved -X- _ O
results -X- _ O
on -X- _ O
XLS. -X- _ B-TaskName
We -X- _ O
refer -X- _ O
our -X- _ O
readers -X- _ O
to -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
for -X- _ O
a -X- _ O
more -X- _ O
comprehensive -X- _ O
literature -X- _ O
review. -X- _ O
Until -X- _ O
recently -X- _ O
, -X- _ O
XLS -X- _ B-TaskName
was -X- _ O
limited -X- _ O
primarily -X- _ O
to -X- _ O
English-Chinese -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
benchmark -X- _ O
datasets. -X- _ O
To -X- _ O
promote -X- _ O
the -X- _ O
task -X- _ O
beyond -X- _ O
this -X- _ O
language -X- _ O
pair -X- _ O
, -X- _ O
Ladhak -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
introduced -X- _ O
Wikilingua -X- _ O
, -X- _ O
a -X- _ O
large-scale -X- _ O
many-to-one -X- _ O
dataset -X- _ O
with -X- _ O
English -X- _ O
as -X- _ O
the -X- _ O
pivot -X- _ O
language -X- _ O
, -X- _ O
while -X- _ O
Perez-Beltrachini -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
introduced -X- _ O
XWikis -X- _ O
, -X- _ O
containing -X- _ O
4 -X- _ O
languages -X- _ O
in -X- _ O
12 -X- _ O
directions -X- _ O
. -X- _ O

More -X- _ O
recently -X- _ O
, -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2023 -X- _ O
) -X- _ O
explored -X- _ O
zeroshot -X- _ O
cross-lingual -X- _ O
summarization -X- _ O
by -X- _ O
prompting -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2023 -X- _ O
) -X- _ O
large -X- _ O
language -X- _ O
models -X- _ O
like -X- _ O
Chat-GPT -X- _ O
13 -X- _ O
, -X- _ O
GPT-4 -X- _ O
( -X- _ O
OpenAI -X- _ O
, -X- _ O
2023 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
BLOOMZ -X- _ O
( -X- _ O
Muennighoff -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O

Conclusion -X- _ O
& -X- _ O
Future -X- _ O
Works -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
presented -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
a -X- _ O
largescale -X- _ O
, -X- _ O
non-English-centric -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
containing -X- _ O
1.68 -X- _ O
million -X- _ O
samples -X- _ O
in -X- _ O
1,500+ -X- _ O
language -X- _ O
pairs. -X- _ O
CrossSum -X- _ B-DatasetName
provides -X- _ O
the -X- _ O
first -X- _ O
publicly -X- _ O
available -X- _ O
XLS -X- _ B-TaskName
dataset -X- _ O
for -X- _ O
many -X- _ O
of -X- _ O
these -X- _ O
pairs. -X- _ O
Performing -X- _ O
a -X- _ O
limited-scale -X- _ O
human -X- _ O
evaluation -X- _ O
of -X- _ O
CrossSum -X- _ B-DatasetName
, -X- _ O
we -X- _ O
introduced -X- _ O
MLS -X- _ B-MethodName
, -X- _ O
a -X- _ O
multistage -X- _ O
sampling -X- _ O
algorithm -X- _ O
for -X- _ O
general-purpose -X- _ O
cross-lingual -X- _ O
generation -X- _ O
, -X- _ O
and -X- _ O
LaSE -X- _ O
, -X- _ O
a -X- _ O
language-agnostic -X- _ O
metric -X- _ O
for -X- _ O
evaluating -X- _ O
summaries -X- _ O
when -X- _ O
reference -X- _ O
summaries -X- _ O
in -X- _ O
the -X- _ O
target -X- _ O
languages -X- _ O
may -X- _ O
not -X- _ O
be -X- _ O
available. -X- _ O
We -X- _ O
demonstrated -X- _ O
that -X- _ O
training -X- _ O
one -X- _ O
multilingual -X- _ O
model -X- _ O
can -X- _ O
help -X- _ O
towards -X- _ O
better -X- _ O
XLS -X- _ B-TaskName
than -X- _ O
baselines. -X- _ O
We -X- _ O
also -X- _ O
shed -X- _ O
light -X- _ O
on -X- _ O
the -X- _ O
potential -X- _ O
to -X- _ O
perform -X- _ O
zero-shot -X- _ B-TaskName
and -X- _ I-TaskName
few-shot -X- _ I-TaskName
XLS -X- _ I-TaskName
with -X- _ O
CrossSum. -X- _ O
We -X- _ O
share -X- _ O
our -X- _ O
findings -X- _ O
and -X- _ O
resources -X- _ O
in -X- _ O
the -X- _ O
hopes -X- _ O
of -X- _ O
making -X- _ O
the -X- _ O
XLS -X- _ B-TaskName
research -X- _ O
community -X- _ O
more -X- _ O
inclusive -X- _ O
and -X- _ O
diverse -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
investigate -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
Cross-Sum -X- _ B-DatasetName
for -X- _ O
other -X- _ O
summarization -X- _ O
tasks -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
multidocument -X- _ O
( -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
multi-modal -X- _ O
summarization -X- _ O
( -X- _ O
Zhu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
would -X- _ O
also -X- _ O
like -X- _ O
to -X- _ O
explore -X- _ O
better -X- _ O
techniques -X- _ O
for -X- _ O
m2m -X- _ O
, -X- _ O
zeroshot -X- _ O
, -X- _ O
and -X- _ O
few-shot -X- _ O
cross-lingual -X- _ O
summarization -X- _ O
. -X- _ O

Limitations -X- _ O
Though -X- _ O
we -X- _ O
believe -X- _ O
that -X- _ O
our -X- _ O
work -X- _ O
has -X- _ O
many -X- _ O
merits -X- _ O
, -X- _ O
some -X- _ O
of -X- _ O
its -X- _ O
limitations -X- _ O
must -X- _ O
be -X- _ O
acknowledged. -X- _ O
Despite -X- _ O
exhaustive -X- _ O
human -X- _ O
annotation -X- _ O
being -X- _ O
the -X- _ O
most -X- _ O
reliable -X- _ O
means -X- _ O
of -X- _ O
ensuring -X- _ O
the -X- _ O
maximum -X- _ O
quality -X- _ O
of -X- _ O
a -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
had -X- _ O
to -X- _ O
resort -X- _ O
to -X- _ O
the -X- _ O
automatic -X- _ O
curation -X- _ O
of -X- _ O
CrossSum -X- _ B-DatasetName
due -X- _ O
to -X- _ O
the -X- _ O
enormous -X- _ O
scale -X- _ O
of -X- _ O
the -X- _ O
dataset. -X- _ O
As -X- _ O
identified -X- _ O
in -X- _ O
the -X- _ O
human -X- _ O
evaluation -X- _ O
, -X- _ O
not -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
alignments -X- _ O
made -X- _ O
by -X- _ O
LaBSE -X- _ B-MethodName
are -X- _ O
correct. -X- _ O
They -X- _ O
are -X- _ O
primarily -X- _ O
summaries -X- _ O
describing -X- _ O
similar -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
having -X- _ O
a -X- _ O
substantial -X- _ O
degree -X- _ O
of -X- _ O
syntactic -X- _ O
or -X- _ O
semantic -X- _ O
similarity -X- _ O
) -X- _ O
but -X- _ O
non-identical -X- _ O
events. -X- _ O
LaBSE -X- _ B-MethodName
also -X- _ O
fails -X- _ O
to -X- _ O
penalize -X- _ O
numerical -X- _ O
mismatches -X- _ O
, -X- _ O
especially -X- _ O
if -X- _ O
the -X- _ O
summaries -X- _ O
depict -X- _ O
the -X- _ O
same -X- _ O
event -X- _ O
. -X- _ O

Consequently -X- _ O
, -X- _ O
any -X- _ O
mistake -X- _ O
made -X- _ O
by -X- _ O
LaBSE -X- _ B-MethodName
in -X- _ O
the -X- _ O
curation -X- _ O
phase -X- _ O
may -X- _ O
propagate -X- _ O
to -X- _ O
the -X- _ O
models -X- _ O
trained -X- _ O
using -X- _ O
CrossSum. -X- _ B-DatasetName
And -X- _ O
since -X- _ O
LaBSE -X- _ B-MethodName
is -X- _ O
a -X- _ O
component -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
LaSE -X- _ B-MetricName
metric -X- _ O
, -X- _ O
these -X- _ O
biases -X- _ O
may -X- _ O
remain -X- _ O
unidentified -X- _ O
by -X- _ O
LaSE -X- _ B-MethodName
in -X- _ O
the -X- _ O
evaluation -X- _ O
stage. -X- _ O
However -X- _ O
, -X- _ O
no -X- _ O
matter -X- _ O
which -X- _ O
automatic -X- _ O
method -X- _ O
we -X- _ O
use -X- _ O
, -X- _ O
there -X- _ O
will -X- _ O
be -X- _ O
such -X- _ O
frailties -X- _ O
in -X- _ O
these -X- _ O
extreme -X- _ O
cases. -X- _ O
Since -X- _ O
the -X- _ O
objective -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
not -X- _ O
to -X- _ O
scrutinize -X- _ O
the -X- _ O
pitfalls -X- _ O
of -X- _ O
LaBSE -X- _ B-MethodName
but -X- _ O
rather -X- _ O
to -X- _ O
use -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
means -X- _ O
of -X- _ O
curation -X- _ O
and -X- _ O
evaluation -X- _ O
, -X- _ O
we -X- _ O
deem -X- _ O
LaBSE -X- _ B-MethodName
the -X- _ O
best -X- _ O
choice -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
extensive -X- _ O
language -X- _ O
coverage -X- _ O
and -X- _ O
empirical -X- _ O
performance -X- _ O
in -X- _ O
cross-lingual -X- _ O
mining -X- _ O
among -X- _ O
existing -X- _ O
alternatives -X- _ O
. -X- _ O

Ethical -X- _ O
Considerations -X- _ O
License -X- _ O
CrossSum -X- _ B-DatasetName
is -X- _ O
a -X- _ O
derivative -X- _ O
of -X- _ O
the -X- _ O
XL-Sum -X- _ B-DatasetName
dataset. -X- _ O
XL-Sum -X- _ B-DatasetName
has -X- _ O
been -X- _ O
released -X- _ O
under -X- _ O
the -X- _ O
Creative -X- _ O
Commons -X- _ O
Attribution-NonCommercial-ShareAlike -X- _ O
4.0 -X- _ O
International -X- _ O
License -X- _ O
( -X- _ O
CC -X- _ O
BY-NC-SA -X- _ O
4.0 -X- _ O
) -X- _ O
, -X- _ O
allowing -X- _ O
modifications -X- _ O
and -X- _ O
distributions -X- _ O
for -X- _ O
non-commercial -X- _ O
research -X- _ O
purposes. -X- _ O
We -X- _ O
are -X- _ O
adhering -X- _ O
to -X- _ O
the -X- _ O
terms -X- _ O
of -X- _ O
the -X- _ O
license -X- _ O
and -X- _ O
releasing -X- _ O
CrossSum -X- _ B-DatasetName
under -X- _ O
the -X- _ O
same -X- _ O
license. -X- _ O
Generated -X- _ O
Text -X- _ O
All -X- _ O
of -X- _ O
our -X- _ O
models -X- _ O
use -X- _ O
the -X- _ O
mT5 -X- _ B-MethodName
model -X- _ O
as -X- _ O
the -X- _ O
backbone -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
pretrained -X- _ O
on -X- _ O
a -X- _ O
large -X- _ O
multilingual -X- _ O
text -X- _ O
corpus. -X- _ O
For -X- _ O
a -X- _ O
text -X- _ O
generation -X- _ O
model -X- _ O
, -X- _ O
even -X- _ O
small -X- _ O
amounts -X- _ O
of -X- _ O
offensive -X- _ O
or -X- _ O
harmful -X- _ O
texts -X- _ O
in -X- _ O
pretraining -X- _ O
could -X- _ O
lead -X- _ O
to -X- _ O
dangerous -X- _ O
biases -X- _ O
in -X- _ O
generated -X- _ O
text -X- _ O
( -X- _ O
Luccioni -X- _ O
and -X- _ O
Viviano -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
our -X- _ O
models -X- _ O
can -X- _ O
potentially -X- _ O
generate -X- _ O
offensive -X- _ O
or -X- _ O
biased -X- _ O
content -X- _ O
learned -X- _ O
during -X- _ O
the -X- _ O
pretraining -X- _ O
phase -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
beyond -X- _ O
our -X- _ O
control. -X- _ O
Text -X- _ O
summarization -X- _ O
systems -X- _ O
have -X- _ O
also -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
generate -X- _ O
unfaithful -X- _ O
and -X- _ O
factually -X- _ O
incorrect -X- _ O
( -X- _ O
albeit -X- _ O
fluent -X- _ O
) -X- _ O
( -X- _ O
Maynez -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
texts. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
suggest -X- _ O
carefully -X- _ O
examining -X- _ O
the -X- _ O
potential -X- _ O
biases -X- _ O
before -X- _ O
considering -X- _ O
them -X- _ O
in -X- _ O
any -X- _ O
real-world -X- _ O
deployment -X- _ O
. -X- _ O

Human -X- _ O
Evaluation -X- _ O
Annotators -X- _ O
were -X- _ O
hired -X- _ O
from -X- _ O
the -X- _ O
graduates -X- _ O
of -X- _ O
an -X- _ O
institute -X- _ O
that -X- _ O
provides -X- _ O
professional -X- _ O
training -X- _ O
for -X- _ O
many -X- _ O
languages -X- _ O
, -X- _ O
including -X- _ O
the -X- _ O
ones -X- _ O
evaluated -X- _ O
in -X- _ O
Section -X- _ O
3. -X- _ O
Each -X- _ O
annotator -X- _ O
was -X- _ O
given -X- _ O
around -X- _ O
200-250 -X- _ O
sequence -X- _ O
pairs -X- _ O
to -X- _ O
evaluate. -X- _ O
Each -X- _ O
annotation -X- _ O
took -X- _ O
an -X- _ O
average -X- _ O
of -X- _ O
one -X- _ O
and -X- _ O
a -X- _ O
half -X- _ O
minutes -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
approximately -X- _ O
5-6 -X- _ O
hours -X- _ O
for -X- _ O
annotating -X- _ O
the -X- _ O
whole -X- _ O
set. -X- _ O
Annotators -X- _ O
were -X- _ O
paid -X- _ O
hourly -X- _ O
per -X- _ O
the -X- _ O
standard -X- _ O
remuneration -X- _ O
of -X- _ O
bilingual -X- _ O
professionals -X- _ O
in -X- _ O
local -X- _ O
currency -X- _ O
. -X- _ O

Environmental -X- _ O
Impact -X- _ O
A -X- _ O
total -X- _ O
of -X- _ O
25 -X- _ O
models -X- _ O
were -X- _ O
trained -X- _ O
as -X- _ O
part -X- _ O
of -X- _ O
this -X- _ O
work. -X- _ O
Each -X- _ O
model -X- _ O
was -X- _ O
trained -X- _ O
for -X- _ O
about -X- _ O
three -X- _ O
days -X- _ O
on -X- _ O
a -X- _ O
4-GPU -X- _ O
Tesla -X- _ O
P100 -X- _ O
server. -X- _ O
Assuming -X- _ O
0.08 -X- _ O
kg -X- _ O
/ -X- _ O
kWh -X- _ O
carbon -X- _ O
emission -X- _ O
14 -X- _ O
, -X- _ O
less -X- _ O
than -X- _ O
175kg -X- _ O
of -X- _ O
carbon -X- _ O
was -X- _ O
released -X- _ O
into -X- _ O
the -X- _ O
environment -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
orders -X- _ O
of -X- _ O
magnitude -X- _ O
below -X- _ O
the -X- _ O
most -X- _ O
computationally -X- _ O
demanding -X- _ O
models -X- _ O
. -X- _ O

Appendix -X- _ O
A -X- _ O
Aligning -X- _ O
Summaries -X- _ O
using -X- _ O
LaBSE -X- _ B-MethodName
In -X- _ O
Section -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
curated -X- _ O
CrossSum -X- _ B-DatasetName
by -X- _ O
aligning -X- _ O
parallel -X- _ O
summaries -X- _ O
in -X- _ O
different -X- _ O
languages. -X- _ O
It -X- _ O
might -X- _ O
be -X- _ O
argued -X- _ O
why -X- _ O
the -X- _ O
articles -X- _ O
themselves -X- _ O
were -X- _ O
not -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
alignment -X- _ O
process. -X- _ O
Initially -X- _ O
, -X- _ O
we -X- _ O
experimented -X- _ O
with -X- _ O
whole-article -X- _ O
embeddings. -X- _ O
However -X- _ O
, -X- _ O
this -X- _ O
resulted -X- _ O
in -X- _ O
many -X- _ O
false-negative -X- _ O
alignments -X- _ O
, -X- _ O
where -X- _ O
similarity -X- _ O
scores -X- _ O
between -X- _ O
parallel -X- _ O
articles -X- _ O
across -X- _ O
languages -X- _ O
were -X- _ O
relatively -X- _ O
low -X- _ O
( -X- _ O
verified -X- _ O
manually -X- _ O
between -X- _ O
English -X- _ O
and -X- _ O
the -X- _ O
authors -X- _ O
' -X- _ O
native -X- _ O
languages -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
most -X- _ O
likely -X- _ O
attributed -X- _ O
to -X- _ O
the -X- _ O
512-token -X- _ O
limit -X- _ O
of -X- _ O
LaBSE -X- _ B-MethodName
and -X- _ O
different -X- _ O
sequence -X- _ O
lengths -X- _ O
of -X- _ O
those -X- _ O
articles -X- _ O
due -X- _ O
to -X- _ O
different -X- _ O
languages -X- _ O
having -X- _ O
different -X- _ O
subword -X- _ O
segmentation -X- _ O
fertility -X- _ O
( -X- _ O
Ács -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
would -X- _ O
entail -X- _ O
that -X- _ O
parallel -X- _ O
articles -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
might -X- _ O
be -X- _ O
truncated -X- _ O
at -X- _ O
different -X- _ O
locations -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
discrepancies -X- _ O
between -X- _ O
their -X- _ O
embeddings. -X- _ O
As -X- _ O
observed -X- _ O
in -X- _ O
the -X- _ O
BUCC -X- _ O
evaluation -X- _ O
, -X- _ O
LaBSE -X- _ B-MethodName
is -X- _ O
well-suited -X- _ O
for -X- _ O
sentence-level -X- _ O
retrieval. -X- _ O
Since -X- _ O
summaries -X- _ O
are -X- _ O
good -X- _ O
representatives -X- _ O
of -X- _ O
entire -X- _ O
articles -X- _ O
, -X- _ O
we -X- _ O
finally -X- _ O
chose -X- _ O
summaries -X- _ O
as -X- _ O
our -X- _ O
candidates -X- _ O
for -X- _ O
the -X- _ O
alignment -X- _ O
. -X- _ O

C.1 -X- _ O
Choice -X- _ O
of -X- _ O
Pretrained -X- _ O
Model -X- _ O
Many -X- _ O
pretrained -X- _ O
multilingual -X- _ O
text-to-text -X- _ O
models -X- _ O
are -X- _ O
currently -X- _ O
available -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
mBART -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
CRISS -X- _ O
( -X- _ O
Tran -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
MARGE -X- _ O
, -X- _ O
and -X- _ O
mT5 -X- _ B-MethodName
( -X- _ O
Xue -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
. -X- _ O
While -X- _ O
mBART -X- _ O
and -X- _ O
mT5 -X- _ B-MethodName
are -X- _ O
pretrained -X- _ O
with -X- _ O
multilingual -X- _ O
objectives -X- _ O
, -X- _ O
CRISS -X- _ O
and -X- _ O
MARGE -X- _ O
are -X- _ O
pretrained -X- _ O
with -X- _ O
a -X- _ O
cross-lingual -X- _ O
one -X- _ O
, -X- _ O
which -X- _ O
better -X- _ O
suits -X- _ O
our -X- _ O
use -X- _ O
case. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
mT5 -X- _ B-MethodName
for -X- _ O
fine-tuning -X- _ O
because -X- _ O
of -X- _ O
its -X- _ O
broad -X- _ O
coverage -X- _ O
of -X- _ O
101 -X- _ O
languages -X- _ O
with -X- _ O
support -X- _ O
for -X- _ O
41 -X- _ O
of -X- _ O
the -X- _ O
45 -X- _ O
languages -X- _ O
from -X- _ O
CrossSum -X- _ O
, -X- _ O
in -X- _ O
contrast -X- _ O
to -X- _ O
only -X- _ O
15 -X- _ O
languages -X- _ O
in -X- _ O
mBART -X- _ O
or -X- _ O
CRISS -X- _ O
and -X- _ O
26 -X- _ O
in -X- _ O
MARGE -X- _ O
. -X- _ O

One -X- _ O
solution -X- _ O
is -X- _ O
to -X- _ O
segment -X- _ O
the -X- _ O
documents -X- _ O
into -X- _ O
sentences -X- _ O
and -X- _ O
then -X- _ O
translate -X- _ O
them. -X- _ O
But -X- _ O
that -X- _ O
increases -X- _ O
the -X- _ O
compute -X- _ O
overhead -X- _ O
, -X- _ O
and -X- _ O
translations -X- _ O
suffer -X- _ O
from -X- _ O
loss -X- _ O
of -X- _ O
context. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
multilingual -X- _ O
summarization -X- _ O
model -X- _ O
( -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
coupled -X- _ O
with -X- _ O
the -X- _ O
multilingual -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
model -X- _ O
, -X- _ O
M2M-100 -X- _ O
( -X- _ O
Fan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
our -X- _ O
pipeline -X- _ O
. -X- _ O

C.2.1 -X- _ O
Multilingual -X- _ O
Summarization -X- _ O
The -X- _ O
pipeline -X- _ O
first -X- _ O
performs -X- _ O
in-language -X- _ O
summarization. -X- _ O
We -X- _ O
train -X- _ O
our -X- _ O
own -X- _ O
model -X- _ O
for -X- _ O
summarization -X- _ O
as -X- _ O
the -X- _ O
model -X- _ O
released -X- _ O
by -X- _ O
Hasan -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
has -X- _ O
been -X- _ O
rendered -X- _ O
unusable -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
change -X- _ O
in -X- _ O
the -X- _ O
dataset -X- _ O
split. -X- _ O
We -X- _ O
extend -X- _ O
our -X- _ O
component -X- _ O
graphs -X- _ O
to -X- _ O
curate -X- _ O
the -X- _ O
in-language -X- _ O
dataset -X- _ O
splits. -X- _ O
We -X- _ O
consider -X- _ O
articles -X- _ O
having -X- _ O
no -X- _ O
parallel -X- _ O
counterpart -X- _ O
in -X- _ O
any -X- _ O
other -X- _ O
language -X- _ O
as -X- _ O
single -X- _ O
node -X- _ O
components -X- _ O
in -X- _ O
the -X- _ O
component -X- _ O
graph. -X- _ O
As -X- _ O
before -X- _ O
, -X- _ O
we -X- _ O
assign -X- _ O
all -X- _ O
articles -X- _ O
originating -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
component -X- _ O
to -X- _ O
the -X- _ O
training -X- _ O
( -X- _ O
dev -X- _ O
/ -X- _ O
test -X- _ O
) -X- _ O
set -X- _ O
of -X- _ O
the -X- _ O
dataset -X- _ O
, -X- _ O
extending -X- _ O
them -X- _ O
to -X- _ O
the -X- _ O
in-language -X- _ O
splits -X- _ O
too. -X- _ O
We -X- _ O
then -X- _ O
train -X- _ O
the -X- _ O
multilingual -X- _ O
model -X- _ O
by -X- _ O
fine-tuning -X- _ O
mT5 -X- _ B-MethodName
with -X- _ O
the -X- _ O
in-language -X- _ O
splits -X- _ O
, -X- _ O
sampling -X- _ O
each -X- _ O
batch -X- _ B-HyperparameterName
of -X- _ O
256 -X- _ B-HyperparameterValue
samples -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
language -X- _ O
with -X- _ O
a -X- _ O
sampling -X- _ O
factor -X- _ O
of -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
0.5 -X- _ B-HyperparameterValue
. -X- _ O

C.2.2 -X- _ O
Multilingual -X- _ O
Translation -X- _ O
For -X- _ O
multilingual -X- _ O
translation -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
M2M-100 -X- _ B-MethodName
( -X- _ O
Fan -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
( -X- _ O
418M -X- _ O
parameters -X- _ O
variant -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
many-to-many -X- _ O
multilingual -X- _ O
translation -X- _ O
model -X- _ O
, -X- _ O
with -X- _ O
support -X- _ O
for -X- _ O
37 -X- _ O
languages -X- _ O
from -X- _ O
CrossSum -X- _ O
. -X- _ O

C.3 -X- _ O
Many-to-One -X- _ O
( -X- _ O
m2o -X- _ B-MethodName
) -X- _ O
Model -X- _ O
Many-to-one -X- _ O
training -X- _ O
is -X- _ O
standard -X- _ O
for -X- _ O
evaluating -X- _ O
cross-lingual -X- _ O
summarization. -X- _ O
In -X- _ O
these -X- _ O
models -X- _ O
, -X- _ O
the -X- _ O
language -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
can -X- _ O
vary -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
remains -X- _ O
the -X- _ O
same -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
pivot -X- _ O
language. -X- _ O
Instead -X- _ O
of -X- _ O
sampling -X- _ O
all -X- _ O
samples -X- _ O
of -X- _ O
a -X- _ O
batch -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
language -X- _ O
pair -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
8 -X- _ B-HyperparameterValue
minibatches -X- _ B-HyperparameterName
of -X- _ O
32 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
using -X- _ O
a -X- _ O
sampling -X- _ O
factor -X- _ O
of -X- _ O
α -X- _ B-HyperparameterValue
= -X- _ O
0.25 -X- _ B-HyperparameterName
, -X- _ O
the -X- _ O
source -X- _ O
side -X- _ O
of -X- _ O
each -X- _ O
originating -X- _ O
from -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O
Languages -X- _ O
are -X- _ O
ordered -X- _ O
by -X- _ O
the -X- _ O
language -X- _ O
taxonomy -X- _ O
from -X- _ O
Joshi -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
show -X- _ O
better -X- _ O
contrast -X- _ O
between -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
we -X- _ O
color -X- _ O
a -X- _ O
bubble -X- _ O
cyan -X- _ O
if -X- _ O
its -X- _ O
frequency -X- _ O
is -X- _ O
below -X- _ O
500 -X- _ O
( -X- _ O
1218 -X- _ O
pairs -X- _ O
) -X- _ O
, -X- _ O
red -X- _ O
for -X- _ O
500 -X- _ O
to -X- _ O
5000 -X- _ O
( -X- _ O
688 -X- _ O
pairs -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
blue -X- _ O
for -X- _ O
frequencies -X- _ O
exceeding -X- _ O
5000 -X- _ O
( -X- _ O
52 -X- _ O
pairs -X- _ O
) -X- _ O
. -X- _ O

a -X- _ O
single -X- _ O
language -X- _ O
while -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
remains -X- _ O
fixed. -X- _ O
We -X- _ O
then -X- _ O
merge -X- _ O
the -X- _ O
mini-batches -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
batch -X- _ O
and -X- _ O
update -X- _ O
the -X- _ O
model -X- _ O
parameters. -X- _ O
This -X- _ O
is -X- _ O
to -X- _ O
ensure -X- _ O
that -X- _ O
there -X- _ O
are -X- _ O
not -X- _ O
many -X- _ O
duplicates -X- _ O
in -X- _ O
a -X- _ O
single -X- _ O
batch -X- _ O
( -X- _ O
if -X- _ O
all -X- _ O
256 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
of -X- _ O
a -X- _ O
batch -X- _ O
are -X- _ O
sampled -X- _ O
from -X- _ O
a -X- _ O
single -X- _ O
language -X- _ O
pair -X- _ O
, -X- _ O
there -X- _ O
might -X- _ O
be -X- _ O
many -X- _ O
duplicates -X- _ O
as -X- _ O
many -X- _ O
language -X- _ O
pairs -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
256 -X- _ B-HyperparameterValue
training -X- _ B-HyperparameterName
samples -X- _ I-HyperparameterName
) -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
still -X- _ O
benefits -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
low-resource -X- _ O
upsampling -X- _ O
. -X- _ O

C.4 -X- _ O
One-to-many -X- _ O
( -X- _ O
o2m -X- _ B-MethodName
) -X- _ O
Model -X- _ O
o2m -X- _ B-MethodName
models -X- _ O
are -X- _ O
complementary -X- _ O
to -X- _ O
m2o -X- _ B-MethodName
models -X- _ O
: -X- _ O
we -X- _ O
train -X- _ O
them -X- _ O
by -X- _ O
keeping -X- _ O
the -X- _ O
source -X- _ O
language -X- _ O
fixed -X- _ O
and -X- _ O
varying -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
We -X- _ O
upsample -X- _ O
the -X- _ O
low-resource -X- _ O
target -X- _ O
languages -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
sampling -X- _ O
factor -X- _ O
of -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
0.25 -X- _ B-HyperparameterValue
and -X- _ O
merge -X- _ O
8 -X- _ B-HyperparameterValue
mini-batches -X- _ B-HyperparameterName
of -X- _ O
32 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
each -X- _ O
, -X- _ O
analogous -X- _ O
to -X- _ O
m2o -X- _ B-MethodName
models -X- _ O
. -X- _ O

C.5 -X- _ O
Many-to-many -X- _ O
( -X- _ O
m2m -X- _ B-MethodName
) -X- _ O
Multistage -X- _ O
Model -X- _ O
This -X- _ O
is -X- _ O
the -X- _ O
model -X- _ O
obtained -X- _ O
from -X- _ O
the -X- _ O
Algorithm -X- _ O
1. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
standard -X- _ O
language -X- _ O
sampling -X- _ O
( -X- _ O
Conneau -X- _ O
( -X- _ O
marked -X- _ O
red -X- _ O
) -X- _ O
in -X- _ O
many-to-one -X- _ O
models -X- _ O
due -X- _ O
to -X- _ O
implicit -X- _ O
data -X- _ O
leakage. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
split -X- _ O
taking -X- _ O
the -X- _ O
issue -X- _ O
into -X- _ O
account -X- _ O
, -X- _ O
and -X- _ O
consequently -X- _ O
, -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
new -X- _ O
set -X- _ O
( -X- _ O
marked -X- _ O
blue -X- _ O
) -X- _ O
do -X- _ O
not -X- _ O
exhibit -X- _ O
any -X- _ O
unusual -X- _ O
spike -X- _ O
in -X- _ O
ROUGE-2. -X- _ B-MetricName
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
and -X- _ O
then -X- _ O
choose -X- _ O
the -X- _ O
source -X- _ O
based -X- _ O
on -X- _ O
that -X- _ O
decision. -X- _ O
We -X- _ O
use -X- _ O
batch -X- _ O
size -X- _ O
256 -X- _ B-HyperparameterValue
, -X- _ O
8 -X- _ B-HyperparameterValue
mini-batches -X- _ B-HyperparameterName
with -X- _ O
size -X- _ B-HyperparameterName
32 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
0.5 -X- _ B-HyperparameterValue
, -X- _ O
β -X- _ B-HyperparameterName
= -X- _ O
0.75 -X- _ B-HyperparameterValue
. -X- _ O

C.6 -X- _ O
Many-to-many -X- _ O
( -X- _ O
m2m -X- _ B-MethodName
) -X- _ O
Unistage -X- _ O
Model -X- _ O
This -X- _ O
algorithm -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
standard -X- _ O
language -X- _ O
sampling -X- _ O
, -X- _ O
the -X- _ O
difference -X- _ O
being -X- _ O
that -X- _ O
languages -X- _ O
are -X- _ O
sampled -X- _ O
as -X- _ O
pairs -X- _ O
from -X- _ O
all -X- _ O
possible -X- _ O
combinations. -X- _ O
Instead -X- _ O
of -X- _ O
sampling -X- _ O
one -X- _ O
language -X- _ O
pair -X- _ O
at -X- _ O
each -X- _ O
training -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
8 -X- _ B-HyperparameterValue
pairs -X- _ B-HyperparameterName
, -X- _ O
one -X- _ O
for -X- _ O
each -X- _ O
mini-batch -X- _ B-HyperparameterName
of -X- _ O
size -X- _ O
32. -X- _ B-HyperparameterValue
We -X- _ O
then -X- _ O
merge -X- _ O
the -X- _ O
mini-batches -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
batch -X- _ O
of -X- _ O
256 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
before -X- _ O
updating -X- _ O
the -X- _ O
model -X- _ O
parameters. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
sampling -X- _ O
factor -X- _ O
of -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
0.25. -X- _ B-HyperparameterValue
In -X- _ O
all -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
discarded -X- _ O
a -X- _ O
language -X- _ O
pair -X- _ O
from -X- _ O
training -X- _ O
if -X- _ O
it -X- _ O
had -X- _ O
fewer -X- _ O
than -X- _ O
30 -X- _ B-HyperparameterValue
training -X- _ B-HyperparameterName
samples -X- _ I-HyperparameterName
to -X- _ O
prevent -X- _ O
too -X- _ O
many -X- _ O
duplicates -X- _ O
in -X- _ O
a -X- _ O
mini-batch. -X- _ O
The -X- _ O
training -X- _ O
was -X- _ O
done -X- _ O
together -X- _ O
with -X- _ O
the -X- _ O
in-language -X- _ O
samples -X- _ O
. -X- _ O

D.1 -X- _ O
Training -X- _ O
Setups -X- _ O
Fine-tuning -X- _ O
generation -X- _ O
models -X- _ O
is -X- _ O
computeintensive -X- _ O
, -X- _ O
and -X- _ O
due -X- _ O
to -X- _ O
computational -X- _ O
limitations -X- _ O
, -X- _ O
we -X- _ O
fine-tune -X- _ O
all -X- _ O
pretrained -X- _ O
models -X- _ O
for -X- _ O
25k -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
with -X- _ O
an -X- _ O
effective -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
256 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
roughly -X- _ O
takes -X- _ O
about -X- _ O
three -X- _ O
days -X- _ O
on -X- _ O
a -X- _ O
4-GPU -X- _ O
NVIDIA -X- _ O
P100 -X- _ O
server. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
base -X- _ O
variant -X- _ O
of -X- _ O
mT5 -X- _ B-MethodName
, -X- _ O
having -X- _ O
250k -X- _ B-HyperparameterValue
vocabulary -X- _ B-HyperparameterName
, -X- _ O
768 -X- _ B-HyperparameterValue
embedding -X- _ B-HyperparameterName
and -X- _ I-HyperparameterName
dimension -X- _ I-HyperparameterName
size -X- _ I-HyperparameterName
, -X- _ O
12 -X- _ B-HyperparameterValue
attention -X- _ B-HyperparameterName
heads -X- _ I-HyperparameterName
, -X- _ O
and -X- _ O
2048 -X- _ B-HyperparameterValue
FFN -X- _ B-HyperparameterName
size -X- _ O
, -X- _ O
with -X- _ O
580M -X- _ B-HyperparameterValue
parameters. -X- _ B-HyperparameterName
We -X- _ O
limit -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
512 -X- _ B-HyperparameterValue
and -X- _ O
output -X- _ O
to -X- _ O
84 -X- _ B-HyperparameterValue
tokens. -X- _ B-HyperparameterName
All -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
respective -X- _ O
subsets -X- _ O
of -X- _ O
the -X- _ O
CrossSum -X- _ B-DatasetName
training -X- _ O
set -X- _ O
. -X- _ O

D.2 -X- _ O
Inference -X- _ O
During -X- _ O
inference -X- _ O
, -X- _ O
we -X- _ O
jump-start -X- _ O
the -X- _ O
decoder -X- _ O
with -X- _ O
language-specific -X- _ O
BOS -X- _ O
( -X- _ O
beginning -X- _ O
of -X- _ O
sequence -X- _ O
) -X- _ O
tokens -X- _ O
( -X- _ O
Johnson -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
at -X- _ O
the -X- _ O
first -X- _ O
decoding -X- _ O
step -X- _ O
for -X- _ O
guiding -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
generate -X- _ O
summaries -X- _ O
in -X- _ O
the -X- _ O
intended -X- _ O
target -X- _ O
language. -X- _ O
We -X- _ O
use -X- _ O
beam -X- _ B-MethodName
search -X- _ I-MethodName
( -X- _ O
Medress -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
1977 -X- _ O
) -X- _ O
with -X- _ O
the -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
4 -X- _ B-HyperparameterValue
and -X- _ O
use -X- _ O
a -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
( -X- _ O
Wu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
of -X- _ O
0.6 -X- _ B-HyperparameterValue
. -X- _ O

E -X- _ O
Ablation -X- _ O
Studies -X- _ O
We -X- _ O
make -X- _ O
several -X- _ O
design -X- _ O
choices -X- _ O
in -X- _ O
the -X- _ O
multistage -X- _ O
sampling -X- _ O
algorithm. -X- _ O
We -X- _ O
break -X- _ O
them -X- _ O
into -X- _ O
two -X- _ O
main -X- _ O
decisions -X- _ O
: -X- _ O

1. -X- _ O
Making -X- _ O
mini-batches -X- _ O
and -X- _ O
sampling -X- _ O
the -X- _ O
language -X- _ O
pair -X- _ O
for -X- _ O
each -X- _ O
mini-batch -X- _ O
. -X- _ O

2. -X- _ O
Keeping -X- _ O
either -X- _ O
the -X- _ O
source -X- _ O
or -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
fixed -X- _ O
for -X- _ O
each -X- _ O
batch -X- _ O
. -X- _ O

To -X- _ O
verify -X- _ O
that -X- _ O
these -X- _ O
choices -X- _ O
indeed -X- _ O
affect -X- _ O
performance -X- _ O
positively -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
five -X- _ O
different -X- _ O
models -X- _ O
for -X- _ O
ablation -X- _ O
: -X- _ O

1. -X- _ O
Sampling -X- _ O
the -X- _ O
language -X- _ O
pair -X- _ O
in -X- _ O
mini-batches -X- _ O
in -X- _ O
one -X- _ O
stage -X- _ O
only -X- _ O
and -X- _ O
then -X- _ O
merging -X- _ O
them -X- _ O
into -X- _ O
large -X- _ O
batches -X- _ O
before -X- _ O
updating -X- _ O
model -X- _ O
parameters -X- _ O
: -X- _ O
m2m-unistage -X- _ B-MethodName
. -X- _ O

2. -X- _ O
Sampling -X- _ O
the -X- _ O
language -X- _ O
pair -X- _ O
with -X- _ O
large -X- _ O
batches -X- _ O
of -X- _ O
256 -X- _ B-HyperparameterValue
samples -X- _ B-HyperparameterName
without -X- _ O
mini-batching -X- _ O
: -X- _ O
m2mlarge -X- _ B-MethodName
. -X- _ O

3. -X- _ O
Multistage -X- _ O
sampling -X- _ O
keeping -X- _ O
only -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
fixed -X- _ O
in -X- _ O
a -X- _ O
batch -X- _ O
: -X- _ O
m2m-tgt -X- _ B-MethodName
[ -X- _ O
our -X- _ O
proposed -X- _ O
model -X- _ O
] -X- _ O
. -X- _ O

4. -X- _ O
Multistage -X- _ O
sampling -X- _ O
keeping -X- _ O
only -X- _ O
the -X- _ O
source -X- _ O
language -X- _ O
fixed -X- _ O
in -X- _ O
a -X- _ O
batch -X- _ O
: -X- _ O
m2m-src -X- _ B-MethodName
; -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
complement -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
model -X- _ O
. -X- _ O

5. -X- _ O
Multistage -X- _ O
sampling -X- _ O
keeping -X- _ O
either -X- _ O
the -X- _ O
source -X- _ O
or -X- _ O
the -X- _ O
target -X- _ O
language -X- _ O
fixed -X- _ O
( -X- _ O
with -X- _ O
equal -X- _ O
probability -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
batch -X- _ O
: -X- _ O
m2m-src-tgt -X- _ B-MethodName
. -X- _ I-MethodName

We -X- _ O
benchmark -X- _ O
on -X- _ O
all -X- _ O
the -X- _ O
language -X- _ O
pairs -X- _ O
done -X- _ O
previously -X- _ O
and -X- _ O
show -X- _ O
the -X- _ O
mean -X- _ O
ROUGE-2 -X- _ B-MetricName
and -X- _ O
LaSE -X- _ B-MetricName
scores -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
. -X- _ O

Model -X- _ O
Scores -X- _ O
Significance -X- _ O
As -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
from -X- _ O
the -X- _ O
table -X- _ O
, -X- _ O
m2m-large -X- _ B-MethodName
, -X- _ I-MethodName
the -X- _ O
standard -X- _ O
m2m -X- _ B-MethodName
model -X- _ O
, -X- _ O
has -X- _ O
the -X- _ O
best -X- _ O
average -X- _ O
ROUGE-2 -X- _ B-MetricName
/ -X- _ O
LaSE -X- _ B-MetricName
scores -X- _ O
among -X- _ O
all -X- _ O
m2m -X- _ B-MethodName
variants. -X- _ O
This -X- _ O
begs -X- _ O
the -X- _ O
question -X- _ O
of -X- _ O
whether -X- _ O
our -X- _ O
proposed -X- _ O
multistage -X- _ O
sampling -X- _ O
is -X- _ O
, -X- _ O
after -X- _ O
all -X- _ O
, -X- _ O
needed -X- _ O
or -X- _ O
not. -X- _ O
But -X- _ O
the -X- _ O
scores -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
m2m-tgt -X- _ B-MethodName
model -X- _ O
do -X- _ O
not -X- _ O
fall -X- _ O
much -X- _ O
below. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
statistical -X- _ O
significance -X- _ O
test -X- _ O
results -X- _ O
of -X- _ O
all -X- _ O
m2m -X- _ B-MethodName
models -X- _ O
, -X- _ O
comparing -X- _ O
them -X- _ O
against -X- _ O
m2o -X- _ B-MethodName
, -X- _ O
o2m -X- _ B-MethodName
, -X- _ O
and -X- _ O
s.+t. -X- _ O
in -X- _ O
one -X- _ O
vs. -X- _ O
all -X- _ O
manner -X- _ O
. -X- _ O

Significance -X- _ O
results -X- _ O
paint -X- _ O
a -X- _ O
different -X- _ O
picture -X- _ O
: -X- _ O
m2m-tgt -X- _ B-MethodName
triumphs -X- _ O
over -X- _ O
all -X- _ O
other -X- _ O
models -X- _ O
, -X- _ O
getting -X- _ O
significantly -X- _ O
better -X- _ O
results -X- _ O
on -X- _ O
42 -X- _ O
% -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
more -X- _ O
than -X- _ O
double -X- _ O
the -X- _ O
m2m-large -X- _ B-MethodName
model. -X- _ O
We -X- _ O
inspected -X- _ O
the -X- _ O
results -X- _ O
individually -X- _ O
and -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
results -X- _ O
are -X- _ O
notably -X- _ O
better -X- _ O
on -X- _ O
language -X- _ O
pairs -X- _ O
that -X- _ O
are -X- _ O
not -X- _ O
adequately -X- _ O
represented -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
set. -X- _ O
m2mtgt -X- _ B-MethodName
performs -X- _ O
comparatively -X- _ O
worse -X- _ O
on -X- _ O
high-resource -X- _ O
language -X- _ O
pairs -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
think -X- _ O
is -X- _ O
a -X- _ O
fair -X- _ O
compromise -X- _ O
to -X- _ O
uplift -X- _ O
low-resource -X- _ O
ones. -X- _ O
As -X- _ O
m2m-large -X- _ B-MethodName
can -X- _ O
sample -X- _ O
a -X- _ O
pair -X- _ O
only -X- _ O
once -X- _ O
per -X- _ O
batch -X- _ O
, -X- _ O
it -X- _ O
fails -X- _ O
to -X- _ O
incorporate -X- _ O
many -X- _ O
language -X- _ O
pairs -X- _ O
due -X- _ O
to -X- _ O
them -X- _ O
having -X- _ O
insufficient -X- _ O
participation -X- _ O
during -X- _ O
training. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
our -X- _ O
proposed -X- _ O
multistage -X- _ O
sampling -X- _ O
algorithm -X- _ O
performs -X- _ O
well -X- _ O
in -X- _ O
this -X- _ O
regard -X- _ O
by -X- _ O
sampling -X- _ O
in -X- _ O
two -X- _ O
stages -X- _ O
. -X- _ O

While -X- _ O
m2m-tgt -X- _ B-MethodName
outperforms -X- _ O
all -X- _ O
the -X- _ O
rest -X- _ O
, -X- _ O
m2msrc -X- _ B-MethodName
falls -X- _ O
behind -X- _ O
all -X- _ O
other -X- _ O
models -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin. -X- _ O
This -X- _ O
phenomenon -X- _ O
also -X- _ O
has -X- _ O
the -X- _ O
same -X- _ O
trend -X- _ O
as -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Section -X- _ O
5 -X- _ O
, -X- _ O
where -X- _ O
o2m -X- _ B-MethodName
models -X- _ O
failed -X- _ O
at -X- _ O
generating -X- _ O
cross-lingual -X- _ O
summaries. -X- _ O
This -X- _ O
is -X- _ O
also -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
our -X- _ O
hypothesis -X- _ O
made -X- _ O
, -X- _ O
as -X- _ O
m2m-src -X- _ B-MethodName
and -X- _ O
m2mtgt -X- _ B-MethodName
mimic -X- _ O
the -X- _ O
training -X- _ O
settings -X- _ O
of -X- _ O
the -X- _ O
o2m -X- _ B-MethodName
and -X- _ O
m2o -X- _ B-MethodName
models -X- _ O
, -X- _ O
respectively -X- _ O
, -X- _ O
at -X- _ O
the -X- _ O
batch -X- _ O
level. -X- _ O
The -X- _ O
m2msrc-tgt -X- _ B-MethodName
is -X- _ O
the -X- _ O
middle -X- _ O
ground -X- _ O
between -X- _ O
m2m-src -X- _ B-MethodName
and -X- _ O
m2m-tgt -X- _ B-MethodName
and -X- _ O
, -X- _ O
likewise -X- _ O
, -X- _ O
scores -X- _ O
between -X- _ O
these -X- _ O
two. -X- _ O
In -X- _ O
our -X- _ O
opinion -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
dynamics -X- _ O
between -X- _ O
the -X- _ O
m2o -X- _ B-MethodName
( -X- _ O
m2m-tgt -X- _ B-MethodName
) -X- _ O
and -X- _ O
o2m -X- _ B-MethodName
( -X- _ O
m2m-src -X- _ B-MethodName
) -X- _ O
models -X- _ O
is -X- _ O
an -X- _ O
interesting -X- _ O
finding -X- _ O
and -X- _ O
should -X- _ O
be -X- _ O
studied -X- _ O
in -X- _ O
depth -X- _ O
as -X- _ O
a -X- _ O
new -X- _ O
research -X- _ O
direction -X- _ O
in -X- _ O
future -X- _ O
works. -X- _ O
B2. -X- _ O
Did -X- _ O
you -X- _ O
discuss -X- _ O
the -X- _ O
license -X- _ O
or -X- _ O
terms -X- _ O
for -X- _ O
use -X- _ O
and -X- _ O
/ -X- _ O
or -X- _ O
distribution -X- _ O
of -X- _ O
any -X- _ O
artifacts -X- _ O
? -X- _ O

Multilingual B-TaskName
Relation I-TaskName
Classification I-TaskName
via O
Efficient O
and O
Effective O
Prompting O
Prompting O
pre-trained O
language O
models O
has O
achieved O
impressive O
performance O
on O
various O
NLP O
tasks, O
especially O
in O
low O
data O
regimes. O
Despite O
the O
success O
of O
prompting O
in O
monolingual O
settings, O
applying O
prompt-based O
methods O
in O
multilingual O
scenarios O
has O
been O
limited O
to O
a O
narrow O
set O
of O
tasks, O
due O
to O

the O
high O
cost O
of O
handcrafting O
multilingual O
prompts. O
In O
this O
paper, O
we O
present O
the O
first O
work O
on O
prompt-based O
multilingual B-TaskName
relation I-TaskName
classification I-TaskName
( O
RC B-TaskName
), O
by O
introducing O
an O
efficient O
and O
effective O
method O
that O
constructs O
prompts O
from O
relation O
triples O
and O
involves O
only O
minimal O
translation O
for O
the O
class O
labels. O
We O
evaluate O
its O
performance O
in O

fully O
supervised, O
few-shot O
and O
zero-shot O
scenarios, O
and O
analyze O
its O
effectiveness O
across O
14 O
languages, O
prompt O
variants, O
and O
English-task O
training O
in O
cross-lingual O
settings. O
We O
find O
that O
in O
both O
fully O
supervised O
and O
few-shot O
scenarios, O
our O
prompt O
method O
beats O
competitive O
baselines: O
fine-tuning O
XLM-R B-MethodName
EM I-MethodName
and O
null O
prompts. O
It O
also O
outperforms O
the O
random O
baseline O
by O

a O
large O
margin O
in O
zero-shot O
experiments. O
Our O
method O
requires O
little O
in-language O
knowledge O
and O
can O
be O
used O
as O
a O
strong O
baseline O
for O
similar O
multilingual O
classification O
tasks. O
Introduction O
Relation B-TaskName
classification I-TaskName
( O
RC B-TaskName
) O
is O
a O
crucial O
task O
in O
information O
extraction O
(IE), O
aiming O
to O
identify O
the O
relation O
between O
entities O
in O
a O
text O
(Alt O

et O
al., O
2019). O
Extending O
RC B-TaskName
to O
multilingual O
settings O
has O
recently O
received O
increased O
interest O
(Zou O
et O
al., O
2018;Kolluru O
et O
al., O
2022), O
but O
the O
majority O
of O
prior O
work O
still O
focuses O
on O
English O
(Baldini O
Soares O
et O
al., O
2019;Lyu O
and O
Chen, O
2021). O
A O
main O
bottleneck O
for O
multilingual B-TaskName
RC I-TaskName
is O
the O
lack O
of O
supervised O
resources, O

comparable O
in O
size O
to O
large O
English O
datasets O
(Riedel O
et O
al., O
2010;Zhang O
et O
al., O
2017). O
The O
SMiLER B-DatasetName
dataset O
(Seganti O
et O
al., O
2021) O
provides O
a O
starting O
point O
to O
test O
fully O
supervised O
and O
more O
efficient O
approaches O
due O
to O
different O
resource O
availability O
for O
different O
languages. O
Previous O
studies O
have O
shown O
the O
promising O
performance O
of O
prompting O

PLMs O
compared O
to O
the O
datahungry O
fine-tuning, O
especially O
in O
low-resource O
scenarios O
(Gao O
et O
al., O
2021;Le O
Scao O
and O
Rush, O
2021;. O
Multilingual O
pre-trained O
language O
models O
(Conneau O
et O
al., O
2020;Xue O
et O
al., O
2021) O
further O
enable O
multiple O
languages O
to O
be O
represented O
in O
a O
shared O
semantic O
space, O
thus O
making O
prompting O
in O
multilingual O
scenarios O
feasible. O
However, O
the O

study O
of O
prompting O
for O
multilingual O
tasks O
so O
far O
remains O
limited O
to O
a O
small O
range O
of O
tasks O
such O
as O
text O
classification O
(Winata O
et O
al., O
2021) O
and O
natural O
language O
inference O
(Lin O
et O
al., O
2022). O
To O
our O
knowledge, O
the O
effectiveness O
of O
prompt-based O
methods O
for O
multilingual B-TaskName
RC I-TaskName
is O
still O
unexplored. O
To O
analyse O
this O
gap, O

we O
pose O
two O
research O
questions O
for O
multilingual B-TaskName
RC I-TaskName
with O
prompts: O
RQ1. O
What O
is O
the O
most O
effective O
way O
to O
prompt? O
We O
investigate O
whether O
prompting O
should O
be O
done O
in O
English O
or O
the O
target O
language O
and O
whether O
to O
use O
soft O
prompt O
tokens. O
RQ2. O
How O
well O
do O
prompts O
perform O
in O
different O
data O
regimes O
and O

languages? O
We O
investigate O
the O
effectiveness O
of O
our O
prompting O
approach O
in O
three O
scenarios: O
fully O
supervised, O
few-shot O
and O
zero-shot. O
We O
explore O
to O
what O
extent O
the O
results O
are O
related O
to O
the O
available O
language O
resources. O
We O
present O
an O
efficient O
and O
effective O
prompt O
method O
for O
multilingual B-TaskName
RC I-TaskName
(see O
Figure O
1) O
that O
derives O
prompts O
from O
relation O

triplets O
(see O
Section O
3.1). O
The O
derived O
prompts O
include O
the O
original O
sentence O
and O
entities O
and O
are O
supposed O
to O
be O
filled O
with O
the O
relation O
label. O
We O
evaluate O
the O
prompts O
with O
three O
variants, O
two O
of O
which O
require O
no O
translation, O
and O
one O
of O
which O
requires O
minimal O
translation, O
i.e., O
of O
the O
relation O
labels O
only. O
We O

find O
that O
our O
method O
outperforms O
fine-tuning O
and O
a O
strong O
taskagnostic O
prompt O
baseline O
in O
fully O
supervised O
and O
few-shot O
scenarios, O
especially O
for O
relatively O
lowresource O
languages. O
Our O
method O
also O
improves O
over O
the O
random O
baseline O
in O
zero-shot O
settings, O
and O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
is O
directed O
by O
Cameron O
. O
Titanic O
_______ O
Cameron O

. O
Goethe O
schrieb O
die O
Tragödie O
Faust O
. O
Relation B-TaskName
Classification I-TaskName
Task O
Definition O
Relation B-TaskName
classification I-TaskName
is O
the O
task O
of O
classifying O
the O
relationship O
such O
as O
date_of_birth, O
founded_by O
or O
parents O
between O
pairs O
of O
entities O
in O
a O
given O
context. O
Formally, O
given O
a O
relation O
set O
R O
and O
a O
text O
x O
= O
[x O
1 O
, O
x O
2 O

, O
. O
. O
. O
, O
x O
n O
] O
(where O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
are O
tokens) O
with O
two O
disjoint O
spans O
e O
h O
and O
e O
t O
denoting O
the O
head O
and O
tail O
entity, O
RC B-TaskName
aims O
to O
predict O
the O
relation O
r O
∈ O
R O
between O
e O
h O
and O
e O
t O

, O
or O
give O
a O
no_relation O
prediction O
if O
no O
relation O
in O
R O
holds. O
RC B-TaskName
is O
a O
multilingual O
task O
if O
the O
token O
sequences O
come O
from O
different O
languages. O
Fine-tuning O
for O
Relation B-TaskName
Classification I-TaskName
In O
fine-tuning, O
a O
task-specific O
linear O
classifier O
is O
added O
on O
top O
of O
the O
PLM. O
Fine-tuning O
hence O
introduces O
a O
different O
scenario O
from O
pre-training, O

since O
language O
model O
(LM) O
pre-training O
is O
usually O
formalized O
as O
a O
cloze-style O
task O
to O
predict O
target O
tokens O
at O
[MASK] O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019) O
or O
a O
corrupted O
span O
(Raffel O
et O
al., O
2020;Lewis O
et O
al., O
2020). O
For O
the O
RC B-TaskName
task, O
the O
classifier O
aims O
to O
predict O
the O
target O
class O
r O
at O

[CLS] O
or O
at O
the O
entity O
spans O
denoted O
by O
MARKER O
(Baldini O
Soares O
et O
al., O
2019). O
Prompting O
for O
Relation B-TaskName
Classification I-TaskName
Prompting O
is O
proposed O
to O
bridge O
the O
gap O
between O
pre-training O
and O
fine-tuning O
. O
The O
essence O
of O
prompting O
is, O
by O
appending O
extra O
text O
to O
the O
original O
text O
according O
to O
a O
task-specific O
template O
T O
(•), O

to O
reformulate O
the O
downstream O
task O
to O
an O
LM O
pre-training O
task O
such O
as O
masked O
language O
modeling O
(MLM), O
and O
apply O
the O
same O
training O
objective O
during O
the O
task-specific O
training. O
For O
the O
RC B-TaskName
task, O
to O
identify O
the O
relation O
between O
"Angela O
Merkel" O
and O
"Joachim O
Sauer" O
in O
the O
text O
"Angela O
Merkel's O
current O
husband O
is O
quantum O
chemist O

Joachim O
Sauer," O
an O
intuitive O
template O
for O
prompting O
can O
be O
"The O
relation O
between O
Angela O
Merkel O
and O
Joachim O
Sauer O
is O
[MASK]," O
and O
the O
LM O
is O
supposed O
to O
assign O
a O
higher O
likelihood O
to O
the O
term O
couple O
than O
to O
e.g. O
friends O
or O
colleagues O
at O
[MASK]. O
This O
"fill-in O
the O
blank" O
paradigm O
is O
well O
aligned O
with O

the O
pre-training O
scenario, O
and O
enables O
prompting O
to O
better O
coax O
the O
PLMs O
for O
pre-trained O
knowledge O
(Petroni O
et O
al., O
2019). O
Methods O
We O
now O
present O
our O
method, O
as O
shown O
in O
Figure O
1. O
We O
introduce O
its O
template O
and O
verbalizer, O
and O
propose O
several O
variants O
of O
the O
prompt. O
Lastly, O
we O
explain O
the O
training O
and O
inference O
process. O

Template O
For O
prompting O
, O
a O
prompt O
often O
consists O
of O
a O
template O
T O
(•) O
and O
a O
verbalizer O
V. O
Given O
a O
plain O
text O
x, O
the O
template O
T O
adds O
taskrelated O
instruction O
to O
x O
to O
yield O
the O
prompt O
input O
x O
prompt O
= O
T O
(x).(1) O
Following O
and O
Han O
et O
al. O
(2021), O
we O
treat O
relations O
as O

predicates O
and O
use O
the O
cloze O
"e O
h O
{relation} O
e O
t O
" O
for O
the O
LM O
to O
fill O
in. O
Our O
template O
is O
formulated O
as O
T O
(x) O
:= O
"x. O
e O
h O
____ O
e O
t O
". O
(2) O
In O
the O
template O
T O
(x), O
x O
is O
the O
original O
text O
and O
the O
two O
entities O
e O
h O
and O

e O
t O
come O
from O
x. O
Therefore, O
our O
template O
does O
not O
introduce O
extra O
tokens, O
thus O
involves O
no O
translation O
at O
all. O
Verbalizer O
After O
being O
prompted O
by O
x O
prompt O
, O
the O
PLM O
M O
predicts O
the O
masked O
text O
y O
at O
the O
blank. O
To O
complete O
an O
NLP O
classification O
task, O
a O
verbalizer O
ϕ O
is O
required O
to O

bridge O
the O
set O
of O
labels O
Y O
and O
the O
set O
of O
predicted O
texts O
(verbalizations O
V). O
For O
the O
simplicity O
of O
our O
prompt, O
we O
use O
the O
one-to-one O
verbalizer: O
ϕ O
: O
Y O
→ O
V, O
r O
→ O
ϕ(r),(3) O
where O
r O
is O
a O
relation, O
and O
ϕ(r) O
is O
the O
simple O
verbalization O
of O
r. O
ϕ(•) O
normally O
only O
involves O

splitting O
r O
by O
"-" O
or O
"_" O
and O
replacing O
abbreviations O
such O
as O
org O
with O
organization. O
E.g., O
the O
relation O
org-has-member O
corresponds O
to O
the O
verbalization O
"organization O
has O
member". O
Then O
the O
prediction O
is O
formalized O
as O
p(r|x) O
∝ O
p(y O
= O
ϕ(r)|x O
prompt O
; O
θ O
M O
), O
(4 O
) O
where O
θ O
M O
denotes O
the O
parameters O
of O

model O
M. O
p(r|x) O
is O
normalized O
by O
the O
likelihood O
sum O
over O
all O
relations. O
Variants O
To O
find O
the O
optimal O
way O
to O
prompt, O
we O
investigate O
three O
variants O
as O
follows. O
Hard O
prompt O
vs O
soft O
prompt O
(SP) O
Hard O
prompts O
(a.k.a. O
discrete O
prompts) O
are O
entirely O
formulated O
in O
natural O
language. O
Soft O
prompts O
(a.k.a. O
continuous O
prompts) O
consist O
of O

learnable O
tokens O
(Lester O
et O
al., O
2021) O
that O
are O
not O
contained O
in O
the O
PLM O
vocabulary. O
Following O
Han O
et O
al. O
(2021), O
we O
insert O
soft O
tokens O
before O
entities O
and O
blanks O
as O
shown O
for O
SP O
in O
Table O
1. O
Code-switch O
(CS) O
vs O
in-language O
(IL) O
Relation O
labels O
are O
in O
English O
across O
almost O
all O
RC O
datasets. O
Given O

a O
text O
from O
a O
non-English O
input O
L O
with O
a O
blank, O
the O
recovered O
text O
is O
code-mixed O
after O
being O
completed O
with O
an O
English O
verbalization, O
corresponding O
to O
code-switch O
prompting. O
It O
is O
probably O
more O
reasonable O
for O
the O
PLM O
to O
fill O
in O
the O
blank O
in O
language O
L. O
Inspired O
by O
Lin O
et O
al. O
(2022) O
(Hendrickx O
et O

al., O
2010) O
10 O
cause O
effect, O
entity O
origin, O
product O
producer, O
... O
2.50 O
0.81 O
NYT O
(Riedel O
et O
al., O
2010) O
24 O
ethnicity, O
major O
shareholder O
of, O
religion, O
... O
show O
that O
the O
label O
space O
of O
the O
RC B-TaskName
task O
is O
more O
complex O
than O
most O
few-class O
classification O
tasks. O
The O
verbalizations O
of O
RC O
datasets O
are O
listed O
in O
Appendix O

B. O
For O
SemEval B-DatasetName
, O
the O
two O
possible O
directions O
of O
a O
relation O
are O
combined. O
For O
NYT B-DatasetName
, O
we O
use O
the O
version O
from O
Zeng O
et O
al. O
(2018). O
For O
SMiLER B-DatasetName
, O
"EN" O
is O
the O
English O
split; O
"ALL" O
contains O
all O
data O
from O
14 O
languages. O
Table O
1 O
visualizes O
both O
code-switch O
(CS) O
and O
inlanguage O
(IL) O
prompting. O

For O
English, O
CS-and O
ILprompting O
are O
equivalent, O
since O
L O
is O
English O
itself. O
Word O
order O
of O
prompting O
For O
the O
RC O
task, O
head-relation-tail O
triples O
involve O
three O
elements. O
Therefore, O
deriving O
natural O
language O
prompts O
from O
them O
requires O
handling O
where O
to O
put O
the O
predicate O
(relation). O
In O
the O
case O
of O
SOV O
languages, O
filling O
in O
a O
relation O
that O

occurs O
between O
e O
h O
and O
e O
t O
seems O
less O
intuitive. O
Therefore, O
to O
investigate O
if O
the O
word O
order O
of O
prompting O
affects O
prediction O
accuracy, O
we O
swap O
the O
entities O
and O
the O
blank O
in O
the O
SVOtemplate O
"x. O
e O
h O
____ O
e O
t O
" O
and O
get O
"x. O
e O
h O
e O
t O
____" O
as O
the O
SOV-template. O

Training O
and O
Inference O
The O
training O
and O
inference O
setups O
depend O
on O
the O
employed O
model. O
Prompting O
autoencoding O
language O
models O
requires O
the O
verbalizations O
to O
be O
of O
fixed O
length, O
since O
the O
length O
of O
masks, O
which O
is O
identical O
with O
verbalization O
length, O
is O
unknown O
during O
inference. O
Encoder-decoders O
can O
handle O
verbalizations O
of O
varying O
length O
by O
nature O
. O

Han O
et O
al. O
(2021) O
adjust O
all O
the O
verbalizations O
in O
TACRED B-DatasetName
to O
a O
length O
of O
3, O
to O
enable O
prompting O
with O
RoBERTa B-MethodName
for O
RC B-TaskName
. O
We O
argue O
that O
for O
multilingual B-TaskName
RC I-TaskName
, O
this O
fix O
is O
largely O
infeasible, O
because: O
(1) O
in O
case O
of O
in-language O
prompting O
on O
SMiLER B-DatasetName
, O
the O
variance B-MetricName
of O
the O
length O

of O
the O
verbalizations O
increases O
from O
0.68 B-HyperparameterValue
to O
1.44 B-HyperparameterValue
after O
translation O
(see O
Table O
2), O
and O
surpasses O
most O
of O
listed O
monolingual O
RC O
datasets O
( O
SemEval B-DatasetName
, O
NYT B-DatasetName
and O
SCIERC B-DatasetName
), O
making O
it O
harder O
to O
unify O
the O
length; O
(2) O
manually O
adjusting O
the O
translated O
prompts O
requires O
manual O
effort O
per O
target O
language, O
making O
it O
much O

more O
expensive O
than O
adjusting O
only O
English O
verbalizations. O
Therefore, O
we O
use O
an O
encoder-decoder O
PLM O
for O
prompting O
Song O
et O
al., O
2022). O
Training O
objective O
For O
an O
encoder-decoder O
PLM O
M, O
given O
the O
prompt O
input O
T O
(x) O
and O
the O
target O
sequence O
ϕ(r) O
(i.e. O
label O
verbalization), O
we O
denote O
the O
output O
sequence O
as O
y. O
The O
probability O
of O

an O
exact-match O
decoding O
is O
calculated O
as O
follows: O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
,(5) O
where O
y O
t O
, O
ϕ O
t O
(r) O
denote O
the O
t-th O
token O
of O
y O
and O
ϕ(r), O
respectively. O
y O
<t O
denotes O
the O
decoded O
sequence O
on O
the O
left. O
θ O
represents O
the O

set O
of O
all O
the O
learnable O
parameters, O
including O
those O
of O
the O
PLM O
θ O
M O
, O
and O
those O
of O
the O
soft O
tokens O
θ O
sp O
in O
case O
of O
variant O
"soft O
prompt". O
Hence, O
the O
final O
objective O
over O
the O
training O
set O
X O
is O
to O
minimize O
the O
negative O
loglikelihood: O
argmin O
θ O
− O
1 O
|X O
| O
x∈X O

|ϕ(r)| O
t=1 O
log O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)|y O
<t O
, O
T O
(x)) O
.(6) O
Inference O
We O
collect O
the O
output O
logits O
of O
the O
decoder, O
L O
∈ O
R O
|V O
|×L O
, O
where O
|V O
| O
is O
the O
vocabulary O
size O
of O
M, O
and O
L O
is O
the O
maximum O
decode O
length. O
For O
each O
relation O

r O
∈ O
R, O
its O
score O
is O
given O
by O
: O
where O
we O
compute O
P O
by O
looking O
up O
in O
the O
t-th O
column O
of O
L O
and O
applying O
softmax O
at O
each O
time O
step O
t. O
We O
aggregate O
P O
by O
addition O
to O
encourage O
partial O
matches O
as O
well, O
instead O
of O
enforcing O
exact O
matches. O
The O
score O
is O
normalized O

by O
the O
length O
of O
verbalization O
in O
order O
to O
avoid O
predictions O
favoring O
longer O
relations. O
Finally, O
we O
select O
the O
relation O
with O
the O
highest O
score O
as O
prediction. O
score O
θ O
(r) O
:= O
1 O
|ϕ(r)| O
|ϕ(r)| O
t=1 O
P O
θ O
(y O
t O
= O
ϕ O
t O
(r)), O
(7) O
Experiments O
We O
implement O
our O
experiments O
using O
the O
Hugging O
Face O

Transformers O
library O
(Wolf O
et O
al., O
2020), O
Hydra O
(Yadan, O
2019) O
and O
PyTorch O
(Paszke O
et O
al., O
2019). O
2 O
We O
use O
micro-F1 B-MetricName
as O
the O
evaluation O
metric, O
as O
the O
SMiLER B-DatasetName
paper O
(Seganti O
et O
al., O
2021) O
suggests. O
To O
measure O
the O
overall O
performance O
over O
multiple O
languages, O
we O
report O
the O
macro B-MetricName
average I-MetricName
across O
languages, O
following O
Zhao O
and O

Schütze O
(2021) O
and O
Lin O
et O
al. O
(2022). O
We O
also O
group O
the O
languages O
by O
their O
available O
resources O
in O
both O
pretraining O
and O
fine-tuning O
datasets O
for O
additional O
aggregate O
results. O
Details O
of O
the O
dataset, O
the O
models, O
and O
the O
experimental O
setups O
are O
as O
follows. O
Further O
experimental O
details O
are O
listed O
in O
Appendix O
A. O
Dataset O
We O
conduct O

an O
experimental O
evaluation O
of O
our O
multilingual O
prompt O
methods O
on O
the O
SMiLER B-DatasetName
(Seganti O
2 O
We O
make O
our O
code O
publicly O
available O
at O
https://github. O
com/DFKI-NLP/meffi-prompt O
for O
better O
reproducibility. O
Grouping O
of O
the O
languages O
We O
visualize O
the O
languages O
in O
Figure O
2 O
based O
on O
the O
sizes O
of O
RC B-TaskName
training O
data, O
but O
include O
the O
pre-training O
data O
as O

well, O
to O
give O
a O
more O
comprehensive O
overview O
of O
the O
availability O
of O
resources O
for O
each O
language. O
We O
divide O
the O
14 O
languages O
into O
4 O
groups, O
according O
to O
the O
detectable O
clusters O
in O
Figure O
2 O
and O
language O
origins. O
Model O
For O
prompting, O
we O
use O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021), O
an O
encoder-decoder O
PLM O
that O
supports O

101 O
languages, O
including O
all O
languages O
in O
SMiLER B-DatasetName
. O
mT5 B-MethodName
BASE I-MethodName
(Xue O
et O
al., O
2021) O
has O
220M B-HyperparameterValue
parameters B-HyperparameterName
. O
XLM-R B-MethodName
EM I-MethodName
To O
provide O
a O
fine-tuning O
baseline, O
we O
re-implement O
BERT B-MethodName
EM I-MethodName
(Baldini O
Soares O
et O
al., O
2019) O
with O
the O
ENTITY O
START O
variant. O
4 O
In O
this O
method, O
the O
top-layer O
representations O
at O
the O
starts O
of O

the O
two O
entities O
are O
concatenated O
for O
linear O
classification. O
To O
adapt O
BERT B-MethodName
EM I-MethodName
to O
multilingual O
tasks, O
we O
change O
the O
PLM O
from O
BERT O
to O
a O
multilingual O
autoencoder, O
XLM-R B-MethodName
BASE I-MethodName
(Conneau O
et O
al., O
2020), O
and O
refer O
to O
this O
model O
as O
XLM-R B-MethodName
EM I-MethodName
. O
XLM-R B-MethodName
BASE I-MethodName
has O
125M B-HyperparameterValue
parameters B-HyperparameterName
. O
Null O
prompts O
(Logan O
IV O

et O
al., O
2022) O
To O
better O
verify O
the O
effectiveness O
of O
our O
method, O
we O
implement O
null O
prompts O
as O
a O
strong O
task-agnostic O
prompt O
baseline. O
Null O
prompts O
involve O
minimal O
prompt O
engineering O
by O
directly O
asking O
the O
LM O
about O
the O
relation, O
without O
giving O
any O
task O
instruction O
(see O
Table O
1). O
Logan O
IV O
et O
al. O
(2022) O
show O
that O

null O
prompts O
surprisingly O
achieve O
on-par O
performance O
with O
handcrafted O
prompts O
on O
many O
tasks. O
For O
best O
comparability, O
we O
use O
the O
same O
PLM O
mT5 B-MethodName
BASE I-MethodName
. O
Fully O
Supervised O
Setup O
We O
evaluate O
the O
performance O
of O
XLM-R B-MethodName
EM I-MethodName
, O
null O
prompts, O
and O
our O
method O
on O
each O
of O
the O
14 O
languages, O
after O
training O
on O
the O
full O

train O
split O
from O
that O
language. O
The O
prompt O
input O
and O
target O
of O
null O
prompts O
and O
our O
prompts O
are O
listed O
in O
Table O
1. O
We O
employ O
the O
randomly O
generated O
seed B-HyperparameterName
319 B-HyperparameterValue
for O
all O
the O
evaluated O
methods. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
follow O
Baldini O
Soares O
et O
al. O
(2019) O
and O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O

be O
64 B-HyperparameterValue
, O
the O
optimizer O
to O
be O
Adam O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
the O
number O
of O
epochs B-HyperparameterName
to O
be O
5 B-HyperparameterValue
. O
For O
null O
prompts O
and O
ours, O
we O
use O
AdamW O
as O
the O
optimizer O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, O
as O
suggest O
for O
most O
of O

the O
sequence-to-sequence O
tasks, O
the O
number O
of O
epochs B-HyperparameterName
to O
5 B-HyperparameterValue
, O
and O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
. O
The O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
for O
all O
methods. O
Few-shot O
Setup O
Few-shot O
learning O
is O
normally O
cast O
as O
a O
K-shot O
problem, O
where O
K O
labelled O
examples O
per O
class O
are O
available. O
We O
follow O
and O
Han O
et O
al. O

(2021), O
and O
evaluate O
on O
8, O
16 O
and O
32 O
shots. O
The O
few-shot O
training O
set O
D O
train O
is O
generated O
by O
randomly O
sampling O
K O
instances O
per O
relation O
from O
the O
training O
split. O
The O
test O
set O
D O
test O
is O
the O
original O
test O
split O
from O
that O
language. O
We O
follow O
Gao O
et O
al. O
(2021) O
and O
sample O
another O

K-shot O
set O
from O
the O
English O
train O
split O
as O
validation O
set O
D O
val O
. O
We O
tune O
hyperparameters O
on O
D O
val O
for O
the O
English O
task, O
and O
apply O
these O
to O
all O
languages. O
We O
evaluate O
the O
same O
methods O
as O
in O
the O
fully O
supervised O
scenarios, O
but O
repeat O
5 O
runs O
as O
suggested O
in O
Gao O
et O
al. O

(2021), O
and O
report O
the O
mean O
and O
standard O
deviation O
of O
micro-F1 B-MetricName
. O
We O
use O
a O
fixed O
set O
of O
random O
seeds B-HyperparameterName
{13, B-HyperparameterValue
36, I-HyperparameterValue
121, I-HyperparameterValue
223, I-HyperparameterValue
319} I-HyperparameterValue
for O
data O
generation O
and O
training O
across O
the O
5 O
runs. O
For O
XLM-R B-MethodName
EM I-MethodName
, O
we O
use O
the O
same O
hyperparameters O
as O
Baldini O
Soares O
et O
al. O
( O
2019), O
a O

batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
, O
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
. O
For O
null O
prompts O
and O
our O
prompts, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
3 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
and O
the O
number O
of O
epochs B-HyperparameterName
to O
20 B-HyperparameterValue
. O
Zero-shot O
Setup O
We O
consider O
two O

scenarios O
for O
zero-shot O
multilingual O
relation O
classification. O
Zero-shot O
in-context O
learning O
Following O
Kojima O
et O
al. O
( O
2022), O
we O
investigate O
whether O
PLMs O
are O
also O
decent O
zero-shot O
reasoners O
for O
RC B-TaskName
. O
This O
scenario O
does O
not O
require O
any O
samples O
or O
training. O
We O
test O
the O
out-of-the-box O
performance O
of O
the O
PLM O
by O
directly O
prompting O
it O
with O
x O

prompt O
. O
Zero-shot O
in-context O
learning O
does O
not O
specify O
further O
hyperparameters O
since O
it O
is O
training-free. O
Zero-shot O
cross-lingual O
transfer O
In O
this O
scenario, O
following O
Krishnan O
et O
al. O
(2021), O
we O
finetune O
the O
model O
with O
in-language O
prompting O
on O
the O
English O
train O
split, O
and O
then O
conduct O
zero-shot O
incontext O
tests O
with O
this O
fine-tuned O
model O
on O
other O
languages O

using O
code-switch O
prompting. O
Through O
this O
setting, O
we O
want O
to O
verify O
if O
task-specific O
pretraining O
in O
a O
high-resource O
language O
such O
as O
English O
helps O
in O
other O
languages. O
In O
zero-shot O
crosslingual O
transfer, O
we O
use O
the O
same O
hyperparameters O
and O
random O
seed O
to O
fine-tune O
on O
the O
English O
task. O
Fully O
Supervised O
Results O
the O
three O
variants O
of O
our O

method O
beat O
the O
finetuning O
baseline O
XLM-R B-MethodName
EM I-MethodName
and O
the O
prompting O
baseline O
null O
prompts, O
according O
to O
the O
macroaveraged O
performance O
across O
14 O
languages. O
Inlanguage O
prompting O
delivers O
the O
most O
promising O
result, O
achieving O
an O
average O
F B-MetricName
1 I-MetricName
of O
85.0 B-MetricValue
, O
which O
is O
higher O
than O
XLM-R B-MethodName
EM I-MethodName
( O
68.2 B-MetricValue
) O
and O
null B-MethodName
prompts I-MethodName
( O
66.2 B-MetricValue

). O
The O
other O
two O
variants, O
code-switch B-MethodName
prompting I-MethodName
with O
and O
w/o B-MethodName
soft I-MethodName
tokens I-MethodName
, O
achieve O
F B-MetricName
1 I-MetricName
scores O
of O
84.1 B-MetricValue
and O
82.7 B-MetricValue
, O
respectively, O
only O
0.9 O
and O
2.3 O
lower O
than O
in-language. O
All O
three O
prompt O
variants O
are O
hence O
effective O
in O
fully O
supervised O
scenarios. O
On O
a O
per-group O
basis, O
we O
find O
that O
the O
lowerresourced O

a O
language O
is, O
the O
greater O
an O
advantage O
prompting O
enjoys O
against O
fine-tuning. O
In O
particular, O
in-language O
prompts O
shows O
better O
robustness O
compared O
to O
XLM-R B-MethodName
EM I-MethodName
in O
low-resource O
languages. O
They O
both O
yield O
95.9 B-MetricValue
- O
96.0 B-MetricValue
F B-MetricName
1 I-MetricName
scores O
for O
English, O
but O
XLM-R B-MethodName
EM I-MethodName
decreases O
to O
54.3 B-MetricValue
and O
3.7 B-MetricValue
F B-MetricName
1 I-MetricName
in O
Group-M O
and O
-L, O

while O
in-language O
prompting O
still O
delivers O
83.5 B-MetricValue
and O
65.2 B-MetricValue
F B-MetricValue
1 I-MetricValue
. O
Few-shot O
Results O
Table O
5 O
presents O
the O
per-group O
results O
in O
few-shot O
experiments. O
All O
the O
methods O
benefit O
from O
larger O
K. O
Similarly, O
in-language O
prompting O
still O
turns O
out O
to O
be O
the O
best O
contender, O
performing O
1st O
in O
8-and O
32-shot, O
and O
the O
2nd O
in O
16-shot. O

We O
see O
that O
inlanguage O
outperforms O
XLM-R B-MethodName
EM I-MethodName
in O
all O
K-shots, O
while O
code-switch B-MethodName
achieves O
comparable O
or O
even O
lower O
F B-MetricName
1 I-MetricName
to O
XLM-R B-MethodName
EM I-MethodName
for O
K O
= O
8, O
suggesting O
that O
the O
choice O
of O
prompt O
affects O
the O
few-shot O
performance O
greatly, O
thus O
needs O
careful O
consideration. O
On O
a O
per-group O
basis, O
we O
find O
that O
in-language O
prompting O

outperforms O
other O
methods O
for O
middleand O
low-resourced O
languages. O
Similar O
observations O
can O
also O
be O
drawn O
from O
fully O
supervised O
results. O
We O
conclude O
that, O
with O
sufficient O
supervision, O
inlanguage O
is O
the O
optimal O
variant O
to O
prompt O
rather O
Table O
5: O
Few-shot O
results O
by O
group O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
(Seganti O
et O
al., O
2021) O
dataset O
averaged O
over O

five O
runs. O
We O
macro-average B-MetricName
results O
for O
each O
language O
group O
(see O
Figure O
2) O
and O
over O
all O
languages O
(X). O
Inlanguage O
prompting O
performs O
best O
in O
most O
settings O
and O
language O
groups. O
Our O
variants O
are O
especially O
strong O
for O
medium-and O
lower-resource O
language O
groups. O
See O
Table O
7 O
in O
Appendix O
C O
for O
detailed O
results O
with O
mean O
and O
std. O

for O
each O
language. O
than O
code-switch B-MethodName
. O
We O
hypothesize O
it O
is O
due O
to O
the O
pre-training O
scenario, O
where O
the O
PLM O
rarely O
sees O
code-mixed O
text O
(Santy O
et O
al., O
2021). O
Zero-shot O
Results O
Table O
6 O
presents O
the O
per-language O
results O
in O
zeroshot O
scenarios. O
We O
consider O
the O
random O
baseline O
for O
comparison O
(Zhao O
and O
Schütze, O
2021;Winata O
et O
al., O

2021). O
We O
notice O
that O
performance O
of O
the O
random O
baseline O
varies O
a O
lot O
across O
languages, O
since O
the O
languages O
have O
different O
number O
of O
classes O
in O
the O
dataset O
(cf. O
Table O
6: O
Zero-shot O
results O
in O
micro-F1 B-MetricName
(%) O
on O
the O
SMiLER B-DatasetName
dataset. O
"SVO" O
and O
"SOV": O
word O
order O
of O
prompting. O
Overall, O
Code-switch B-MethodName
prompting I-MethodName
performs O
the O
best O

in O
the O
zero-shot O
in-context O
scenario. O
In O
cross-lingual O
transfer O
experiments, O
English-task O
training O
greatly O
improves O
the O
performance O
on O
all O
the O
other O
13 O
languages. O
margin, O
in O
both O
word O
orders, O
while O
in-language B-MethodName
prompting I-MethodName
performs O
worse O
than O
the O
random O
baseline O
in O
6 O
languages. O
Code-switch B-MethodName
prompting I-MethodName
outperforms O
in-language O
prompting O
across O
all O
the O
13 O
non-English O
languages, O
using O

SVO-template. O
We O
assume O
that, O
without O
in-language O
training, O
the O
PLM O
understands O
the O
task O
best O
when O
prompted O
in O
English. O
The O
impressive O
performance O
of O
code-switch B-MethodName
shows O
the O
PLM O
is O
able O
to O
transfer O
its O
pre-trained O
knowledge O
in O
English O
to O
other O
languages. O
We O
also O
find O
that O
the O
performance O
is O
also O
highly O
indicated O
by O
the O
number O

of O
classes, O
with O
worst O
F B-MetricName
1 I-MetricName
scores O
achieved O
in O
EN, O
KO O
and O
PT O
(36, O
28 O
and O
22 O
classes), O
and O
best O
scores O
in O
AR, O
RU O
and O
UK O
(9, O
8 O
and O
7 O
classes). O
In O
addition, O
we O
observe O
that O
word O
order O
does O
not O
play O
a O
significant O
role O
for O
most O
languages, O
except O
for O
FA, O

which O
is O
an O
SOV-language O
and O
has O
54.5 B-MetricValue
F B-MetricName
1 I-MetricName
gain O
from O
in-language O
prompting O
with O
an O
SOV-template. O
For O
zero-shot O
cross-lingual O
transfer, O
we O
see O
that O
non-English O
tasks O
benefit O
from O
English O
in-domain B-MethodName
prompt-based I-MethodName
fine-tuning I-MethodName
, O
and O
the O
F B-MetricName
1 I-MetricName
gain O
improves O
with O
the O
English O
data O
size. O
For O
5 O
languages O
(ES, O
FA, O
NL, O
SV, O

and O
UK), O
zero-shot O
transfer O
after O
training O
on O
268k O
English O
examples O
delivers O
even O
better O
results O
than O
in-language O
fully O
supervised O
training O
(cf. O
Table O
4). O
Sanh O
et O
al. O
(2022) O
show O
that O
including O
RC B-TaskName
-specific O
prompt O
input O
in O
English O
during O
pre-training O
can O
help O
in O
other O
languages. O
Discussion O
Based O
on O
the O
results O
above, O
we O
answer O

the O
research O
questions O
from O
Section O
1. O
RQ1. O
Which O
is O
the O
most O
effective O
way O
to O
prompt? O
In O
the O
fully-supervised O
and O
few-shot O
scenario, O
in-language B-MethodName
prompting I-MethodName
displays O
the O
best O
re-sults. O
This O
appears O
to O
stem O
from O
a O
solid O
performance O
across O
all O
languages O
in O
both O
settings. O
Its O
worst O
performance O
is O
31.8 O
F O
1 O
for O
Polish O

8-shot O
(see O
Table O
7 O
in O
Appendix O
C). O
All O
other O
methods O
have O
results O
lower O
than O
15.0 B-MetricValue
F B-MetricName
1 I-MetricName
for O
some O
language. O
This O
indicates O
that O
with O
little O
supervision O
mT5 B-MethodName
is O
able O
to O
perform O
the O
task O
when O
prompted O
in O
the O
language O
of O
the O
original O
text. O
However, O
zero-shot O
results O
strongly O
prefer O
code-switch B-MethodName
prompting I-MethodName
. O

It O
could O
follow O
that, O
without O
fine-tuning, O
the O
model's O
understanding O
of O
this O
task O
is O
much O
better O
in O
English. O
RQ2. O
How O
well O
does O
our O
method O
perform O
in O
different O
data O
regimes O
and O
languages? O
Averaged O
over O
all O
languages, O
all O
our O
variants O
outperform O
the O
baselines, O
except O
for O
8-shot. O
For O
some O
high-resource O
languages, O
XLM-R B-MethodName
EM I-MethodName
is O

able O
to O
outperform O
our O
method. O
On O
the O
other O
hand, O
for O
low-resource O
languages O
null O
prompts O
are O
a O
better O
baseline O
which O
we O
consistently O
outperform. O
This O
could O
indicate O
that O
prompting O
the O
underlying O
mT5 B-MethodName
model O
is O
better O
suited O
for O
multilingual B-TaskName
RC I-TaskName
on O
SMiLER B-DatasetName
. O
Overall, O
the O
results O
suggest O
that O
minimal O
translation O
can O
be O
very O

helpful O
for O
multilingual O
relation O
classification. O
B O
Verbalizers O
for O
SMiLER B-DatasetName
• O
EN O
"birth-place": O
"birth O
place", O
"eats": O
"eats", O
"event-year": O
"event O
year", O
"firstproduct": O
"first O
product", O
"from-country": O
"from O
country", O
"has-author": O
"has O
author", O
"has-child": O
"has O
child", O
"has-edu": O
"has O
education", O
"has-genre": O
"has O
genre", O
"has-height": O
"has O
height", O
"has-highestmountain": O
"has O
highest O
mountain", O
"haslength": O
"has O
length", O
"has-lifespan": O
"has O
lifespan", O

"has-nationality": O
"has O
nationality", O
"has-occupation": O
"has O
occupation", O
"has-parent": O
"has O
parent", O
"has-population": O
"has O
population", O
"has-sibling": O
"has O
sibling", O
"has-spouse": O
"has O
spouse", O
"hastourist-attraction": O
"has O
tourist O
attraction", O
"has-type": O
"has O
type", O
"has-weight": O
"has O
weight", O
"headquarters": O
"headquarters", O
"invented-by": O
"invented O
by", O
"inventedwhen": O
"invented O
when", O
"is-member-of": O
"is O
member O
of", O
"is-where": O
"located O
in", O
"loc-leader": O
"location O
leader", O
"movie-hasdirector": O
"movie O
has O
director", O

"no_relation": O
"no O
relation", O
"org-has-founder": O
"organization O
has O
founder", O
"org-has-member": O
"organization O
has O
member", O
"org-leader": O
"organization O
leader", O
"post-code": O
"post O
code", O
"starring": O
"starring", O
"won-award": O
"won O
award"; O
1 O
Introduction O
010 O
• O
AR O
"event-year": O
" O
", O
"hasedu": O
" O
", O
"has-genre": O
" O
"has-population": O
" O
", O
"has-type": O
" O
", O
"is-member-of": O
" O
", O
• O
DE O
"birth-place": O
"Geburtsort", O
"eventyear": O
"Veranstaltungsjahr", O

"from-country": O
"vom O
Land", O
"has-author": O
"hat O
Autor", O
"haschild": O
"hat O
Kind", O
"has-edu": O
"hat O
Bildung", O
"has-genre": O
"hat O
Genre", O
"has-occupation": O
"hat O
Beruf", O
"has-parent": O
"hat O
Elternteil", O
"has-population": O
"hat O
Bevölkerung", O
"has-spouse": O
"hat O
Ehepartner", O
"has-type": O
"hat O
Typ", O
"headquarters": O
"Hauptsitz", O
"ismember-of": O
"ist O
Mitglied O
von", O
"is-where": O
"gelegen O
in", O
"loc-leader": O
"Standortleiter", O
"movie-has-director": O
"Film O
hat O
Regisseur", O
"no_relation": O
"keine O
Beziehung", O
"org-hasfounder": O
"Organisation O

hat O
Gründer", O
"orghas-member": O
"Organisation O
hat O
Mitglied", O
"org-leader": O
"Organisationsleiter", O
"wonaward": O
"gewann O
eine O
Auszeichnung"; O
• O
ES O
"birth-place": O
"lugar O
de O
nacimiento", O
"event-year": O
"año O
del O
evento", O
"fromcountry": O
"del O
país", O
"has-author": O
"tiene O
autor", O
"has-child": O
"tiene O
hijo", O
"hasedu": O
"tiene O
educación", O
"has-genre": O
"tiene O
género", O
"has-occupation": O
"tiene O
ocupación", O
"has-parent": O
"tiene O
padre", O
"has-population": O
"tiene O
población", O
"has-spouse": O
"tiene O
cónyuge", O
"has-type": O

"tiene O
tipo", O
"headquarters": O
"sede O
central", O
"is-member-of": O
"es O
miembro O
de", O
"is-where": O
"situado O
en", O
"loc-leader": O
"líder O
de O
ubicación", O
"moviehas-director": O
"película O
cuenta O
con O
el O
director", O
"no_relation": O
"sin O
relación", O
"orghas-founder": O
"organización O
cuenta O
con O
el O
fundador", O
"org-has-member": O
"organización O
tiene O
miembro", O
"won-award": O
"ganó O
el O
premio"; O
• O
AR O
"event-year": O
" O
", O
"has-011 O
edu": O
" O
", O
"has-genre": O
" O

A.1 O
Hyperparameter O
Search O
We O
investigated O
the O
following O
possible O
hyperparameters O
for O
few-shot O
settings. O
For O
fully-supervised, O
we O
take O
hyperparameters O
from O
literature O
(see O
Section O
4.4). O
Number O
of O
epochs B-HyperparameterName
: O
[10,20] B-HyperparameterValue
; O
Learning B-HyperparameterName
rate I-HyperparameterName
: O
[1 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
, I-HyperparameterValue
1 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue

] I-HyperparameterValue
. O
Batch B-HyperparameterName
size I-HyperparameterName
: O
[16,64,256] B-HyperparameterValue
, O
not O
tuned O
but O
selected O
based O
on O
available O
GPU O
VRAM. O
We O
manually O
tune O
these O
hyperparameters, O
based O
on O
the O
micro-F B-MetricName
1 I-MetricName
score O
on O
the O
validation O
set. O
A.2 O
Computing O
Infrastructure O
Fully O
supervised O
experiments O
are O
conducted O
on O
a O
single O
A100-80GB O
GPU. O
Few-shot O
and O
zero-shot O
experiments O
are O
conducted O

on O
a O
single O
A100 O
GPU. O
A.3 O
Average O
Running O
Time O
Fully O
supervised O
It O
takes O
5 O
hours O
to O
train O
for O
1 O
run O
with O
mT5 B-MethodName
BASE I-MethodName
and O
a O
prompt O
method O
(null O
prompts, O
CS, O
SP O
and O
IL) O
on O
either O
English, O
or O
all O
other O
languages O
in O
total. O
With O
XLM-R B-MethodName
EM I-MethodName
the O
running O
time O
is O
3 O

hours. O
Few O

Inducer-tuning B-MethodName
: O
Connecting O
Prefix-tuning B-TaskName
and O
Adapter-tuning B-TaskName
Prefix-tuning B-TaskName
, O
or O
more O
generally O
continuous O
prompt O
tuning, O
has O
become O
an O
essential O
paradigm O
of O
parameter-efficient O
transfer O
learning. O
Using O
a O
large O
pre-trained O
language O
model O
(PLM), O
prefix-tuning B-TaskName
can O
obtain O
strong O
performance O
by O
training O
only O
a O
small O
portion O
of O
parameters. O
In O
this O
paper, O
we O
propose O
to O
understand O

and O
further O
develop O
prefix-tuning B-TaskName
through O
the O
kernel O
lens. O
Specifically, O
we O
make O
an O
analogy O
between O
prefixes O
and O
inducing O
variables O
in O
kernel O
methods O
and O
hypothesize O
that O
prefixes O
serving O
as O
inducing O
variables O
would O
improve O
their O
overall O
mechanism. O
From O
the O
kernel O
estimator O
perspective, O
we O
suggest O
a O
new O
variant O
of O
prefix-tuning-inducer-tuning B-MethodName
, O
which O
shares O
the O

exact O
mechanism O
as O
prefix-tuning B-TaskName
while O
leveraging O
the O
residual O
form O
found O
in O
adaptertuning B-TaskName
. O
This O
mitigates O
the O
initialization O
issue O
in O
prefix-tuning B-TaskName
. O
Through O
comprehensive O
empirical O
experiments O
on O
natural O
language O
understanding O
and O
generation O
tasks, O
we O
demonstrate O
that O
inducer-tuning B-MethodName
can O
close O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
fine-tuning. O
* O
Equal O
contribution. O
This O
work O

was O
performed O
while O
the O
first O
author O
was O
interning O
at O
Amazon O
Alexa O
AI. O
Introduction O
Transfer O
learning O
from O
large O
pre-trained O
language O
models O
(PLMs) O
has O
been O
the O
de-facto O
method O
to O
tackle O
downstream O
natural O
language O
processing O
(NLP) O
tasks O
with O
proven O
performance O
and O
scalability O
(Peters O
et O
al., O
2018). O
Among O
all O
the O
adaption O
techniques, O
fine-tuning O
(Howard O

and O
Ruder, O
2018;Kale O
and O
Rastogi, O
2020) O
is O
predominant O
for O
PLMs O
and O
maintains O
the O
models' O
architecture O
while O
updating O
all O
the O
parameters O
within. O
Though O
powerful, O
fine-tuning O
is O
considered O
parameter-inefficient O
since O
it O
results O
in O
separate O
copies O
of O
model O
parameters O
for O
each O
task/client O
after O
training. O
With O
the O
sizes O
of O
PLMs O
increasing O
to O
hundreds O
of O

millions O
(Brown O
et O
al., O
2020) O
or O
even O
up O
to O
tril-lion O
(Fedus O
et O
al., O
2021) O
parameters, O
the O
trend O
motivates O
a O
range O
of O
parameter-efficient O
adaptation O
techniques, O
including O
adapter-tuning B-TaskName
and O
prompting, O
as O
promising O
lightweight O
alternatives O
to O
finetuning O
to O
reduce O
computational O
consumption O
and O
storage O
space. O
Adapter-tuning B-TaskName
inserts O
bottlenecked O
Multi-layer O
Perception O
(MLP) O
modules O
between O
the O

pre-trained O
layers O
of O
PLMs O
and O
tunes O
only O
these O
new O
parameters O
for O
task O
adaptation O
(Houlsby O
et O
al., O
2019;Pfeiffer O
et O
al., O
2020a). O
Prompting, O
instead, O
aims O
to O
adapt O
the O
general-purpose O
PLMs O
through O
prompts, O
whose O
effectiveness O
has O
been O
shown O
on O
a O
frozen O
GPT-3 O
model O
(Brown O
et O
al., O
2020). O
An O
implicit O
drawback O
of O
the O
prompt-based O

adaptation O
is O
the O
difficulty O
of O
searching O
for O
the O
proper O
prompt. O
To O
avoid O
manually O
designing O
the O
prompts, O
Shin O
et O
al. O
(2020) O
propose O
a O
search O
algorithm O
to O
find O
the O
effective O
prompt O
over O
discrete O
space O
of O
vocabularies; O
prefix-tuning O
(Li O
and O
Liang, O
2021) O
and O
other O
concurrent O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b,a) O

further O
extend O
the O
discrete O
search O
to O
continuous O
prompts, O
attaining O
performance O
close O
to O
fine-tuning O
in O
some O
tasks. O
Despite O
the O
effort, O
there O
is O
still O
a O
performance O
gap O
between O
" O
prefixtuning B-TaskName
" O
and O
"fine-tuning" O
in O
many O
tasks, O
especially O
when O
the O
model O
size O
is O
small O
(Lester O
et O
al., O
2021;He O
et O
al., O
2021a). O
In O
addition, O

the O
mechanism O
of O
prefix-tuning O
is O
still O
poorly O
understood O
and O
underexplored. O
Prefix-tuning B-TaskName
is O
also O
similar O
to O
adaptertuning B-TaskName
, O
since O
they O
both O
insert O
additional O
modules O
into O
each O
transformer O
layer O
(classical O
prompt-based O
methods O
(Lester O
et O
al., O
2021;Liu O
et O
al., O
2021b) O
only O
add O
prompts O
to O
the O
embedding O
layer). O
Scrutinizing O
the O
evolution O
of O
prompt-based O
methods, O

we O
can O
observe O
they O
have O
gradually O
deviated O
from O
the O
concept O
of O
"prompts". O
Compared O
to O
the O
manually O
designed O
prompts, O
the O
discrete O
search O
usually O
results O
in O
counter-intuitive O
prompt O
tokens, O
which O
vaguely O
match O
the O
topic O
but O
are O
not O
as O
sensible O
as O
the O
manual O
one; O
for O
continuous O
prompt O
tuning, O
it O
even O
breaks O
the O
limit O

of O
the O
existing O
vo-cabulary. O
All O
these O
pieces O
imply O
that O
the O
mechanism O
behind O
prompt-based O
tuning O
might O
be O
more O
complicated O
than O
guiding O
the O
output O
through O
hint O
prompts. O
To O
open O
the O
black O
box O
of O
"prompts", O
in O
this O
work, O
we O
propose O
to O
consider O
the O
prompts O
(either O
hard O
or O
soft) O
as O
"inducing O
variables" O
in O
kernel O

methods O
(Titsias, O
2009). O
This O
analogy O
is O
justified O
due O
to O
the O
close O
connection O
between O
attention O
modules O
in O
PLMs O
and O
kernel O
estimators O
(Choromanski O
et O
al., O
2020;Tsai O
et O
al., O
2019). O
This O
kernel O
perspective O
explains O
the O
potential O
mechanism O
of O
prefix-tuning B-TaskName
and O
motivates O
a O
new O
method, O
inducer-tuning B-MethodName
. O
Specifically, O
inducertuning B-MethodName
freezes O
all O
the O
original O
parameters O

in O
the O
PLMs O
as O
other O
prompt-based O
methods; O
when O
computing O
the O
attention O
output O
for O
a O
certain O
input O
token O
in O
each O
layer, O
inducer-tuning O
utilizes O
a O
point O
close O
to O
the O
query O
vector O
as O
the O
"inducer". O
This O
unique O
"soft O
prompt" O
eases O
the O
search O
for O
appropriate O
prompts O
and O
builds O
a O
new O
connection O
between O
"prompting" O
and O

" O
adapter-tuning B-TaskName
". O
In O
summary, O
the O
contribution O
of O
this O
work O
is O
three-fold: O
1 O
We O
explain O
the O
underlying O
mechanism O
of O
prefix-tuning B-TaskName
as O
the O
inducing O
variables O
in O
kernel O
learning. O
2 O
We O
propose O
a O
new O
parameterefficient O
adaptation O
technique, O
inducer-tuning B-MethodName
, O
to O
further O
improve O
prefix-tuning B-TaskName
. O
3 O
Through O
comprehensive O
empirical O
studies, O
we O
verify O
our O

proposed O
method O
can O
close O
the O
gap O
between O
" O
prefix-tuning B-TaskName
" O
and O
"fine-tuning" O
on O
relatively O
small O
PLMs, O
and O
provide O
a O
tighter O
lower O
bound O
on O
the O
potential O
of O
continuous O
prompt O
tuning. O
Related O
Work O
In O
this O
section, O
we O
briefly O
introduce O
the O
classical O
form O
of O
adapter-tuning B-TaskName
and O
mainly O
focus O
on O
the O
different O
variants O
of O

prompting. O
Adapter-tuning B-TaskName
. O
Compared O
to O
fine-tuning O
all O
the O
parameters O
in O
the O
PLMs, O
Houlsby O
et O
al. O
(2019), O
Pfeiffer O
et O
al. O
(2020a) O
propose O
to O
modulate O
the O
output O
of O
a O
transformer O
layer O
through O
inserting O
additional O
small-bottleneck O
MLP O
layers O
(adapters) O
(Houlsby O
et O
al., O
2019) O
1 O
: O
Adapter(h) O
= O
h O
+ O
ReLU(hW O
1 O
)W O
2 O

, O
(1) O
where O
h O
is O
the O
dimension-d O
hidden O
state O
in O
the O
transformer O
and O
W O
1 O
, O
W O
2 O
are O
d-by-r O
and O
r-by-d O
projection O
matrices. O
Adapters O
have O
a O
residual O
form O
similar O
to O
skip O
connection, O
while O
only O
W O
1 O
, O
W O
2 O
will O
be O
trained, O
greatly O
decreasing O
the O
size O
of O
tunable O
parameters. O

Up O
to O
now, O
the O
adapter-based O
method O
has O
been O
widely O
used O
for O
multiple O
NLP O
tasks O
(Stickland O
and O
Murray, O
2019;Pfeiffer O
et O
al., O
2020a;Wang O
et O
al., O
2020;Pfeiffer O
et O
al., O
2020b;Üstün O
et O
al., O
2020;Vidoni O
et O
al., O
2020;Pfeiffer O
et O
al., O
2021;He O
et O
al., O
2021b;Xu O
et O
al., O
2021;Rücklé O
et O
al., O
2020;Karimi O
Mahabadi O
et O
al., O
2021), O
and O

adapters O
are O
also O
intrinsically O
connected O
to O
many O
other O
parameter-efficient O
adaptation O
techniques, O
as O
detailed O
in O
He O
et O
al. O
(2021a). O
Prompting. O
Prompting O
prepends O
task-specific O
instructions O
to O
the O
task O
input O
and O
was O
originally O
demonstrated O
in O
Brown O
et O
al. O
(2020). O
As O
manual O
prompts O
rely O
on O
trial O
and O
error, O
, O
Shin O
et O
al. O
(2020) O
suggests O

search O
algorithms O
to O
specify O
the O
prompts O
among O
all O
the O
tokens O
in O
the O
vocabulary. O
Prompt-tuning B-TaskName
(Lester O
et O
al., O
2021) O
and O
P-tuning O
(Liu O
et O
al., O
2021b) O
remove O
the O
vocabulary O
restriction O
on O
prompts O
by O
using O
trainable O
"soft O
prompts". O
The O
prompts O
in O
the O
aforementioned O
methods O
are O
only O
inserted O
into O
the O
bottom O
embedding O
layer O
of O

PLMs, O
while O
Prefix-tuning B-TaskName
(Li O
and O
Liang, O
2021;Liu O
et O
al., O
2021a) O
adds O
soft O
prompts O
to O
all O
the O
transformer O
layers O
to O
further O
increase O
the O
capacity O
of O
prompting. O
Though O
effective, O
proper O
initialization O
of O
the O
soft O
prompts O
remains O
challenging. O
To O
mitigate O
the O
issue, O
Li O
and O
Liang O
(2021) O
used O
an O
extra O
MLP O
to O
reparameterize O
the O

prompts O
in O
each O
layer, O
thus O
adding O
more O
parameters O
that O
need O
training; O
SPoT O
(Vu O
et O
al., O
2021) O
suggests O
performing O
pre-training O
for O
soft O
prompts O
using O
a O
wide O
range O
of O
NLP O
tasks, O
which O
requires O
additional O
computational O
resources. O
In O
contrast, O
though O
adapters O
have O
a O
similar O
expression O
form O
to O
prefix-tuning B-TaskName
(He O
et O
al., O
2021a), O
adaptertuning B-TaskName

only O
requires O
regular O
initialization. O
We O
speculate O
that O
the O
residual O
form O
of O
adapters O
mitigates O
the O
initialization O
issue O
since O
the O
output O
of O
each O
layer O
in O
the O
new O
model O
would O
be O
centered O
around O
the O
output O
in O
the O
frozen O
PLMs, O
and O
the O
residual O
form O
contributes O
to O
gradient O
back-propagation O
as O
in O
skip O
connection. O
We O
rely O

on O
this O
intuition O
and O
utilize O
the O
above-mentioned O
advantages O
of O
adapters O
to O
guide O
the O
design O
of O
our O
proposed O
inducer-tuning B-MethodName
. O
Preliminaries: O
Transformer O
Layers O
Before O
discussing O
the O
mechanism O
of O
prompt-tuning, O
we O
introduce O
the O
structure O
of O
transformer O
layers O
and O
necessary O
notations O
in O
this O
section. O
A O
general O
transformer-based O
PLM O
is O
mainly O
composed O
of O
L O

stacked O
layers. O
Each O
layer O
contains O
a O
multi-headed O
self-attention O
and O
a O
fully O
connected O
feed-forward O
network O
(FFN) O
sub-layer, O
both O
followed O
by O
an O
"Add O
& O
Norm" O
module O
(Vaswani O
et O
al., O
2017). O
2 O
Hereon, O
we O
shall O
focus O
on O
the O
structure O
of O
the O
attention O
sub-layer O
since O
prefix-tuning B-TaskName
directly O
works O
on O
this O
sub-layer. O
Passing O
a O
length-n O

input O
sequence O
X O
∈ O
R O
n×N O
h O
p O
to O
an O
attention O
sub-layer O
(assuming O
N O
h O
heads O
and O
dimension O
size O
p O
for O
each O
head), O
we O
first O
perform O
linear O
transforms O
to O
the O
input O
X O
and O
obtain O
the O
query O
matrix O
(Q), O
the O
key O
matrix O
(K), O
and O
the O
value O
matrix O
(V O
) O
as: O
Q/K/V O

= O
XW O
[q/k/v] O
+ O
1b O
T O
[q/k/v] O
, O
(2) O
where O
Q, O
K, O
V O
∈ O
R O
n×N O
h O
p O
are O
the O
query/ O
key/ O
value O
matrix; O
W O
[q/k/v] O
∈ O
R O
N O
h O
p×N O
h O
p O
are O
the O
weight O
matrices, O
and O
b O
[q/k/v] O
∈ O
R O
N O
h O
p O
are O
the O
bias O
terms O
in O

the O
corresponding O
transformations. O
3 O
To O
increase O
the O
model O
capacity, O
the O
three O
components O
Q, O
K, O
V O
are O
respectively O
divided O
into O
N O
h O
blocks, O
contributing O
to O
the O
attention O
output O
in O
each O
head O
of O
the O
multi-headed O
self-attention O
module. O
For O
instance, O
we O
represent O
Q O
as O
Q O
= O
Q O
(1) O
, O
• O
• O
• O
, O

Q O
(N O
h O
) O
, O
where O
each O
block O
Q O
(h) O
= O
XW O
(h) O
q O
+ O
1(b O
(h) O
q O
) O
T O
is O
an O
n-by-p O
matrix, O
and O
W O
(h) O
q O
, O
b O
(h) O
q O
are O
the O
corresponding O
parts O
in O
W O
q O
, O
b O
q O
. O
The O
attention O
output O
for O
the O
h O
th O

head O
is: O
L O
(h) O
V O
(h) O
:= O
softmax(Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p)V O
(h) O
= O
(D O
(h) O
) O
−1 O
M O
(h) O
V O
(h) O
,(3) O
where O
M O
(h) O
:= O
exp O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
and O
D O
(h) O
is O
a O
diagonal O
matrix O
in O
which O

D O
(h) O
ii O
is O
the O
sum O
of O
the O
i-th O
row O
in O
M O
(h) O
, O
serving O
as O
the O
normalization O
procedure O
in O
softmax. O
The O
attention O
outputs O
in O
each O
head O
are O
then O
concatenated O
as O
L O
:= O
(L O
(1) O
V O
(1) O
, O
. O
. O
. O
, O
L O
(N O
h O
) O
V O
(N O
h O
) O

). O
After O
concatenating O
the O
heads, O
there O
is O
a O
linear O
transform O
following O
the O
output O
LW O
o O
+ O
1b O
T O
o O
,(4) O
where O
W O
o O
and O
b O
o O
are O
similarly O
sized O
as O
the O
other O
matrices O
in O
Equation O
(2). O
This O
is O
the O
overall O
output O
of O
the O
attention O
sub-layer, O
which O
we O
shall O
revisit O
in O

§ O
4.4. O
Attention O
as O
Kernel O
Estimators O
Traditionally, O
attention O
operation O
(Equation O
( O
3)) O
is O
viewed O
as O
a O
transformation O
g(•) O
of O
the O
input O
sequence O
X. O
However, O
in O
prefix-tuning B-TaskName
, O
parameters O
within O
PLMs O
are O
frozen, O
which O
implies O
that O
given O
the O
input O
X, O
the O
represenattions O
Q, O
K, O
and O
V O
are O
invariant. O
4 O
This O
observation O

allows O
us O
to O
reinterpret O
attention O
as O
a O
kernel O
estimator O
f O
(•) O
with O
Q O
as O
its O
input. O
Specifically, O
we O
denote O
the O
i-th O
input O
vector O
X O
i O
's O
attention O
operation O
as O
f O
(Q O
i O
) O
:= O
g(X O
i O
). O
This O
attention O
representation O
can O
be O
seen O
as O
modifying O
the O
input O
query O
vector O
Q O

i O
to O
f O
(Q O
i O
) O
via O
supporting O
points O
{K O
j O
} O
n O
j=1 O
(Choromanski O
et O
al., O
2020;Peng O
et O
al., O
2020;, O
which O
can O
be O
considered O
as O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39): O
row-normalize O
(κ O
(Q, O
K)) O
V O
, O
where O
κ(•, O
•) O
is O
a O
kernel O
function. O
(Refer O
to O
Appendix O

C O
for O
more O
details O
on O
this O
claim.) O
Prefix-Tuning B-TaskName
and O
Inducing O
Variables O
Prefix-tuning O
(Li O
and O
Liang, O
2021) O
alters O
the O
attention O
output O
in O
each O
layer. O
Concretely, O
it O
prepends O
length-l O
prefix O
vectors O
P O
k O
, O
P O
v O
∈ O
R O
l×p O
to O
K O
and O
V O
, O
respectively; O
for O
a O
certain O
query O
token O
Q O
i O

(the O
i-th O
row O
of O
the O
query O
matrix O
Q), O
its O
attention O
output O
f O
(Q O
i O
) O
:= O
Attn(Q O
i O
, O
K, O
V O
) O
is O
updated O
as O
a O
weighted O
sum O
of O
f O
(Q O
i O
) O
and O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
) O
(He O
et O
al., O
2021a, O
Equation O
(7)). O
Remark. O

From O
the O
kernel O
estimator O
perspective, O
the O
two O
categories O
of O
virtual O
tokens O
play O
different O
roles. O
The O
virtual O
key O
vectors O
P O
k O
apply O
to O
the O
empirical O
kernel O
matrix O
part O
and O
can O
alter O
the O
attention O
scores O
(and O
thus O
the O
weights O
for O
Attn(Q O
i O
, O
P O
k O
, O
P O
v O
)); O
whereas O
P O
v O

takes O
effect O
in O
the O
value O
part. O
It O
might O
not O
be O
optimal O
for O
prefix-tuning B-TaskName
to O
model O
the O
two O
categories O
of O
virtual O
tokens O
similarly. O
In O
§ O
4.3 O
we O
will O
show O
how O
inducer-tuning O
addresses O
the O
two O
parts O
through O
different O
residual O
forms. O
We O
suggest O
that O
the O
mechanism O
of O
prefix-tuning B-TaskName
can O
be O
further O
understood O
through O

the O
concept O
of O
inducing O
variables O
in O
kernel O
learning O
literature O
(Titsias, O
2009). O
Many O
computational O
methods O
in O
kernel O
learning O
utilize O
a O
small O
set O
of O
support O
points O
(inducing O
variables) O
to O
improve O
the O
inference O
performance O
(Musco O
and O
Musco, O
2017;. O
Snelson O
and O
Ghahramani O
(2005) O
specifically O
consider O
the O
inducing O
variables O
as O
auxiliary O
pseudo-inputs O
and O
infer O
them O

using O
continuous O
optimization, O
which O
is O
similar O
to O
prefix-tuning B-TaskName
. O
We O
emphasize O
that O
from O
the O
first O
sight O
the O
main O
character O
of O
inducing-point O
methods O
is O
representing O
a O
vast O
amount O
of O
training O
examples O
through O
a O
small O
number O
of O
points, O
so O
as O
to O
reduce O
the O
computational O
cost; O
however, O
here O
we O
instead O
aim O
to O
leverage O

the O
mechanism O
of O
inducing O
variables O
to O
well-steer O
the O
estimation: O
the O
goal O
we O
try O
to O
attain O
is O
to O
strengthen O
prefix-tuning B-TaskName
by O
making O
the O
prefixes O
better O
modulate O
the O
attention O
output. O
We O
introduce O
and O
analyze O
the O
mechanism O
as O
follows. O
Mechanism O
for O
well-steering O
inference O
outputs O
in O
inducing-point O
methods. O
Conceptually, O
inducing O
variables O
help O
the O
inference O

because O
they O
can O
represent O
the O
distribution O
of O
the O
query O
inputs O
and O
steer O
the O
kernel O
methods O
without O
changing O
the O
kernel O
in O
use. O
In O
particular, O
we O
consider O
the O
distribution O
pattern O
of O
unconstrained O
inducing O
points O
X O
M O
(Snelson O
and O
Ghahramani, O
2005, O
Figure O
1). O
We O
observe O
that O
most O
of O
them O
are O
close O
to O
the O

testing O
examples O
X O
* O
, O
and O
in O
the O
new O
estimation O
(Snelson O
and O
Ghahramani, O
2005, O
Equation O
( O
8)) O
the O
inducers O
X O
M O
will O
receive O
great O
weights O
through O
the O
weights O
assignment O
mechanism O
in O
kernel O
methods O
(we O
recall O
kernel O
methods O
can O
assign O
the O
weights O
of O
samples O
as O
attention O
(Choromanski O
et O
al., O
2020;Tsai O
et O

al., O
2019); O
for O
inducing O
variables O
close O
to O
the O
query, O
they O
would O
automatically O
receive O
more O
attention), O
and O
thus O
effectively O
modulate O
the O
output. O
From O
this O
mechanism, O
we O
draw O
an O
inductive O
bias O
"the O
prefix O
should O
be O
close O
to O
the O
query" O
(which O
is O
not O
enforced O
in O
the O
method O
of O
prefix-tuning B-TaskName
) O
and O
accordingly O
propose O

inducer-tuning B-MethodName
. O
We O
remark O
since O
we O
are O
not O
pursuing O
the O
original O
goal, O
reducing O
computational O
cost, O
of O
inducing O
variables, O
it O
is O
ordinary O
that O
the O
concrete O
design O
in O
the O
next O
subsection O
is O
different O
from O
the O
usual O
form O
of O
inducing O
points, O
a O
small O
number O
of O
samples. O
We O
speculate O
prefix-tuning B-TaskName
partially O
benefits O
from O
the O

above O
mechanism O
as O
well. O
Furthermore, O
some O
indirect O
evidence O
is O
stated O
as O
follows. O
As O
discussed O
in O
previous O
studies, O
to O
make O
the O
full O
potential O
of O
prompting, O
the O
manually O
designed O
prompts O
are O
expected O
to O
be O
related O
to O
the O
topic O
of O
the O
input O
sequence O
(Brown O
et O
al., O
2020) O
(close O
to O
the O
query); O
even O
for O

the O
soft O
prompts O
they O
are O
recommended O
to O
be O
initialized O
with O
the O
token O
relevant O
to O
the O
specific O
tasks O
(Li O
and O
Liang, O
2021), O
which O
also O
requires O
the O
prompts O
to O
be O
close O
to O
the O
query O
to O
provide O
effective O
adaptation. O
With O
this O
belief, O
we O
propose O
inducer-tuning B-MethodName
to O
exploit O
further O
the O
mechanism O
of O
inducing O
variables O

and O
improve O
upon O
prefix-tuning B-TaskName
. O
Method O
Inducer-tuning B-MethodName
follows O
the O
same O
design O
principle O
as O
prefix-tuning B-TaskName
, O
which O
modulates O
the O
attention O
output O
through O
inserting O
virtual O
tokens O
(vectors). O
However, O
unlike O
prefix-tuning B-TaskName
, O
our O
virtual O
tokens O
are O
not O
shared O
among O
the O
input O
sequences. O
Inducertuning O
also O
incorporates O
the O
benefits O
of O
residual O
forms O
to O
ease O
the O

initialization O
and O
remove O
the O
reparametrization O
trick O
in O
prefix-tuning B-TaskName
. O
Specifically, O
we O
suggest O
the O
following O
modifications: O
1 O
The O
"inducers" O
are O
adaptive O
to O
and O
customized O
for O
each O
input O
token O
to O
strengthen O
the O
expressiveness O
of O
the O
new O
attention O
output. O
2 O
We O
propose O
to O
model O
the O
virtual O
vectors O
in O
a O
residual O
form O
as O
an O

adapter, O
which O
makes O
the O
final O
attention O
output O
be O
in O
a O
residual O
form O
as O
well. O
We O
now O
dive O
into O
discussing O
the O
intuitions O
behind O
the O
modifications O
in O
detail. O
Adaptive B-TaskName
inducers I-TaskName
. O
There O
is O
an O
important O
difference O
between O
language O
models O
and O
kernel O
methods, O
making O
fixed O
prefixes O
less O
effective O
than O
inducing O
variables O
in O
kernel O

methods. O
In O
language O
models, O
the O
distribution O
of O
the O
input O
queries O
keeps O
changing, O
and O
for O
some O
inputs, O
the O
fixed O
prefixes O
fail O
to O
be O
qualified O
as O
"inducing O
variables". O
Even O
worse, O
for O
a O
long O
input, O
there O
probably O
exists O
some O
query O
vectors O
away O
(regarding O
ℓ O
2 O
distance) O
from O
all O
the O
virtual O
vectors O
in O
the O

fixed O
prefixes, O
which O
are O
thus O
unable O
to O
modulate O
the O
attention O
output O
well. O
The O
phenomenon O
that O
prefix-tuning B-TaskName
has O
a O
relatively O
poorer O
performance O
on O
tasks O
with O
longer O
inputs O
can O
be O
observed O
in O
our O
experiments O
( O
§ O
6). O
To O
alleviate O
the O
above O
issue, O
we O
propose O
adaptive O
modeling O
of O
the O
virtual O
key O
vectors. O
For O

a O
query O
Q O
i O
, O
we O
suggest O
taking O
a O
vector O
close O
to O
Q O
i O
itself O
as O
the O
corresponding O
virtual O
key O
vector O
(the O
length O
of O
the O
new O
prefix O
is O
thus O
1), O
in O
the O
hope O
of O
leading O
to O
better O
inference. O
As O
for O
the O
virtual O
value O
vectors, O
we O
relate O
them O
to O
the O
corresponding O

virtual O
key O
vectors. O
The O
motivation O
comes O
from O
traditional O
(non-self-)attention, O
whose O
mechanism O
coincides O
with O
a O
kernel O
estimator: O
the O
value O
V O
is O
independent O
of O
the O
query O
sequence O
Q O
and O
related O
to O
the O
supporting O
points O
K. O
Specifically, O
considering O
our O
design O
above O
that O
the O
virtual O
key O
vectors O
are O
close O
to O
Q O
i O
(we O
take O

the O
virtual O
key O
vectors O
as O
transforms O
of O
the O
input O
query O
vectors O
Q O
i O
's), O
we O
propose O
to O
accordingly O
model O
the O
virtual O
value O
vectors O
as O
a O
map O
of O
Q O
i O
as O
well, O
which O
implies O
the O
virtual O
value O
vectors O
are O
also O
adaptive O
to O
the O
input O
query O
vectors. O
Adapter O
Structures. O
To O
stabilize O
the O

training O
procedure, O
we O
propose O
incorporating O
the O
adapter O
structures O
into O
modeling O
the O
virtual O
key/value O
vectors. O
Specifically, O
for O
the O
i-th O
token O
Q O
i O
(in O
a O
certain O
head), O
we O
represent O
the O
corresponding O
virtual O
key/value O
vectors O
respectively O
as O
P O
k,i O
= O
Q O
i O
+ O
MLP O
k O
(Q O
i O
)(5) O
P O
v,i O
= O
f O
(Q O

i O
) O
+ O
MLP O
v O
(Q O
i O
),(6) O
where O
MLP O
k/v O
will O
both O
return O
a O
vector O
of O
the O
same O
dimension O
as O
the O
input O
Q O
i O
. O
5 O
It O
is O
natural O
to O
model O
P O
k,i O
in O
a O
residual O
form O
as O
in O
Equation O
( O
1), O
considering O
P O
k,i O
is O
expected O
to O
center O

around O
Q O
i O
; O
as O
for O
P O
v,i O
, O
we O
claim O
the O
specific O
form O
in O
Equation O
( O
6) O
allows O
the O
complete O
expression O
of O
inducer-tuning B-MethodName
to O
be O
adapter-like, O
and O
the O
justification O
is O
stated O
as O
the O
following O
derivation. O
To O
derive O
the O
expression O
for O
inducer-tuning B-MethodName
, O
we O
denote O
the O
new O
key O
matrix O
and O

value O
matrix O
(specific O
to O
the O
input O
query O
vector O
Q O
i O
) O
as O
K O
(i) O
= O
P O
T O
k,i O
K O
, O
V O
(i) O
= O
P O
T O
v,i O
V O
T O
. O
The O
new O
attention O
outputf O
(Q O
i O
) O
for O
the O
query O
Q O
i O
is O
thus O
(omitting O
the O
factor O
1/ O
√ O
p O
for O

clarity) O
Attn(Q O
i O
, O
K O
(i) O
, O
V O
(i) O
) O
= O
exp(⟨Q O
i O
, O
P O
k,i O
⟩)P O
v,i O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩)V O
j O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
=λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O

)f O
(Q O
i O
)(7) O
where O
we O
define O
the O
weight O
λ O
i O
as, O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
exp(⟨Q O
i O
, O
P O
k,i O
⟩) O
+ O
j O
exp(⟨Q O
i O
, O
K O
j O
⟩) O
. O
Combining O
the O
pieces, O
we O
state O
the O
complete O
equation O
for O
the O
new O
attention O
outputf O
(Q O
i O
) O
as, O

λ O
i O
P O
v,i O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=λ O
i O
(f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
)) O
+ O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
=f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
(8 O
) O
We O
observe O

inducer-tuning B-MethodName
now O
perturbs O
the O
output O
f O
(Q O
i O
) O
in O
a O
residual O
form, O
which O
therefore O
connects O
prefix-tuning B-TaskName
and O
adapter-tuning B-TaskName
. O
The O
procedure O
of O
inducer-tuning B-MethodName
is O
summarized O
in O
Figure O
1, O
and O
§ O
6.2 O
shows O
the O
residual O
form O
greatly O
impacts O
the O
model O
performance. O
Extending O
the O
Scope O
of O
Value O
Besides O
the O
representation O
of O

the O
virtual O
vectors, O
we O
propose O
another O
improvement O
via O
the O
self-attention O
decomposition O
proposed O
by O
Hou O
et O
al. O
(2020). O
Considering O
the O
linear O
transform O
right O
after O
the O
attention O
module, O
we O
can O
accordingly O
rewrite O
the O
attention O
sub-layer O
as O
(ignoring O
the O
bias O
term O
in O
the O
linear O
transform) O
N O
h O
h=1 O
softmax(Q O
(h) O
(K O
(h) O
) O

T O
/ O
√ O
p)V O
(h) O
W O
(h) O
o O
, O
where O
W O
(h) O
o O
is O
the O
h-th O
row O
block O
in O
W O
o O
. O
No- O
tably, O
W O
(h) O
o O
is O
attached O
to O
the O
value O
matrix O
V O
(h) O
, O
suggesting O
that O
W O
(h) O
o O
's O
should O
be O
counted O
into O
the O
complete O
kernel O
structure O

of O
a O
head. O
We O
therefore O
define O
the O
complete O
attention O
outputf O
(Q O
(h) O
) O
as O
softmax O
Q O
(h) O
(K O
(h) O
) O
T O
/ O
√ O
p O
V O
(h) O
W O
(h) O
o O
, O
(9) O
and O
align O
the O
prefix O
vectors O
P O
v,i O
's O
with O
the O
rows O
in O
(h) O
as O
in O
prefixtuning. O
The O
detailed O
implementation O

of O
the O
extended O
P O
v,i O
is O
provided O
in O
Appendix O
B.3. O
We O
can O
verify O
the O
improvement O
by O
this O
extension O
through O
the O
ablation O
studies O
in O
§ O
6.2. O
V O
(h) O
W O
(h) O
o O
, O
instead O
of O
solely O
V O
A O
Potential O
Limitation O
of O
Prompting O
A O
potential O
limitation O
of O
prompt-based O
methods O
comes O
from O
the O
frozen O

weight O
matrices O
W O
q O
and O
W O
k O
. O
For O
all O
the O
n(n O
+ O
l) O
pairs O
of O
query O
/ O
key O
vectors O
in O
a O
head, O
most O
of O
the O
pairs O
(corresponding O
to O
the O
elements O
within O
QK O
T O
) O
have O
invariant O
. O
. O
MLPk O
+ O
MLPv O
+ O
P O
k O
K O
V O
P O
V O
Q O

i O
Q O
i O
P O
k,i O
K O
P O
v,i O
V O
f(Q O
i O
) O
Prefix-Tuning B-TaskName
Inducer-Tuning B-MethodName
Attention O
Scores O
Attention O
Scores O
Figure O
1: O
The O
mechanisms O
of O
prefix-tuning B-TaskName
(left) O
and O
inducer-tuning B-MethodName
(right) O
in O
inference O
(the O
MLP O
module O
for O
reparameterization O
in O
prefix-tuning O
is O
dropped). O
For O
prefix-tuning B-TaskName
, O
the O
virtual O
tokens O
(P O
k O
, O
P O
v O

) O
are O
shared O
among O
all O
the O
query O
vectors; O
inducer-tuning O
instead O
prepends O
customized O
inducers O
(P O
k,i O
, O
P O
v,i O
) O
for O
a O
certain O
vector O
Q O
i O
. O
pairwise O
positional O
interactions O
due O
to O
the O
frozen O
weight O
matrices O
W O
q O
and O
W O
k O
. O
However, O
on O
downstream O
tasks, O
there O
can O
be O
a O
mismatch O

between O
W O
q O
and O
W O
k O
maintained O
from O
pre-training: O
the O
distribution O
of O
Q, O
K O
will O
substantially O
change O
due O
to O
the O
distinct O
task-specific O
datasets O
as O
well O
as O
the O
virtual O
tokens O
added O
in O
the O
previous O
layers. O
There O
is O
no O
adaptation O
to O
ensure O
the O
positional O
interactions O
between O
Q, O
K O
still O
contribute O
to O
the O

proper O
representation O
f O
(Q O
i O
). O
To O
resolve O
the O
potential O
issue O
of O
prefixtuning B-TaskName
, O
we O
suggest O
applying O
low-rank O
adaptation O
(LoRA) O
(Hu O
et O
al., O
2021) O
to O
W O
q O
as O
a O
complement O
to O
prompt-based O
methods, O
including O
inducer-tuning B-MethodName
. O
Specifically, O
before O
we O
compute O
the O
attention O
output O
in O
each O
layer, O
W O
q O
will O
be O

updated O
as O
W O
q O
← O
W O
q O
+ O
BA,(10) O
where O
W O
q O
is O
kept O
frozen O
and O
B O
∈ O
R O
N O
h O
p×r O
, O
A O
∈ O
R O
r×N O
h O
p O
will O
be O
tunable O
in O
training. O
We O
report O
in O
§ O
6 O
that O
combining O
both O
inducer-tuning O
and O
LoRA O
outperforms O
their O
individual O
counterparts. O
Final O

Model. O
Our O
final O
proposed O
model O
does O
the O
inferencex O
as O
follows: O
1 O
in O
each O
layer, O
we O
first O
apply O
Equation O
(10) O
to O
update O
W O
q O
before O
obtaining O
Q, O
K, O
V O
; O
2 O
construct O
the O
inducer O
matrices O
P O
k O
= O
Q O
+ O
MLP O
k O
(Q), O
and O
compute O
the O
vector O
a O
with O
the O
i-th O

component O
a O
i O
= O
⟨Q O
i O
, O
P O
k,i O
⟩; O
3 O
compute O
the O
matrix O
product O
[a; O
QK O
T O
]/ O
√ O
p O
and O
then O
perform O
softmax O
over O
the O
product-the O
first O
column O
(denoted O
as O
p) O
is O
the O
weights O
λ O
i O
's O
in O
Equation O
(7); O
4 O
obtainf O
(Q) O
as O
in O
Equation O
( O
9), O

and O
returnf O
(Q) O
+ O
diag(p)MLP O
v O
(Q) O
(corresponding O
to O
Equation O
( O
8)) O
as O
the O
complete O
attention O
output. O
Experiments O
While O
prefix-tuning B-TaskName
has O
been O
shown O
comparable O
to O
fine-tuning O
on O
some O
natural O
language O
understanding O
(NLU) O
tasks O
(Liu O
et O
al., O
2021a), O
there O
is O
still O
a O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
finetuning O
on O
natural O
language O

generation O
(NLG) O
tasks, O
especially O
for O
those O
tasks O
with O
long O
input O
sequences. O
Complete O
settings O
of O
the O
experiments O
below O
can O
be O
found O
in O
Appendix O
A O
and O
Appendix O
B. O
The O
code O
for O
our O
algorithms O
is O
publicly O
available O
at O
https://github.com/ychen-stat-ml/kerneladapters. O
Sketch O
of O
the O
Tasks O
We O
test O
the O
performance O
of O
our O
methods O
on O
both O
NLU B-TaskName

and O
NLG B-TaskName
tasks. O
For O
NLU B-TaskName
tasks, O
we O
follow O
(He O
et O
al., O
2021a) O
to O
use O
RoBERTa B-MethodName
BASE I-MethodName
(Liu O
et O
al., O
2019) O
on O
MNLI B-DatasetName
(Williams O
et O
al., O
2018) O
and O
SST2 B-DatasetName
(Socher O
et O
al., O
2013) O
from O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019); O
in O
SST2, O
the O
models O
predict O
the O
two-way O
sentiment O
(positive/negative) O
of O

a O
given O
sentence, O
and O
the O
MNLI B-DatasetName
task O
is O
to O
decide, O
given O
a O
premise O
and O
a O
hypothesis, O
whether O
there O
is O
entailment, O
contradiction, O
or O
neither. O
We O
use O
GPT-2 B-MethodName
SMALL I-MethodName
(Radford O
et O
al., O
2019) O
for O
NLG B-TaskName
tasks: O
WebNLG-challenge B-DatasetName
(Gardent O
et O
al., O
2017) O
focuses O
on O
table-to-text O
tasks, O
in O
which O
the O
language O
models O
generate O
some O

relatively O
long O
and O
sensible O
sentences O
based O
on O
the O
triples O
with O
solely O
a O
few O
words; O
in O
contrast, O
CoQA B-DatasetName
(Reddy O
et O
al., O
2019) O
provides O
the O
data O
for O
conversational O
question O
answering O
6 O
, O
which O
requires O
the O
language O
model O
to O
return O
short O
answers O
to O
questions O
based O
on O
long O
conversational O
materials. O
More O
details O
about O
the O

datasets O
(including O
the O
average O
sequence O
length) O
and O
the O
evaluation O
metrics O
used O
are O
provided O
in O
Appendix O
A. O
Baselines O
We O
compare O
our O
method O
with O
other O
representative O
methods: O
Fine-Tuning O
(Howard O
and O
Ruder, O
2018) O
We O
differentiate O
the O
number O
of O
parameters O
to O
store O
and O
tune, O
as O
for O
prefix-tuning B-TaskName
, O
the O
two O
numbers O
are O
inconsistent O
due O

to O
a O
re-parametrization O
trick O
(Li O
and O
Liang, O
2021) O
to O
mitigate O
the O
initialization O
issue. O
Instead O
of O
directly O
setting O
up O
an O
embedding O
matrix O
for O
virtual O
tokens, O
an O
additional O
MLP O
module O
in O
each O
layer O
is O
used O
in O
prefix-tuning O
to O
model O
the O
representation O
for O
those O
virtual O
tokens; O
after O
the O
fine-tuning O
stage, O
the O
additional O
MLP O

modules O
are O
dropped O
and O
only O
the O
output O
embedding O
for O
virtual O
tokens O
needs O
storing, O
which O
leads O
to O
a O
regular O
number O
of O
parameters O
to O
store. O
For O
the O
proposed O
inducer-tuning B-MethodName
, O
we O
adopt O
the O
residual O
form O
to O
address O
the O
initialization O
issue O
and O
avoid O
the O
usage O
of O
the O
extra O
MLP, O
which O
makes O
inducer-tuning B-MethodName
have O

the O
same O
number O
of O
parameters O
to O
store O
as O
to O
train O
and O
behave O
more O
like O
a O
regular O
adapter. O
To O
make O
a O
fair O
comparison, O
we O
intentionally O
choose O
the O
number O
of O
parameters O
to O
store O
in O
prefixtuning B-TaskName
roughly O
the O
same O
as O
its O
adapter O
counterpart O
by O
adjusting O
the O
prefix O
length. O
Detailed O
settings O
are O
available O
in O

Appendix O
B.4. O
Main O
Results O
We O
conclude O
our O
experimental O
results O
in O
Tables O
1 O
and O
2, O
comparing O
the O
proposed O
inducertuning B-MethodName
( O
§ O
4.3), O
or O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
( O
§ O
4.5), O
against O
other O
baselines. O
The O
benefit O
of O
using O
Mix-And-Match O
(MAM) O
techniques O
(He O
et O
al., O
2021a) O
The O
MAM O
technique O
benefits O
inducer-tuning B-MethodName
. O
As O
remarked O

by O
He O
et O
al. O
(2021a), O
the O
"Mix-And-Match" O
of O
adapters O
in O
both O
self-attention O
and O
FFN O
sub-layers O
can O
better O
exploit O
parameter-efficient O
transfer O
learning O
than O
only O
modulating O
a O
single O
sublayer. O
We O
obtain O
a O
similar O
conclusion O
by O
replacing O
prefix-tuning B-TaskName
with O
inducer-tuning B-MethodName
(+ I-MethodName
LoRA) I-MethodName
in O
self-attention O
sub-layers. O
The O
combination O
( O
MAM B-MethodName
inducer-tuning I-MethodName
) O
performs O
well O

on O
most O
of O
the O
tasks; O
especially O
on O
the O
tasks O
with O
relatively O
longer O
sequences, O
MNLI B-DatasetName
and O
CoQA B-DatasetName
, O
MAM O
inducer-tuning O
attains O
respectively O
0.6% B-MetricValue
and O
1.2% B-MetricValue
performance O
improvement O
over O
vanilla O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
. O
Long O
inputs O
deteriorate O
prefix-tuning. O
Notably, O
the O
performance O
of O
prefix-tuning B-TaskName
is O
sensitive O
to O
the O
input O
length O
(c.f. O
§ O
4.3). O

For O
WebNLG B-DatasetName
with O
short O
inputs, O
prefix-tuning B-TaskName
attains O
comparable O
performance O
with O
fine-tuning O
and O
other O
parameter-efficient O
methods. O
On O
CoQA B-DatasetName
, O
however, O
prefix-tuning B-TaskName
has O
a O
substantially O
lower O
exact-match O
/ O
F1 B-MetricName
score O
than O
others O
(e.g., O
over O
7% B-MetricValue
decrease O
in O
F1 B-MetricName
score O
compared O
with O
fine-tuning). O
The O
similar O
pattern O
can O
be O
observed O
on O
the O
two O
NLU B-TaskName

tasks O
as O
well: O
the O
performance O
gap O
between O
prefix-tuning B-TaskName
and O
other O
candidate O
methods O
is O
much O
smaller O
on O
SST2 B-DatasetName
, O
whose O
mean O
sequence O
length O
is O
shorter O
than O
MNLI B-DatasetName
. O
We O
remark O
our O
proposed O
adaptive O
inducers O
somewhat O
resolve O
the O
issue: O
both O
variants O
of O
inducer-tuning B-DatasetName
in O
Table O
1 O
obtain O
a O
5%+ B-MetricValue
improvement O
on O
CoQA B-DatasetName

. O
Enhance O
inducer-tuning B-MethodName
through O
adapting O
pairwise O
positional O
interactions. O
In O
§ O
4.5, O
we O
speculate O
the O
prompt-based O
methods O
can O
benefit O
from O
adapting O
pairwise O
positional O
interactions, O
and O
we O
investigate O
it O
on O
both O
NLU B-TaskName
and O
NLG B-TaskName
tasks. O
With O
the O
same O
parameter O
budgets, O
the O
inducer-tuning B-MethodName
+ I-MethodName
LoRA I-MethodName
outperforms O
the O
pure O
inducer-tuning B-MethodName
on O
all O
tasks. O
The O

improvement O
is O
more O
evident O
in O
CoQA B-DatasetName
, O
the O
more O
challenging O
generation O
task O
with O
longer O
input O
sequences. O
We O
remark O
that O
inducer-tuning O
more O
effectively O
exploits O
the O
tunable O
parameters O
than O
LoRA-54 O
for O
the O
value O
part, O
as O
the O
combination O
variant O
also O
performs O
better O
than O
pure O
LoRA. O
Ablation O
Studies O
We O
perform O
ablation O
studies O
on O
generation O

tasks O
to O
analyze O
the O
efficacy O
of O
the O
different O
components O
in O
our O
proposed O
method. O
We O
recall O
there O
are O
four O
different O
features O
in O
inducer-tuning B-MethodName
compared O
to O
prefix-tuning B-TaskName
, O
including O
the O
usage O
of O
adaptive O
inducers, O
the O
extension O
of O
virtual O
value O
vectors, O
the O
residual O
form O
of O
P O
k O
, O
and O
the O
design O
for O
P O

v,i O
to O
concentrate O
around O
attention O
output. O
Accordingly, O
we O
implement O
three O
other O
variants O
of O
inducer-tuning O
to O
help O
ablate O
the O
effects O
of O
the O
above-mentioned O
components. O
Among O
them, O
Adaptive O
directly O
takes O
Q O
i O
as O
P O
k,i O
but O
still O
models O
P O
v,i O
as O
f O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
upon O

Adaptive, O
Extension O
changes O
P O
v,i O
tof O
(Q O
i O
) O
+ O
MLP O
v O
(Q O
i O
); O
compared O
to O
Extension, O
Inducer-tuning B-MethodName
just O
mod-ifies O
P O
k,i O
to O
Q O
i O
+ O
MLP O
k O
(Q O
i O
); O
to O
justify O
the O
design O
that O
P O
v,i O
centers O
around O
the O
attention O
output, O
Gating O
models O
P O
v,i O
simply O
as O

MLP O
v O
(Q O
i O
), O
and O
the O
new O
complete O
attention O
output O
thus O
becomes O
(1 O
− O
λ O
i O
)f O
(Q O
i O
) O
+ O
λ O
i O
MLP O
v O
(Q O
i O
). O
The O
concrete O
setting O
of O
each O
variant O
is O
deferred O
to O
Appendix O
B.4 O
due O
to O
limited O
space. O
The O
usage O
of O
adaptive B-TaskName
inducers I-TaskName
. O

To O
demonstrate O
the O
benefits O
of O
adaptive B-TaskName
inducers I-TaskName
, O
we O
compare O
Prefix-tuning-108 O
with O
the O
basic O
counterpart-Adaptive. O
Table O
3 O
shows O
Adaptive O
attains O
close O
performance O
to O
Prefix-tuning B-TaskName
-108 O
on O
WebNLG B-DatasetName
while O
obtaining O
a O
substantial O
improvement O
on O
CoQA B-DatasetName
, O
which O
has O
longer O
inputs. O
The O
extension O
of O
virtual O
value O
vectors. O
We O
observe O
an O
obvious O
improvement O

attributed O
to O
extending O
the O
scope O
of O
virtual O
value O
vectors O
by O
comparing O
the O
performance O
of O
Adaptive O
and O
Extension. O
For O
almost O
all O
the O
metrics, O
Extension O
obtains O
better O
performance O
than O
Adaptive, O
with O
the O
same O
number O
of O
tunable O
parameters. O
The O
residual O
form O
of O
P O
k O
. O
A O
natural O
design O
for O
P O
k O
is O
to O

directly O
model O
it O
as O
Q, O
which O
would O
automatically O
be O
the O
closest O
vectors O
to O
the O
ones O
in O
Q. O
To O
ablate O
the O
usage O
of O
MLP O
k O
, O
we O
compare O
Inducertuning B-MethodName
against O
Extension, O
which O
follows O
the O
natural O
design O
to O
model O
P O
k O
. O
Through O
the O
empirical O
results, O
we O
find O
assigning O
parameters O
to O
MLP O

k O
can O
still O
slightly O
help O
the O
performance O
of O
inducer-tuning B-MethodName
. O
P O
v,i O
centers O
aroundf O
(Q O
i O
). O
Lastly, O
to O
show O
the O
benefits O
of O
modeling O
P O
v,i O
as O
centering O
around O
f O
(Q O
i O
), O
we O
compare O
the O
variant O
Gating B-MethodName
against O
Inducer-tuning B-MethodName
. O
While O
Gating B-MethodName
has O
a O
weighted O
sum O
form O
similar O
to O

prefix-tuning B-TaskName
, O
it O
suffers O
from O
a O
great O
performance O
drop O
on O
both O
tasks, O
which O
justifies O
the O
effectiveness O
of O
our O
design O
for O
P O
v,i O
's. O
Ethics O
Statement O
As O
an O
efficient O
method O
for O
NLP, O
we O
consider O
our O
work O
to O
have O
a O
low O
ethical O
risk O
since O
the O
outcomes O
of O
the O
algorithm O
mainly O
depend O
on O

the O
downstream O
applications. O
The O
usage O
of O
the O
method O
would O
be O
the O
same O
as O
some O
previous O
methods, O
i.e., O
the O
practical O
deployment O
for O
some O
applications. O
Also, O
our O
method O
doesn't O
assume O
any O
specific O
structure O
of O
the O
input O
and O
thus O
doesn't O
leverage O
biases O
in O
the O
data. O
We O
conclude O
that O
our O
work O
will O
not O
likely O

have O
a O
negative O
ethical O
impact. O
A O
Dataset O
Details O
• O
The O
Multi-Genre B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
Corpus O
(Williams O
et O
al., O
2018, O
MNLI B-DatasetName
) O
involves O
433k O
sentence O
pairs O
of O
premises O
and O
hypotheses, O
labeled O
with O
textual O
entailment O
annotations. O
The O
premise O
sentences O
include O
ten O
distinct O
genres, O
and O
the O
classification O
can O
be O
performed O
on O
both O
the O

matched O
(in-domain) O
and O
mismatched O
(cross-domain) O
sections. O
Concatenating O
premises O
and O
hypothesis O
as O
the O
inputs, O
we O
obtain O
the O
sequence O
lengths O
are O
on O
average O
39.9 O
and O
max O
444. O
For O
the O
results O
reported O
in O
Table O
2, O
we O
follow O
Hu O
et O
al. O
( O
2021) O
and O
take O
mismatched O
accuracy O
as O
the O
metric. O
• O
The O
Stanford B-DatasetName
Sentiment I-DatasetName

Treebank I-DatasetName
(Socher O
et O
al., O
2013, O
SST2) O
is O
a O
corpus O
of O
movie O
reviews O
and O
human O
annotations O
of O
their O
sentiment. O
This O
task O
is O
incorporated O
into O
the O
GLUE B-DatasetName
benchmark O
(Wang O
et O
al., O
2019), O
and O
the O
dataset O
split O
assigns O
67k O
sentences O
to O
the O
training O
set O
and O
0.9k O
to O
the O
dev O
set. O
In O
SST2 B-DatasetName
, O

the O
sequence O
lengths O
are O
on O
average O
13.3 O
and O
max O
66, O
much O
shorter O
than O
in O
MNLI B-DatasetName
. O
As O
specified O
in O
the O
GLUE B-DatasetName
benchmark, O
we O
test O
the O
accuracy O
metric O
on O
whether O
the O
sentiment O
of O
a O
review O
sentence O
is O
positive O
or O
negative. O
• O
The O
instances O
in O
WebNLG B-DatasetName
dataset O
are O
the O
mapping O
set O
of O

RDF O
triples O
to O
text. O
They O
are O
Data/Text O
pairs, O
where O
the O
"Data" O
is O
in O
a O
format O
of O
(subject, O
property, O
object) O
triples. O
For O
the O
train O
and O
the O
validation O
set, O
they O
involve O
nine O
categories O
which O
are O
extracted O
from O
DBpedia B-DatasetName
; O
while O
in O
the O
test O
set, O
there O
are O
five O
extra O
unseen O
categories, O
which O
can O

partially O
reflect O
the O
generalization O
of O
the O
adaptation O
methods. O
The O
input O
sequences O
in O
the O
training O
set O
consist O
of O
1 O
to O
7 O
triples, O
and O
the O
lengths O
of O
most O
sequences O
are O
bounded O
by O
50 O
(as O
each O
triple O
only O
includes O
three O
short O
phrases). O
The O
official O
evaluation O
script O
is O
used O
in O
our O
experiments, O
and O
we O

report O
BLEU B-MetricName
(Papineni O
et O
al., O
2002), O
METEOR B-MetricName
, O
(Lavie O
and O
Agarwal, O
2007) O
and O
TER B-MetricName
(Snover O
et O
al., O
2006) O
as O
the O
metrics. O
• O
CoQA B-DatasetName
is O
a O
large-scale O
dataset, O
mainly O
for O
conversational O
question O
answering. O
It O
collects O
more O
than O
8K O
conversations O
over O
text O
passages, O
involving O
over O
127K O
questions O
with O
answers O
in O
5 O
domains. O

The O
average O
conversation O
length O
is O
15 O
turns O
(each O
turn O
consists O
of O
a O
question O
and O
an O
answer). O
The O
task O
requires O
the O
language O
model O
to O
generate O
answers O
to O
the O
given O
questions O
based O
on O
related O
conversation O
histories O
and O
documents O
in O
the O
dataset. O
The O
average O
passage O
length O
in O
CoQA B-DatasetName
is O
271 O
(Reddy O
et O
al., O

2019, O
Table O
3). O
We O
simply O
follow O
the O
evaluation O
script O
provided O
on O
the O
official O
website, O
reporting O
both O
the O
macro-average B-MetricName
F1 I-MetricName
score O
of O
word O
overlap O
and O
the O
exact-match O
metric O
(Reddy O
et O
al., O
2019). O
B O
Training O
Details O
We O
mainly O
implement O
our O
methods O
based O
on O
the O
GitHub O
repositories O
provided O
by O
Lin O
et O
al. O
(2020) O

and O
He O
et O
al. O
(2021a). O
Our O
code O
will O
be O
made O
public O
after O
the O
review O
procedure. O
B.1 O
General O
Training O
Settings O
For O
the O
NLU B-TaskName
tasks, O
we O
exactly O
follow O
the O
experimental O
setup O
used O
by O
He O
et O
al. O
(2021a), O
and O
more O
details O
can O
be O
found O
in O
Appendix O
B.2. O
For O
the O
two O
NLG B-TaskName
tasks, O
we O

mainly O
follow O
the O
experimental O
setting O
adopted O
by O
Lin O
et O
al. O
(2020), O
and O
specifically, O
keep O
using O
"task O
embeddings" O
in O
our O
experiments, O
as O
they O
are O
also O
applied O
in O
the O
original O
GPT-2 B-MethodName
model. O
These O
task O
embeddings O
are O
specialized O
segment O
embeddings O
used O
to O
indicate O
the O
different O
components O
of O
the O
text O
input O
(e.g., O
the O
three O

components O
of O
a O
triple O
in O
WebNLG B-DatasetName
, O
questions, O
and O
answers O
in O
CoQA B-DatasetName
, O
etc.). O
8 O
We O
list O
the O
task O
embedding O
used O
in O
each O
NLG B-TaskName
task: O
for O
CoQA B-DatasetName
, O
we O
follow O
the O
task O
embedding O
suggested O
by O
Lin O
et O
al. O
(2020); O
for O
WebNLG B-DatasetName
, O
we O
simply O
use O
the O
special O
tokens O
to O
indicate O

the O
different O
components O
in O
the O
triples. O
The O
details O
of O
the O
special O
tokens O
in O
each O
task O
are O
summarized O
in O
Table O
4. O
Notably, O
the O
parameter O
budget O
for O
task O
embedding O
is O
much O
smaller O
than O
the O
number O
of O
tunable O
parameters O
in O
the O
aforementioned O
parameter-efficient O
adaptation O
methods O
(around O
2M). O
B.2 O
Hyper-parameters O
for O
Training O
For O
NLU B-TaskName

tasks, O
we O
train O
the O
models O
with O
Adam O
(Kingma O
and O
Ba, O
2015) O
optimizer O
and O
use O
a O
polynomial O
learning O
rate O
scheduler O
to O
make O
the O
learning O
rate O
linearly O
decay; O
specifically, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
linearly O
warmed O
up O
from O
0 O
for O
the O
first O
6For O
NLG B-TaskName
tasks, O
an O
AdamW O
(Loshchilov O
and O
Hutter, O
2018) O
optimizer O
is O

applied O
to O
train O
the O
models, O
and O
a O
linear O
learning O
rate O
scheduler O
with O
a O
500-step B-HyperparameterValue
warmup B-HyperparameterName
duration I-HyperparameterName
is O
used. O
For O
the O
evaluation O
of O
NLG B-TaskName
tasks, O
we O
follow O
the O
script O
provided O
by O
Lin O
et O
al. O
(2020) O
to O
generate O
the O
texts O
through O
a O
greedy O
search O
for O
both O
WebNLG B-DatasetName
and O
CoQA B-DatasetName
. O
As O
for O

the O
number O
of O
epochs O
and O
the O
argument O
for O
weight O
decay, O
we O
mainly O
follow O
the O
setting O
used O
by O
Lin O
et O
al. O
(2020); O
Hu O
et O
al. O
( O
2021): O
for O
WebNLG B-DatasetName
, O
we O
train O
the O
model O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
; O
for O
CoQA B-DatasetName
, O
we O
train O
the O
model O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
. O
For O
the O

model-specific O
hyper-parameters, O
namely O
batch O
size O
(gradient O
accumulation O
is O
used O
if O
necessary) O
and O
learning O
rate, O
we O
decide O
them O
for O
different O
methods O
based O
on O
the O
loss O
on O
the O
validation O
set. O
For O
the O
proposed O
method O
inducer-tuning B-MethodName
with/without O
LoRA O
and O
MAM B-MethodName
inducer-tuning I-MethodName
in O
Table O
1, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.00125 B-HyperparameterValue
, O
and O

the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
for O
WebNLG B-DatasetName
; O
for O
CoQA B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
we O
use O
is O
0.001 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
16 B-HyperparameterValue
. O
On O
MNLI B-DatasetName
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
0.0002 B-HyperparameterValue
for O
both O
two O
methods O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
as O
32 B-HyperparameterValue
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName

and O
16 B-HyperparameterValue
for O
MAM B-MethodName
inducer-tuning I-MethodName
; O
on O
SST2 B-DatasetName
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
similarly O
set O
as O
0.0002 B-HyperparameterValue
, O
the O
batch B-HyperparameterName
size I-HyperparameterName
for O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
is O
16 B-HyperparameterValue
, O
and O
for O
MAM B-MethodName
inducer-tuning I-MethodName
64 B-HyperparameterValue
. O
To O
reduce O
the O
random O
variability O
in O
the O
results, O
all O
the O
methods O
reported O
are O
trained O
for O
multiple O

independent O
runs. O
In O
particular, O
for O
WebNLG B-DatasetName
, O
we O
train O
models O
over O
5 B-HyperparameterValue
runs B-HyperparameterName
, O
and O
for O
CoQA B-DatasetName
, O
MNLI B-DatasetName
, O
and O
SST2 B-DatasetName
3 B-HyperparameterValue
runs B-HyperparameterName
. O
The O
reported O
numbers O
in O
the O
cells O
in O
Tables O
1 O
and O
2 O
Rücklé O
et O
al. O
(2020). O
For O
the O
implementation O
of O
MLP O
v O
, O
we O
provide O
the O

exact O
expression O
for O
MLP O
(h) O
v O
(Q O
(h) O
) O
in O
head O
h O
as O
follows: O
σ O
Q O
(h) O
W O
(h) O
1 O
+ O
1(b O
(h) O
1 O
) O
T O
W O
(h) O
2 O
+ O
1b O
T O
2 O
, O
(11 O
) O
where O
σ O
is O
the O
activation O
function. O
As O
the O
superscript O
suggests, O
W O
h) O
1 O
∈ O

R O
p×r O
, O
b O
(h) O
1 O
∈ O
R O
r O
,( O
and O
W O
(h) O
2 O
∈ O
R O
r×N O
h O
p O
are O
specific O
to O
the O
head O
h, O
while O
b O
2 O
∈ O
R O
N O
h O
p O
are O
shared O
among O
all O
the O
heads, O
which O
is O
the O
same O
case O
as O
in O
Equation O
( O
4) O
(in O

the O
original O
attention O
sub-layer, O
the O
bias O
term O
b O
o O
applies O
to O
all O
the O
heads O
as O
well). O
B.4 O
Specific O
settings O
for O
baseline O
methods O
In O
this O
subsection, O
we O
provide O
the O
detailed O
setting O
for O
the O
methods O
in O
Tables O
1, O
2, O
and O
3 O
that O
need O
further O
specification. O
In O
Table O
1, O
the O
settings O
for O
Adapter-108 O

and O
Prefix-tuning-108 O
are O
clear, O
as O
the O
only O
arguments O
are O
the O
bottleneck O
size O
/ O
prefix O
length; O
for O
LoRA-54, O
we O
apply O
rank-54 O
updates O
for O
both O
W O
q O
and O
W O
v O
, O
as O
suggested O
by O
Hu O
et O
al. O
(2021); O
for O
MAM O
adapter, O
we O
mimic O
the O
parameter O
assignment O
scheme O
( O
bottleneck B-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
for O

FFN O
and O
prefix B-HyperparameterName
length I-HyperparameterName
30 B-HyperparameterValue
) O
by O
He O
et O
al. O
(2021a), O
and O
use O
the O
ratio O
102 O
: O
6 O
to O
implement O
MAM O
adapters O
with O
1.61% O
tunable O
parameters. O
For O
the O
variants O
of O
inducer O
tuning, O
their O
settings O
are O
summarized O
in O
Table O
5. O
In O
this O
table, O
the O
numbers O
in O
column O
MLP O
k O
and O
MLP O

v O
are O
the O
bottleneck O
sizes O
used O
for O
computing O
P O
k O
and O
P O
v O
; O
notice O
for O
Adaptive, O
the O
scope O
of O
virtual O
value O
tokens O
is O
not O
extended O
and O
thus O
has O
a O
larger O
bottleneck O
size O
than O
others. O
(Recall O
for O
MLP O
v O
, O
the O
size O
of O
W O
(h) O
2 O
in O
Equation O
( O
11) O

is O
larger O
than O
the O
counterparts O
in O
MLP O
v O
. O
For O
the O
numbers O
in O
column O
LoRA, O
they O
are O
the O
rank O
of O
the O
update O
used O
in O
the O
LoRA O
component O
to O
adjust O
W O
q O
; O
only O
for O
our O
proposed O
method O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
, O
the O
number O
will O
be O
nonzero. O
C O
Attention O
as O
Kernels O

To O
justify O
the O
claim O
that O
attention O
is O
a O
kernel O
operation, O
we O
construct O
a O
Nadaraya-Watson O
kernel O
estimator O
(Wasserman, O
2006, O
Definition O
5.39) O
of O
a O
query O
vector O
Q O
i O
(taking O
{K O
j O
} O
n O
j=1 O
as O
the O
supporting O
points) O
as O
follows: O
f O
(Q O
i O
) O
= O
n O
j=1 O
ℓ O
j O
(Q O
i O
)C O

j O
,(12) O
where O
ℓ O
j O
(Q O
i O
) O
:= O
κ(Q O
i O
, O
K O
j O
) O
n O
k=1 O
κ(Q O
i O
, O
K O
j O
) O
. O
κ(•, O
•) O
is O
a O
kernel O
function, O
and O
C O
j O
's O
are O
the O
coefficients O
corresponding O
to O
the O
rows O
V O
j O
's O
in O
the O
value O
matrix O
V O
. O

Take O
kernel O
function O
κ(x, O
y) O
= O
exp O
⟨x, O
y⟩ O
/ O
√ O
p O
. O
We O
slightly O
abuse O
the O
notation O
κ(Q, O
K) O
to O
represent O
the O
n-by-n O
empirical O
kernel O
matrix O
M O
, O
in O
which O
the O
i-th O
row O
and O
the O
j-th O
column O
is O
κ(Q O
i O
, O
K O
j O
), O
∀i O
∈ O
[n], O
j O
∈ O

[N O
]. O
With O
these O
notations, O
the O
output O
of O
the O
kernel O
estimator O
will O
be, O
D O
−1 O
M O
C, O
(13 O
) O
where O
D O
is O
a O
diagonal O
matrix O
serving O
as O
the O
row O
normalization O
in O
Equation O
( O
12), O
and O
C O
is O
an O
nby-p O
matrix O
with O
C O
j O
as O
its O
j-th O
row. O
We O
observe O
an O

obvious O
correspondence O
between O
Equation O
( O
13) O
and O
the O
standard O
attention O
in O
Equation O
(3). O
The O
correspondence O
implies O
a O
finer O
division O
of O
the O
attention O
module: O
the O
empirical O
kernel O
matrix O
M O
(D O
is O
decided O
by O
κ(Q, O
K)) O
and O
the O
value O
part O
C. O
(In O
Section O
4.4, O
we O
show O
that O
C O
includes O
but O
is O
not O

limited O
to O
the O
value O
matrix O
in O
attention.) O
D O
Example O
We O
provide O
an O
example O
answer O
generated O
by O
finetuning O
and O
our O
inducer-tuning B-MethodName
with I-MethodName
LoRA I-MethodName
on O
CoQA B-DatasetName
in O
Table O
6. O
Acknowledgements O
We O
appreciate O
all O
the O
valuable O
feedback O
from O
the O
anonymous O
reviewers. O

Unsupervised O
Boundary-Aware O
Language O
Model O
Pretraining O
for O
Chinese B-TaskName
Sequence I-TaskName
Labeling I-TaskName
Chinese O
language O
processing O
tasks, O
such O
as O
word O
segmentation, O
part-of-speech O
tagging, O
and O
named O
entity O
recognition. O
Previous O
studies O
usually O
resorted O
to O
the O
use O
of O
a O
high-quality O
external O
lexicon, O
where O
lexicon O
items O
can O
offer O
explicit O
boundary O
information. O
However, O
to O
ensure O
the O
quality O
of O
the O

lexicon, O
great O
human O
effort O
is O
always O
necessary, O
which O
has O
been O
generally O
ignored. O
In O
this O
work, O
we O
suggest O
unsupervised O
statistical O
boundary O
information O
instead, O
and O
propose O
an O
architecture O
to O
encode O
the O
information O
directly O
into O
pre-trained O
language O
models, O
resulting O
in O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
). O
We O
apply O
BABERT B-MethodName
for O
feature O
induction O
of O
Chinese B-TaskName

sequence I-TaskName
labeling I-TaskName
tasks. O
Experimental O
results O
on O
ten O
benchmarks O
of O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
demonstrate O
that O
BABERT B-MethodName
can O
provide O
consistent O
improvements O
on O
all O
datasets. O
In O
addition, O
our O
method O
can O
complement O
previous O
supervised O
lexicon O
exploration, O
where O
further O
improvements O
can O
be O
achieved O
when O
integrated O
with O
external O
lexicon O
information. O
Introduction O
The O
representative O
sequence O
labeling O
tasks O

for O
the O
Chinese O
language, O
such O
as O
word B-TaskName
segmentation I-TaskName
, O
partof-speech B-TaskName
(POS) I-TaskName
tagging I-TaskName
and O
named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
(Emerson, O
2005;Jin O
and O
Chen, O
2008), O
have O
been O
inclined O
to O
be O
performed O
at O
the O
characterlevel O
in O
an O
end-to-end O
manner O
(Shen O
et O
al., O
2016). O
The O
paradigm, O
naturally, O
is O
standard O
to O
Chinese B-TaskName
word I-TaskName
segmentation I-TaskName

( O
CWS B-TaskName
), O
while O
for O
Chinese B-TaskName
POS I-TaskName
tagging I-TaskName
and O
NER B-TaskName
, O
it O
can O
better O
help O
reduce O
the O
error O
propagation O
(Sun O
and O
Uszkoreit, O
2012;Yang O
et O
al., O
2016;Liu O
et O
al., O
2019a) O
compared O
with O
word-based O
counterparts O
by O
straightforward O
modeling. O
Recently, O
all O
the O
above O
tasks O
have O
reached O
stateof-the-art O
performances O
with O
the O
help O
of O
BERTlike O

pre-trained O
language O
models O
(Yan O
et O
al., O
2019;Meng O
et O
al., O
2019). O
The O
BERT O
variants, O
such O
as O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021), O
ERNIE B-MethodName
, O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
NEZHA B-MethodName
, O
etc., O
further O
improve O
the O
vanilla O
BERT O
by O
either O
using O
external O
knowledge O
or O
larger-scale O
training O
corpus. O
The O
improvements O
can O
also O
benefit O
character-level B-TaskName

Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Notably, O
since O
the O
output O
tags O
of O
all O
these O
character-level B-TaskName
Chinese I-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
involve O
identifying O
Chinese O
words O
or O
entities O
(Zhang O
and O
Yang, O
2018;Yang O
et O
al., O
2019), O
prior O
boundary O
knowledge O
could O
be O
highly O
helpful O
for O
them. O
A O
number O
of O
studies O
propose O
the O
integration O
of O
an O
external O
lexicon O

to O
enhance O
their O
baseline O
models O
by O
feature O
representation O
learning O
(Jia O
et O
al., O
2020;Tian O
et O
al., O
2020a;. O
Moreover, O
some O
works O
suggest O
injecting O
similar O
resources O
into O
the O
pre-trained O
BERT O
weights. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
the O
representatives, O
which O
leverage O
an O
external O
lexicon O
for O
masked O
word O
prediction O
in O
Chinese O
BERT. O

The O
lexicon-based O
methods O
have O
indeed O
achieved O
great O
success O
for O
boundary O
integration. O
However, O
there O
are O
two O
major O
drawbacks. O
First, O
the O
lexicon O
resources O
are O
always O
constructed O
manually O
(Zhang O
and O
Yang, O
2018;Diao O
et O
al., O
2020;Jia O
et O
al., O
2020;, O
which O
is O
expensive O
and O
time-consuming. O
The O
quality O
of O
the O
lexicon O
is O
critical O
to O
our O
tasks. O

Second, O
different O
tasks O
as O
well O
as O
different O
domains O
require O
different O
lexicons O
(Jia O
et O
al., O
2020;. O
A O
well-studied O
lexicon O
for O
word O
segmentation O
might O
be O
inappropriate O
for O
NER, O
and O
a O
lexicon O
for O
news O
NER O
might O
also O
be O
problematic O
for O
finance O
NER. O
The O
two O
drawbacks O
can O
be O
due O
to O
the O
supervised O
characteristic O
of O

these O
lexicon-based O
enhancements. O
Thus, O
it O
is O
more O
desirable O
to O
offer O
boundary O
information O
in O
an O
unsupervised O
manner. O
In O
this O
paper, O
we O
propose O
an O
unsupervised O
Boundary-Aware B-MethodName
BERT I-MethodName
( O
BABERT B-MethodName
), O
which O
is O
achieved O
by O
fully O
exploring O
the O
potential O
of O
statisti-cal O
features O
mined O
from O
a O
large-scale O
raw O
corpus. O
We O
extract O
a O
set O
of O

N-grams O
(a O
predefined O
fixed O
N) O
no O
matter O
they O
are O
valid O
words O
or O
entities, O
and O
then O
calculate O
their O
corresponding O
unsupervised O
statistical O
features, O
which O
are O
mostly O
related O
to O
boundary O
information. O
We O
inject O
the O
boundary O
information O
into O
the O
internal O
layer O
of O
a O
pre-trained O
BERT, O
so O
that O
our O
final O
BABERT B-MethodName
model O
can O
approximate O
the O

boundary O
knowledge O
softly O
by O
using O
inside O
representations. O
The O
BABERT B-MethodName
model O
has O
no O
difference O
from O
the O
original O
BERT, O
so O
that O
we O
can O
use O
it O
in O
the O
same O
way O
as O
the O
standard O
BERT O
exploration. O
We O
conduct O
experiments O
on O
three O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks O
to O
demonstrate O
the O
effectiveness O
of O
our O
proposed O
method. O
Experimental O

results O
show O
that O
our O
approach O
can O
significantly O
outperform O
other O
Chinese O
pre-trained O
language O
models. O
In O
addition, O
compared O
with O
supervised O
lexicon-based O
methods, O
BABERT B-MethodName
obtains O
competitive O
results O
on O
all O
tasks O
and O
achieves O
further O
improvements O
when O
integrated O
with O
external O
lexicon O
knowledge. O
We O
also O
conduct O
extensive O
analyses O
to O
understand O
our O
method O
comprehensively. O
The O
pre-trained O
model O

and O
code O
are O
publicly O
available O
at O
http://github.com/modelscope/ O
adaseq/examples/babert. O
Our O
contributions O
in O
this O
paper O
include O
the O
following: O
1) O
We O
design O
a O
method O
to O
encode O
unsupervised O
statistical O
boundary O
information O
into O
boundary-aware O
representation, O
2) O
propose O
a O
new O
pre-trained O
language O
model O
called O
BABERT B-MethodName
as O
a O
boundary-aware O
extension O
for O
BERT, O
3) O
verify O
BABERT B-MethodName
on O
ten O

benchmark O
datasets O
of O
three O
Chinese O
sequence O
labeling O
tasks. O
Related O
Work O
In O
the O
past O
decades, O
machine O
learning O
has O
achieved O
good O
performance O
on O
sequence O
labeling O
tasks O
with O
statistical O
information O
(Bellegarda, O
2004;Low O
et O
al., O
2005;Bouma, O
2009). O
Recently, O
neural O
models O
have O
led O
to O
state-of-the-art O
results O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Lample O
et O
al., O
2016;Ma O
and O

Hovy, O
2016;Chiu O
and O
Nichols, O
2016). O
In O
addition, O
the O
presence O
of O
language O
representation O
models O
such O
as O
BERT O
(Devlin O
et O
al., O
2019) O
has O
led O
to O
impressive O
improvements. O
In O
particular, O
many O
variants O
of O
BERT O
are O
devoted O
to O
integrating O
boundary O
information O
into O
BERT O
to O
improve O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
(Diao O
et O
al., O
2020;Jia O
et O
al., O

2020;. O
Statistical O
Machine O
Learning O
Statistical O
information O
is O
critical O
for O
sequence O
labeling. O
Previous O
works O
attempt O
to O
count O
such O
information O
from O
large O
corpora O
in O
order O
to O
combine O
it O
with O
machine O
learning O
methods O
for O
sequence O
labeling O
(Bellegarda, O
2004;Liang, O
2005;Bouma, O
2009). O
Peng O
et O
al. O
(2004) O
attempts O
to O
conduct O
sequence O
labeling O
by O
CRF O
and O
a O

statistical-based O
new O
word O
discovery O
method. O
Low O
et O
al. O
(2005) O
introduce O
a O
maximum O
entropy O
approach O
for O
sequence O
labeling. O
Liang O
(2005) O
utilizes O
unsupervised O
statistical O
information O
in O
Markov O
models, O
and O
gets O
a O
boost O
on O
Chinese O
NER O
and O
CWS. O
Pre-trained O
Language O
Model O
Pre-trained O
language O
model O
is O
a O
hot O
topic O
in O
natural O
language O
processing O
(NLP) O

communities O
(Devlin O
et O
al., O
2019;Liu O
et O
al., O
2019b;Clark O
et O
al., O
2020;Diao O
et O
al., O
2020; O
and O
has O
been O
extensively O
studied O
for O
Chinese O
sequence O
labeling. O
For O
instance, O
TENER O
(Yan O
et O
al., O
2019) O
adopts O
Transformer O
encoder O
to O
model O
characterlevel O
features O
for O
Chinese O
NER. O
Glyce O
(Meng O
et O
al., O
2019) O
uses O
BERT O
to O
capture O
the O

contextual O
representation O
combined O
with O
glyph O
embeddings O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Lexicon-based O
Methods O
In O
recent O
studies, O
lexicon O
knowledge O
has O
been O
applied O
to O
improve O
model O
performance. O
There O
are O
two O
mainstream O
categories O
to O
the O
work O
of O
lexicon O
enhancement. O
The O
first O
aims O
to O
enhance O
the O
original O
BERT O
with O
implicit O
boundary O
information O
by O
using O

the O
multi-granularity O
word O
masking O
mechanism. O
BERT-wwm B-MethodName
(Cui O
et O
al., O
2021) O
and O
ERNIE B-MethodName
are O
representatives O
of O
this O
category, O
which O
propose O
to O
mask O
tokens, O
entities, O
and O
phrases O
as O
the O
mask O
units O
in O
the O
masked O
language O
modeling O
(MLM) O
task O
to O
learn O
the O
coarse-grained O
lexicon O
information O
during O
pre-training. O
ERNIE-Gram B-MethodName
, O
an O
extension O
of O
ERNIE, O

utilizes O
statistical O
boundary O
information O
for O
unsupervised O
word O
extraction O
to O
support O
masked O
word O
prediction, O
The O
second O
category, O
which O
includes O
ZEN B-MethodName
(Diao O
et O
al., O
2020), O
EEBERT B-MethodName
(Jia O
et O
al., O
2020), O
and O
LEBERT B-MethodName
, O
exploits O
the O
potential O
of O
directly O
injecting O
lexicon O
information O
into O
BERT O
via O
extra O
modules, O
leading O
to O
better O
performance O
but O
is O

limited O
in O
predefined O
external O
knowledge. O
Our O
work O
follows O
the O
first O
line O
of O
work, O
most O
similar O
to O
ERNIE-Gram B-MethodName
. O
However, O
different O
from O
ERNIE-Gram B-MethodName
, O
we O
do O
not O
discretize O
the O
real-valued O
statistical O
information O
ex- O
tracted O
from O
corpus, O
but O
adopt O
a O
regression O
manner O
to O
leverage O
the O
information O
fully. O
PMI O
LRE O
1 O
2 O
… O

… O
−1 O
MLM O
Loss O
ℒ O
MLM O
MSE O
Loss O
ℒ O
BA O
(c). O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Input O
Sentence O
Raw O
Corpus O
N-gram O
Statistical O
Dictionary O
Contextual O
N-gram O
Sets O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
• O
N-gram O
Set O
of O
N-gram O
Set O
of O
+1 O
… O
+ O
−1 O
• O
• O
• O
• O

• O
• O
−1 O
− O
/2 O
… O
+ O
/2 O
• O
• O
• O
• O
• O
• O
+1 O
+2 O
− O
+1 O
… O
−1 O
+1 O
N-gram O
Set O
1 O
of O
1 O
Method O
Figure O
1 O
shows O
the O
overall O
architecture O
of O
our O
unsupervised O
boundary-aware O
pre-trained O
language O
model, O
which O
mainly O
consists O
of O
three O
components: O
1) O
boundary O
information O
extractor O

for O
unsupervised O
statistical O
boundary O
information O
mining, O
2) O
boundary-aware O
representation O
to O
integrate O
statistical O
information O
at O
the O
character-level, O
and O
3) O
boundary-aware B-MethodName
BERT I-MethodName
learning O
which O
injects O
boundary O
knowledge O
into O
the O
internal O
layer O
of O
BERT. O
In O
this O
section, O
we O
first O
focus O
on O
the O
details O
of O
the O
above O
components, O
and O
then O
introduce O
the O
fine-tuning O
method O

for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Boundary O
Information O
Extractor O
Statistical O
boundary O
information O
has O
been O
shown O
with O
a O
positive O
influence O
on O
a O
variety O
of O
Chinese O
NLP O
tasks O
(Song O
and O
Xia, O
2012;Higashiyama O
et O
al., O
2019;Ding O
et O
al., O
2020;. O
We O
follow O
this O
line O
of O
work, O
designing O
a O
boundary O
information O
extractor O
to O
mine O
statistical O
information O

from O
a O
large O
raw O
corpus O
in O
an O
unsupervised O
way. O
The O
overall O
flow O
of O
the O
extractor O
includes O
two O
steps: O
I) O
First, O
we O
collect O
all O
N-grams O
from O
the O
raw O
corpus O
to O
build O
a O
dictionary O
N O
, O
in O
which O
we O
count O
the O
frequencies O
of O
each O
N-gram O
and O
filter O
out O
the O
low O
frequencies O
items; O

II) O
second, O
considering O
that O
word O
frequency O
is O
insufficient O
for O
representing O
the O
flexible O
boundary O
relation O
in O
the O
Chinese O
context, O
we O
further O
compute O
two O
unsupervised O
indicators O
which O
can O
capture O
most O
of O
the O
boundary O
information O
in O
the O
corpus. O
In O
the O
following, O
we O
will O
describe O
these O
two O
indicators O
in O
detail. O
Pointwise O
Mutual O
Information O
(PMI) O

Given O
an O
N-gram, O
we O
split O
it O
into O
two O
sub-strings O
and O
compute O
the O
mutual O
information O
(MI) O
between O
them O
as O
a O
candidate. O
Then, O
we O
enumerate O
all O
sub-string O
pairs O
and O
choose O
the O
minimum O
MI O
as O
the O
overall O
PMI O
to O
estimate O
the O
tightness O
of O
the O
N-gram. O
Let O
g O
= O
{c O
1 O
...c O
m O
} O

be O
an O
N-gram O
that O
consists O
of O
m O
characters, O
we O
calculate O
PMI O
using O
this O
formula: O
PMI(g) O
= O
min O
i∈[1:m−1] O
{ O
p(g) O
p(c1...ci) O
• O
p(ci+1...cm) O
}, O
(1) O
where O
p(•) O
denotes O
the O
probability O
over O
the O
corpus. O
Note O
that, O
when O
m O
= O
1, O
the O
corresponding O
PMI O
is O
constantly O
equal O
to O
1. O
The O
higher O
PMI O

indicates O
that O
the O
N-gram O
(e.g., O
"贝克汉姆 O
(Beckham)") O
has O
a O
similar O
occurrence O
probability O
to O
the O
sub-string O
pair O
(e.g., O
"贝克 O
(Beck)" O
and O
"汉姆 O
(Ham)"), O
leading O
to O
a O
higher O
association O
between O
internal O
sub-string O
pairs, O
which O
makes O
the O
N-gram O
more O
likely O
to O
be O
a O
word/entity. O
In O
contrast, O
a O
lower O
PMI O
means O
the O
Ngram O
(e.g., O

"克汉(Kehan)") O
is O
possibly O
an O
invalid O
word/entity. O
Left O
and O
Right O
Entropy O
(LRE) O
Given O
an O
Ngram O
g, O
we O
first O
collect O
a O
left-adjacent O
character O
set O
S O
l O
m O
= O
{c O
l O
1 O
, O
..., O
c O
l O
n O
l O
} O
with O
n O
l O
characters. O
Then, O
we O
utilize O
the O
conditional O
probability O
between O
g O
and O
its O

left O
adjacent O
characters O
in O
S O
l O
m O
to O
compute O
the O
left O
entropy O
(LE), O
which O
measures O
sufficient O
boundary O
information. O
LE O
can O
be O
defined O
as: O
LE(g) O
= O
− O
n O
l O
i O
p(c O
l O
i O
g|g) O
log O
p(c O
l O
i O
g|g).(2) O
Similar O
to O
LE, O
we O
further O
collect O
a O
right O
adjacent O
set O
S O
r O

m O
= O
{c O
r O
1 O
, O
..., O
c O
r O
nr O
} O
with O
n O
r O
characters O
to O
calculate O
the O
right O
entropy O
(RE) O
for O
the O
N-gram O
g: O
RE(g) O
= O
− O
nr O
i O
p(gc O
r O
i O
|g) O
log O
p(gc O
r O
i O
|g).(3) O
Intuitively, O
LRE O
represents O
the O
abundance O
of O
neighboring O
characters O
for O
the O
N-gram. O

With O
a O
lower O
LRE, O
the O
N-gram O
(e.g., O
"汉姆 O
") O
has O
a O
more O
fixed O
context, O
indicating O
it O
is O
more O
likely O
to O
be O
a O
part O
of O
a O
phrase O
or O
entity. O
Conversely, O
the O
N-gram O
with O
a O
higher O
LRE O
(e.g., O
"贝克汉姆") O
will O
interact O
more O
with O
context, O
which O
prefers O
to O
be O
an O
independent O
word O
or O

phrase. O
Finally, O
we O
utilize O
PMI O
and O
LRE O
to O
measure O
the O
flexible O
boundary O
relations O
in O
the O
Chinese O
context, O
and O
then O
update O
each O
N-gram O
in O
N O
with O
the O
unsupervised O
statistical O
indicators O
above. O
Boundary-Aware O
Representation O
By O
using O
the O
boundary O
information O
extractor, O
we O
can O
obtain O
an O
N-gram O
dictionary O
N O
with O
unsupervised O
statistical O
boundary O
information. O

Unfortunately, O
since O
the O
context O
independence O
and O
the O
high O
relevance O
to O
N-gram, O
previous O
works O
(Ding O
et O
al., O
2020; O
use O
such O
statistical O
features O
for O
word O
extraction O
only, O
which O
ignore O
the O
potential O
of O
statistical O
boundary O
information O
in O
representation O
learning. O
To O
alleviate O
this O
problem, O
we O
propose O
boundary-aware O
representation, O
a O
highly O
extensible O
method, O
to O
fully O

benefit O
from O
the O
statistical O
boundary O
information O
for O
representation O
learning. O
To O
achieve O
boundary-aware O
representation, O
we O
first O
build O
contextual O
N-gram O
sets O
from O
the O
sentence. O
As O
shown O
in O
Figure O
1 O
(b), O
given O
a O
sentence O
x O
= O
{c O
1 O
, O
c O
2 O
, O
..., O
c O
n O
} O
with O
n O
characters O
and O
the O
maximum O
N-gram O

length O
N O
, O
we O
extract O
all O
N-grams O
that O
include O
c O
i O
as O
the O
contextual O
N-gram O
set O
S O
c O
i O
= O
{c O
i O
, O
c O
i O
c O
i+1 O
, O
• O
• O
• O
, O
c O
i−N O
+1 O
...c O
i O
} O
for O
char- O
acter O
c O
i O
. O
Then, O
we O
design O
a O
composition O
method O

to O
integrate O
the O
statistical O
features O
of O
N-grams O
in O
S O
c O
i O
by O
using O
specific O
conditions O
and O
rules, O
aiming O
to O
avoid O
the O
sparsity O
and O
contextual O
independence O
limitations O
of O
statistical O
information. O
Concretely, O
we O
divide O
the O
information O
composition O
method O
into O
PMI O
and O
entropy O
representation. O
First, O
we O
concatenate O
the O
PMI O
of O
all O
N-grams O
in O

S O
c O
i O
to O
generate O
PMI O
representation: O
e O
p O
i O
=PMI( O
ci O
) O
⊕PMI( O
ci O
ci+1) O
⊕ O
PMI(ci−1 O
ci O
) O
⊕PMI( O
ci O
ci+1ci+2) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−2ci−1 O
ci O
) O
• O
• O
• O
• O
• O
• O
⊕PMI( O
ci O
...ci+N−1) O
⊕ O
• O
• O
• O
⊕ O
PMI(ci−N+1... O
ci O
),(4) O
where O
e O

p O
i O
∈ O
R O
a O
, O
and O
a O
= O
1+2+• O
• O
•+N O
is O
the O
number O
of O
the O
N-grams O
that O
contain O
c O
i O
. O
Note O
that O
the O
position O
of O
each O
N-gram O
is O
fixed O
in O
PMI O
representation. O
We O
strictly O
follow O
the O
order O
of O
N-gram O
length O
and O
the O
position O
of O
c O
i O
in O

N-gram O
to O
concatenate O
their O
corresponding O
PMI, O
ensuring O
that O
the O
position O
and O
context O
information O
can O
be O
encoded O
into O
e O
p O
i O
. O
Entropy O
representation O
focuses O
on O
the O
contextual O
interactions O
of O
each O
character. O
When O
c O
i O
is O
the O
border O
of O
N-grams O
in O
S O
c O
i O
, O
we O
separately O
aggregate O
the O
LE O
and O

RE O
as O
left O
and O
right O
entropy O
representation: O
e O
le O
i O
=LE( O
ci O
) O
⊕ O
LE( O
ci O
ci+1) O
⊕ O
• O
• O
• O
⊕ O
LE( O
ci O
...ci+N−1), O
e O
re O
i O
=RE( O
ci O
) O
⊕ O
RE(ci−1 O
ci O
) O
⊕ O
• O
• O
• O
⊕ O
RE(ci−N+1... O
ci O
),(5) O
where O
e O
le O
i O
∈ O
R O
b O

, O
e O
re O
i O
∈ O
R O
b O
, O
and O
b O
= O
N O
1 O
is O
the O
number O
of O
integrated O
N-grams. O
Similar O
to O
PMI O
representation, O
the O
position O
of O
each O
N-gram O
in O
e O
le O
i O
and O
e O
le O
i O
is O
fixed O
and O
symmetric. O
Therefore, O
the O
boundary-aware O
representation O
e O
i O
of O
c O
i O
can O

be O
formalized O
as: O
e O
i O
= O
e O
le O
i O
⊕ O
e O
p O
i O
⊕ O
e O
re O
i O
,(6) O
where O
e O
i O
∈ O
R O
a+2b O
. O
Finally, O
by O
composing O
multigranularity O
statistical O
boundary O
information O
in O
a O
specific O
order, O
we O
are O
able O
to O
obtain O
the O
boundaryaware O
representation, O
which O
explicitly O
contains O
the O
boundary O
and O

context O
information. O
Figure O
2 O
shows O
an O
example O
of O
the O
boundaryaware O
representation. O
Given O
a O
sentence O
"南 O
京 O
市 O
长 O
江 O
大 O
桥 O
(Nanjing O
Yangtze O
River O
Bridge)" O
and O
a O
maximum O
N-gram O
length O
N O
= O
3, O
we O
first O
build O
a O
contextual O
N-gram O
set O
for O
the O
character O
"长 O
(Long)". O
Then, O
we O
integrate O
the O
PMI O

of O
all O
N-grams O
in O
a O
specific O
order O
(from O
N-gram O
"长" O
to O
"京市长 O
(Mayor O
of O
Jing)") O
to O
compute O
PMI O
representation. O
Furthermore, O
left O
and O
right O
entropy O
representations O
are O
also O
calculated O
in O
a O
particular O
order O
(from O
Ngram O
"长" O
to O
"长江大 O
(Yangtze O
River O
Big)" O
and O
"京市长", O
respectively). O
Finally, O
we O
concatenate O
the O
above O
features O
to O

produce O
the O
overall O
boundaryaware O
representation O
of O
the O
character O
"长". O
Contextual O
N-grams O
Set O
of O
"长" O
长 O
Long O
长江 O
Yangtze O
River O
+1 O
市长 O
Mayor O
−1 O
京市长 O
Mayor O
of O
Jing O
−2 O
−1 O
长江大 O
Yangtze O
River O
Big O
+1 O
+2 O
PMI O
Representation O
市长江 O
Mayor O
Jiang O
−1 O
+1 O
RE O
Representation O
LE O
Representation O
• O
• O
• O
• O

• O
• O
• O
• O
• O
• O
• O
• O
⊕ O
Boundary-Aware O
Representation O
Boundary-Aware B-MethodName
BERT I-MethodName
Learning O
Boundary-aware B-MethodName
BERT I-MethodName
is O
a O
variant O
of O
BERT, O
enhanced O
with O
boundary O
information O
simply O
and O
effectively. O
In O
this O
subsection, O
we O
describe O
how O
the O
boundary O
information O
can O
be O
integrated O
into O
BERT O
during O
pre-training O
by O
boundary-aware O
learning. O
Boundary-Aware O
Objective O
As O

mentioned O
in O
Section O
3.2, O
given O
a O
sentence O
x O
with O
characterlength O
n, O
we O
can O
compute O
the O
corresponding O
boundary-aware O
representation O
E O
= O
{e O
1 O
, O
..., O
e O
n O
}. O
Then, O
we O
transfer O
the O
BERT O
feature O
into O
the O
boundary O
information O
space O
and O
approximate O
it O
to O
E O
for O
boundary-aware O
learning. O
Moreover, O
shows O
that O
encoding O

basic O
lexical O
knowledge O
in O
the O
shallow O
BERT O
layers O
is O
a O
more O
effective O
approach. O
Hence, O
we O
use O
the O
hidden O
features O
H O
l O
= O
{h O
l O
1 O
, O
..., O
h O
l O
n O
} O
of O
the O
l-th O
shallow O
layer O
to O
achieve O
the O
boundary-aware O
objective: O
L O
BA O
= O
n O
i O
MSE(W O
B O
h O
l O

i O
, O
e O
i O
),(7) O
where O
MSE(•) O
denotes O
the O
mean O
square O
error O
loss. O
W O
B O
is O
a O
trainable O
matrix O
used O
to O
project O
BERT O
representation O
into O
boundary O
information O
space. O
Previous O
classification-based O
word-level O
masking O
methods O
use O
statistical O
information O
as O
thresholds O
to O
filter O
valid O
words O
for O
masked O
word O
prediction. O
Unlike O
the O
above O
works, O

we O
softly O
utilize O
such O
information O
in O
a O
regression O
manner, O
avoiding O
possible O
errors O
in O
empirically O
filtering O
valid O
tags, O
thereby O
fully O
exploring O
the O
potential O
of O
this O
information. O
Jia O
et O
al. O
(2020) O
and O
Gao O
and O
Callan O
(2021), O
we O
opt O
to O
initialize O
our O
model O
with O
a O
pre-trained O
BERT O
model O
released O
by O
Google O
2 O
and O

randomly O
initialize O
the O
other O
parameters, O
alleviating O
the O
enormous O
cost O
of O
train-ing O
BABERT B-MethodName
from O
scratch. O
In O
particular, O
we O
discard O
the O
next O
sentence O
prediction O
task O
during O
pretraining, O
which O
is O
confirmed O
to O
be O
not O
essential O
for O
the O
pre-trained O
language O
models O
(Lan O
et O
al., O
2020;Liu O
et O
al., O
2019b). O
The O
total O
pre-training O
loss O
of O
BABERT B-MethodName

can O
be O
formalized O
as: O
Pre-training O
Following O
L O
pre O
= O
L O
MLM O
+ O
L O
BA O
,(8) O
where O
L O
MLM O
is O
the O
standard O
objective O
of O
MLM O
task. O
Fine-tuning O
for O
Sequence O
Labeling O
Straightforward O
Fine-tuning O
As O
shown O
in O
Figure O
1 O
(c), O
because O
BABERT B-MethodName
has O
the O
same O
architecture O
as O
BERT, O
we O
can O
adopt O
the O
identical O

procedure O
that O
BERT O
uses O
for O
fine-tuning, O
where O
the O
output O
of O
BABERT O
can O
be O
used O
as O
the O
contextual O
character O
representation O
for O
sequence O
labeling. O
Concretely, O
given O
a O
sequence O
labeling O
dataset O
D O
= O
{(x O
j O
, O
y O
j O
)} O
N O
j=1 O
, O
where O
y O
j O
is O
the O
label O
sequence O
of O
x O
j O
, O

we O
utilize O
the O
output O
of O
BABERT B-MethodName
and O
a O
CRF O
layer O
to O
calculate O
the O
sentence-level O
output O
probability O
p(y O
j O
|x O
j O
), O
which O
is O
exactly O
the O
same O
as O
. O
The O
negative O
log-likelihood O
loss O
for O
training O
can O
be O
defined O
as: O
L O
sq O
= O
− O
N O
j O
log O
p(y O
j O
|x O
j O
),(9) O

At O
the O
inference O
stage, O
we O
use O
the O
Viterbi O
algorithm O
(Viterbi, O
1967) O
to O
generate O
the O
final O
label O
sequence. O
Combining O
with O
Supervised O
Lexicon O
Features O
We O
can O
naturally O
combine O
BABERT B-MethodName
with O
other O
supervised O
lexicon-based O
methods O
because O
of O
the O
unsupervised O
setting O
of O
BABERT B-MethodName
. O
To O
this O
end, O
we O
propose O
a O
lexicon-enhanced B-MethodName
BABERT I-MethodName
( O
BABERT-LE B-MethodName

) O
for O
the O
fine-tuning O
stage, O
which O
utilizes O
the O
lexicon O
adapter O
proposed O
by O
to O
incorporate O
external O
lexicon O
knowledge O
into O
BABERT B-MethodName
feature: O
h O
i O
= O
LA(h O
i O
, O
S O
lex O
i O
),(10) O
where O
LA(•) O
is O
the O
lexicon O
adapter, O
S O
lex O
i O
is O
a O
set O
of O
related O
N-gram O
embeddings O
of O
character O
c O
i O

, O
andĥ O
i O
is O
the O
lexicon-enhanced O
version O
of O
original O
BERT O
feature O
h O
i O
. O
We O
apply O
the O
lexicon O
adapter O
after O
the O
l-th O
layer O
to O
be O
consistent O
with O
boundary-aware O
learning. O
Finally, O
BABERT-LE B-MethodName
performs O
a O
similar O
fine-tuning O
procedure O
as O
BABERT B-MethodName
for O
training: O
L O
lex O
= O
− O
N O
j O
log O
p(yj|xj, O
[S O
lex O

1 O
, O
..., O
S O
lex O
n O
j O
]).(11) O
4 O
Experiments O
Datasets O
Following O
previous O
works O
(Devlin O
et O
al., O
2019;, O
we O
draw O
the O
mixed O
corpus O
of O
Chinese B-DatasetName
Wikipedia I-DatasetName
3 O
and O
Baidu B-DatasetName
Baike I-DatasetName
4 O
as O
our O
pretraining O
corpus, O
which O
contains O
3B O
tokens O
and O
62M O
sentences. O
To O
further O
confirm O
the O
effectiveness O
of O
our O
proposed O

method O
for O
Chinese O
sequence O
labeling, O
we O
evaluate O
BABERT B-MethodName
on O
ten O
benchmark O
datasets O
of O
three O
representative O
tasks: O
Chinese O
Word O
Segmentation O
We O
use O
three O
CWS O
benchmarks O
to O
evaluate O
our O
BABERT. O
Penn B-DatasetName
Chinese I-DatasetName
TreeBank I-DatasetName
version I-DatasetName
6.0 I-DatasetName
CTB6 B-DatasetName
) O
is O
from O
Xue O
et O
al. O
(2005), O
and O
MSRA B-DatasetName
and O
PKU B-DatasetName
are O
from O
SIGHAN O
2005 O

Bakeoff O
(Emerson, O
2005). O
Part-Of-Speech O
Tagging O
For O
Chinese O
POS O
tagging, O
we O
conduct O
experiments O
on O
CTB6 B-DatasetName
(Xue O
et O
al., O
2005) O
and O
the O
Chinese O
part O
of O
Universal B-DatasetName
Dependencies I-DatasetName
( O
UD B-DatasetName
) O
(Nivre O
et O
al., O
2016). O
The O
UD B-DatasetName
dataset O
uses O
two O
different O
POS O
tagsets, O
which O
are O
universal O
and O
language-specific O
tagsets. O
We O
follow O
Shao O
et O

al. O
(2017), O
referring O
to O
the O
corpus O
with O
the O
two O
tagsets O
as O
UD1 O
and O
UD2, O
respectively. O
Named O
Entity O
Recognition O
For O
the O
Chinese O
NER O
task, O
we O
conduct O
experiments O
on O
OntoNotes B-DatasetName
4.0 I-DatasetName
( O
Onto4 B-DatasetName
) O
(Weischedel O
et O
al., O
2011) O
and O
News B-DatasetName
datasets O
(Jia O
et O
al., O
2020), O
both O
of O
which O
are O
from O
the O
standard O

newswire O
domain. O
Moreover, O
we O
evaluate O
BABERT B-MethodName
in O
the O
internet O
novel O
(Book) O
and O
financial O
report O
(Finance) O
domains O
(Jia O
et O
al., O
2020) O
to O
further O
verify O
the O
robustness O
of O
our O
method. O
The O
statistics O
of O
the O
benchmark O
datasets O
are O
shown O
in O
Table O
1. O
For O
a O
fair O
comparison, O
we O
split O
these O
datasets O
into O
training, O
development, O

and O
test O
sections O
following O
previous O
works O
(Jia O
et O
al., O
2020;. O
Note O
that O
MSRA B-DatasetName
, O
PKU B-DatasetName
, O
and O
Finance B-DatasetName
do O
not O
have O
development O
sections. O
Therefore, O
we O
randomly O
select O
10% O
instances O
from O
the O
training O
set O
as O
the O
development O
set O
for O
these O
datasets. O
Experimental O
Settings O
Hyperparameters O
During O
pre-training, O
we O
use O
the O
hyperparameters O
of O

BERT B-MethodName
BASE I-MethodName
to O
initialize O
BABERT B-MethodName
and O
Adam O
(Kingma O
and O
Ba, O
2014) O
for O
optimizing. O
The O
number O
of O
BERT O
layers B-HyperparameterName
L B-HyperparameterName
is O
12 B-HyperparameterValue
, O
with O
12 B-HyperparameterValue
self-attention B-HyperparameterName
heads I-HyperparameterName
, O
768 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
hidden B-HyperparameterName
states I-HyperparameterName
, O
and O
64 B-HyperparameterValue
dimensions I-HyperparameterValue
for O
each B-HyperparameterName
head I-HyperparameterName
. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
set O
to O
32 B-HyperparameterValue
, O
the O
learning B-HyperparameterName

rate I-HyperparameterName
is O
1e-4 B-HyperparameterValue
with O
a O
warmup B-HyperparameterName
ratio I-HyperparameterName
of O
0.1 B-HyperparameterValue
, O
and O
the O
max B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
input I-HyperparameterName
sequence I-HyperparameterName
is O
512 B-HyperparameterValue
. O
To O
extract O
unsupervised O
boundary O
information, O
we O
set O
the O
maximum B-HyperparameterName
N-gram I-HyperparameterName
length I-HyperparameterName
N B-HyperparameterName
to O
4 B-HyperparameterValue
5 I-HyperparameterValue
and O
the O
frequency B-HyperparameterName
filtering I-HyperparameterName
threshold I-HyperparameterName
to O
50 B-HyperparameterValue
. O
Then O
we O
use O
the O
3-th O
BERT O

layer O
to O
compute O
boundary-aware O
objective. O
BABERT B-MethodName
has O
no O
extra O
modules, O
which O
is O
why O
the O
parameter O
size O
and O
model O
architecture O
are O
the O
same O
as O
those O
of O
BERT B-MethodName
BASE I-MethodName
. O
Finally, O
we O
train O
the O
BABERT B-MethodName
on O
8 O
NVIDIA O
Tesla O
V100 O
GPUs O
with O
32GB O
memory. O
For O
Chinese O
sequence O
labeling, O
we O
empirically O
set O
hyperparameters O

based O
on O
previous O
studies O
(Jia O
et O
al., O
2020; O
and O
preliminary O
experiments. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
32 B-HyperparameterValue
, O
the O
max B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
is O
256 B-HyperparameterValue
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
fixed O
to O
2e-5 B-HyperparameterValue
. O
Baselines O
To O
verify O
the O
effectiveness O
of O
our O
proposed O
BABERT B-MethodName
, O
we O
build O
systems O
on O
the O
following O
methods O

to O
conduct O
fair O
comparisons: O
• O
BERT O
is O
the O
Chinese O
version O
BERT B-MethodName
BASE I-MethodName
model O
released O
by O
Google. O
• O
BERT-wwm B-MethodName
performs O
segmentation O
on O
the O
corpus O
and O
further O
conduct O
word-level O
masking O
in O
pre-training O
(Cui O
et O
al., O
2021). O
• O
ERNIE B-MethodName
is O
an O
extension O
of O
BERT, O
which O
leverages O
external O
lexicons O
for O
word-level O
masking O
. O
• O

ERNIE-Gram B-MethodName
is O
an O
extension O
of O
ERNIE B-MethodName
, O
which O
alleviates O
the O
limitations O
of O
external O
lex- O
icons O
by O
using O
statistical O
information O
for O
entity O
and O
phrase O
extraction O
. O
• O
ZEN B-MethodName
uses O
an O
extra O
N-gram O
encoder O
to O
integrate O
external O
lexicon O
knowledge O
into O
BERT O
during O
pre-training O
(Diao O
et O
al., O
2020). O
• O
NEZHA B-MethodName
leverages O
functional O
relative O

positional O
encoding, O
supervised O
word-level O
masking O
strategy, O
and O
enormous O
training O
data O
6 O
to O
enhance O
vanilla O
BERT O
. O
• O
BERT-LE B-MethodName
is O
a O
lexicon-enhanced O
BERT O
, O
which O
introduces O
a O
lexicon O
adapter O
between O
BERT O
layers O
to O
incorporate O
external O
lexicon O
embeddings. O
We O
strictly O
follow. O
to O
reimplement O
it O
with O
open-source O
word O
embeddings O
7 O
Main O
Results O
The O

overall O
Chinese O
sequence O
labeling O
results O
are O
shown O
in O
Table O
2. O
We O
report O
the O
F1 B-MetricName
-score O
of O
the O
test O
datasets O
on O
CWS B-TaskName
, O
POS B-TaskName
, O
and O
NER B-TaskName
tasks. O
Here, O
we O
first O
compare O
our O
BABERT B-MethodName
with O
various O
Chinese O
pre-trained O
language O
models O
to O
evaluate O
its O
effectiveness. O
Then, O
we O
compare O
BABERT-LE B-MethodName
with O
other O
supervised O

lexicon-based O
methods O
to O
show O
the O
potential O
of O
BABERT B-MethodName
in O
combining O
with O
external O
lexicon O
knowledge. O
6 O
NEZHA B-MethodName
uses O
three O
large O
corpora, O
including O
Chinese B-DatasetName
Wikipedia I-DatasetName
, O
Baidu B-DatasetName
Baike I-DatasetName
, O
and O
Chinese B-DatasetName
News I-DatasetName
, O
which O
contain O
11B O
tokens O
and O
are O
four O
times O
more O
than O
us. O
7 O
https://ai.tencent.com/ailab/nlp/en/embedding.html O
First, O
we O
examine O
the O
F1 B-MetricName
values O

of O
the O
BERT B-MethodName
baseline. O
As O
shown, O
BERT B-MethodName
obtains O
comparable O
results O
on O
all O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks, O
which O
is O
similar O
to O
that O
of O
Diao O
et O
al. O
(2020), O
Tian O
et O
al. O
(2020a) O
and O
. O
BABERT B-MethodName
significantly O
outperforms O
BERT B-MethodName
, O
resulting O
in O
an O
increase O
of O
90.47 O
− O
89.80 O
= O
0.67 B-MetricValue
on O
average. O
This O

observation O
clearly O
indicates O
the O
advantage O
of O
introducing O
boundary O
information O
into O
BERT O
pre-training. O
Compared O
with O
various O
BERT O
extensions, O
our O
BABERT B-MethodName
can O
achieve O
competitive O
performances O
as O
a O
whole. O
First, O
in O
comparison O
with O
BERT-wwm B-MethodName
, O
ERNIE B-MethodName
, O
ERNIE-gram B-MethodName
, O
and O
ZEN B-MethodName
, O
which O
leverage O
external O
lexicons O
that O
include O
high-frequency O
words O
for O
pre-training, O
BABERT B-MethodName

outperforms O
all O
of O
them O
by O
averaging O
0.54+0.41+0.40+0.57 O
4 O
= O
0.48 B-MetricValue
point, O
and O
achieves O
top O
scores O
on O
eight O
of O
the O
ten O
benchmarks. O
This O
result O
is O
consistent O
with O
our O
intuition O
that O
directly O
exploiting O
a O
supervised O
lexicon O
can O
only O
achieve O
good O
performance O
in O
specific O
tasks, O
indicating O
the O
limitation O
of O
these O
methods O
when O
the O

chosen O
lexicon O
is O
incompatible O
with O
the O
target O
tasks. O
Second, O
we O
find O
that O
BABERT B-MethodName
surpasses O
NEZHA B-MethodName
in O
the O
average O
F1 B-MetricName
values, O
indicating O
that O
the O
boundary O
information O
is O
more O
critical O
than O
the O
data O
scale O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
. O
Then, O
we O
compare O
our O
method O
with O
supervised O
lexicon-based O
methods. O
Lattice-based O
methods O
(Zhang O
and O

Yang, O
2018;Yang O
et O
al., O
2019) O
2020a,b) O
designs O
an O
external O
memory O
network O
after O
the O
BERT O
encoder O
to O
incorporate O
lexicon O
knowledge. O
EEBERT B-MethodName
(Jia O
et O
al., O
2020) O
builds O
entity O
embeddings O
from O
the O
corpus O
and O
further O
utilizes O
them O
in O
the O
multi-head O
attention O
mechanism. O
The O
results O
are O
shown O
in O
Table O
2 O
(II). O
All O
the O
above O

methods O
lead O
to O
significant O
improvements O
over O
the O
base O
BERT O
model, O
which O
shows O
the O
effectiveness O
of O
external O
lexicon O
knowledge. O
Moreover, O
BABERT B-MethodName
can O
achieve O
comparable O
performance O
with O
the O
above O
methods, O
which O
further O
demonstrates O
the O
potential O
of O
our O
unsupervised O
manner. O
BABERT B-MethodName
learns O
boundary O
information O
from O
unsupervised O
statistical O
features O
with O
vanilla O
BERT, O
which O
means O

it O
has O
excellent O
scalability O
to O
fuse O
with O
other O
BERT-based O
supervised O
lexicon O
models. O
As O
shown, O
we O
can O
see O
that O
our O
BABERT-LE B-MethodName
achieves O
further O
improvements O
and O
state-of-the-art O
performances O
on O
all O
tasks, O
showing O
the O
advantages O
of O
our O
unsupervised O
setting O
and O
boundary-aware O
learning. O
Interestingly, O
compared O
with O
MEM-ZEN B-MethodName
, O
BABERT-LE B-MethodName
has O
larger O
improvements O
over O
their O

corresponding O
baselines. O
One O
reason O
might O
be O
that O
both O
ZEN B-MethodName
and O
the O
memory O
network O
module O
exploits O
supervised O
lexicons, O
which O
leads O
to O
a O
duplication O
of O
introduced O
knowledge. O
Analysis O
In O
this O
subsection, O
we O
conduct O
detailed O
experimental O
analyses O
for O
an O
in-depth O
comprehensive O
understanding O
of O
our O
method. O
Few-Shot O
Setting O
To O
further O
verify O
the O
effectiveness O
of O

BABERT B-MethodName
, O
we O
conduct O
experiments O
under O
the O
few-shot O
setting, O
where O
we O
randomly O
sample O
10, O
50, O
and O
100 O
instances O
of O
the O
original O
training O
data O
from O
PKU B-DatasetName
( O
CWS B-TaskName
) O
and O
Onto4 B-DatasetName
( O
NER B-TaskName
). O
For O
fair O
comparisons, O
we O
compare O
BABERT B-MethodName
with O
the O
pretrained O
language O
models O
without O
external O
supervised O
knowledge. O
The O
results O

are O
presented O
in O
Table O
3. O
As O
the O
size O
of O
training O
data O
is O
reduced, O
the O
adding O
T-test O
does O
not O
bring O
further O
improvements. O
One O
possible O
reason O
is O
that O
the O
T-test O
is O
essentially O
similar O
to O
the O
entropy O
measure O
of O
2-grams, O
which O
has O
already O
been O
injected O
into O
our O
BABERT B-MethodName
model. O
Boundary O
Information O
Encoding O
Layer O

Previous O
works O
(Jawahar O
et O
al., O
2019; O
exploit O
the O
fact O
that O
different O
BERT O
layers O
would O
generate O
different O
concept O
representations. O
The O
shallow O
BERT O
layers O
are O
more O
likely O
to O
capture O
basic O
lexicon O
information, O
while O
the O
top O
layers O
focus O
on O
the O
semantic O
representation. O
We O
empirically O
set O
l O
in O
{1, O
3, O
6, O
12} O
to O
explore O

the O
effect O
of O
computing O
boundary-aware O
loss O
by O
the O
hidden O
features O
H O
l O
of O
different O
BERT O
layers O
on O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
tasks. O
Table O
5 O
shows O
the O
results. O
We O
can O
see O
that O
the O
best O
F1 B-MetricName
-score O
can O
be O
achieved O
when O
l O
= O
3 O
on O
all O
datasets, O
which O
indicates O
that O
the O
BABERT B-MethodName
still O

needs O
sufficient O
parameters O
to O
learn O
the O
basic O
boundary O
information. O
Interestingly, O
the O
BABERT B-MethodName
performs O
poorly O
when O
l O
= O
12, O
which O
might O
be O
due O
to O
a O
conflict O
between O
the O
MLM O
loss O
and O
our O
boundary-aware O
regression O
loss O
during O
pretraining. O
Qualitative O
Analysis O
To O
explore O
how O
BABERT B-MethodName
improves O
the O
performance O
for O
Chinese B-TaskName
sequence I-TaskName
labeling I-TaskName
, O

we O
conduct O
qualitative O
analysis O
on O
the O
News B-DatasetName
test O
dataset, O
which O
consists O
of O
four O
different O
subdomains, O
namely O
game O
(GAM), O
entertainment O
(ENT), O
lottery O
(LOT) O
and O
finance O
(FIN). O
The O
results O
are O
shown O
in O
Table O
6. O
We O
can O
see O
that O
compared O
with O
other O
pre-trained O
language O
models, O
BABERT B-MethodName
can O
obtain O
consistent O
improvement O
in O
all O
domains O

with O
unsupervised O
statistical O
boundary O
information, O
while O
the O
other O
models O
only O
improve O
performance O
on O
specific O
domains. O
Moreover, O
as O
shown O
in O
Table O
7, O
we O
also O
give O
an O
example O
from O
the O
game O
domain O
to O
further O
demonstrate O
the O
effectiveness O
of O
our O
method. O
BABERT B-MethodName
is O
the O
only O
model O
that O
correctly O
recognizes O
all O
entities. O
In O
particular, O

the O
prediction O
of O
BABERT B-MethodName
for O
the O
entity O
"WCG2011 O
org O
" O
indicates O
the O
potential O
of O
boundary O
information. O

Co-guiding B-MethodName
Net I-MethodName
: O
Achieving O
Mutual O
Guidances O
between O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
and O
Slot B-TaskName
Filling I-TaskName
via O
Heterogeneous O
Semantics-Label O
Graphs O
Recent O
graph-based O
models O
for O
joint B-TaskName
multiple I-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
have O
obtained O
promising O
results O
through O
modeling O
the O
guidance O
from O
the O
prediction O
of O
intents O
to O
the O
decoding O
of O
slot B-TaskName
filling I-TaskName
. O
However, O
existing O

methods O
(1) O
only O
model O
the O
unidirectional O
guidance O
from O
intent O
to O
slot; O
(2) O
adopt O
homogeneous O
graphs O
to O
model O
the O
interactions O
between O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes, O
which O
limit O
the O
performance. O
In O
this O
paper, O
we O
propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
achieving O

the O
mutual O
guidances O
between O
the O
two O
tasks. O
In O
the O
first O
stage, O
the O
initial O
estimated O
labels O
of O
both O
tasks O
are O
produced, O
and O
then O
they O
are O
leveraged O
in O
the O
second O
stage O
to O
model O
the O
mutual O
guidances. O
Specifically, O
we O
propose O
two O
heterogeneous O
graph O
attention O
networks O
working O
on O
the O
proposed O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName

, O
which O
effectively O
represent O
the O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes. O
Experiment O
results O
show O
that O
our O
model O
outperforms O
existing O
models O
by O
a O
large O
margin, O
obtaining O
a O
relative O
improvement O
of O
19.3% B-MetricValue
over O
the O
previous O
best O
model O
on O
Mix-ATIS O
dataset O
in O
overall O
accuracy B-MetricName
. O
Introduction O
Spoken B-TaskName
language I-TaskName
understanding I-TaskName
( O
SLU B-TaskName

) O
(Young O
et O
al., O
2013) O
is O
a O
fundamental O
task O
in O
dialog O
systems. O
Its O
objective O
is O
to O
capture O
the O
comprehensive O
semantics O
of O
user O
utterances, O
and O
it O
typically O
includes O
two O
subtasks: O
intent B-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
(Tur O
and O
De O
Mori, O
2011). O
Intent B-TaskName
detection I-TaskName
aims O
to O
predict O
the O
intention O
of O
the O
user O
utterance O

and O
slot O
filling O
aims O
to O
extract O
additional O
information O
or O
constraints O
expressed O
in O
the O
utterance. O
Recently, O
researchers O
discovered O
that O
these O
two O
tasks O
are O
closely O
tied, O
and O
a O
bunch O
of O
models O
(Goo O
et O
al., O
2018;Liu O
et O
al., O
2019a;E O
et O
al., O
2019;Qin O
et O
al., O
2019) O
are O
proposed O
to O
combine O
the O
single-intent O
detection O
and O

slot B-TaskName
filling I-TaskName
in O
multi-task O
frameworks O
to O
leverage O
their O
correlations. O
However, O
in O
real-world O
scenarios, O
a O
user O
usually O
expresses O
multiple O
intents O
in O
a O
single O
utterance. O
To O
this O
end, O
(Kim O
et O
al., O
2017) O
begin O
to O
tackle O
the O
multi-intent B-TaskName
detection I-TaskName
task O
and O
(Gangadharaiah O
and O
Narayanaswamy, O
2019) O
make O
the O
first O
attempt O
to O
jointly O
model O
the O

multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
in O
a O
multi-task O
framework. O
(Qin O
et O
al., O
2020) O
propose O
an O
AGIF B-MethodName
model O
to O
adaptively O
integrate O
the O
fine-grained O
multi-intent O
prediction O
information O
into O
the O
autoregressive O
decoding O
process O
of O
slot B-TaskName
filling I-TaskName
via O
graph O
attention O
network O
(GAT) O
(Velickovic O
et O
al., O
2018). O
And O
(Qin O
et O
al., O
2021b) O
further O
propose O

a O
non-autoregressive B-MethodName
GAT-based I-MethodName
model I-MethodName
which O
enhances O
the O
interactions O
between O
the O
predicted O
multiple O
intents O
and O
the O
slot O
hidden O
states, O
obtaining O
state-of-the-art O
results O
and O
significant O
speedup. O
Despite O
the O
promising O
progress O
that O
existing O
multi-intent O
SLU O
joint O
models O
have O
achieved, O
we O
discover O
that O
they O
suffer O
from O
two O
main O
issues: O
(1) O
Ignoring O
the O
guidance O
from O

slot O
to O
intent. O
Since O
previous O
researchers O
realized O
that O
"slot O
labels O
could O
depend O
on O
the O
intent" O
(Gangadharaiah O
and O
Narayanaswamy, O
2019), O
existing O
models O
leverage O
the O
information O
of O
the O
predicted O
intents O
to O
guide O
slot B-TaskName
filling I-TaskName
, O
as O
shown O
in O
Fig. O
1(a). O
However, O
they O
ig- O
However, O
in O
previous O
works, O
the O
only O
guidance O
that O
the O

multiple B-TaskName
intent I-TaskName
detection I-TaskName
task O
can O
get O
from O
the O
joint O
model O
is O
sharing O
the O
basic O
semantics O
with O
the O
slot B-TaskName
filling I-TaskName
task. O
As O
a O
result, O
the O
lack O
of O
guidance O
from O
slot O
to O
intent O
limits O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
so O
the O
joint O
task. O
(2) O
Node O
and O
edge O
ambiguity O
in O
the O
semanticslabel O
graph. O

(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b O
apply O
GATs B-MethodName
over O
the O
constructed O
graphs O
to O
model O
the O
interactions O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
However, O
their O
graphs O
are O
homogeneous, O
in O
which O
all O
nodes O
and O
edges O
are O
treated O
as O
the O
same O
type. O
For O
a O
slot O
semantics O
node, O
the O

information O
from O
intent O
label O
nodes O
and O
other O
slot O
semantics O
nodes O
play O
different O
roles, O
while O
the O
homogeneous O
graph O
cannot O
discriminate O
their O
specific O
contributions, O
causing O
ambiguity. O
Therefore, O
the O
heterogeneous O
graphs O
should O
be O
designed O
to O
represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes O
to O
facilitate O
better O
interactions. O
In O
this O
paper, O
we O

propose O
a O
novel O
model O
termed O
Co-guiding B-MethodName
Net I-MethodName
to O
tackle O
the O
above O
two O
issues. O
For O
the O
first O
issue, O
Co-guiding B-MethodName
Net I-MethodName
implements O
a O
two-stage O
framework O
as O
shown O
in O
Fig. O
1 O
(b). O
The O
first O
stage O
produces O
the O
initial O
estimated O
labels O
for O
the O
two O
tasks O
and O
the O
second O
stage O
leverages O
the O
estimated O
labels O
as O

prior O
label O
information O
to O
allow O
the O
two O
tasks O
mutually O
guide O
each O
other. O
For O
the O
second O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
( O
HSLGs B-MethodName
): O
(1) O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
intent O
semantics O
nodes O
and O
slot O
label O
nodes; O
(2) O
an O
intent-to-slot B-MethodName

semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
effectively O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
intent O
label O
nodes. O
Moreover, O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
( O
HGATs B-MethodName
) O
are O
proposed O
to O
work O
on O
the O
two O
proposed O
graphs O
for O
modeling O
the O
guidances O
from O
slot O
to O
intent O
and O
intent O
to O
slot, O
respectively. O

Experiment O
results O
show O
that O
our O
Co-guiding B-MethodName
Net I-MethodName
significantly O
outperforms O
previous O
models, O
and O
model O
analysis O
further O
verifies O
the O
advantages O
of O
our O
model. O
The O
contributions O
of O
our O
work O
are O
three-fold: O
(1) O
We O
propose O
Co-guiding B-MethodName
Net I-MethodName
1 I-MethodName
, O
which O
implements O
a O
two-stage O
framework O
allowing O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
mutually O
guide O
each O

other. O
We O
make O
the O
first O
attempt O
to O
achieve O
the O
mutual O
guidances O
between O
the O
two O
tasks. O
(2) O
We O
propose O
two O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
as O
appropriate O
platforms O
for O
interactions O
between O
semantics O
nodes O
and O
label O
nodes. O
And O
we O
propose O
two O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
to O
model O
the O
mutual O
guidances O
between O
the O
two O
tasks. O

(3) O
Experiment O
results O
demonstrate O
that O
our O
model O
achieves O
new O
state-of-the-art O
performance. O
Co-guiding O
Problem O
Definition O
Given O
a O
input O
utterance O
denoted O
as O
U O
= O
{u O
i O
} O
n O
1 O
, O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
can O
be O
formulated O
as O
a O
multi-label O
classification O
task O
that O
outputs O
multiple O
intent O
labels O
corresponding O
to O
the O
input O
utterance. O
And O

slot B-TaskName
filling I-TaskName
is O
a O
sequence O
labeling O
task O
that O
maps O
each O
u O
i O
into O
a O
slot O
label. O
Next, O
before O
diving O
into O
the O
details O
of O
Coguiding B-MethodName
Net I-MethodName
's O
architecture, O
we O
first O
introduce O
the O
construction O
of O
the O
two O
heterogeneous B-MethodName
graphs I-MethodName
. O
Slot-to-Intent B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
provide O
an O
appropriate O
platform O
for O
modeling O
the O
guidance O

from O
the O
estimated O
slot O
labels O
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
design O
a O
slot-to-intent B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
S2I-SLG B-MethodName
), O
which O
represents O
the O
relations O
among O
the O
semantics O
of O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
the O
estimated O
slot O
labels. O
S2I-SLG B-MethodName
is O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
3 O
(a). O
It O

contains O
two O
types O
of O
nodes: O
intent O
semantics O
nodes O
(e.g., O
I O
1 O
, O
..., O
I O
5 O
) O
and O
slot O
label O
(SL) O
nodes O
(e.g., O
SL O
1 O
, O
..., O
SL O
5 O
). O
And O
there O
are O
four O
types O
of O
edges O
in O
S2I-SLG B-MethodName
, O
as O
shown O
in O
Fig. O
3 O
(b). O
Each O
edge O
type O
corresponds O
to O

an O
individual O
kind O
of O
information O
aggregation O
on O
the O
graph. O
Mathematically, O
the O
S2I-SLG B-MethodName
can O
be O
denoted O
as O
G O
s2i O
= O
(V O
s2i O
, O
E O
s2i O
, O
A O
s2i O
, O
R O
s2i O
), O
in O
which O
V O
s2i O
is O
the O
set O
of O
all O
nodes, O
E O
s2i O
is O
the O
set O
of O
all O
edges, O
A O

s2i O
is O
the O
set O
of O
two O
node O
types O
and O
R O
s2i O
is O
the O
set O
of O
four O
edge O
types. O
Each O
node O
v O
s2i O
and O
each O
edge O
e O
s2i O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
s2i O
) O
: O
V O
s2i O
→ O
A O
s2i O
and O
ϕ(e O
s2i O
) O
: O
E O

s2i O
→ O
R O
s2i O
. O
For O
instance, O
in O
Fig. O
3, O
the O
SL O
2 O
node O
belongs O
to O
V O
s2i O
, O
while O
its O
node O
type O
SL O
belongs O
to O
A O
s2i O
; O
the O
edge O
from O
SL O
2 O
to O
I O
3 O
belongs O
to O
E O
s2i O
, O
while O
its O
edge O
type O
slot_to_intent_guidance O
belongs O
to O
R O

s2i O
. O
Besides, O
edges O
in O
S2I-SLG B-MethodName
are O
based O
on O
local O
connections. O
For O
example, O
node O
I O
i O
is O
connected O
to O
{I O
i−w O
, O
..., O
I O
i+w O
} O
and O
{SL O
i−w O
, O
..., O
SL O
i+w O
}, O
where O
w O
is O
a O
hyper-parameter O
of O
the O
local O
window O
size. O
Intent-to-Slot B-MethodName
Semantics-Label I-MethodName
Graph I-MethodName
To O
present O
a O

platform O
for O
accommodating O
the O
guidance O
from O
the O
estimated O
intent O
labels O
to O
slot O
filling, O
we O
design O
an O
intent-to-slot B-MethodName
semantics-label I-MethodName
graph I-MethodName
( O
I2S-SLG B-MethodName
) O
that O
represents O
the O
relations O
among O
the O
slot O
semantics O
nodes O
and O
the O
intent O
label O
nodes. O
I2S-SLG B-MethodName
is O
also O
a O
heterogeneous B-MethodName
graph I-MethodName
and O
an O
example O
is O
shown O
in O
Fig. O
4 O

(a). O
It O
contains O
two O
types O
of O
nodes: O
slot O
semantics O
nodes O
(e.g., O
S O
1 O
, O
..., O
S O
5 O
) O
and O
intent O
label O
(IL) O
nodes O
(e.g., O
IL O
1 O
, O
..., O
IL O
5 O
). O
And O
Fig. O
4 O
(b) O
shows O
the O
four O
edge O
types. O
Each O
edge O
type O
corresponds O
to O
an O
individual O
kind O
of O
information O

aggregation O
on O
the O
graph. O
Mathematically, O
the O
I2S-SLG B-MethodName
can O
be O
denoted O
as O
G O
i2s O
= O
(V O
i2s O
, O
E O
i2s O
, O
A O
i2s O
, O
R O
i2s O
). O
Each O
node O
v O
i2s O
and O
each O
edge O
e O
i2s O
are O
associated O
with O
their O
type O
mapping O
functions O
τ O
(v O
i2s O
) O
and O
ϕ(e O
i2s O
). O

The O
connections O
in O
I2S-SLG B-MethodName
are O
a O
little O
different O
from O
S2I-SLG B-MethodName
. O
Since O
intents O
are O
sentence-level, O
each O
IL O
node O
is O
globally O
connected O
with O
all O
nodes. O
For O
S O
i O
node, O
it O
is O
connected O
to O
{S O
i−w O
, O
..., O
S O
i+w O
} O
and O
{IL O
1 O
, O
..., O
IL O
m O
}, O
where O
w O
is O

the O
local O
window O
size O
and O
m O
is O
the O
number O
of O
estimated O
intents. O
Shared O
Self-Attentive O
Encoder O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
adopt O
a O
shared O
self-attentive O
encoder O
to O
produce O
the O
initial O
hidden O
states O
containing O
the O
basic O
semantics. O
It O
includes O
a O
BiLSTM O
and O
a O
self-attention O
module. O
BiLSTM O
captures O

the O
temporal O
dependencies: O
hi O
= O
BiLSTM O
xi, O
hi−1, O
hi+1(1) O
where O
x O
i O
is O
the O
word O
vector O
of O
u O
i O
. O
Now O
we O
obtain O
the O
context-sensitive O
hidden O
statesĤ O
= O
{ĥ O
i O
} O
n O
1 O
. O
Self-attention O
captures O
the O
global O
dependencies: O
H O
′ O
= O
softmax O
QK O
⊤ O
√ O
d O
k O
V O
(2) O

where O
H O
′ O
is O
the O
global O
contextual O
hidden O
states O
output O
by O
self-attention; O
Q, O
K O
and O
V O
are O
matrices O
obtained O
by O
applying O
different O
linear O
projections O
on O
the O
input O
utterance O
word O
vector O
matrix. O
Then O
we O
concatenate O
the O
output O
of O
BiLSTM O
and O
self-attention O
to O
form O
the O
output O
of O
the O
shared O
selfattentive O
encoder: O
H O

=Ĥ∥H O
′ O
, O
where O
H O
= O
{h O
i O
} O
n O
1 O
and O
∥ O
denotes O
concatenation O
operation. O
Initial O
Estimation O
Multiple B-TaskName
Intent I-TaskName
Detection I-TaskName
To O
obtain O
the O
taskspecific O
features O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
apply O
a O
BiLSTM O
layer O
over O
H: O
Following O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b, O
we O
conduct O
token-level O

multi-intent B-TaskName
detection I-TaskName
. O
Each O
h O
[I,0] O
i O
is O
fed O
into O
the O
intent O
decoder. O
Specifically, O
the O
intent O
label O
distributions O
of O
the O
i-th O
word O
are O
obtained O
by: O
h O
[I,0] O
i O
= O
BiLSTM O
I O
hi, O
h O
[I,0] O
i−1 O
, O
h O
[I,0] O
i+1 O
(3) O
Shared O
Self- O
y O
[I,0] O
i O
= O
sigmoid O
W O
1 O
I O

σ(W O
2 O
I O
h O
[I,0] O
i O
+b O
2 O
I O
) O
+b O
1 O
I O
(4) O
where O
σ O
denotes O
the O
non-linear O
activation O
function; O
W O
* O
and O
b O
* O
are O
model O
parameters. O
Then O
the O
estimated O
sentence-level O
intent O
labels O
{IL O
1 O
, O
..., O
IL O
m O
} O
are O
obtained O
by O
the O
token-level O
intent O
voting O
(Qin O

et O
al., O
2021b). O
Slot B-TaskName
Filling I-TaskName
(Qin O
et O
al., O
2021b) O
propose O
a O
nonautoregressive O
paradigm O
for O
slot B-TaskName
filling I-TaskName
decoding, O
which O
achieves O
significant O
speedup. O
In O
this O
paper, O
we O
also O
conduct O
parallel O
slot O
filling O
decoding. O
We O
first O
apply O
a O
BiLSTM O
over O
H O
to O
obtain O
the O
task-specific O
features O
for O
slot O
filling: O
h O
[S,0] O
i O
= O

BiLSTM O
S O
(hi, O
h O
[S,0] O
i−1 O
, O
h O
[S,0] O
i+1 O
)(5) O
Then O
use O
a O
softmax O
classifier O
to O
generate O
the O
slot O
label O
distribution O
for O
each O
word: O
y O
[S,0] O
i O
= O
softmax O
W O
1 O
S O
σ(W O
2 O
S O
h O
[S,0] O
i O
+b O
2 O
S O
) O
+b O
1 O
S O
(6) O
And O
the O
estimated O

slot O
label O
for O
each O
word O
is O
obtained O
by O
SL O
i O
= O
arg O
max(y O
[S,0] O
i O
). O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
State-of-the-art O
models O
(Qin O
et O
al., O
2020(Qin O
et O
al., O
, O
2021b) O
use O
a O
homogeneous O
graph O
to O
connect O
the O
semantic O
nodes O
of O
slot O
filling O
and O
the O
intent O
label O
nodes. O
And O
GAT B-MethodName
(Velickovic O

et O
al., O
2018) O
is O
adopted O
to O
achieve O
information O
aggregation. O
In O
Sec. O
1, O
we O
propose O
that O
this O
manner O
cannot O
effectively O
learn O
the O
interactions O
between O
one O
task's O
semantics O
and O
the O
estimated O
labels O
of O
the O
other O
task. O
To O
tackle O
this O
issue, O
we O
propose O
two O
heterogeneous B-MethodName
graphs I-MethodName
( O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
) O
to O
effectively O

represent O
the O
relations O
among O
the O
semantic O
nodes O
and O
label O
nodes. O
To O
model O
the O
interactions O
between O
semantics O
and O
labels O
on O
the O
proposed O
graphs, O
we O
propose O
a O
Heterogeneous B-MethodName
Graph I-MethodName
Attention I-MethodName
Network I-MethodName
( O
HGAT B-MethodName
). O
When O
aggregating O
the O
information O
into O
a O
node, O
HGAT B-MethodName
can O
discriminate O
the O
specific O
information O
from O
different O
types O
of O
nodes O

along O
different O
relations. O
And O
two O
HGATs B-MethodName
( O
S2I-HGAT B-MethodName
and O
I2S-HGAT B-MethodName
) O
are O
applied O
on O
S2I-SLG B-MethodName
and O
I2S-SLG B-MethodName
, O
respectively. O
Specifically, O
S2I-HGAT B-MethodName
can O
be O
formulated O
as O
follows: O
h O
l+1 O
i O
= O
K O
∥ O
k=1 O
σ O
 O
 O
j∈N O
i O
s2i O
W O
[r,k,1] O
s2i O
α O
[r,k] O
ij O
h O
l O
j O
 O
 O

, O
r O
= O
ϕ O
e O
[j,i] O
s2i O
α O
[r,k] O
ij O
= O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
j O
T O
/ O
√ O
d O
u∈N O
r,i O
s2i O
exp O
W O
[r,k,2] O
s2i O
h O
l O
i O
W O
[r,k,3] O
s2i O
h O
l O
u O
T O
/ O
√ O
d O
(7) O
where O

K O
denotes O
the O
total O
head O
number; O
N O
i O
s2i O
denotes O
the O
set O
of O
incoming O
neighbors O
of O
node O
i O
on O
S2I-SLG B-MethodName
; O
W O
[r,k, O
* O
] O
s2i O
are O
weight O
matrices O
of O
edge O
type O
r O
on O
the O
k-th O
head; O
e O
[j,i] O
s2i O
denotes O
the O
edge O
from O
node O
j O
to O
node O
i O
on O

S2I-SLG B-MethodName
; O
N O
r,i O
s2i O
denotes O
the O
nodes O
connected O
to O
node O
i O
with O
r-type O
edges O
on O
S2I-SLG B-MethodName
; O
d O
is O
the O
dimension O
of O
node O
hidden O
state. O
I2S-HGAT B-MethodName
can O
be O
derived O
like O
Eq. O
7. O
Intent O
Decoding O
with O
Slot O
Guidance O
In O
the O
first O
stage, O
we O
obtain O
the O
initial O
intent O
features O
H O
[I,0] O

= O
{h O
I,0 O
i O
} O
n O
i O
and O
the O
initial O
estimated O
slot O
labels O
sequence O
{SL O
1 O
, O
..., O
SL O
n O
}. O
Now O
we O
project O
the O
slot O
labels O
into O
vector O
form O
using O
the O
slot O
label O
embedding O
matrix, O
obtaining O
E O
sl O
= O
{e O
1 O
sl O
, O
..., O
e O
n O
sl O
}. O
Then O

we O
feed O
H O
[I,0] O
and O
E O
sl O
into O
S2I-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
slot O
label O
information O
to O
guide O
the O
intent O
decoding: O
where O
[H O
[I,0] O
, O
E O
sl O
] O
denotes O
the O
input O
node O
representation; O
θ O
I O
denotes O
S2I-HGAT B-MethodName
's O
parameters. O
L O
denotes O
the O
total O
layer O
number. O
Finally, O
H O

[I,L] O
is O
fed O
to O
intent O
decoder, O
producing O
the O
intent O
label O
distributions O
for O
the O
utterance O
words: O
Y O
[I,1] O
= O
{y O
[I,1] O
i O
, O
..., O
y O
[I,1] O
n O
}. O
And O
the O
final O
output O
sentence-level O
intents O
are O
obtained O
via O
applying O
token-level O
intent O
voting O
over O
Y O
[I,1] O
. O
Slot O
Decoding O
with O
Intent O
Guidance O
Intent-aware O

BiLSTM O
Since O
the O
B-I-O O
tags O
of O
slot O
labels O
have O
temporal O
dependencies, O
we O
use O
an O
intent-aware O
BiLSTM O
to O
model O
the O
temporal O
dependencies O
among O
slot O
hidden O
states O
with O
the O
guidance O
of O
estimated O
intents: O
h O
[S,0] O
i O
= O
BiLSTM(y O
[I,0] O
i O
∥h O
[S,0] O
i O
,h O
[S,0] O
i−1 O
,h O
[S,0] O
i+1 O
)(9) O
I2S-HGAT B-MethodName
We O

first O
project O
the O
estimated O
intent O
labels O
{IL O
j O
} O
m O
1 O
into O
vectors O
using O
the O
intent O
label O
embedding O
matrix, O
obtaining O
E O
il O
= O
{e O
1 O
il O
, O
..., O
e O
m O
il O
}. O
Then O
we O
feedH O
S O
and O
E O
il O
into O
I2S-HGAT B-MethodName
to O
model O
their O
interactions, O
allowing O
the O
estimated O
intent O
label O

information O
to O
guide O
the O
slot O
decoding: O
H O
[S,L] O
= O
I2S-HGAT B-MethodName
[H O
S O
, O
E O
il O
], O
Gi2s, O
θS(10) O
where O
[H O
[S] O
, O
E O
il O
] O
denotes O
the O
input O
node O
representation; O
θ O
S O
denotes O
I2S-HGAT B-MethodName
's O
parameters. O
Finally, O
H O
[S,L] O
is O
fed O
to O
slot O
decoder, O
producing O
the O
slot O
label O
distributions O
for O

each O
word: O
Y O
[S,1] O
= O
{y O
[S,1] O
i O
, O
..., O
y O
[S,1] O
n O
}. O
And O
the O
final O
output O
slot O
labels O
are O
obtained O
by O
applying O
arg O
max O
over O
Y O
[S,1] O
. O
Training O
Objective O
Loss O
Function O
The O
loss O
function O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
is: O
CE(ŷ, O
y) O
=ŷ O
log(y) O
+ O
(1 O
−ŷ) O
log(1 O

− O
y) O
LI O
= O
1 O
t=0 O
n O
i=1 O
N O
I O
j=1 O
CE O
ŷ O
I O
i O
[j], O
y O
[I,t] O
i O
[j](11) O
And O
the O
loss O
function O
for O
slot B-TaskName
filling I-TaskName
is: O
LS O
= O
1 O
t=0 O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
log O
y O
[S,t] O
i O
[j](12) O
where O
N O
I O
and O
N O

S O
denote O
the O
total O
numbers O
of O
intent O
labels O
and O
slot O
labels;ŷ O
I O
i O
andŷ O
S O
i O
denote O
the O
ground-truth O
intent O
labels O
and O
slot O
labels. O
Margin O
Penalty O
The O
core O
of O
our O
model O
is O
to O
let O
the O
two O
tasks O
mutually O
guide O
each O
other. O
Intuitively, O
the O
predictions O
in O
the O
second O
stage O
should O
be O

better O
than O
those O
in O
the O
first O
stage. O
To O
force O
our O
model O
obey O
this O
rule, O
we O
design O
a O
margin O
penalty O
(L O
mp O
) O
for O
each O
task, O
whose O
aim O
is O
to O
improve O
the O
probabilities O
of O
the O
correct O
labels. O
Specifically, O
the O
formulations O
of O
L O
mp O
I O
and O
L O
mp O
S O
are: O
L O
mp O

I O
= O
n O
i=1 O
N O
I O
j=1ŷ O
I O
i O
[j] O
max O
0, O
y O
[I,0] O
i O
[j] O
− O
y O
[I,1] O
i O
[j] O
L O
mp O
S O
= O
n O
i=1 O
N O
S O
j=1ŷ O
S O
i O
[j] O
max O
0, O
y O
[S,0] O
i O
[j] O
− O
y O
[S,1] O
i O
[j](13) O
Model O
Training O
The O
training O
objective O
L O

is O
the O
weighted O
sum O
of O
loss O
functions O
and O
margin O
regularizations O
of O
the O
two O
tasks: O
L O
= O
γ O
(LI O
+ O
βI O
L O
mp O
I O
) O
+ O
(1 O
− O
γ) O
(LS O
+ O
βSL O
mp O
S O
) O
(14) O
where O
γ O
is O
the O
coefficient O
balancing O
the O
two O
tasks; O
β O
I O
and O
β O
S O
are O

the O
coefficients O
of O
the O
margin O
regularization O
for O
the O
two O
tasks. O
3 O
Experiments O
Datasets O
and O
Metrics O
Following O
previous O
works, O
MixATIS B-DatasetName
and O
MixS-NIPS B-DatasetName
(Hemphill O
et O
al., O
1990;Coucke O
et O
al., O
2018;Qin O
et O
al., O
2020) O
As O
for O
evaluation O
metrics, O
following O
previous O
works, O
we O
adopt O
accuracy B-MetricName
( O
Acc B-MetricName
) O
for O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
F1 B-MetricName

score O
for O
slot B-TaskName
filling I-TaskName
, O
and O
overall B-MetricName
accuracy I-MetricName
for O
the O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
. O
Overall O
accuracy O
denotes O
the O
ratio O
of O
sentences O
whose O
intents O
and O
slots O
are O
all O
correctly O
predicted. O
Implementation O
Details O
Following O
previous O
works, O
the O
word O
and O
label O
embeddings O
are O
trained O
from O
scratch O
2 O
. O
The O
dimensions O
of O
word B-HyperparameterName

embedding I-HyperparameterName
, O
label B-HyperparameterName
embedding I-HyperparameterName
, O
and O
hidden B-HyperparameterName
state I-HyperparameterName
are O
256 B-HyperparameterValue
on O
MixATIS B-DatasetName
, O
while O
on O
MixS-NIPS B-DatasetName
they O
are O
256 B-HyperparameterValue
, O
128 B-HyperparameterValue
, O
and O
256 B-HyperparameterValue
. O
The O
layer B-HyperparameterName
number I-HyperparameterName
of O
all O
GNNs O
is O
2 B-HyperparameterValue
. O
Adam O
(Kingma O
and O
Ba, O
2015) O
is O
used O
to O
train O
our O
model O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O

1e B-HyperparameterValue
−3 I-HyperparameterValue
and O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e B-HyperparameterValue
−6 I-HyperparameterValue
. O
As O
for O
the O
coefficients O
Eq.14, O
γ B-HyperparameterName
is O
0.9 B-HyperparameterValue
on O
MixATIS B-DatasetName
and O
0.8 B-HyperparameterValue
on O
MixSNIPS B-DatasetName
; O
on O
both O
datasets, O
β B-HyperparameterName
I I-HyperparameterName
is O
1e B-HyperparameterValue
−6 I-HyperparameterValue
and O
β B-HyperparameterName
S I-HyperparameterName
is O
1e B-HyperparameterValue
0 I-HyperparameterValue
. O
The O
model O
performing O
best O
on O
the O
dev O
set O
is O
selected O

then O
we O
report O
its O
results O
on O
the O
test O
set. O
All O
experiments O
are O
conducted O
on O
RTX O
6000. O
Our O
source O
code O
will O
be O
released. O
Main O
Results O
The O
performance O
comparison O
of O
Co-guiding B-MethodName
Net I-MethodName
and O
baselines O
are O
shown O
in O
Table O
1, O
from O
which O
we O
have O
the O
following O
observations: O
(1) O
Co-guiding O
Net O
gains O
significant O
and O

consistent O
improvements O
on O
all O
tasks O
and O
datasets. O
Specifically, O
on O
MixATIS B-DatasetName
dataset, O
it O
overpasses O
the O
previous O
state-of-the-art O
model O
GL-GIN B-MethodName
by O
19.3% B-MetricValue
, O
1.8% B-MetricValue
, O
and O
3.7% B-MetricValue
on O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
, O
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively; O
on O
MixSNIPS B-DatasetName
dataset, O
it O
overpasses O
GL-GIN B-MethodName
by O
5.2% B-MetricValue
, O
1.2% B-MetricValue

and O
2.1% B-MetricValue
on O
sentencelevel B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
, O
slot B-TaskName
filling I-TaskName
and O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
respectively. O
This O
is O
because O
our O
model O
achieves O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
allowing O
the O
two O
tasks O
to O
provide O
crucial O
clues O
for O
each O
other. O
Besides, O
our O
designed O
HSLGs B-MethodName
and O
HGATs B-MethodName

can O
effectively O
model O
the O
interactions O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
extracting O
the O
indicative O
clues O
from O
initial O
predictions. O
(2) O
Co-guiding B-MethodName
Net I-MethodName
achieves O
a O
larger O
improvement O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
than O
slot B-TaskName
filling I-TaskName
. O
The O
reason O
is O
that O
except O
for O
the O
guidance O
from O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
to O
slot B-TaskName
filling I-TaskName
, O

our O
model O
also O
achieves O
the O
guidance O
from O
slot B-TaskName
filling I-TaskName
to O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
while O
previous O
models O
all O
ignore O
this. O
Besides, O
previous O
methods O
model O
the O
semantics-label O
interactions O
by O
homogeneous B-MethodName
graph I-MethodName
and O
GAT B-MethodName
, O
limiting O
the O
performance. O
Differently, O
our O
model O
uses O
the O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
to O
represent O
different O
relations O
among O
the O

semantic O
nodes O
and O
the O
label O
nodes, O
then O
applies O
the O
proposed O
HGATs B-MethodName
over O
the O
graphs O
to O
achieve O
the O
interactions. O
Consequently, O
their O
performances O
(especially O
on O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
) O
are O
significantly O
inferior O
to O
our O
model. O
(3) O
The O
improvements O
in O
overall B-MetricName
accuracy I-MetricName
are O
much O
sharper. O
We O
suppose O
the O
reason O
is O
that O
the O
achieved O

mutual O
guidances O
make O
the O
two O
tasks O
deeply O
coupled O
and O
allow O
them O
to O
stimulate O
each O
other O
using O
their O
initial O
predictions. O
For O
each O
task, O
its O
final O
outputs O
are O
guided O
by O
its O
and O
another O
task's O
initial O
predictions. O
By O
this O
means, O
the O
correct O
predictions O
of O
the O
two O
tasks O
can O
be O
better O
aligned. O
As O
a O

result, O
more O
test O
samples O
get O
correct O
sentence-level B-TaskName
semantic I-TaskName
frame I-TaskName
parsing I-TaskName
results, O
and O
then O
overall B-MetricName
accuracy I-MetricName
is O
boosted. O
Model O
Analysis O
We O
conduct O
a O
set O
of O
ablation O
experiments O
to O
verify O
the O
advantages O
of O
our O
work O
from O
different O
perspectives, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
Effect O
of O
Slot-to-Intent O
Guidance O
One O
of O

the O
core O
contributions O
of O
our O
work O
is O
achieving O
the O
mutual O
guidances O
between O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
and O
slot B-TaskName
filling I-TaskName
, O
while O
previous O
works O
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot. O
Therefore, O
compared O
with O
previous O
works, O
one O
of O
the O
advantages O
of O
our O
work O
is O
modeling O
the O
slot-tointent O
guidance. O
To O
verify O

this, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
S2I-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O
can O
observe O
that O
Intent B-MetricName
Acc I-MetricName
drops O
by O
2.0% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.8% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
Moreover, O
Overall B-MetricName
Acc I-MetricName
drops O
more O
significantly: O
3.6% B-MetricValue
on O
MixATIS B-DatasetName
and O
0.9% B-MetricValue
on O
MixSNIPS B-DatasetName
. O
This O
proves O
that O
the O

guidance O
from O
slot O
to O
intent O
can O
effectively O
benefit O
multiple B-TaskName
intent I-TaskName
detection I-TaskName
, O
and O
achieving O
the O
mutual O
guidances O
between O
the O
two O
tasks O
can O
significantly O
improve O
Overall B-MetricName
Acc I-MetricName
. O
Besides, O
although O
both O
of O
w/o B-MetricName
S2I-guidance I-MetricName
and O
GL-GIN B-MethodName
only O
leverage O
the O
one-way O
message O
from O
intent O
to O
slot, O
w/o B-MetricName
S2I-guidance I-MetricName
outperforms O
GL-GIN B-MethodName
by O
large O

margins. O
We O
attribute O
this O
to O
our O
proposed O
heterogeneous B-MethodName
semantics-label I-MethodName
graphs I-MethodName
and O
heterogeneous B-MethodName
graph I-MethodName
attention I-MethodName
networks I-MethodName
, O
whose O
advantages O
are O
verified O
in O
Sec. O
3.4.3. O
Effect O
of O
Intent-to-Slot O
Guidance O
To O
verify O
the O
effectiveness O
of O
intent-to-slot O
guidance, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
I2S-guidance I-MetricName
and O
its O
result O
is O
shown O
in O
Table O
2. O
We O

can O
find O
that O
the O
intent-to-slot O
guidance O
has O
a O
significant O
impact O
on O
performance. O
Specifically, O
w/o B-MetricName
I2S-guidance I-MetricName
cause O
nearly O
the O
same O
extent O
of O
performance O
drop O
on O
Overall B-MetricName
Acc I-MetricName
, O
proving O
that O
both O
of O
the O
intent-toslot O
guidance O
and O
slot-to-intent O
guidance O
are O
indispensable O
and O
achieving O
the O
mutual O
guidances O
can O
significantly O
boost O
the O
performance. O
Effect O

of O
HSLGs B-MethodName
and O
HGATs B-MethodName
In O
this O
paper, O
we O
design O
two O
HSLGs B-MethodName
: O
(i.e., O
S2I-SLG B-MethodName
, O
I2S-SLG B-MethodName
) O
and O
two O
HGATs B-MethodName
(i.e., O
S2I-HGAT B-MethodName
, O
I2S-HGAT B-MethodName
). O
To O
verify O
their O
effectiveness, O
we O
design O
a O
variant O
termed O
w/o B-MetricName
relations I-MetricName
by O
removing O
the O
relations O
on O
the O
two O
HSLGs B-MethodName
. O
In O
this O
case, O
S2I-SLG B-MethodName
/ O

I2S-SLG B-MethodName
collapses O
to O
a O
homogeneous B-MethodName
graph I-MethodName
, O
and O
S2I-HGAT B-MethodName
/ O
I2S-HGAT B-MethodName
collapses O
to O
a O
general O
GAT B-MethodName
based O
on O
multi-head O
attentions. O
From O
Table O
2, O
we O
can O
observe O
that O
w/o B-MetricName
relations I-MetricName
obtains O
dramatic O
drops O
on O
all O
metrics O
on O
both O
datasets. O
The O
apparent O
performance O
gap O
between O
w/o B-MetricName
relations I-MetricName
and O
Co-guiding B-MethodName
Net I-MethodName
verifies O
that O

(1) O
our O
proposed O
HSLGs B-MethodName
can O
effectively O
represent O
the O
different O
relations O
among O
the O
semantics O
nodes O
and O
label O
nodes, O
providing O
appropriate O
platforms O
for O
modeling O
the O
mutual O
guidances O
between O
the O
two O
tasks; O
(2) O
our O
proposed O
HGATs B-MethodName
can O
sufficiently O
and O
effectively O
model O
interactions O
between O
the O
semantics O
and O
indicative O
label O
information O
via O
achieving O
the O
relation-specific O

attentive O
information O
aggregation O
on O
the O
HSLGs B-MethodName
. O
Besides, O
although O
w/o B-MetricName
relations I-MetricName
obviously O
un-derperforms O
Co-guiding B-MethodName
Ne I-MethodName
t, O
it O
still O
significantly O
outperforms O
all O
baselines. O
We O
attribute O
this O
to O
the O
fact O
that O
our O
model O
achieves O
the O
mutual O
guidances O
between O
the O
two O
tasks, O
which O
allows O
them O
to O
promote O
each O
other O
via O
cross-task O
correlations. O
et O

al. O
(2021b) O
propose O
a O
Local O
Slot-aware O
GAT O
module O
to O
alleviate O
the O
uncoordinated O
slot O
problem O
(e.g., O
B-singer O
followed O
by O
I-song) O
(Wu O
et O
al., O
2020) O
caused O
by O
the O
non-autoregressive O
fashion O
of O
slot O
filling. O
And O
the O
ablation O
study O
in O
(Qin O
et O
al., O
2021b) O
proves O
that O
this O
module O
effectively O
improves O
the O
slot O
filling O
performance O

by O
modeling O
the O
local O
dependencies O
among O
slot O
hidden O
states. O
Effect O
of O
I2S-HGAT B-MethodName
for O
Capturing O
Local O
Slot O
Dependencies O
Qin O
In O
their O
model O
( O
GL-GIN B-MethodName
), O
the O
local O
dependencies O
are O
modeled O
in O
both O
of O
the O
local B-MethodName
slot-aware I-MethodName
GAT I-MethodName
and O
subsequent O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
. O
We O
suppose O
the O
reason O
why O
GL-GIN B-MethodName
needs O
the O

local B-MethodName
Slotaware I-MethodName
GAT I-MethodName
is O
that O
the O
global B-MethodName
intent-slot I-MethodName
GAT I-MethodName
in O
GL-GIN B-MethodName
cannot O
effectively O
capture O
the O
local O
slot O
dependencies. O
GL-GIN's B-MethodName
global B-MethodName
slot-intent I-MethodName
graph I-MethodName
is O
homogeneous, O
and O
the O
GAT B-MethodName
working O
on O
it O
treats O
the O
slot O
semantics O
nods O
and O
the O
intent O
label O
nodes O
equally O
without O
discrimination. O
Therefore, O
each O
slot O
hidden O
state O
receives O
indiscriminate O

information O
from O
both O
of O
its O
local O
slot O
hidden O
states O
and O
all O
intent O
labels, O
making O
it O
confusing O
to O
capture O
the O
local O
slot O
dependencies. O
In O
contrast, O
we O
believe O
our O
I2S-HLG B-MethodName
and O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
slot O
local O
dependencies O
along O
the O
specific O
slot_semantics_dependencies O
relation, O
which O
is O
modeled O
together O
with O
other O
relations. O
Therefore, O

our O
Co-guiding B-MethodName
Net I-MethodName
does O
not O
include O
another O
module O
to O
capture O
the O
slot O
local O
dependencies. O
To O
verify O
this, O
we O
design O
a O
variant O
termed O
+Local B-MetricName
Slot-aware I-MetricName
GAT I-MetricName
, O
which O
is O
implemented O
by O
augmenting O
Co-guiding B-MethodName
Net I-MethodName
with O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
(Qin O
et O
al., O
2021b) O
that O
not O
only O
the O
Local B-MethodName
Slot-aware I-MethodName
GAT I-MethodName
does O

not O
bring O
improvement, O
it O
even O
causes O
performance O
drops. O
This O
proves O
that O
our O
I2S-HGAT B-MethodName
can O
effectively O
capture O
the O
local O
slot O
dependencies. O
A.1 O
Settings O
To O
evaluate O
Co-guiding B-MethodName
Net's I-MethodName
performance O
based O
on O
the O
pre-trained O
language O
model, O
we O
use O
the O
pretrained O
RoBERTa O
(Liu O
et O
al., O
2019b) O
encoder O
to O
replace O
the O
original O
self-attentive O
encoder. O
We O

adopt O
the O
pre-trained O
RoBERTa-base O
version O
provided O
by O
Transformers O
(Wolf O
et O
al., O
2020). O
For O
each O
word, O
its O
first O
subwords' O
hidden O
state O
generated O
by O
RoBERTa O
is O
taken O
as O
the O
word O
representation. O
AdamW O
(Loshchilov O
and O
Hutter, O
2019) O
optimizer O
is O
used O
for O
model O
training O
with O
the O
default O
setting, O
and O
RoBERTa O
is O
fine-tuned O
with O
model O

training. O
Other O
model O
components O
are O
identical O
to O
the O
Coguiding B-MethodName
Net I-MethodName
based O
on O
LSTM, O
and O
we O
use O
the O
same O
hyper-parameters O
of O
the O
model O
rather O
than O
search O
for O
the O
optimal O
ones O
for O
RoBERTa+Co-guiding O
Net O
due O
to O
our O
limited O
computation O
resource. O
A.2 O
Results O
Table O
3 O
shows O
the O
result O
comparison O
of O
Coguiding B-MethodName
Net I-MethodName
, O

RoBERTa+Co-guiding B-MethodName
Net I-MethodName
, O
and O
their O
state-of-the-art O
counterparts: O
AGIF B-MethodName
, O
GL-GIN B-MethodName
, O
RoBERTa+AGIF B-MethodName
, O
and O
RoBERTa+GL-GIN B-MethodName
. O
We O
can O
find O
that O
although O
RoBERTa O
boosts O
the O
models' O
performance, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
 I-MethodName
still O
significantly O
outperforms O
RoBERTa+AGIF B-MethodName
and O
RoBERTa+GL-GIN B-MethodName
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
although O
the O
pre-trained O
language O
model O
(PTLM) O

can O
enhance O
the O
word O
representations, O
it O
cannot O
achieve O
the O
guidance O
between O
the O
two O
tasks O
or O
the O
interactions O
between O
the O
semantics O
and O
label O
information, O
which O
are O
exactly O
the O
advantages O
of O
our O
Co-guiding B-MethodName
Net I-MethodName
. O
Therefore, O
collaborating O
with O
PTLM O
that O
has O
strong O
ability O
of O
language O
modeling, O
RoBERTa+Co-guiding B-MethodName
Net I-MethodName
gets O
its O
performance O
further O

boosted, O
achieving O
new O
state-of-the-art. O

Learning B-TaskName
a I-TaskName
Grammar I-TaskName
Inducer I-TaskName
from O
Massive O
Uncurated O
Instructional O
Videos O
Video-aided B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
to O
leverage O
video O
information O
for O
finding O
more O
accurate O
syntactic O
grammars O
for O
accompanying O
text. O
While O
previous O
work O
focuses O
on O
building O
systems O
for O
inducing O
grammars O
on O
text O
that O
are O
well-aligned O
with O
video O
content, O
we O
investigate O
the O
scenario, O
in O
which O

text O
and O
video O
are O
only O
in O
loose O
correspondence. O
Such O
data O
can O
be O
found O
in O
abundance O
online, O
and O
the O
weak O
correspondence O
is O
similar O
to O
the O
indeterminacy O
problem O
studied O
in O
language O
acquisition. O
Furthermore, O
we O
build O
a O
new O
model O
that O
can O
better O
learn O
video-span O
correlation O
without O
manually O
designed O
features O
adopted O
by O
previous O
work. O

Experiments O
show O
that O
our O
model O
trained O
only O
on O
large-scale O
YouTube O
data O
with O
no O
textvideo O
alignment O
reports O
strong O
and O
robust O
performances O
across O
three O
unseen O
datasets, O
despite O
domain O
shift O
and O
noisy O
label O
issues. O
Furthermore O
our O
model O
yields O
higher O
F1 B-MetricName
scores O
than O
the O
previous O
state-of-the-art O
systems O
trained O
on O
in-domain O
data. O
Introduction O
Grammar B-TaskName
induction I-TaskName

is O
a O
fundamental O
and O
longlasting O
(Lari O
and O
Young, O
1990;Clark, O
2001;Klein O
and O
Manning, O
2002) O
problem O
in O
computational O
linguistics, O
which O
aims O
to O
find O
hierarchical O
syntactic O
structures O
from O
plain O
sentences. O
Unlike O
supervised O
methods O
(Charniak, O
2000;Collins, O
2003;Petrov O
and O
Klein, O
2007;Zhang O
and O
Clark, O
2011;Cross O
and O
Huang, O
2016;Kitaev O
and O
Klein, O
2018) O
that O
require O
human O
annotated O
treebanks, O

e.g., O
Penn B-MethodName
Treebank I-MethodName
(Marcus O
et O
al., O
1993), O
grammar O
inducers O
do O
not O
rely O
on O
any O
human O
annotations O
for O
training. O
Grammar B-TaskName
induction I-TaskName
is O
attractive O
since O
annotating O
syntactic O
trees O
by O
human O
language O
experts O
is O
expensive O
and O
time O
consuming, O
while O
the O
current O
treebanks O
are O
limited O
to O
several O
major O
languages O
and O
domains. O
Recently, O
deep O
learning O

models O
have O
achieved O
remarkable O
success O
across O
NLP O
tasks, O
and O
neural O
models O
have O
been O
designed O
(Shen O
et O
al., O
2018b,a;Kim O
et O
al., O
2019a,b;Jin O
et O
al., O
2018) O
for O
grammar B-TaskName
induction I-TaskName
, O
which O
greatly O
advanced O
model O
performance O
on O
induction O
with O
raw O
text. O
Recent O
efforts O
have O
started O
to O
consider O
other O
useful O
information O
from O
multiple O
modalities, O

such O
as O
images O
(Shi O
et O
al., O
2019;Jin O
and O
Schuler, O
2020) O
and O
videos O
(Zhang O
et O
al., O
2021). O
Specifically, O
Zhang O
et O
al. O
(2021) O
show O
that O
multi-modal O
information O
(e.g. O
motion, O
sound O
and O
objects) O
from O
videos O
can O
significantly O
improve O
the O
induction O
accuracy O
on O
verb O
and O
noun O
phrases. O
Such O
work O
uses O
curated O
multi-modal O
data O
publicly O

available O
on O
the O
web, O
which O
all O
assume O
that O
the O
meaning O
of O
a O
sentence O
needs O
to O
be O
identical O
(e.g., O
being O
a O
caption) O
to O
the O
corresponding O
video O
or O
image. O
This O
assumption O
limits O
usable O
data O
to O
several O
small-scale O
benchmarks O
(Lin O
et O
al., O
2014;Xu O
et O
al., O
2016;Hendricks O
et O
al., O
2017) O
with O
expensive O
human O
annotations O

on O
image/video O
captions. O
The O
noisy O
correspondence O
between O
form O
and O
meaning O
is O
one O
of O
the O
main O
research O
questions O
in O
language O
acquisition O
(Akhtar O
and O
Montague, O
1999;Gentner O
et O
al., O
2001;Dominey O
and O
Dodane, O
2004), O
where O
different O
proposals O
attempt O
to O
address O
this O
indeterminacy O
faced O
by O
children. O
There O
has O
been O
computational O
work O
incorporating O
such O
indeterminacy O
into O

their O
models O
(Yu O
and O
Siskind, O
2013;Huang O
et O
al., O
2021). O
For O
modeling O
empirical O
grammar O
learning O
with O
multi-modal O
inputs, O
two O
important O
questions O
still O
remain O
open: O
1) O
how O
can O
a O
grammar O
inducer O
benefit O
from O
large-scale O
multi-media O
data O
(e.g., O
YouTube O
videos) O
with O
noisy O
text-to-video O
correspondence? O
and O
2) O
how O
can O
a O
grammar O
inducer O
show O
robust O

performances O
across O
multiple O
domains O
and O
datasets? O
By O
using O
data O
with O
only O
weak O
cross-modal O
correspondence, O
such O
as O
YouTube O
videos O
and O
their O
automatically O
generated O
subtitles, O
we O
allow O
the O
computational O
models O
to O
face O
a O
similar O
indeterminacy O
problem, O
and O
exam-ine O
how O
indeterminacy O
interacts O
with O
data O
size O
to O
influence O
learning O
behavior O
and O
performance O
of O
the O

induction O
models. O
In O
this O
paper, O
we O
conduct O
the O
first O
investigation O
on O
both O
questions. O
Specifically, O
we O
collect O
2.4 O
million O
video O
clips O
and O
the O
corresponding O
subtitles O
from O
instructional O
YouTube O
videos O
( O
HowTo100M B-DatasetName
Miech O
et O
al. O
2019) O
to O
train O
multi-modal O
grammar O
inducers, O
instead O
of O
using O
the O
training O
data O
from O
a O
benchmark O
where O
text O

and O
video O
are O
in O
alignment. O
We O
then O
propose O
a O
novel O
model, O
named O
Pre-Trained B-MethodName
Compound I-MethodName
Probabilistic I-MethodName
Context-Free I-MethodName
Grammars I-MethodName
( O
PTC-PCFG B-MethodName
), O
that O
extends O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
incorporating O
a O
videospan O
matching O
loss O
term O
into O
the O
Compound B-MethodName
PCFG I-MethodName
(Kim O
et O
al., O
2019a) O
model. O
To O
better O
capture O

the O
video-span O
correlation, O
it O
leverages O
CLIP O
(Miech O
et O
al., O
2020), O
a O
state-of-the-art O
model O
pretrained O
on O
video O
subtitle O
retrieval, O
as O
the O
encoders O
for O
both O
video O
and O
text. O
Compared O
with O
previous O
work O
(Zhang O
et O
al., O
2021) O
that O
independently O
extracts O
features O
from O
each O
modality O
before O
merging O
them O
using O
a O
simple O
Transformer O
(Vaswani O
et O

al., O
2017) O
encoder, O
the O
encoders O
of O
our O
model O
have O
been O
pretrained O
to O
merge O
such O
multi-modal O
information, O
and O
no O
human O
efforts O
are O
needed O
to O
select O
useful O
modalities O
from O
the O
full O
set. O
Experiments O
on O
three O
benchmarks O
show O
that O
our O
model, O
which O
is O
trained O
on O
noisy O
YouTube O
video O
clips O
and O
no O
data O
from O

these O
benchmarks, O
produces O
substantial O
gains O
over O
the O
previous O
state-of-the-art O
system O
(Zhang O
et O
al., O
2021) O
trained O
on O
in-domain O
video O
clips O
with O
human O
annotated O
captions. O
Furthermore, O
our O
model O
demonstrates O
robust O
performances O
across O
all O
three O
datasets. O
We O
suggest O
the O
limitations O
of O
our O
model O
and O
future O
directions O
for O
improvements O
through O
analysis O
and O
discussions. O
Code O

will O
be O
released O
upon O
paper O
acceptance. O
In O
summary, O
the O
main O
contributions O
are: O
• O
We O
are O
the O
first O
to O
study O
training O
a O
grammar O
inducer O
with O
massive O
general-domain O
noisy O
video O
clips O
instead O
of O
benchmark O
data, O
introducing O
the O
indeterminacy O
problem O
to O
the O
induction O
model. O
• O
We O
propose O
PTC-PCFG B-MethodName
, O
a O
novel O
model O
for O

unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
It O
is O
simpler O
in O
design O
than O
previous O
models O
and O
can O
better O
capture O
the O
video-text O
matching O
information. O
• O
Trained O
only O
on O
noisy O
YouTube O
videos O
without O
finetuning O
on O
benchmark O
data, O
PTC-PCFG B-MethodName
reports O
stronger O
performances O
than O
previous O
mod-els O
trained O
on O
benchmark O
data O
across O
three O
benchmarks. O
Background O
and O
Motivation O
Compound B-MethodName

PCFGs I-MethodName
A O
PCFG B-MethodName
model O
in O
Chomsky O
Normal O
Form O
can O
be O
defined O
as O
a O
tuple O
of O
6 O
terms O
(S, O
N O
, O
P, O
Σ, O
R, O
Π), O
where O
they O
correspond O
to O
the O
start O
symbol, O
the O
sets O
of O
non-terminals, O
pre-terminals, O
terminals, O
production O
rules O
and O
their O
probabilities. O
Given O
pre-defined O
numbers O
of O
non-terminals O
and O
pre-terminals, O
a O

PCFG B-MethodName
induction O
model O
tries O
to O
estimate O
the O
probabilities O
for O
all O
production O
rules. O
The O
compound B-MethodName
PCFG I-MethodName
( O
C-PCFG B-MethodName
) O
model O
(Kim O
et O
al., O
2019a) O
adopts O
a O
mixture O
of O
PCFGs. O
Instead O
of O
a O
corpus-level O
prior O
used O
in O
previous O
work O
(Kurihara O
and O
Sato, O
2006;Johnson O
et O
al., O
2007;Wang O
and O
Blunsom, O
2013;Jin O
et O
al., O
2018), O

C-PCFG B-MethodName
imposes O
a O
sentence-specific O
prior O
on O
the O
distribution O
of O
possible O
PCFGs. O
Specifically O
in O
the O
generative O
story, O
the O
probability O
π O
r O
for O
production O
rule O
r O
is O
estimated O
by O
model O
g O
that O
assigns O
a O
latent O
variable O
z O
for O
each O
sentence O
σ, O
and O
z O
is O
drawn O
from O
a O
prior O
distribution: O
π O
r O
= O

g(r, O
z; O
θ), O
z O
∼ O
p(z). O
( O
)1 O
where O
θ O
represents O
the O
model O
parameters. O
The O
probabilities O
for O
all O
three O
types O
of O
CFG O
rules O
are O
defined O
as O
follows: O
π O
S→A O
= O
exp(u O
⊤ O
A O
f O
s O
([w O
S O
; O
z])) O
A O
′ O
∈N O
exp(u O
⊤ O
A O
′ O
f O
s O
([w O
S O

; O
z])) O
, O
π O
A→BC O
= O
exp(u O
⊤ O
BC O
[w O
A O
; O
z]) O
B O
′ O
,C O
′ O
∈N O
∪P O
exp(u O
⊤ O
B O
′ O
C O
′ O
[w O
A O
; O
z])) O
, O
π O
T O
→w O
= O
exp(u O
⊤ O
w O
f O
t O
([w O
T O
; O
z])) O
w O
′ O
∈Σ O
exp(u O
⊤ O
w O
′ O

f O
t O
([w O
T O
; O
z])) O
,(2) O
where O
A O
∈ O
N O
, O
B O
and O
C O
∈ O
N O
∪ O
P, O
T O
∈ O
P, O
w O
∈ O
Σ. O
Both O
w O
and O
u O
are O
dense O
vectors O
representing O
words O
and O
all O
types O
of O
non-terminals, O
and O
f O
s O
and O
f O
t O
are O
neural O
encoding O
functions. O
Optimizing O

the O
C-PCFG B-MethodName
model O
involves O
maximizing O
the O
marginal O
likelihood O
p(σ) O
of O
each O
training O
sentence O
σ O
for O
all O
possible O
z: O
log O
p O
θ O
(σ) O
= O
log O
z O
t∈T O
G O
(σ) O
p O
θ O
(t|z)p(z)dz O
(3) O
where O
T O
G O
(σ) O
indicates O
all O
possible O
parsing O
trees O
for O
sentence O
σ. O
Since O
computing O
the O
integral O
over O
z O

is O
intractable, O
this O
objective O
is O
optimized O
by O
maximizing O
its O
evidence O
lower O
bound O
ELBO(σ; O
ϕ, O
θ): O
ELBO(σ; O
ϕ, O
θ) O
= O
E O
q O
ϕ O
(z|σ) O
[log O
p O
θ O
(σ|z)] O
−KL[q O
ϕ O
(z|σ)||p(z)],(4) O
where O
q O
ϕ O
(z|σ) O
is O
the O
variational O
posterior O
calculated O
by O
another O
neural O
network O
with O
parameters O
ϕ. O
Given O
a O
sampled O
z, O

the O
log-likelihood O
term O
log O
p O
θ O
(σ|z) O
is O
calculated O
via O
the O
inside O
algorithm. O
The O
KL O
term O
can O
be O
computed O
analytically O
when O
both O
the O
prior O
p(z) O
and O
the O
variational O
posterior O
q O
ϕ O
(z|σ) O
are O
Gaussian O
(Kingma O
and O
Welling, O
2014). O
Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
Multi-Modal B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
MMC-PCFG B-MethodName
) O
(Zhang O
et O
al., O

2021) O
extends O
C-PCFG B-MethodName
with O
a O
model O
to O
match O
a O
video O
v O
with O
a O
span O
c O
in O
a O
parse O
tree O
t O
of O
a O
sentence O
σ. O
It O
extracts O
M O
visual O
and O
audio O
features O
from O
a O
video O
v O
and O
encodes O
them O
via O
a O
multi-modal O
transformer O
(Gabeur O
et O
al., O
2020), O
denoted O
as O
Ψ O
= O

{ψ O
i O
} O
M O
i=1 O
. O
The O
word O
representation O
h O
i O
of O
the O
ith O
word O
is O
computed O
by O
BiLSTM. O
Given O
a O
particular O
span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
, O
its O
representation O
c O
is O
the O
weighted O
sum O
of O
all O
label-specific O
span O
representations: O
c O
= O
|N O

| O
k=1 O
p(k|c, O
σ)f O
k O
1 O
j O
− O
i O
+ O
1 O
j O
l=i O
h O
l O
, O
(5) O
where O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|} O
are O
the O
phrasal O
label O
probabilities O
of O
span O
c. O
The O
representation O
of O
a O
span O
c O
is O
then O
correspondingly O
projected O
to O
M O
separate O
embeddings O
via O
gated O
embedding O

(Miech O
et O
al., O
2018), O
denoted O
as O
Ξ O
= O
{ξ O
i O
} O
M O
i=1 O
. O
Finally O
the O
video-text O
matching O
loss O
is O
defined O
as O
a O
sum O
over O
all O
video-span O
matching O
losses O
weighted O
by O
the O
marginal O
probability O
of O
a O
span O
from O
the O
parser: O
s O
mm O
(v, O
σ) O
= O
c∈σ O
p(c|σ)h O
mm O
(Ξ, O
Ψ), O

(6) O
where O
h O
mm O
(Ξ, O
Ψ) O
is O
a O
hinge O
loss O
measuring O
the O
distances O
from O
video O
v O
to O
the O
matched O
and O
unmatched O
(i.e. O
span O
from O
another O
sentence) O
span O
c O
and O
c O
′ O
and O
the O
distances O
from O
span O
c O
to O
the O
matched O
and O
unmatched O
(i.e. O
another O
video) O
video O
v O
and O
v O
′ O

: O
ω O
i O
(c) O
= O
exp(u O
⊤ O
i O
c) O
M O
j=1 O
exp(u O
⊤ O
j O
c) O
,(7) O
o(Ξ,Ψ) O
= O
M O
i=1 O
ω O
i O
(c)cos(ξ O
i O
, O
ψ O
i O
),(8) O
h O
mm O
(Ξ,Ψ) O
= O
E O
c O
′ O
[o(Ξ O
′ O
, O
Ψ) O
− O
o(Ξ, O
Ψ)) O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[o(Ξ, O

Ψ O
′ O
) O
− O
o(Ξ, O
Ψ) O
+ O
ϵ] O
+ O
, O
(9) O
where O
Ξ O
′ O
is O
a O
set O
of O
unmatched O
span O
expert O
embeddings O
of O
Ψ, O
Ψ O
′ O
is O
a O
set O
of O
unmatched O
video O
representations O
of O
Ξ, O
ϵ O
is O
a O
positive O
margin, O
[•] O
+ O
= O
max(0, O
•), O
{u O
i O
} O
M O
i=1 O

are O
learned O
weights, O
and O
the O
expectations O
are O
approximated O
with O
one O
sample O
drawn O
from O
the O
training O
data. O
During O
training, O
both O
ELBO O
and O
the O
video-text O
matching O
loss O
are O
jointly O
optimized. O
Limitation O
and O
Motivation O
Existing O
work O
on O
multi-modal B-TaskName
grammar I-TaskName
induction I-TaskName
aims O
at O
leveraging O
strict O
correspondence O
between O
image/video O
and O
text O
for O
information O
about O
syntactic O

categories O
and O
structures O
of O
the O
words O
and O
spans O
in O
the O
text. O
However, O
such O
datasets O
are O
expensive O
to O
annotate. O
Besides, O
the O
ambiguous O
correspondence O
between O
language O
and O
real-world O
context, O
observed O
in O
language O
acquisition, O
is O
not O
really O
reflected O
in O
such O
training O
setups. O
As O
a O
result, O
we O
believe O
that O
the O
previous O
work O
fails O
to O

answer O
the O
following O
important O
questions: O
1) O
how O
well O
a O
grammar O
inducer O
would O
perform O
when O
it O
is O
trained O
only O
on O
noisy O
multi-media O
data; O
2) O
how O
the O
scale O
of O
training O
data O
would O
affect O
the O
performance O
and O
cross-domain O
robustness? O
Training O
a O
Grammar O
Inducer O
with O
Massive O
YouTube O
Videos O
We O
make O
the O
first O
investigation O
into O

the O
above O
questions O
by O
leveraging O
massive O
video O
clips O
from O
instructional O
YouTube O
videos O
to O
train O
our O
grammar O
inducer. O
Different O
from O
the O
benchmark O
data O
used O
by O
previous O
work, O
the O
YouTube O
video O
clips O
do O
not O
contain O
paired O
sentences. O
This O
section O
will O
first O
introduce O
the O
method O
for O
generating O
noisy O
training O
instances O
(video O
clip O
and O

sentence O
pairs) O
from O
YouTube O
videos O
( O
§3.1), O
before O
describing O
a O
novel O
grammar O
induction O
model O
( O
§3.2) O
with O
pre-trained O
text O
and O
video O
encoders. O
Harvesting O
Training O
Instances O
from O
YouTube O
Videos O
Given O
a O
YouTube O
video, O
we O
would O
like O
to O
generate O
a O
set O
of O
video O
clip O
and O
subtitle O
pairs O
Ω O
= O
{(v, O
σ)}, O
where O

each O
subtitle O
σ O
is O
a O
complete O
sentence O
and O
is O
aligned O
in O
time O
with O
its O
paired O
video O
clip O
v. O
To O
this O
end, O
the O
YouTube O
API O
is O
chosen O
to O
obtain O
all O
subtitles O
of O
the O
video. O
But, O
our O
observation O
finds O
that O
most O
obtained O
subtitles O
are O
not O
complete O
sentences, O
and O
in O
some O
cases, O
a O

complete O
sentence O
can O
last O
for O
several O
continuous O
video O
fragments. O
Meanwhile, O
they O
do O
not O
contain O
any O
punctuation, O
which O
is O
a O
key O
factor O
for O
sentence O
segmentation. O
As O
shown O
in O
the O
top O
part O
of O
Figure O
1, O
we O
design O
an O
algorithm O
that O
takes O
the O
following O
steps O
to O
find O
each O
complete O
sentence O
and O
its O
corresponding O

video O
clip. O
Sentence B-TaskName
segmentation I-TaskName
. O
In O
the O
first O
step, O
we O
try O
to O
find O
complete O
sentences O
from O
the O
subtitles. O
We O
first O
concatenate O
all O
subtitles O
from O
the O
same O
video O
are O
concatenated O
into O
a O
very O
long O
sequence O
of O
tokens. O
Next, O
a O
punctuation O
restoration O
model O
1 O
(Tilk O
and O
Alumäe, O
2016) O
is O
adopted O
to O
insert O

punctuation O
into O
the O
sequence. O
Lastly, O
sentences O
are O
segmented O
based O
on O
certain O
punctuation O
(e.g., O
".", O
"?", O
"!"). O
Video B-TaskName
clip I-TaskName
extraction I-TaskName
. O
In O
the O
second O
step, O
we O
trim O
the O
corresponding O
video O
clips. O
Each O
raw O
subtitle O
contains O
its O
start O
and O
an O
end O
times. O
We O
assume O
each O
word O
within O
the O
raw O
subtitle O
occupies O
equal O

time O
and O
record O
the O
start O
and O
end O
times O
for O
1 O
We O
manually O
punctuate O
subtitles O
from O
10 O
videos O
randomly O
selected O
from O
HowTo100M B-DatasetName
, O
which O
contains O
461 O
sentences O
after O
annotation. O
The O
punctuation B-MethodName
restoration I-MethodName
model I-MethodName
has O
an O
overall O
F1 B-MetricName
score O
of O
74.1% B-MetricValue
with O
the O
manual O
labels. O
each O
word. O
After O
that, O
given O
a O
complete O

sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
..., O
w O
N O
, O
we O
use O
the O
start O
time O
of O
its O
first O
word O
w O
1 O
and O
the O
end O
time O
of O
its O
last O
word O
w O
N O
as O
the O
start O
and O
end O
times O
of O
σ. O
Lastly, O
we O
segment O
a O
complete O
sentence O
σ's O
corresponding O

video O
clip O
v O
based O
on O
its O
start O
and O
end O
times. O
Model: O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
After O
harvesting O
large-scale O
sentence O
and O
video O
pairs, O
the O
next O
step O
is O
to O
build O
a O
strong O
grammar O
induction O
model O
that O
can O
benefit O
from O
them. O
In O
this O
section, O
we O
introduce O
our O
Pre-Trained B-MethodName
Compound I-MethodName
PCFGs I-MethodName
( O
PTC-PCFG B-MethodName
) O
model O

for O
unsupervised B-TaskName
grammar I-TaskName
induction I-TaskName
. O
As O
shown O
in O
the O
lower O
part O
of O
Figure O
1, O
the O
PTC-PCFG B-MethodName
model O
composes O
of O
a O
video O
encoder, O
a O
span O
encoder O
and O
a O
parsing O
model. O
Both O
the O
video O
encoder O
and O
the O
span O
encoder O
are O
initialized O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020), O
a O
pre-trained O
video-text O

matching O
model O
that O
takes O
a O
simple O
design O
and O
has O
shown O
superior O
zero-shot O
results O
on O
many O
video O
understanding O
tasks, O
such O
as O
video O
retrieval, O
video O
question O
answering, O
etc. O
We O
first O
introduce O
the O
pre-trained O
video O
and O
span O
encoders, O
before O
covering O
the O
training O
and O
inference O
details O
of O
PTC-PCFG B-MethodName
. O
Video O
encoding. O
The O
first O
step O

is O
to O
encode O
a O
video O
v O
to O
its O
representation O
v. O
To O
do O
this, O
we O
first O
segment O
v O
into O
small O
video O
clips, O
where O
each O
video O
clip O
v O
i O
consists O
of O
T O
frames. O
Following O
Zhang O
et O
al. O
(2021), O
we O
sample O
L O
video O
clips O
with O
equal O
interval O
for O
efficiency. O
We O
use O
the O
video O

encoder O
from O
the O
MIL-NCE B-MethodName
model O
(Miech O
et O
al., O
2020) O
as O
our O
video O
encoder O
and O
only O
fine-tune O
its O
last O
fully O
connected O
layer O
f O
v O
for O
efficiency. O
In O
more O
detail, O
for O
each O
sampled O
video O
clip, O
we O
pre-compute O
the O
input O
of O
f O
v O
as O
its O
representation, O
denoted O
as O
{h O
v O
i O
} O
L O

i=1 O
. O
Then O
we O
feed O
them O
into O
f O
v O
and O
average O
the O
output O
as O
its O
representation O
v, O
denoted O
as, O
v O
= O
AvgPool({f O
v O
(h O
v O
i O
)} O
L O
i=1 O
),(10) O
where O
AvgPool O
indicates O
average O
pooling. O
Span O
encoding. O
The O
next O
step O
is O
to O
compute O
a O
span O
representation O
c O
for O
each O
particular O

span O
c O
= O
w O
i O
, O
. O
. O
. O
, O
w O
j O
(1 O
≤ O
i O
< O
j O
≤ O
N O
) O
in O
sentence O
σ O
= O
w O
1 O
, O
w O
2 O
, O
. O
. O
. O
, O
w O
N O
. O
The O
pre-trained O
text O
encoder O
of O
MIL-NCE B-MethodName
consists O
of O
a O
word O
embedding O
layer O
and O

two O
stacked O
fully O
connected O
layers, O
f O
c O
0 O
and O
f O
c O
1 O
. O
Motivated O
by O
Zhao O
and O
Titov O
(2020); O
Zhang O
et O
al. O
(2021), O
we O
expect O
to O
learn O
|N O
| O
different O
span O
representations, O
each O
is O
specified O
for O
one O
non-terminal O
node. O
However, O
directly O
applying O
the O
pre-trained O
text O
encoder O
is O
not O
feasible, O
since O

it O
has O
only O
one O
output O
layer O
f O
c O
1 O
. O
Therefore, O
we O
duplicate O
f O
c O
1 O
for O
|N O
| O
times, O
denoted O
as O
{f O
c O
k O
} O
|N O
| O
k=1 O
, O
and O
compose O
|N O
| O
label-specific O
output O
layers. O
In O
more O
detail, O
we O
first O
encode O
each O
word O
w O
i O
with O
the O
word O

embedding O
layer, O
denoted O
as O
h O
c O
i O
. O
Then O
we O
feed O
the O
word O
embeddings O
to O
f O
c O
0 O
, O
ReLU, O
maximum O
pooling O
and O
each O
label-specific O
output O
layer O
sequentially. O
we O
also O
compute O
the O
probabilities O
of O
its O
phrasal O
labels O
{p(k|c, O
σ)|1 O
≤ O
k O
≤ O
|N O
|}, O
as O
illustrated O
in O
Section O
2.1. O
Lastly, O

the O
span O
representation O
c O
is O
the O
sum O
of O
all O
label-specific O
span O
representations O
weighted O
by O
the O
probabilities O
we O
predicted, O
denoted O
as: O
τ O
= O
MaxPool(ReLU(f O
c O
0 O
(h O
c O
i O
))) O
c O
= O
|N O
| O
k=1 O
p(k|c, O
σ)f O
c O
k O
(τ O
),(11) O
where O
MaxPool O
is O
a O
maximum O
pooling O
operation O
and O
ReLU O
is O

a O
ReLU O
activation O
function. O
Training. O
As O
shown O
in O
lower O
left O
of O
Figure O
1, O
we O
optimize O
both O
the O
video-text O
matching O
loss O
and O
evidence O
lower O
bound O
during O
training. O
We O
first O
compute O
the O
similarity O
between O
a O
video O
clip O
v O
and O
a O
particular O
span O
c O
via O
dot O
product O
and O
then O
compute O
a O
triplet B-MetricName
hinge I-MetricName

loss I-MetricName
as O
following, O
h(v, O
c) O
= O
E O
c O
′ O
[c O
′ O
• O
v O
− O
c O
• O
v O
+ O
ϵ] O
+ O
+ O
E O
v O
′ O
[c O
• O
v O
′ O
− O
c O
• O
v O
+ O
ϵ] O
+ O
, O
(12 O
) O
where O
ϵ O
is O
a O
positive O
margin, O
[•] O
+ O
= O
max(0, O
•), O
v O

′ O
is O
a O
clip O
from O
a O
different O
video O
and O
c O
′ O
is O
a O
span O
from O
a O
different O
sentence. O
The O
video-text B-MetricName
matching I-MetricName
loss I-MetricName
is O
correspondingly O
defined O
as, O
s(v, O
σ) O
= O
Σ O
c∈σ O
p(c|σ)h(v, O
c),(13) O
where O
p(c|σ) O
is O
the O
probability O
of O
a O
particular O
span O
c O
being O
a O
syntactic O
phrase. O
Finally, O
the O
overall O

loss O
function O
is O
composed O
by O
the O
ELBO O
and O
the O
videotext B-MetricName
matching I-MetricName
loss I-MetricName
: O
L(ϕ, O
θ) O
= O
(v,σ)∈Ω O
−ELBO(σ; O
ϕ, O
θ) O
+ O
αs(v, O
σ),(14) O
where O
α O
is O
a O
constant O
balancing O
these O
two O
terms. O
Inference. O
During O
inference, O
given O
a O
sentence O
σ, O
we O
predict O
the O
most O
likely O
tree O
t O
* O
without O
accessing O
videos, O

as O
shown O
in O
the O
lower O
right O
of O
Figure O
1. O
Since O
computing O
the O
integral O
over O
z O
is O
intractable, O
we O
estimate O
t O
* O
with O
the O
following O
approximation, O
t O
* O
= O
arg O
max O
t O
z O
p O
θ O
(t|z)p O
θ O
(z|σ)dz O
≈ O
arg O
max O
t O
p O
θ O
(t|σ, O
µ O
ϕ O
(σ)),(15) O
where O
µ O
ϕ O

(σ) O
is O
the O
mean O
vector O
of O
the O
variational O
posterior O
q O
ϕ O
(z|σ), O
and O
t O
* O
is O
obtained O
by O
the O
CYK O
algo. O
(Cocke, O
1969;Younger, O
1967;Kasami, O
1966). O
Evaluation O
We O
discard O
punctuation, O
lowercase O
all O
words, O
replace O
numbers O
with O
a O
special O
token O
and O
ignore O
trivial O
single-word O
and O
sentence-level O
spans O
during O
testing O
following O
Kim O
et O

al. O
(2019a). O
Besides, O
we O
follow O
previous O
work O
(Shi O
et O
al., O
2019;Zhang O
et O
al., O
2021) O
by O
using O
a O
state-of-the-art O
constituency O
parser O
(Benepar O
Kitaev O
et O
al. O
2019) O
to O
obtain O
the O
reference O
trees O
for O
evaluation O
2 O
. O
Following O
Shi O
et O
al. O
(2020); O
Zhang O
et O
al. O
(2021), O
all O
models O
are O
run O
5 O
times O
for O

1 O
epoch O
with O
different O
random O
seeds. O
For O
each O
model, O
we O
report O
the O
averaged B-MetricName
sentence-level I-MetricName
F1 I-MetricName
( O
S-F1 B-MetricName
) O
and O
corpus-level B-MetricName
F1 I-MetricName
( O
C-F1 B-MetricName
) O
of O
its O
runs O
on O
each O
testing O
set. O
Implementation O
Details O
We O
use O
Spacy O
3 O
for O
tokenization O
and O
keep O
sentences O
with O
fewer O
than O
40 O
words O
for O
training O
due O

to O
the O
limited O
computational O
resources. O
Each O
video O
is O
decoded O
at O
16 B-HyperparameterValue
fps B-HyperparameterName
and O
L B-HyperparameterName
= O
8 B-HyperparameterValue
video B-HyperparameterName
clips I-HyperparameterName
are O
sampled O
in O
total, O
where O
each O
clip O
contains O
T B-HyperparameterName
= O
16 B-HyperparameterValue
frames B-HyperparameterName
. O
We O
train O
baseline O
models, O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
with O
the O
same O
hyper-parameters O
suggested O
by O
Kim O
et O
al. O
(2019a) O
and O
Zhang O

et O
al. O
(2021). O
The O
parsing O
model O
of O
PTC-PCFG B-MethodName
has O
the O
same O
hyperparameter O
setting O
as O
C-PCFG B-MethodName
and O
MMC-PCFG B-MethodName
(Please O
refer O
their O
papers O
for O
details). O
The O
constant O
α B-HyperparameterName
is O
set O
to O
1 B-HyperparameterValue
. O
We O
select O
the O
top O
20 O
000 O
most O
common O
words O
in O
HowTo100M B-DatasetName
as O
vocabulary O
for O
all O
datasets. O
All O
baseline O
methods O

and O
ours O
are O
optimized O
by O
Adam O
(Kingma O
and O
Ba, O
2015) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
, O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.75 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O
All O
parameters O
(except O
the O
video-text O
matching O
model O
in O
PTC-PCFG B-MethodName
) O
are O
initialized O
with O
Xavier O
uniform O
initializer O
(Glorot O
and O
Bengio, O
2010). O
All O
our O
models O

in O
experiments O
are O
trained O
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
without O
finetuning O
on O
the O
2 O
For O
each O
dataset, O
we O
randomly O
select O
50 O
sentences O
and O
manually O
label O
their O
constituency O
parse O
trees. O
Benepar O
has O
S-F1 B-MetricName
scores O
of O
98.1% B-MetricValue
( O
DiDeMo B-DatasetName
), O
97.2% B-MetricValue
( O
YouCook2 B-DatasetName
) O
and O
98.1% B-MetricValue
( O
MSRVTT B-DatasetName

) O
with O
manual O
labels. O
3 O
https://spacy.io/ O
target O
dataset. O
Main O
Results O
Figure O
Cross-dataset O
Evaluation O
We O
evaluate O
the O
robustness O
of O
models O
across O
different O
datasets, O
as O
shown O
in O
Table O
1. O
Comparing O
MMC-PCFG B-MethodName
trained O
on O
in-domain O
datasets O
(Row O
1-3), O
we O
can O
observe O
that O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
achieves O
the O
best O
overall O
performance, O
while O
MMC-PCFG B-MethodName

trained O
on O
YouCook2 B-DatasetName
is O
the O
worst. O
We O
believe O
this O
is O
due O
to O
the O
different O
number O
of O
training O
instances O
5 O
and O
the O
domain O
gap O
between O
different O
datasets. O
Comparing O
Rows O
1-4, O
we O
can O
observe O
that O
the O
MMC-PCFG B-MethodName
model O
trained O
on O
HT(592k) B-DatasetName
(Row O
4) O
is O
the O
best O
or O
the O
second O
place O
regarding O
C-F1 B-MetricName

and O
S-F1 B-MetricName
compared O
with O
its O
variants O
trained O
on O
in-domain O
datasets O
(Rows O
1-3). O
This O
demonstrates O
that O
the O
our O
processed O
videotext O
training O
instances O
are O
abundant, O
rich O
in O
content O
and O
can O
serve O
for O
general O
purpose. O
Comparing O
Rows O
4 O
and O
5, O
PTC-PCFG B-MethodName
outperforms O
MMC-PCFG B-MethodName
in O
both O
C-F1 B-MetricName
and O
S-F1 B-MetricName
in O
all O
three O
datasets O
and O

has O
smaller O
variance. O
This O
demonstrate O
that O
our O
model O
can O
leverage O
pre-trained O
video-text O
matching O
knowledge O
and O
learn O
consistent O
grammar O
induction. O
Effectiveness O
of O
Pre-Training O
In O
this O
section, O
we O
explore O
how O
different O
pretrained O
video O
and O
text O
encoders O
can O
affect O
the O
parsing O
performance, O
and O
the O
results O
are O
shown O
in O
Table O
2. O
In O
particular, O
we O

study O
different O
(Zhang O
et O
al., O
2021;Zhao O
and O
Titov, O
2020), O
a O
pre-trained O
TinyBERT O
(Jiao O
et O
al., O
2020) O
model, O
the O
text O
encoder O
from O
MIL-NCE B-MethodName
(Miech O
et O
al., O
2020), O
and O
the O
text O
encoder O
from O
CLIP O
(Radford O
et O
al., O
2021). O
Comparing O
Rows O
1 O
with O
2, O
we O
can O
observe O
that O
MM O
is O
better O
than O
the O

video O
encoder O
of O
MIL-NCE B-MethodName
regarding O
C-F1 B-MetricName
and O
S-F1 B-MetricName
on O
all O
three O
datasets, O
as O
MM O
provides O
more O
comprehensive O
video O
features. O
By O
comparing O
row O
1 O
with O
3, O
we O
can O
also O
observe O
that O
TinyBERT B-MethodName
, O
which O
is O
distilled O
from O
BERT O
(Devlin O
et O
al., O
2019), O
outperforms O
the O
randomly O
initialized O
LSTM O
encoder. O
However, O
both O
MM B-MethodName

and O
TinyBERT B-MethodName
are O
independently O
trained O
only O
on O
vision O
or O
language O
tasks, O
where O
the O
vision-language O
correspondences O
are O
not O
considered O
during O
pretraining. O
Therefore, O
we O
further O
investigate O
the O
encoders O
jointly O
pre-trained O
on O
large O
scale O
multimedia O
datasets, O
including O
the O
video-text O
matching O
model O
MIL-NCE B-MethodName
(Row O
4) O
and O
the O
image-text O
matching O
model O
CLIP O
(Row O
5). O
We O

can O
observe O
that O
by O
leveraging O
both O
video O
and O
text O
encoders O
in O
MIL-NCE B-MethodName
can O
improve O
the O
parsing O
performance O
by O
a O
large O
margin O
on O
all O
three O
datasets. O
On O
the O
other O
hand, O
CLIP O
does O
not O
perform O
well, O
since O
it O
is O
designed O
for O
static O
images O
and O
other O
multi-modal O
information O
(e.g., O
motion) O
is O
ignored. O
Qualitative O

Analysis O
In O
figure O
5, O
we O
visualize O
a O
parser O
tree O
predicted O
by O
the O
best O
run O
of O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-MethodName
, O
MMC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
and O
PTC-PCFG B-MethodName
trained O
on O
HT(296k) B-DatasetName
, O
as O
well O
as O
its O
reference O
tree. O
We O
can O
observe O
that O
C-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
fails O

at O
noun O
phrase O
"a O
lady", O
while O
MMC-PCFG B-MethodName
trained O
on O
MSRVTT B-DatasetName
succeeds. O
MMC-PCFG B-MethodName
can O
be O
further O
improved O
by O
training O
on O
HT(296k) B-DatasetName
, O
however, O
fails O
at O
noun O
phrase O
"the O
groceries O
she O
had O
kept O
in O
her O
refrigerator". O
Our O
PTC-PCFG B-MethodName
can O
leverage O
the O
pretrained O
matching O
knowledge O
and O
make O
the O
correct O
prediction. O

Learning O
to O
Adapt O
to O
Low-Resource B-TaskName
Paraphrase I-TaskName
Generation I-TaskName
Paraphrase B-TaskName
generation I-TaskName
is O
a O
longstanding O
NLP O
task O
and O
achieves O
great O
success O
with O
the O
aid O
of O
large O
corpora. O
However O
, O
transferring O
a O
paraphrasing O
model O
to O
another O
domain O
encounters O
the O
problem O
of O
domain O
shifting O
especially O
when O
the O
data O
is O
sparse. O
At O
the O
same O
time O

, O
widely O
using O
large O
pre-trained O
language O
models O
( O
PLMs O
) O
faces O
the O
overfitting O
problem O
when O
training O
on O
scarce O
labeled O
data. O
To O
mitigate O
these O
two O
issues O
, O
we O
propose O
, O
LAPA B-MethodName
, O
an O
effective O
adapter O
for O
PLMs O
optimized O
by O
meta-learning. O
LAPA B-MethodName
has O
three-stage O
training O
on O
three O
types O
of O
related O
resources O
to O

solve O
this O
problem O
: O
1. O
pre-training O
PLMs O
on O
unsupervised O
corpora O
, O
2. O
inserting O
an O
adapter O
layer O
and O
meta-training O
on O
source O
domain O
labeled O
data O
, O
and O
3. O
fine-tuning O
adapters O
on O
a O
small O
amount O
of O
target O
domain O
labeled O
data. O
This O
method O
enables O
paraphrase O
generation O
models O
to O
learn O
basic O
language O
knowledge O
first O
, O

then O
learn O
the O
paraphrasing O
task O
itself O
later O
, O
and O
finally O
adapt O
to O
the O
target O
task. O
Our O
experimental O
results O
demonstrate O
that O
LAPA B-MethodName
achieves O
state-of-the-art O
in O
supervised O
, O
unsupervised O
, O
and O
low-resource O
settings O
on O
three O
benchmark O
datasets. O
With O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
labeled O
data O
of O
the O
target O

task O
, O
our O
approach O
can O
achieve O
a O
competitive O
performance O
with O
previous O
work O
. O
Introduction O
Paraphrase B-TaskName
generation I-TaskName
can O
comprehend O
a O
sentence O
and O
generate O
another O
with O
the O
same O
semantics O
but O
with O
variations O
in O
lexicon O
or O
syntax O
, O
which O
has O
various O
applications O
on O
downstream O
tasks O
including O
query O
rewriting O
( O
Dong O
et O
al. O
, O

2017 O
) O
, O
data O
augmentation O
( O
Iyyer O
et O
al. O
, O
2018 O
) O
and O
language O
model O
pre-training O
( O
Lewis O
et O
al. O
, O
2020a O
) O
. O
Conventional O
approaches O
( O
Prakash O
et O
al. O
, O
2016 O
; O
Chowdhury O
et O
al. O
, O
2022 O
) O
model O
the O
paraphrase B-TaskName
generation I-TaskName
as O
a O
supervised O
encoding-decoding O
problem O
, O
inspired O

by O
machine O
translation O
systems. O
However O
, O
the O
success O
of O
these O
methods O
often O
relies O
on O
a O
large O
number O
of O
parallel O
paraphrases O
, O
whose O
collection O
is O
timeconsuming O
and O
requires O
a O
lot O
of O
domain O
knowledge. O
Therefore O
, O
in O
real O
scenarios O
with O
a O
small O
amount O
of O
parallel O
data O
, O
the O
model O
suffers O
from O
performance O

drops O
facing O
domain O
gaps. O
This O
phenomenon O
, O
known O
as O
domain O
shift O
problem O
( O
Pan O
and O
Yang O
, O
2009 O
) O
, O
comes O
from O
the O
representation O
gap O
between O
training O
and O
testing O
domains O
with O
different O
writing O
styles O
or O
forms O
. O
To O
tackle O
this O
problem O
, O
unsupervised O
methods O
such O
as O
editing-based O
approaches O
( O
Bowman O

et O
al. O
, O
2016 O
; O
Miao O
et O
al. O
, O
2019 O
) O
or O
reinforcement O
learning O
( O
Li O
et O
al. O
, O
2018 O
; O
Siddique O
et O
al. O
, O
2020 O
) O
, O
and O
weakly-supervised O
methods O
such O
as O
retrievalenhanced O
( O
Ding O
et O
al. O
, O
2021 O
; O
Yin O
et O
al. O
, O
2022 O
) O
or O
prompt-based O
do O

not O
introduce O
or O
only O
introduce O
a O
small O
number O
of O
supervised O
signals O
, O
which O
limits O
their O
performance O
such O
that O
underperforms O
supervised O
methods. O
In O
fact O
, O
largescale O
unlabeled O
corpus O
data O
( O
UCD O
) O
and O
labeled O
source O
domain O
data O
( O
LSDD O
) O
, O
as O
well O
as O
a O
few O
labeled O
target O
domain O
data O
( O

LTDD O
) O
, O
can O
be O
easily O
achieved. O
Therefore O
, O
we O
propose O
a O
new O
three-stage O
learning O
paradigm O
: O
pre-training O
, O
meta-learning O
, O
and O
fine-tuning O
, O
aiming O
to O
leverage O
the O
pre-trained O
knowledge O
on O
UCD O
, O
source O
domain O
knowledge O
on O
LSDD O
, O
and O
adapt O
to O
target O
domain O
on O
LSDD O
to O
improve O
the O
performance O

of O
low-resource O
paraphrase O
generation. O
In O
order O
to O
successfully O
implement O
this O
learning O
paradigm O
, O
we O
propose O
a O
simple O
yet O
effective O
model O
which O
combined O
pre-trained O
language O
model O
( O
PLM O
) O
and O
MAML O
( O
Finn O
et O
al. O
, O
2017 O
) O
, O
named O
Learning O
to O
Adapt O
to O
low-resource B-MethodName
PAraphrase I-MethodName
generation I-MethodName
( O
LAPA B-MethodName
) O
. O

Specifically O
, O
before O
meta-learning O
, O
we O
insert O
an O
adapter O
layer O
into O
each O
transformer O
layer O
of O
PLM. O
An O
adapter O
layer O
is O
composed O
of O
a O
few O
parameters O
of O
feedforward O
layer O
and O
residual O
connection. O
During O
meta-training O
and O
fine-tuning O
, O
only O
the O
adapter O
layer O
and O
normalization O
layer O
are O
trainable. O
Parameter O
freezing O
and O
residual O
connection O

can O
retain O
the O
prior O
knowledge O
of O
PLM O
to O
avoid O
negative O
transfer O
effects. O
Smaller-scale O
parameter O
updating O
can O
prevent O
MAML O
from O
gradient O
explosion O
or O
diminishing O
problems O
when O
the O
number O
of O
MAML O
inner O
loop O
iterations O
and O
model O
depth O
increase O
( O
Antoniou O
et O
al. O
, O
2019 O
) O
or O
training O
data O
is O
extremely O
scarce O
. O

Overall O
, O
we O
hold O
the O
idea O
that O
paraphrasing O
is O
a O
fundamental O
ability O
of O
human O
beings. O
The O
paraphrase O
model O
should O
not O
rely O
on O
domain O
and O
seen O
data. O
Therefore O
, O
we O
are O
committed O
to O
characterizing O
the O
basic O
ability O
of O
the O
paraphrase O
model O
, O
obtaining O
gains O
from O
each O
domain O
, O
and O
applying O
it O

to O
a O
specific O
domain. O
Our O
contributions O
are O
summarized O
as O
follows O
: O
• O
We O
define O
a O
novel O
three O
stages O
learning O
paradigm O
for O
low-resource B-TaskName
paraphrase I-TaskName
generation I-TaskName
in O
data O
scarcity O
scenarios O
. O
• O
We O
propose O
that O
LAPA B-MethodName
implement O
this O
learning O
paradigm O
, O
which O
transferred O
the O
PLM O
knowledge O
and O
source O
domain O
knowledge O
to O
complete O

the O
low-resource O
learning O
in O
the O
target O
domain O
quickly O
and O
with O
high O
quality O
. O
• O
The O
supervised O
, O
unsupervised O
and O
weakly O
supervised O
experimental O
results O
of O
LAPA B-MethodName
on O
three O
benchmark O
datasets O
achieve O
state-of-theart O
( O
SOTA O
) O
. O
LAPA B-MethodName
with O
only O
2 O
% O
of O
trainable O
parameters O
and O
1 O
% O
target O
task O
labeled O
data O

can O
achieve O
a O
competitive O
performance O
with O
previous O
works O
. O
Related O
Work O
While O
the O
paraphrase O
generation O
performance O
is O
greatly O
improved O
with O
various O
supervised O
techniques O
( O
Zhao O
et O
al. O
, O
2008 O
; O
Prakash O
et O
al. O
, O
2016 O
; O
Egonmwan O
and O
Chali O
, O
2019 O
; O
Cao O
and O
Wan O
, O
2020 O
; O
Hosking O
and O

Lapata O
, O
2021 O
; O
Chowdhury O
et O
al. O
, O
2022 O
) O
, O
there O
are O
few O
studies O
regarding O
the O
lowresource O
setting. O
West O
et O
al. O
( O
2021 O
) O
and O
Meng O
et O
al. O
( O
2021 O
) O
proposed O
novel O
unsupervised O
paraphrasing O
strategies O
by O
data O
augmentation O
based O
on O
reflective O
decoding O
or O
diverse O
decoding. O
Ding O
et O
al. O

( O
2021 O
) O
and O
Yin O
et O
al. O
( O
2022 O
) O
achieved O
improvements O
on O
various O
low-resource O
datasets O
with O
retrieved O
data O
and O
meta O
reinforcement O
learning. O
However O
, O
these O
studies O
only O
use O
a O
single O
large O
corpus O
for O
training O
the O
full O
PLM O
, O
which O
suffers O
from O
domainshifting O
problems O
( O
Wang O
et O
al. O
, O
2019 O

) O
. O
Besides O
, O
under O
the O
extreme O
low-resource O
setting O
, O
directly O
fine-tuning O
the O
full O
PLM O
will O
cause O
an O
over-fitting O
problem O
( O
Antoniou O
et O
al. O
, O
2019 O
) O
. O
Meta-learning O
helps O
improve O
low-resource O
performance O
in O
various O
recent O
studies O
, O
such O
as O
image O
classification O
( O
Soh O
et O
al. O
, O
2020 O
) O
, O

vehicle O
tracking O
and O
natural O
language O
processing O
( O
Park O
et O
al. O
, O
2021 O
; O
Chen O
and O
Shuai O
, O
2021 O
; O
Hong O
and O
Jang O
, O
2022 O
) O
. O
Finn O
et O
al. O
( O
2017 O
) O
proposed O
a O
meta O
learner O
named O
MAML B-MethodName
, O
which O
uses O
other O
example O
tasks O
to O
learn O
how O
to O
effectively O
initialize O

a O
basic O
learner O
, O
which O
can O
be O
quickly O
generalized O
to O
new O
tasks. O
Adapter O
modules O
have O
been O
mainly O
used O
for O
parameter-efficient O
and O
quick O
fine-tuning O
of O
a O
basic O
PLMs O
to O
new O
tasks O
( O
Houlsby O
et O
al. O
, O
2019 O
; O
Bapna O
and O
Firat O
, O
2019 O
; O
Pfeiffer O
et O
al. O
, O
2020Pfeiffer O
et O
al. O

, O
, O
2021. O
Our O
paper O
proposes O
to O
incorporate O
meta-learning O
approaches O
to O
realize O
multi-domain O
migration O
and O
task O
adapter O
to O
realize O
parameter O
effective O
transfer O
learning O
( O
i.e. O
, O
limited O
trainable O
parameters O
) O
to O
mitigate O
the O
above O
problems O
of O
paraphrase O
generation O
. O
3 O
The O
Approach O
Learning O
Paradigm O
As O
shown O
in O
Figure O
1 O
, O

the O
workflow O
of O
our O
learning O
paradigm O
including O
three O
stages O
: O
1. O
Backbone O
model O
pre-training O
on O
large O
unlabeled O
corpora O
2. O
Adapter O
model O
meta-training O
on O
large O
source O
corpora O
using O
the O
meta-learning O
and O
3. O
Adapter O
model O
fine-tuning O
on O
target O
corpora O
and O
evaluate O
model O
performance. O
The O
prior O
knowledge O
K O
pri O
comes O
from O
first O
two O

stages O
: O
pre-training O
and O
meta-learning. O
We O
denote O
our O
backbone O
model O
by O
f O
( O
θ O
) O
with O
parameters O
θ. O
The O
first O
stage O
is O
pretraining O
on O
unlabeled O
corpora O
D O
pre O
, O
and O
we O
get O
f O
( O
θ O
pre O
) O
. O
The O
second O
stage O
is O
meta-training O
on O
adapter O
model O
f O
[ O
θ O
pre O

, O
Φ O
] O
with O
additional O
parameters O
Φ O
and O
frozen O
θ O
pre O
on O
related O
source O
corpora O
D O
src O
, O
and O
we O
got O
f O
[ O
θ O
pre O
, O
Φ O
src O
] O
. O
Finally O
, O
we O
initialize O
the O
adapter O
model O
with O
[ O
θ O
pre O
, O
Φ O
src O
] O
and O
finetune O
Φ O
src O
on O

the O
target O
corpus O
D O
tgt O
to O
obtain O
a O
target O
model O
f O
[ O
θ O
pre O
, O
Φ O
tgt O
] O
which O
are O
model O
parameters O
after O
target O
adapter O
, O
i.e. O
, O
the O
posterior O
knowledge O
K O
por O
. O
Backbone O
Model O
Because O
PLM O
is O
equipped O
with O
prior O
knowledge O
K O
pri O
and O
exhibits O
strong O
capabilities O
in O

a O
range O
of O
different O
generative O
tasks O
, O
we O
choose O
the O
pretrained B-MethodName
BART I-MethodName
( O
Lewis O
et O
al. O
, O
2020b O
) O
as O
the O
backbone O
model O
for O
paraphrase O
generation. O
Specifically O
, O
given O
a O
labeled O
paraphrase O
pair O
i O
= O
( O
x O
, O
ŷ O
) O
, O
where O
x O
= O
[ O
x O
1 O
, O
. O
. O

. O
, O
x O
N O
] O
, O
ŷ O
= O
[ O
ŷ O
1 O
, O
. O
. O
. O
, O
ŷ O
M O
] O
, O
and O
inputting O
x O
, O
the O
model O
has O
produced O
a O
predicted O
segment O
sequence O
y O
< O
t O
= O
[ O
y O
1 O
, O
. O
. O
. O
, O
y O
t−1 O
] O
before O
time O
t O

, O
then O
the O
probability O
that O
the O
token O
generated O
at O
time O
t O
is O
y O
t O
is O
p O
( O
y O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O
The O
model O
is O
optimized O
by O
minimizing O
the O
negative O
log-likelihood O
: O
L O
i O
( O
f O
( O
θ O
) O
) O
= O
− O
M O
t=1 O

log O
p O
( O
ŷ O
t O
|y O
< O
t O
, O
x O
, O
θ O
) O
. O
Adapter B-MethodName
Model I-MethodName
The O
adapter O
model O
is O
obtained O
by O
inserting O
the O
adapter O
layer O
into O
each O
transformer O
layer O
of O
the O
backbone O
model. O
An O
adapter O
layer O
is O
a O
bottlenecked O
feed-forward O
network O
consisting O
of O
a O
downproject O
layer O
, O
a O
nonlinearity O

function O
and O
an O
upproject O
layer. O
In O
addition O
, O
a O
skip O
connection O
layer O
from O
input O
to O
output O
prevents O
the O
noised O
initialization O
from O
interference O
with O
the O
training O
initially. O
For O
the O
adapter O
in O
layer O
l O
, O
the O
function O
can O
be O
formulated O
as O
: O
Adapter O
( O
z O
l O
) O
= O
W O
l O
u O
ReLU O

( O
W O
l O
d O
z O
l O
) O
+ O
z O
l O
where O
z O
l O
represents O
the O
inputs O
of O
the O
adapter O
in O
layer O
l. O
Besides O
, O
the O
normalization O
layers O
are O
trainable O
and O
initialized O
from O
the O
previous O
training O
stage O
. O
Meta-Learning O
The O
second O
stage O
is O
adapter O
model O
meta O
traning O
based O
on O
MAML B-MethodName
( O

Finn O
et O
al. O
, O
2017 O
) O
. O
The O
learning O
process O
is O
shown O
in O
Algorithm O
1. O
First O
, O
we O
freeze O
the O
backbone O
model O
parameters O
θ O
pre O
that O
have O
been O
pre-trained O
in O
the O
pre-training O
stage O
, O
then O
, O
add O
new O
adapters O
with O
parameters O
Φ O
to O
get O
adapter O
model O
f O
[ O
θ O
pre O

, O
Φ O
] O
. O
Based O
on O
Algorithm O
1 O
, O
we O
first O
complete O
the O
meta-learning O
of O
the O
adapter O
model O
on O
the O
source O
corpus O
D O
src O
to O
help O
the O
adapters O
Φ O
find O
the O
initialization O
parameters O
Φ O
src O
suitable O
for O
paraphrase O
generation O
to O
adapt O
faster O
target O
task. O
At O
this O
Compute O
adapted O
parameters O
with O

gradient O
descent O
: O
[ O
θ O
, O
Φ O
] O
= O
[ O
θ O
, O
Φ O
] O
− O
α∇ O
Φ O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O
) O
8 O
: O
end O
for O
9 O
: O
Update O
[ O
θ O
, O
Φ O
] O
← O
[ O
θ O
, O
Φ O
] O
− O
β∇ O
Φ O
T O
i O

∼p O
( O
T O
) O
L O
i O
( O
f O
[ O
θ O
, O
Φ O
] O
Datasets O
We O
conducted O
experiments O
on O
Quora B-DatasetName
1 I-DatasetName
, O
Twitter B-DatasetName
( O
Lan O
et O
al. O
, O
2017 O
) O
and O
MSCOCO B-DatasetName
( O
Lin O
et O
al. O
, O
2014 O
) O
benchmark O
datasets O
, O
and O
followed O
the O
same O
setting O
in O
previous O
works O
( O

Lin O
et O
al. O
, O
2014 O
; O
Liu O
et O
al. O
, O
2020 O
; O
Ding O
et O
al. O
, O
2021 O
) O
. O
For O
meta-learning O
, O
we O
choose O
a O
different O
source O
task O
's O
labeld O
train-set O
from O
the O
target O
task O
to O
randomly O
construct O
meta O
tasks. O
Appendix O
Table O
4 O
describes O
more O
details O
. O
Baselines O
Supervised O
methods O

are O
trained O
with O
all O
parallel O
sentences O
of O
target O
task. O
Unsupervised O
baselines O
( O
Lewis O
et O
al. O
, O
2020b O
) O
. O
Like O
our O
work O
, O
they O
all O
used O
BART B-MethodName
as O
PLM. O
To O
compare O
the O
performance O
of O
our O
method O
against O
the O
previous O
works O
, O
we O
use O
BLEU B-MetricName
( O
Papineni O
et O
al. O
, O
2002 O

) O
, O
iBLEU B-MetricName
( O
Sun O
and O
Zhou O
, O
2012 O
) O
and O
ROUGE B-MetricName
( O
Hovy O
et O
al. O
, O
2006 O
) O
metrics. O
All O
metrics O
are O
computed O
between O
the O
generated O
and O
the O
reference O
paraphrases O
in O
the O
test O
set O
( O
Kumar O
et O
al. O
, O
2020 O
) O
. O
We O
also O
separately O
analyze O
the O
impact O
of O

target O
task O
Example O
Input O
Can O
we O
ever O
store O
energy O
produced O
in O
lightning O
? O
Experimental O
Results O
How O
does O
a O
pencil O
and O
a O
liquid O
eyeliner O
differ O
? O
How O
come O
there O
's O
no O
physical O
evidence O
for O
sea O
dragons O
existing O
if O
they O
're O
the O
largestanimal O
in O
the O
sea. O
Table O
2 O
: O
Examples O
of O
the O

generated O
paraphrases O
on O
Quora B-DatasetName
dataset. O
We O
highlight O
the O
key O
phrases O
in O
the O
paraphrases O
generated O
and O
use O
wavy O
underline O
to O
show O
the O
matched O
parts O
between O
LAPA B-MethodName
and O
reference O
. O
labeled O
data O
scale O
under O
low-resource O
setting. O
Figure O
2 O
shows O
the O
experimental O
results O
on O
the O
Quora B-DatasetName
dataset. O
It O
can O
be O
conclused O
that O
LAPA B-MethodName

has O
a O
significant O
effect O
compared O
with O
BART B-MethodName
under O
the O
same O
small O
data O
size. O
LAPA B-MethodName
can O
achieve O
the O
effect O
of O
89 B-MetricValue
% I-MetricValue
to O
93 B-MetricValue
% I-MetricValue
of O
the O
full O
amount O
of O
data O
when O
not O
using O
any O
target O
task O
labeled O
data O
; O
when O
using O
a O
very O
small O
amount O
of O
data O
such O
as O
0.5k O

( O
i.e O
0.5 O
% O
of O
the O
full O
data O
) O
, O
it O
can O
be O
improved O
to O
94 B-MetricValue
% I-MetricValue
to O
96 B-MetricValue
% I-MetricValue
; O
when O
the O
amount O
of O
data O
increases O
to O
10k O
( O
i.e O
10 O
% O
of O
the O
full O
data O
) O
, O
the O
performance O
is O
almost O
the O
same O
as O
the O
full O
amount O
of O

data O
100k. O
It O
should O
be O
pointed O
out O
that O
which O
dataset O
is O
selected O
as O
the O
source O
data O
can O
not O
have O
a O
substantial O
impact O
on O
the O
migration O
results O
, O
as O
shown O
in O
Figure O
3. O
The O
results O
independent O
of O
the O
source O
dataset O
prove O
that O
LAPA B-MethodName
can O
learn O
the O
paraphrasing O
task O
itself O
on O
any O

dataset O
, O
so O
it O
has O
strong O
adaptability O
to O
the O
target O
task. O
We O
conduct O
an O
ablation O
study O
with O
three O
variants O
under O
the O
low-resource O
setting O
of O
the O
Quora B-DatasetName
dataset O
to O
investigate O
the O
contribution O
of O
each O
component O
in O
the O
proposed O
method. O
The O
experimental O
results O
are O
shown O
in O
Table O
3. O
We O
can O
get O
: O

first O
, O
using O
pre-trained B-MethodName
BART I-MethodName
can O
get O
good O
results O
; O
second O
, O
by O
adding O
the O
source O
task O
dataset O
for O
pre-trained B-MethodName
BART I-MethodName
, O
the O
knowledge O
of O
the O
source O
domain O
can O
be O
effectively O
learned O
, O
thereby O
improving O
the O
performance O
of O
the O
model O
in O
the O
target O
domain O
; O
third O
, O
adding O
our O
proposed O

meta-learning O
framework O
can O
again O
effectively O
improve O
the O
speed O
and O
quality O
of O
learning O
the O
source O
domain O
( O
LAPA B-MethodName
only O
has O
2.8 O
% O
training O
parameters O
compared O
with O
BART B-MethodName
) O
and O
achieve O
the O
best O
performance O
. O
Case O
Study O
Table O
2 O
lists O
some O
paraphrases O
generated O
by O
LAPA B-MethodName
and O
BART B-MethodName
with O
different O
experimental O
settings. O
We O

can O
observe O
that O
paraphrases O
produced O
by O
LAPA B-MethodName
are O
not O
only O
grammatically O
correct O
but O
preserve O
the O
semantics O
of O
Input O
more O
completely O
, O
and O
the O
expression O
is O
closer O
to O
Reference O
than O
the O
other O
methods. O
This O
benefits O
from O
the O
fact O
that O
our O
LAPA B-MethodName
approach O
can O
make O
full O
use O
of O
source O
domain O
data O
and O

task O
features O
, O
and O
better O
preserve O
the O
prior O
knowledge O
of O
PLM O
, O
so O
as O
to O
adapt O
to O
new O
target O
tasks O
quickly O
and O
efficiently O
. O
Conclusion O
In O
this O
work O
, O
we O
investigate O
the O
problem O
of O
paraphrase B-TaskName
generation I-TaskName
under I-TaskName
the I-TaskName
low-resource I-TaskName
setting I-TaskName
and O
propose O
a O
simple O
yet O
effective O
approach O
LAPA. B-MethodName
We O
effectively O

combine O
transfer O
learning O
and O
meta-learning O
by O
using O
adapter O
modules O
as O
the O
bridge. O
Whether O
in O
supervised O
, O
unsupervised O
or O
low-resource O
setting O
, O
the O
results O
that O
our O
approach O
achieves O
the O
SOTA O
results O
on O
benchmark O
datasets. O
In O
the O
future O
, O
we O
plan O
to O
explore O
how O
to O
choose O
a O
smaller O
but O
suitable O
high-quality O
source O

corpus O
for O
learning O
in O
the O
source O
domain O
to O
improve O
the O
effect O
of O
transferring O
to O
the O
target O
domain O
, O
because O
not O
all O
source O
domain O
data O
has O
a O
positive O
effect. O
Second O
, O
we O
plan O
to O
extend O
this O
framework O
to O
other O
AI O
fields O
to O
solve O
low-resource O
problems O
in O
other O
scenarios O
and O
enable O
more O

industrial O
applications O
. O
Limitations O
The O
major O
limitation O
of O
present O
study O
is O
the O
need O
for O
source O
domain O
annotated O
data O
that O
can O
adapt O
to O
the O
target O
domain. O
Because O
this O
is O
the O
source O
of O
data O
for O
the O
knowledge O
of O
the O
learning O
task O
itself O
, O
it O
can O
not O
be O
avoided. O
In O
the O
real O
world O

, O
we O
can O
find O
it O
from O
public O
free O
datasets O
, O
exchange O
it O
commercially O
with O
other O
institutions O
, O
or O
annotate O
a O
batch O
of O
raw O
data O
ourselves O
as O
a O
cold O
start O
to O
solve O
this O
problem. O
Secondly O
, O
this O
study O
also O
has O
insufficient O
research O
on O
related O
variables. O
Due O
to O
the O
limitation O
of O
time O

and O
article O
length O
, O
we O
have O
not O
been O
able O
to O
study. O
These O
findings O
provide O
the O
following O
insights O
for O
future O
research O
: O
What O
is O
the O
lower O
bound O
of O
the O
amount O
of O
source O
domain O
data O
that O
can O
be O
well O
adapted O
to O
the O
target O
task O
? O
Whether O
we O
can O
apply O
weak O
supervision O
, O

data O
augmentation O
and O
other O
methods O
to O
create O
source O
domain O
data O
? O
How O
to O
select O
high-quality O
source O
domain O
data O
to O
get O
a O
better O
adapter O
model O
? O
We O
leave O
these O
questions O
to O
future O
research. O
Twitter O
The O
twitter O
URL O
paraphrasing O
corpus O
is O
built O
by O
Lan O
et O
al. O
( O
2017 O
) O
for O
paraphrase O
identification. O

We O
follow O
the O
setting O
in O
Li O
et O
al. O
( O
2018 O
) O
, O
Kazemnejad O
et O
al. O
( O
2020 O
) O
and O
Siddique O
et O
al. O
( O
2020 O
) O
. O
The O
detailed O
dataset O
statistics O
are O
summarized O
in O
Table O
4 O
. O
A.2 O
Evaluation O
Details O
To O
make O
a O
fair O
and O
comprehensive O
assessment O
, O
we O
follow O
the O

same O
experiment O
setting O
of O
each O
comparison O
work O
( O
Li O
et O
al. O
, O
2018 O
; O
Liu O
et O
al. O
, O
2020 O
; O
Ding O
et O
al. O
, O
2021 O
) O
and O
conduct O
the O
comparison O
respectively. O
For O
data O
preprocessing O
, O
all O
the O
sentences O
are O
lower O
cased O
, O
and O
truncate O
all O
sentences O
to O
up O
to O
20 O

words. O
< O
s O
> O
and O
< O
/ O
s O
> O
are O
spliced O
to O
the O
front O
and O
back O
end O
of O
the O
sentence O
as O
start O
and O
end O
markers O
. O
For O
evaluation O
metrics O
, O
we O
use O
BLEU B-MetricName
, O
i-BLEU B-MetricName
and O
ROUGE B-MetricName
that O
have O
been O
widely O
used O
in O
the O
previous O
work O
to O
measure O
the O
quality O

of O
the O
paraphrases. O
The O
i-BLUE B-MetricName
aims O
to O
measure O
the O
diversity O
of O
expression O
in O
the O
generated O
paraphrases O
by O
penalizing O
copying O
words O
from O
input O
sentences. O
Specifically O
, O
we O
follow O
the O
unsupervised O
paraphrase O
generation O
baselines O
and O
set O
the O
balancing B-HyperparameterName
parameter I-HyperparameterName
α B-HyperparameterName
= O
0.9 B-HyperparameterName
. O
A.3 O
Implementation O
Our O
experiments O
were O
conducted O
with O
PyToch O
on O

NVIDIA O
Tesla O
V100 O
16GB O
GPU. O
Following O
the O
comparison O
methods O
, O
we O
used O
BART-large B-MethodName
as O
the O
pre-trained O
language O
model O
and O
use O
its O
pre-trained O
parameters. O
For O
adapter O
modules O
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
is O
128. B-HyperparameterValue
For O
meta-training O
, O
unless O
otherwise O
specified O
, O
a O
meta O
batch O
includes O
3 B-HyperparameterValue
tasks B-HyperparameterName
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName

of O
each O
task O
is O
10. B-HyperparameterValue
Both O
basic O
learners O
and O
meta O
learners O
use O
the O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
optimizer O
for O
optimization O
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
is O
set O
by O
grid O
search O
in O
1e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
1e-6 B-HyperparameterValue
and O
5e-6. B-HyperparameterValue
The O
internal B-HyperparameterName
gradient I-HyperparameterName
step I-HyperparameterName
size I-HyperparameterName
is O
4 B-HyperparameterValue
, O

and O
the O
whole O
model O
has O
enough O
step O
size O
for O
training. O
For O
meta O
verification O
, O
we O
use O
a O
corpus O
excluded O
from O
the O
source O
task O
and O
the O
target O
task. O
For O
fine-tuning O
, O
we O
use O
validation O
set O
to O
select O
the O
best O
model O
for O
metrics O
calculation O
. O

Crossmodal-3600 B-DatasetName
: O
A O
Massively O
Multilingual O
Multimodal O
Evaluation O
Dataset O
Research O
in O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
has O
been O
severely O
hampered O
by O
a O
lack O
of O
high-quality O
evaluation O
datasets. O
In O
this O
paper O
we O
present O
the O
Crossmodal-3600 B-DatasetName
dataset I-DatasetName
( O
XM3600 B-DatasetName
in O
short O
) O
, O
a O
geographically-diverse O
set O
of O
3600 O
images O
annotated O
with O
humangenerated O
reference O

captions O
in O
36 O
languages. O
The O
images O
were O
selected O
from O
across O
the O
world O
, O
covering O
regions O
where O
the O
36 O
languages O
are O
spoken O
, O
and O
annotated O
with O
captions O
that O
achieve O
consistency O
in O
terms O
of O
style O
across O
all O
languages O
, O
while O
avoiding O
annotation O
artifacts O
due O
to O
direct O
translation. O
We O
apply O
this O
benchmark O
to O

model O
selection O
for O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
and O
show O
strong O
correlation O
results O
with O
human O
evaluations O
when O
using O
XM3600 B-DatasetName
as O
golden O
references O
for O
automatic O
metrics O
. O
Introduction O
Image B-TaskName
captioning I-TaskName
is O
the O
task O
of O
automatically O
generating O
a O
fluent O
natural O
language O
description O
for O
a O
given O
image. O
This O
task O
is O
important O
for O

enabling O
accessibility O
for O
visually O
impaired O
users O
, O
and O
is O
a O
core O
task O
in O
multimodal O
research O
encompassing O
both O
vision O
and O
language O
modeling. O
However O
, O
datasets O
for O
this O
task O
are O
primarily O
available O
in O
English O
( O
Young O
et O
al. O
, O
2014 O
; O
Chen O
et O
al. O
, O
2015 O
; O
Krishna O
et O
al. O
, O
2017 O

; O
Sharma O
et O
al. O
, O
2018 O
; O
. O
Beyond O
English O
, O
there O
are O
a O
few O
datasets O
such O
as O
Multi30K O
with O
captions O
in O
German O
( O
Elliott O
et O
al. O
, O
2016 O
) O
, O
French O
( O
Elliott O
et O
al. O
, O
2017 O
) O
and O
Czech O
( O
Barrault O
et O
al. O
, O
2018 O
) O
, O
but O

they O
are O
limited O
to O
only O
a O
few O
languages O
that O
cover O
a O
small O
fraction O
of O
the O
world O
's O
population O
, O
while O
featuring O
images O
that O
severely O
under-represent O
the O
richness O
and O
diversity O
of O
cultures O
from O
across O
the O
globe. O
These O
aspects O
have O
hindered O
research O
on O
image O
captioning O
for O
a O
wide O
variety O
of O
languages O
, O

and O
directly O
hamper O
deploying O
accessibility O
solutions O
for O
a O
large O
potential O
audience O
around O
the O
world O
. O
Creating O
large O
training O
and O
evaluation O
datasets O
in O
multiple O
languages O
is O
a O
resource-intensive O
endeavor. O
Recent O
works O
( O
Thapliyal O
and O
Soricut O
, O
2020 O
) O
have O
shown O
that O
it O
is O
feasible O
to O
build O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models O
trained O

on O
machine-translated O
data O
( O
with O
English O
captions O
as O
the O
starting O
point O
) O
. O
This O
work O
also O
shows O
that O
the O
effectiveness O
of O
some O
of O
the O
most O
reliable O
automatic O
metrics O
for O
image O
captioning O
, O
such O
as O
CIDEr O
1 O
is O
severely O
diminished O
when O
applied O
to O
translated O
evaluation O
sets O
, O
resulting O
in O
poorer O
agreement O

with O
human O
evaluations O
compared O
to O
the O
English O
case. O
As O
such O
, O
the O
current O
situation O
is O
that O
trustworthy O
model O
evaluation O
can O
only O
be O
based O
on O
extensive O
and O
expensive O
human O
evaluations. O
However O
, O
such O
evaluations O
can O
not O
usually O
be O
replicated O
across O
different O
research O
efforts O
, O
and O
therefore O
do O
not O
offer O
a O
fast O

and O
robust O
mechanism O
for O
model O
hill-climbing O
and O
comparison O
of O
multiple O
lines O
of O
research O
. O
The O
proposed O
XM3600 B-DatasetName
image O
captioning O
evaluation O
dataset O
provides O
a O
robust O
benchmark O
for O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
, O
and O
can O
be O
reliably O
used O
to O
compare O
research O
contributions O
in O
this O
emerging O
field. O
Our O
contributions O
are O
as O
follows O
: O
( O

i O
) O
for O
human O
caption O
annotations O
, O
we O
have O
devised O
a O
protocol O
that O
allows O
annotators O
for O
a O
specific O
target O
language O
to O
produce O
image O
captions O
in O
a O
style O
that O
is O
consistent O
across O
languages O
; O
this O
protocol O
results O
in O
image-caption O
annotations O
that O
are O
free O
of O
direct O
translation O
artefacts O
, O
an O
issue O
that O

has O
plagued O
Machine O
Translation O
research O
for O
many O
years O
and O
is O
now O
well O
understood O
( O
Freitag O
et O
al. O
, O
2020 O
) O
; O
( O
ii O
) O
for O
image O
selection O
, O
we O
have O
devised O
an O
algorithmic O
approach O
to O
sample O
a O
set O
of O
3600 O
geographically-diverse O
images O
from O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O

al. O
, O
2020 O
) O
, O
aimed O
at O
creating O
a O
representative O
set O
of O
images O
from O
across O
the O
world O
; O
( O
iii O
) O
for O
the O
resulting O
XM3600 B-DatasetName
bench-Figure O
1 O
: O
Sample O
captions O
in O
three O
different O
languages O
( O
out O
of O
36 O
-see O
full O
list O
of O
captions O
in O
Appendix O
A O
) O
, O
showcasing O
the O

creation O
of O
annotations O
that O
are O
consistent O
in O
style O
across O
languages O
, O
while O
being O
free O
of O
directtranslation O
artefacts O
( O
e.g. O
the O
Spanish O
" O
number O
42 O
" O
or O
the O
Thai O
" O
convertibles O
" O
would O
not O
be O
possible O
when O
directly O
translating O
from O
the O
English O
versions O
) O
. O
mark O
, O
we O
empirically O
measure O
its O

ability O
to O
rank O
image O
captioning O
model O
variations O
, O
and O
show O
that O
it O
provides O
high O
levels O
of O
agreement O
with O
human O
judgements O
, O
therefore O
validating O
its O
usefulness O
as O
a O
benchmark O
and O
alleviating O
the O
need O
for O
human O
judgement O
in O
the O
future O
. O
Fig. O
1 O
shows O
a O
few O
sample O
captions O
for O
an O
image O
in O

XM3600 B-DatasetName
that O
exemplify O
point O
( O
i O
) O
above O
, O
and O
Fig. O
2 O
shows O
the O
variety O
of O
cultural O
aspects O
captured O
by O
the O
image O
sampling O
approach O
from O
point O
( O
ii O
) O
. O
We O
provide O
detailed O
explanations O
and O
results O
for O
each O
of O
the O
points O
above O
in O
the O
rest O
of O
the O
paper. O
We O
have O

released O
XM3600 B-DatasetName
under O
a O
CC-BY4.0 O
license O
at O
https O
: O
/ O
/ O
google.github.io O
/ O
crossmodal-3600 O
/ O
. O
The O
XM3600 B-DatasetName
Dataset O
In O
this O
section O
, O
we O
describe O
the O
heuristics O
used O
for O
language O
and O
image O
selection O
, O
the O
design O
of O
the O
caption O
annotation O
process O
, O
caption O
statistics O
including O
quality O
, O
and O
annotator O
details O

. O
Language O
Selection O
In O
this O
section O
, O
we O
describe O
the O
heuristic O
used O
for O
selecting O
the O
languages. O
As O
a O
first O
step O
, O
we O
take O
a O
quantitative O
stance O
and O
choose O
30 O
languages O
( O
L30 O
) O
roughly O
based O
on O
their O
percent O
of O
web O
content O
2 O
. O
As O
a O
second O
step O
, O
we O
consider O

an O
additional O
five O
languages O
( O
L5 O
) O
3 O
to O
cover O
low-resource O
languages O
with O
ish O
( O
da O
) O
, O
Dutch O
( O
nl O
) O
, O
Filipino O
( O
fil O
) O
, O
Finnish O
( O
fi O
) O
, O
French O
( O
fr O
) O
, O
German O
( O
de O
) O
, O
Greek O
( O
el O
) O
, O
Hebrew O
( O

he O
) O
, O
Hindi O
( O
hi O
) O
, O
Hungarian O
( O
hu O
) O
, O
Indonesian O
( O
id O
) O
, O
Italian O
( O
it O
) O
, O
Japanese O
( O
ja O
) O
, O
Korean O
( O
ko O
) O
, O
Norwegian O
( O
no O
) O
, O
Persian O
( O
fa O
) O
, O
Polish O
( O
pl O
) O
, O
Portuguese O
( O

pt O
) O
, O
Romanian O
( O
ro O
) O
, O
Russian O
( O
ru O
) O
, O
Spanish O
( O
es O
) O
, O
Swedish O
( O
sv O
) O
, O
Thai O
( O
th O
) O
, O
Turkish O
( O
tr O
) O
, O
Ukrainian O
( O
uk O
) O
, O
Vietnamese O
( O
vi O
) O
. O
3 O
Bengali O
( O
bn O
) O
, O
Cusco O

Quechua O
( O
quz O
) O
, O
Maori O
( O
mi O
) O
, O
Swahili O
( O
sw O
) O
, O
Telugu O
( O
te O
) O
. O
many O
native O
speakers O
, O
or O
major O
native O
languages O
from O
continents O
that O
would O
not O
be O
covered O
otherwise. O
The O
protocol O
for O
caption O
annotation O
( O
Sec. O
2.3 O
) O
has O
been O
applied O
to O
the O

resulting O
union O
of O
languages O
plus O
English O
, O
for O
a O
total O
of O
36 B-HyperparameterValue
languages B-HyperparameterName
. O
Image O
Selection O
In O
this O
section O
, O
we O
consider O
the O
heuristics O
used O
for O
selecting O
a O
geographically O
diverse O
set O
of O
images O
. O
For O
each O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
, O
we O
select O
100 O
images O
that O
, O
as O
far O
as O

it O
is O
possible O
for O
us O
to O
identify O
, O
are O
taken O
in O
an O
area O
where O
the O
given O
language O
is O
spoken. O
The O
images O
are O
selected O
among O
those O
in O
the O
Open B-DatasetName
Images I-DatasetName
Dataset I-DatasetName
( O
Kuznetsova O
et O
al. O
, O
2020 O
) O
that O
have O
GPS O
coordinates O
stored O
in O
their O
EXIF O
metadata. O
Since O
there O
are O
many O

regions O
where O
more O
than O
one O
language O
is O
spoken O
, O
and O
given O
that O
some O
areas O
are O
not O
well O
covered O
by O
Open B-DatasetName
Images I-DatasetName
, O
we O
design O
an O
algorithm O
that O
maximizes O
the O
percentage O
of O
selected O
images O
taken O
in O
an O
area O
in O
which O
the O
assigned O
language O
is O
spoken. O
This O
is O
a O
greedy O
algorithm O
that O

starts O
the O
selection O
of O
images O
by O
the O
languages O
for O
which O
we O
have O
the O
smallest O
pool O
( O
e.g. O
Persian O
) O
and O
processes O
them O
in O
increasing O
order O
of O
their O
candidate O
image O
pool O
size. O
Whenever O
there O
are O
not O
enough O
images O
in O
the O
area O
where O
a O
language O
is O
spoken O
, O
we O
have O
several O
back-off O

levels O
: O
( O
i O
) O
selecting O
from O
a O
country O
where O
the O
language O
is O
spoken O
; O
( O
ii O
) O
a O
continent O
where O
the O
language O
is O
spoken O
, O
and O
, O
as O
last O
resort O
, O
( O
iii O
) O
from O
anywhere O
in O
the O
world O
. O
This O
strategy O
succeeds O
in O
providing O
our O
target O
number O
of O

100 O
images O
from O
an O
appropriate O
region O
for O
most O
of O
the O
36 B-HyperparameterValue
languages B-HyperparameterName
except O
for O
Persian O
( O
where O
14 O
continent-level O
images O
are O
used O
) O
and O
Hindi O
( O
where O
all O
100 O
images O
are O
at O
the O
global O
level O
because O
the O
in-region O
images O
are O
assigned O
to O
Bengali O
and O
Telugu O
) O
. O
We O
keep O
the O

region O
each O
image O
is O
selected O
from O
as O
part O
of O
our O
data O
annotation O
, O
so O
that O
future O
evaluations O
can O
choose O
to O
either O
evaluate O
on O
images O
relevant O
to O
particular O
regions O
of O
interest O
or O
on O
the O
entire O
dataset O
. O
Caption B-TaskName
Annotation I-TaskName
In O
this O
section O
we O
detail O
the O
design O
of O
the O
caption B-TaskName
annotation I-TaskName
process. O

For O
a O
massively O
multilingual O
benchmark O
such O
as O
XM3600 B-DatasetName
, O
consistency O
in O
the O
style O
of O
the O
description O
language O
is O
critical O
, O
since O
language O
can O
serve O
multiple O
communication O
goals. O
For O
a O
more O
in-depth O
discussion O
on O
these O
issues O
as O
they O
relate O
to O
image O
captions O
, O
we O
refer O
the O
reader O
to O
( O
Alikhani O
et O

al. O
, O
2020 O
) O
. O
We O
borrow O
from O
their O
terminology O
, O
as O
it O
identifies O
coherence O
relations O
between O
image O
and O
captions O
such O
as O
VISIBLE O
, O
META O
, O
SUB-JECTIVE O
, O
and O
STORY. O
The O
goal O
for O
our O
caption B-TaskName
annotation I-TaskName
is O
to O
generate O
VISIBLE O
image O
captions O
, O
i.e. O
, O
use O
the O
target O
language O
to O

formulate O
a O
sentence O
that O
is O
intended O
to O
recognizably O
characterize O
what O
is O
visually O
depicted O
in O
the O
image O
. O
One O
possible O
approach O
to O
generating O
such O
captions O
is O
to O
generate O
them O
as O
such O
in O
English O
, O
and O
have O
them O
translated O
( O
automatically O
, O
semiautomatically O
, O
or O
manually O
) O
into O
all O
the O
other O
languages. O

However O
, O
this O
approach O
results O
in O
an O
English-language O
bias O
, O
as O
well O
as O
other O
problems O
that O
have O
been O
already O
identified O
in O
the O
literature. O
For O
instance O
, O
translations O
are O
often O
less O
fluent O
compared O
to O
natural O
target O
sentences O
, O
due O
to O
word O
order O
and O
lexical O
choices O
influenced O
by O
the O
source O
language. O
The O

impact O
of O
this O
phenomenon O
on O
metrics O
and O
modeling O
has O
recently O
received O
increased O
attention O
in O
the O
evaluation O
literature O
( O
Toral O
et O
al. O
, O
2018 O
; O
Zhang O
and O
Toral O
, O
2019 O
; O
Freitag O
et O
al. O
, O
2020 O
) O
, O
and O
references O
created O
in O
this O
style O
are O
thought O
to O
cause O
overlap-based O
metrics O
to O

favor O
model O
outputs O
that O
use O
such O
unnatural O
language O
. O
We O
have O
designed O
our O
caption B-TaskName
annotation I-TaskName
process O
to O
achieve O
two O
main O
goals O
: O
( O
i O
) O
produce O
caption O
annotations O
in O
a O
VISIBLE O
relation O
with O
respect O
to O
the O
image O
content O
, O
and O
, O
strongly O
, O
create O
consistency O
in O
the O
description O
style O
across O

languages O
; O
( O
ii O
) O
be O
free O
of O
translation O
artefacts. O
To O
achieve O
this O
, O
we O
use O
bi-lingual O
annotators O
with O
a O
requirement O
to O
be O
reading-proficient O
in O
English O
and O
fluent O
/ O
native O
in O
the O
target O
language. O
As O
a O
preliminary O
step O
, O
we O
train O
an O
image-captioning O
model O
on O
English-annotated O
data O
, O
which O
results O

in O
captions O
in O
the O
VISIBLE O
style O
of O
COCO-CAP O
( O
Chen O
et O
al. O
, O
2015 O
) O
. O
The O
annotation O
process O
proceeds O
as O
follows. O
Each O
annotation O
session O
is O
done O
over O
batches B-HyperparameterName
of O
N B-HyperparameterName
= O
15 B-HyperparameterValue
images O
, O
using O
the O
images O
selected O
as O
described O
in O
Sec. O
2.2. O
The O
first O
screen O
shows O
the O
N O

images O
with O
their O
captions O
in O
English O
as O
generated O
by O
the O
captioning O
model O
, O
and O
asks O
the O
annotators O
if O
the O
captions O
are O
EXCELLENT O
, O
GOOD O
, O
MEDIUM O
, O
BAD O
, O
or O
there O
is O
NOT-ENOUGH-INFO. O
We O
refer O
to O
this O
rating O
scale O
as O
the O
5-level O
quality O
scale O
in O
the O
subsequent O
text. O
We O
provide O

the O
annotators O
with O
clear O
guidelines O
about O
what O
constitutes O
an O
EXCEL-LENT O
caption O
, O
and O
how O
to O
evaluate O
degradations O
from O
that O
quality. O
This O
step O
forces O
the O
annotators O
to O
carefully O
assess O
caption O
quality O
and O
it O
primes O
them O
into O
internalizing O
the O
style O
of O
the O
captions O
without O
the O
need O
for O
complicated O
and O
lengthy O
annotation O
instructions O

. O
The O
second O
round O
shows O
the O
same O
N O
images O
again O
, O
but O
one O
image O
at O
a O
time O
without O
the O
English O
captions O
, O
and O
the O
annotators O
are O
asked O
to O
produce O
descriptive O
captions O
in O
the O
target O
language O
for O
each O
image. O
In O
the O
absence O
of O
the O
English O
captions O
, O
the O
annotators O
rely O
on O

the O
internalized O
caption O
style O
, O
and O
generate O
their O
annotations O
mostly O
based O
on O
the O
image O
content O
-with O
no O
support O
from O
the O
text O
modality O
, O
other O
than O
potentially O
from O
memory. O
Note O
, O
however O
, O
that O
we O
have O
designed O
the O
system O
to O
support O
N O
annotations O
simultaneously O
, O
and O
we O
have O
empirically O
selected O
the O

value O
of O
N O
as O
to O
be O
large O
enough O
to O
" O
overwrite O
" O
the O
memory O
of O
the O
annotators O
with O
respect O
to O
the O
exact O
textual O
formulation O
of O
the O
English O
captions. O
As O
a O
result O
, O
we O
observe O
that O
the O
produced O
annotations O
are O
free O
of O
translation O
artefacts O
: O
See O
the O
example O
in O
Fig. O
1 O

for O
Spanish O
mentioning O
" O
number O
42 O
" O
, O
and O
for O
Thai O
mentioning O
" O
convertibles O
" O
. O
We O
also O
provide O
the O
annotators O
with O
an O
annotation O
protocol O
to O
use O
when O
creating O
the O
captions O
, O
which O
provides O
useful O
guidance O
in O
achieving O
consistent O
annotations O
across O
all O
the O
targeted O
languages. O
We O
provide O
the O
annotation O
guidelines O

in O
Appendices O
B O
and O
C. O
For O
each O
language O
, O
we O
annotate O
all O
3600 O
images O
with O
captions O
using O
replication O
2 O
( O
two O
different O
annotators O
working O
independently O
) O
4 O
, O
except O
Bengali O
( O
bn O
) O
with O
replication O
1 O
and O
Maori O
( O
mi O
) O
with O
roughly O
1 O
for O
2 O
/ O
3 O
and O
2 O

for O
1 O
/ O
3 O
of O
the O
images O
, O
see O
Table O
1 O
. O
Caption O
Statistics O
In O
this O
section O
, O
we O
take O
a O
look O
at O
the O
the O
basic O
statistics O
of O
the O
captions O
in O
the O
dataset. O
captions O
per O
language O
. O
For O
languages O
with O
natural O
space O
tokenization O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
words I-HyperparameterName
per I-HyperparameterName
caption I-HyperparameterName

can O
be O
as O
low O
as O
5 B-HyperparameterValue
or O
6 B-HyperparameterValue
for O
some O
agglutinative O
languages O
like O
Cusco O
Quechua O
( O
quz O
) O
and O
Czech O
( O
cs O
) O
, O
and O
as O
high O
as O
18 B-HyperparameterValue
for O
an O
analytic O
language O
like O
Vietnamese O
( O
vi O
) O
. O
The O
number O
of O
characters O
per O
caption O
also O
varies O
drastically O
-from O
mid-20s O

for O
Korean O
( O
ko O
) O
to O
mid-90s O
for O
Indonesian O
( O
id O
) O
-depending O
on O
the O
alphabet O
and O
the O
script O
of O
the O
language O
. O
Caption O
Quality O
In O
this O
section O
, O
we O
describe O
the O
process O
for O
ensuring O
the O
creation O
of O
high O
quality O
annotations O
, O
and O
present O
quality O
statistics O
of O
the O
annotations O
produced O

. O
In O
order O
to O
ensure O
quality O
, O
the O
annotation O
process O
is O
initially O
started O
with O
pilot B-HyperparameterName
runs I-HyperparameterName
on O
150 B-HyperparameterValue
images. O
The O
caption O
ratings O
are O
spot O
checked O
by O
the O
authors O
to O
verify O
that O
the O
raters O
have O
a O
good O
understanding O
of O
the O
rating O
scale. O
Further O
, O
the O
generated O
captions O
go O
through O
a O
verification O

round O
where O
they O
are O
rated O
by O
the O
human O
annotators O
on O
the O
5-level O
quality O
scale O
described O
in O
Sec.2.3. O
If O
the O
annotations O
are O
below O
the O
desired O
quality O
, O
we O
clarify O
the O
guidelines O
and O
add O
more O
examples O
to O
provide O
feedback O
to O
the O
human O
annotators O
and O
then O
conduct O
another O
pilot. O
This O
process O
is O
repeated O

until O
very O
few O
low-quality O
captions O
are O
being O
produced O
5 O
. O
After O
this O
, O
for O
every O
language O
, O
we O
run O
the O
main O
annotation O
and O
finally O
a O
verification O
round O
where O
we O
select O
one O
caption O
for O
600 O
randomly O
selected O
images O
and O
have O
the O
annotator O
pool O
( O
per O
language O
) O
rate O
them O
on O
the O

5-level O
quality O
scale O
mentioned O
in O
Sec. O
2.3. O
The O
quality O
scores O
are O
presented O
in O
Table O
2 O
. O
Annotator O
Details O
We O
use O
an O
in-house O
annotation O
platform O
with O
professional O
( O
paid O
) O
annotators O
and O
quality O
assurance. O
Annotators O
are O
chosen O
to O
be O
native O
in O
the O
target O
language O
whenever O
possible O
, O
and O
fluent O
otherwise O
( O

for O
low-resource O
languages O
, O
they O
are O
usually O
linguists O
that O
have O
advanced-level O
knowledge O
of O
that O
language O
) O
. O
All O
annotators O
are O
required O
to O
be O
proficient O
in O
English O
since O
the O
instructions O
and O
guidelines O
are O
given O
in O
English O
. O
Model O
Comparison O
using O
XM3600 B-DatasetName
In O
this O
section O
, O
we O
detail O
our O
experiments O
for O
comparing O

several O
models O
using O
human O
evaluations O
, O
and O
also O
using O
XM3600 B-DatasetName
annotations O
as O
gold O
6 O
references O
for O
automated O
metrics O
. O
For O
model O
comparison O
, O
we O
train O
several O
multilingual O
image O
captioning O
models O
with O
different O
sizes O
over O
different O
datasets O
, O
and O
compare O
them O
on O
XM3600. B-DatasetName
As O
our O
main O
result O
, O
we O
show O
a O

high O
level O
of O
correlation O
between O
model O
rankings O
based O
on O
human-evaluation O
scores O
and O
the O
scores O
obtained O
using O
CIDEr B-MetricName
with O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
. O
Datasets O
We O
build O
two O
multilingual O
datasets O
for O
training O
, O
CC3M-35L B-DatasetName
and O
COCO-35L B-DatasetName
, O
by O
translating O
Conceptual O
Captions O
3M O
( O
Sharma O
et O
al. O
, O
2018 O
) O
and O

COCO O
Captions O
( O
Chen O
et O
al. O
, O
2015 O
) O
to O
the O
other O
34 O
languages O
using O
Google O
's O
machine O
translation O
API O
7 O
. O
The O
remaining O
language O
, O
Cusco O
Quechua O
( O
quz O
) O
, O
is O
not O
supported O
by O
the O
API O
8 O
. O
We O
use O
the O
standard O
train O
and O
validation O
splits O
for O
CC3M O

9 O
. O
For O
COCO O
, O
we O
use O
the O
Karpathy O
split O
( O
Karpathy O
and O
Fei-Fei O
, O
2014 O
) O
Models O
In O
this O
section O
we O
detail O
the O
model O
architecture O
we O
used O
for O
the O
experiments. O
Our O
Transformer-based O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
model O
architecture O
for O
image O
captioning O
is O
shown O
in O
Figure O
3. O

On O
the O
vision O
side O
, O
each O
input O
image O
is O
modeled O
by O
a O
Vision B-MethodName
Transformer I-MethodName
( O
ViT B-MethodName
) O
( O
Dosovitskiy O
et O
al. O
, O
2020 O
; O
Zhai O
et O
al. O
, O
2021 O
) O
. O
The O
visual O
features O
produced O
by O
ViT O
for O
every O
patch O
of O
the O
image O
are O
pooled O
into O
a O
single O
dense O
feature O

vector. O
On O
the O
text O
side O
, O
a O
Language O
Identifier O
( O
LangId O
) O
string O
is O
used O
to O
specify O
the O
language. O
The O
LangId O
string O
is O
tokenized O
and O
embedded O
into O
dense O
token O
embeddings O
, O
which O
are O
merged O
with O
the O
dense O
visual O
embeddings O
as O
the O
input O
to O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Encoder O

, O
followed O
by O
a O
multi-layer O
Transformer O
Image O
and O
Text O
Decoder O
to O
generate O
the O
predicted O
captions O
. O
We O
take O
advantage O
of O
existing O
pretrained O
models O
to O
initialize O
different O
parts O
of O
our O
model O
: O
ViT O
( O
Zhai O
et O
al. O
, O
2021 O
) O
( O
green O
in O
Fig. O
3 O
) O
and O
mT5 O
( O
Xue O
et O

al. O
, O
2021 O
) O
( O
orange O
in O
Fig. O
3 O
) O
. O
We O
consider O
different O
model O
sizes O
: O
mT5-base O
, O
mT-large O
, O
ViT-B O
/ O
16 O
, O
and O
ViT-g O
/ O
14 O
, O
where O
16 O
and O
14 O
are O
the O
corresponding O
patch O
sizes. O
We O
choose O
three O
combinations O
resulting O
in O
three O
different O
model O
architectures O
: O

mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
, O
mT5-base B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
and O
mT5large B-MethodName
+ I-MethodName
ViT-g I-MethodName
/ I-MethodName
14 I-MethodName
. O
We O
train O
these O
three O
models O
on O
COCO-35L. B-DatasetName
In O
addition O
, O
we O
consider O
a O
fourth O
model O
based O
on O
mT5base B-MethodName
+ I-MethodName
ViT-B I-MethodName
/ I-MethodName
16 I-MethodName
and O
trained O
on O
CC3M-35L. B-DatasetName
The O
models O
are O
trained O
on O
a O

4x4x4 O
TPU-v4 O
architecture O
using O
an O
Adafactor O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
optimizer O
with O
a O
constant B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
period I-HyperparameterName
between O
{ B-HyperparameterValue
1k I-HyperparameterValue
, I-HyperparameterValue
10k I-HyperparameterValue
} I-HyperparameterValue
steps I-HyperparameterValue
, O
followed O
by O
a O
reversed O
square-root O
decay O
with O
the O
number O
of O
steps. O
The O
batch B-HyperparameterName
size I-HyperparameterName
is O
2048 B-HyperparameterValue
in O
all O
the O
experiments. O
The O
initial B-HyperparameterName

learning I-HyperparameterName
rate I-HyperparameterName
is O
between O
{ B-HyperparameterValue
1e-4 I-HyperparameterValue
, I-HyperparameterValue
3e-4 I-HyperparameterValue
} I-HyperparameterValue
. O
We O
use O
the O
same O
vocabulary B-HyperparameterName
( O
size B-HyperparameterValue
250k I-HyperparameterValue
) O
as O
mT5 O
( O
Xue O
et O
al. O
, O
2021 O
) O
. O
The O
model O
trained O
with O
CC3M-35L B-DatasetName
is O
subsequently O
finetuned O
on O
COCO-35L B-DatasetName
with O
constant O
learning B-HyperparameterName
rate I-HyperparameterName
3e-5 B-HyperparameterValue
for O
1 B-HyperparameterValue
epoch B-HyperparameterName
. O
Human O
Evaluation O

In O
this O
section O
, O
we O
detail O
the O
process O
used O
for O
human O
evaluations O
comparing O
the O
performance O
of O
two O
models O
. O
Our O
main O
goal O
in O
creating O
XM3600 B-DatasetName
is O
to O
automate O
the O
evaluation O
of O
massively B-TaskName
multilingual I-TaskName
image I-TaskName
captioning I-TaskName
models O
, O
by O
eliminating O
expensive O
and O
timeconsuming O
human O
evaluations. O
Our O
results O
indicate O
that O
they O
can O

be O
substituted O
by O
using O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
for O
automated O
metrics O
such O
as O
CIDEr B-MetricName
. O
To O
quantify O
the O
correlation O
between O
the O
two O
methods O
, O
we O
train O
four O
different O
models O
( O
Tab. O
3 O
) O
and O
conduct O
side-by-side O
human O
evaluations O
using O
the O
outputs O
of O
these O
models O
in O
several O
languages. O
We O

observe O
strong O
correlations O
( O
Sec. O
3.4 O
) O
between O
the O
human O
evaluations O
and O
the O
CIDEr B-MetricName
scores O
using O
the O
XM3600 B-DatasetName
references O
. O
Specifically O
, O
we O
use O
a O
randomly O
selected O
subset O
of O
600 O
images O
from O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
, O
which O
we O
call O
XM600. O
Image O
captions O
generated O
by O
a O
given O
pairing O
of O
models O

( O
m O
1 O
vs O
m O
2 O
, O
where O
m O
1 O
is O
considered O
as O
the O
base O
condition O
and O
m O
2 O
as O
the O
test O
condition O
) O
are O
compared O
and O
rated O
side-by-side O
, O
using O
a O
similar O
pool O
of O
annotators O
as O
described O
in O
Sec. O
2.6. O
Each O
side-by-side O
pair O
( O
shown O
in O
a O
random O
per-example O

left-vs-right O
order O
) O
is O
rated O
using O
a O
7-point O
scale O
: O
MUCH-BETTER O
, O
BETTER O
, O
SLIGHTLY-BETTER O
, O
SIMILAR O
, O
SLIGHTLY-WORSE O
, O
WORSE O
, O
MUCH-WORSE O
, O
with O
a O
replication O
factor O
of O
3 O
( O
three O
annotators O
rate O
each O
pair O
) O
. O
We O
denote O
by O
WINS B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O

raters O
( O
i.e. O
2 O
out O
of O
3 O
) O
mark O
m O
2 O
's O
captions O
as O
better O
, O
and O
by O
LOSSES B-MetricName
the O
percentage O
of O
images O
where O
the O
majority O
of O
raters O
mark O
m O
2 O
's O
captions O
as O
worse. O
We O
then O
define O
the O
overall B-MetricName
side-by-side I-MetricName
gain I-MetricName
of O
m O
2 O
over O
m O
1 O
as O
∆S×S O

= O
WINS O
-LOSSES O
. O
Conducting O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
for O
each O
pair O
of O
models O
over O
the O
35 O
languages O
would O
require O
210 O
human O
evaluation O
sessions. O
This O
is O
prohibitively O
expensive O
and O
time O
consuming. O
Thus O
, O
we O
conduct O
the O
full O
set O
of O
six O
side-by-side O
evaluations O
of O
the O
pairs O
of O
models O

, O
on O
a O
core O
set O
of O
four O
languages O
called O
LCORE O
11 O
. O
We O
call O
this O
set O
of O
24 O
evaluation O
sessions O
OCORE. O
Furthermore O
, O
we O
also O
conduct O
a O
sparser O
set O
of O
side-by-side O
evaluations O
over O
languages O
where O
the O
CIDEr B-MetricName
differences O
on O
XM3600 B-DatasetName
and O
on O
COCO-DEV O
12 O
indicate O
11 O
Chinese-Simplified O
( O
zh O
) O

, O
English O
( O
en O
) O
, O
Hindi O
( O
hi O
) O
, O
Spanish O
( O
es O
) O
12 O
COCO O
validation O
split O
with O
machine-translated O
references O
disagreement O
or O
ambiguity O
( O
e.g. O
, O
opposite O
sign O
of O
the O
CIDEr O
differences O
, O
and O
/ O
or O
small O
CIDEr O
differences O
) O
; O
this O
gives O
us O
a O
set O
of O
28 O

languages O
called O
LEXT O
13 O
. O
We O
call O
the O
resulting O
set O
of O
41 O
evaluation O
sessions O
OEXT. O
The O
set O
of O
all O
evaluations O
is O
called O
OALL O
=OCORE O
+ O
OEXT O
, O
which O
are O
conducted O
over O
the O
languages O
LALL O
= O
LCORE O
+ O
LEXT O
. O
The O
choice O
of O
which O
model O
is O
called O
m O
1 O
and O
which O

model O
is O
called O
m O
2 O
is O
arbitrary O
in O
the O
sideby-side O
evaluations O
, O
since O
we O
randomly O
flip O
left O
vs O
right O
before O
presenting O
the O
captions O
to O
the O
raters. O
Hence O
a O
single O
side-by-side O
evaluation O
gives O
two O
points O
for O
the O
correlation O
calculations O
: O
one O
with O
the O
m O
1 O
and O
m O
2 O
assigned O
as O
per O

the O
actual O
evaluation O
conducted O
, O
and O
one O
more O
with O
the O
m O
1 O
and O
m O
2 O
assignment O
flipped O
and O
the O
∆S×S O
sign O
flipped O
correspondingly O
. O
Results O
We O
present O
results O
that O
show O
that O
it O
is O
feasible O
to O
use O
the O
XM3600 B-DatasetName
annotations O
as O
gold O
references O
with O
automated O
metrics O
such O
as O
CIDEr B-MetricName
to O
compare O

models O
in O
lieu O
of O
human B-MetricName
evaluations I-MetricName
, O
and O
that O
this O
option O
is O
superior O
to O
using O
silver O
references O
created O
via O
automated O
translation. O
Table O
5 O
presents O
the O
results O
for O
the O
OCORE O
set O
of O
evaluations O
on O
XM600 B-DatasetName
on O
the O
LCORE O
languages O
, O
while O
Table O
7 O
in O
the O
appendix O
shows O
the O
results O
on O
the O

LEXT O
languages. O
The O
reference O
for O
the O
relative O
strength O
of O
each O
pairing O
is O
given O
by O
∆S×S O
, O
with O
positive O
numbers O
indicating O
the O
superiority O
of O
m O
2 O
, O
and O
negative O
numbers O
indicating O
a O
superiority O
of O
m O
1 O
. O
As O
can O
be O
seen O
from O
the O
table O
, O
the O
model O
comparisons O
span O
a O
range O

of O
model O
differences O
, O
from O
low O
∆S×S O
to O
high O
∆S×S. O
∆CIDEr O
XM600and B-DatasetName
∆CIDEr O
XM3600 B-DatasetName
capture O
similar O
information O
, O
except O
these O
numbers O
are O
based O
on O
CIDEr O
scores O
using O
as O
references O
XM600 B-DatasetName
and O
XM3600 B-DatasetName
, O
respectively O
, O
while O
∆CIDEr O
COCO-DEV O
is O
based O
on O
machine-translated O
references O
from O
the O
validation O
split O
of O
COCO O
. O

We O
use O
the O
results O
from O
Table O
5 O
( O
and O
Table O
7 O
) O
to O
compute O
the O
correlation O
between O
human O
judgements O
of O
the O
relative O
quality O
of O
the O
captioning O
models O
and O
the O
ability O
of O
the O
CIDEr B-DatasetName
14 O
metric O
-or O
, O
rather O
, O
of O
the O
underlying O
references O
used O
by O
the O
metricto O
perform O
an O
equivalent O

task. O
Table O
6 O
presents O
the O
correlation O
results O
using O
three O
correlation O
metrics O
: O
Pearson B-MetricName
, O
Spearman B-MetricName
, O
and O
Kendall. B-MetricName
The O
first O
section O
shows O
the O
correlations O
over O
all O
the O
side-by-side O
evaluations O
( O
i.e. O
OCORE O
and O
OEXT O
) O
; O
These O
cover O
the O
LCORE O
and O
the O
LEXT O
languages. O
The O
second O
section O
shows O
the O
correlations O

for O
the O
OEXT O
covering O
the O
LEXT O
languages. O
The O
third O
section O
shows O
the O
correlations O
for O
the O
OCORE O
evaluations O
covering O
the O
LCORE O
languages O
. O
We O
observe O
that O
∆CIDEr O
XM3600 B-DatasetName
is O
highly O
correlated O
with O
human O
judgement O
according O
to O
all O
the O
correlation O
metrics O
( O
Bonett O
and O
Wright O
, O
2000 O
) O
, O
over O
all O
the O

evaluations O
OALL O
, O
over O
the O
OCORE O
evaluations O
, O
and O
also O
the O
OEXT O
evaluations. O
Furthermore O
, O
for O
the O
OEXT O
evaluations O
, O
where O
most O
of O
the O
instances O
have O
opposite O
signs O
for O
∆CIDEr O
COCO-DEV O
and O
∆CIDEr O
XM3600 B-DatasetName
, O
we O
find O
that O
the O
former O
is O
strongly O
anti-correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results O
while O
the O

latter O
is O
highly O
correlated O
with O
the O
human B-MetricName
evaluation I-MetricName
results. O
Overall O
, O
these O
results O
indicate O
that O
: O
( O
i O
) O
we O
can O
reliably O
substitute O
∆CIDEr O
XM3600 B-DatasetName
for O
human B-MetricName
evaluations I-MetricName
on O
XM600 B-DatasetName
when O
comparing O
models O
similar O
to O
the O
ones O
we O
used O
; O
( O
ii O
) O
the O
gold O
XM3600 B-DatasetName
references O
are O
preferable O
over O

the O
silver O
references O
obtained O
from O
translating O
COCO O
captions O
, O
in O
terms O
of O
approximating O
the O
judgements O
of O
the O
human O
evaluators O
15 O
. O
Based O
on O
the O
results O
from O
Table O
6 O
, O
we O
recommend O
the O
use O
of O
the O
XM3600 B-DatasetName
references O
as O
a O
means O
to O
achieve O
high-quality O
automatic O
comparisons O
between O
multilingual O
image O
captioning O
models. O

We O
have O
provided O
the O
CIDEr B-MetricName
scores O
for O
XM3600 B-DatasetName
in O
35 O
languages O
for O
all O
the O
models O
, O
in O
Tables O
8-11 O
in O
the O
Appendix. O
These O
can O
be O
used O
as O
baselines O
in O
future O
work. O
15 O
However O
, O
it O
is O
unclear O
whether O
machine O
translated O
references O
for O
one O
particular O
language O
in O
XM3600 B-DatasetName
translated O
to O
all O

others O
, O
are O
worse O
than O
using O
the O
human O
generated O
references. O
In O
particular O
, O
we O
studied O
the O
correlations O
of O
CIDEr B-MetricName
computed O
using O
XM3600-en-MT B-DatasetName
( O
i.e. O
the O
XM3600 O
English O
references O
, O
machine O
translated O
to O
all O
the O
other O
languages O
) O
, O
with O
the O
human B-MetricName
evaluations. I-MetricName
We O
found O
that O
even O
though O
the O
translations O
have O

artifacts O
and O
disfluencies O
, O
CIDEr B-MetricName
differences O
calculated O
using O
them O
show O
comparable O
correlations O
with O
human O
judgement O
observations. O
We O
also O
studied O
such O
correlations O
for O
machine O
translated O
references O
from O
German O
, O
Greek O
, O
Hebrew O
, O
Hungarian O
and O
Swahili. O
We O
found O
that O
the O
correlations O
are O
similar O
and O
sometimes O
even O
a O
bit O
higher O
than O
using O

the O
human O
generated O
references. O
We O
believe O
this O
happens O
because O
the O
rater O
guidelines O
weigh O
informativeness O
over O
fluency O
and O
the O
CIDEr B-MetricName
metric O
is O
also O
not O
as O
sensitive O
to O
fluency. O
Further O
work O
is O
needed O
to O
understand O
the O
use O
of O
translated O
references O
as O
compared O
to O
human O
generated O
references. O
We O
believe O
that O
using O
the O
human O

generated O
references O
along O
with O
the O
set O
of O
machine O
translated O
references O
from O
all O
the O
other O
languages O
may O
provide O
even O
stronger O
correlations O
and O
show O
greater O
diversity O
in O
the O
coverage O
of O
the O
image O
constituents O
. O
Conclusions O
We O
introduce O
the O
XM3600 B-DatasetName
dataset O
as O
a O
benchmark O
for O
evaluating O
the O
performance O
of O
multilingual B-TaskName
image I-TaskName
captioning I-TaskName
models. O

The O
images O
in O
the O
dataset O
are O
geographically O
diverse O
, O
covering O
all O
inhabited O
continents O
and O
a O
large O
fraction O
of O
the O
world O
population. O
We O
believe O
this O
benchmark O
has O
the O
potential O
to O
positively O
impact O
both O
the O
research O
and O
the O
applications O
of O
this O
technology O
, O
and O
enable O
( O
among O
other O
things O
) O
better O
accessibility O

for O
visually-impaired O
users O
across O
the O
world O
, O
including O
speakers O
of O
lowresource O
languages O
. O
The O
main O
appeal O
of O
this O
benchmark O
is O
that O
it O
alleviates O
the O
need O
for O
extensive O
human B-MetricName
evaluation I-MetricName
, O
which O
is O
difficult O
to O
achieve O
across O
multiple O
languages O
and O
hinders O
direct O
comparison O
between O
different O
research O
ideas O
and O
results. O
We O
show O

significant O
improvements O
in O
correlation O
with O
human O
judgements O
when O
using O
the O
XM3600 B-DatasetName
dataset O
as O
references O
for O
automatic O
metrics O
, O
and O
therefore O
hope O
that O
the O
adoption O
of O
this O
dataset O
as O
a O
standard O
benchmark O
will O
facilitate O
faster O
progress O
and O
better O
comparisons O
among O
competing O
ideas O
. O
Our O
empirical O
observations O
are O
primarily O
on O
the O
full O

set O
of O
side-by-side O
comparisons O
over O
English O
and O
three O
other O
languages O
( O
Spanish O
, O
Hindi O
, O
Chinese O
) O
. O
Due O
to O
the O
similarity O
in O
the O
data O
collection O
and O
the O
quality O
control O
process O
, O
we O
expect O
similar O
results O
to O
hold O
for O
all O
the O
other O
languages O
as O
well O
; O
we O
validated O
this O
expectation O

with O
additional O
empirical O
observations O
covering O
an O
additional O
28 O
languages O
. O
-DOCSTART- O
Semantic B-MethodName
Diversity I-MethodName
in I-MethodName
Dialogue I-MethodName
with I-MethodName
Natural I-MethodName
Language I-MethodName
Inference I-MethodName
Generating O
diverse O
, O
interesting O
responses O
to O
chitchat O
conversations O
is O
a O
problem O
for O
neural O
conversational O
agents. O
This O
paper O
makes O
two O
substantial O
contributions O
to O
improving O
diversity O
in O
dialogue B-TaskName
generation. I-TaskName
First O
, O
we O
propose O
a O
novel O
metric O
which O
uses O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( I-TaskName
NLI I-TaskName

) I-TaskName
to O
measure O
the O
semantic B-MetricName
diversity I-MetricName
of O
a O
set O
of O
model O
responses O
for O
a O
conversation. O
We O
evaluate O
this O
metric O
using O
an O
established O
framework O
( O
Tevet O
and O
Berant O
, O
2021 O
) O
and O
find O
strong O
evidence O
indicating O
NLI B-TaskName
Diversity B-MetricName
is O
correlated O
with O
semantic B-MetricName
diversity. I-MetricName
Specifically O
, O
we O
show O
that O
the O
contradiction O
relation O

is O
more O
useful O
than O
the O
neutral O
relation O
for O
measuring O
this O
diversity O
and O
that O
incorporating O
the O
NLI B-TaskName
model O
's O
confidence B-MetricName
achieves O
state-of-the-art O
results. O
Second O
, O
we O
demonstrate O
how O
to O
iteratively O
improve O
the O
semantic B-MetricName
diversity I-MetricName
of O
a O
sampled O
set O
of O
responses O
via O
a O
new O
generation O
procedure O
called O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
which O

results O
in O
an O
average O
137 B-MetricValue
% I-MetricValue
increase O
in O
NLI B-MetricName
Diversity O
compared O
to O
standard O
generation O
procedures O
. O
Introduction O
Dialogue O
models O
often O
struggle O
to O
produce O
engaging O
utterances O
in O
conversations O
, O
tending O
to O
generate O
responses O
which O
are O
common O
in O
the O
training O
data O
, O
such O
as O
" O
OK O
, O
" O
" O
Yeah O
, O
" O

or O
" O
I O
do O
n't O
know O
" O
( O
Li O
et O
al. O
, O
2016 O
) O
. O
While O
these O
responses O
are O
appropriate O
for O
a O
wide O
variety O
of O
contexts O
, O
their O
over-production O
can O
result O
in O
a O
dull O
conversation O
( O
See O
et O
al. O
, O
2019 O
) O
. O
An O
evaluation O
task O
has O
emerged O
that O
consists O

of O
measuring O
the O
diversity B-MetricName
of O
chitchat O
model O
responses O
over O
a O
test O
set. O
While O
some O
past O
work O
uses O
human B-MetricName
evaluation I-MetricName
to O
measure O
model O
response O
diversity B-MetricName
according O
to O
engagingness B-MetricName
, O
specificity B-MetricName
, O
or O
interestingness B-MetricName
( O
Li O
et O
al. O
, O
2016 O
; O
See O
et O
al. O
, O
2019 O
; O
Ghandeharioun O
et O
al. O
, O
2019 O

) O
, O
several O
automated O
metrics O
have O
also O
been O
proposed O
to O
measure O
diversity O
of O
model O
responses. O
Some O
metrics O
measure O
lexical B-MetricName
diversity I-MetricName
, O
typically O
via O
n-gram B-MetricName
overlap I-MetricName
( O
Li O
Figure O
1 O
: O
Illustration O
of O
NLI O
Diversity O
using O
human O
responses O
from O
DailyDialog++. O
Contradictions O
are O
weighted O
by O
1 O
, O
entailments O
by O
-1 O
, O
and O

neutrals O
by O
0 O
, O
so O
the O
score O
is O
( O
2 O
× O
1 O
) O
+ O
( O
3 O
× O
0 O
) O
+ O
( O
1 O
× O
−1 O
) O
= O
1. O
et O
al. O
, O
2016 O
) O
or O
computing O
the O
BLEU B-MetricName
score O
( O
Zhu O
et O
al. O
, O
2018 O
) O
among O
model O
responses O
generated O
from O
the O

test O
set. O
Other O
past O
work O
attempts O
to O
measure O
semantic B-MetricName
diversity I-MetricName
via O
repurposing O
sentence B-MetricName
similarity I-MetricName
metrics O
( O
Tevet O
and O
Berant O
, O
2021 O
; O
Zhang O
et O
al. O
, O
2020a O
; O
Cer O
et O
al. O
, O
2017 O
) O
. O
We O
propose O
a O
new O
metric O
aimed O
at O
measuring O
semantic B-MetricName
diversity I-MetricName
by O
leveraging O
a O
Natural B-TaskName
Language I-TaskName

Inference I-TaskName
( I-TaskName
NLI I-TaskName
) I-TaskName
model O
to O
score O
a O
set O
of O
multiple O
dialogue O
model O
responses O
for O
a O
single O
conversation O
, O
as O
illustrated O
in O
Figure O
1. O
NLI B-TaskName
is O
a O
three-way O
classification O
task O
to O
determine O
whether O
one O
sentence O
entails O
, O
contradicts O
, O
or O
is O
neutral O
toward O
a O
second O
sentence. O
We O
hypothesize O
that O
a O

diverse O
set O
of O
responses O
for O
a O
conversation O
captures O
contradictory O
ways O
one O
could O
respond O
, O
which O
can O
be O
measured O
by O
the O
NLI B-TaskName
model. O
We O
aggregate O
the O
contradiction O
, O
neutral O
, O
and O
entailment O
predictions O
among O
pairs O
of O
responses O
from O
the O
set O
and O
combine O
the O
predictions O
into O
a O
new O
diversity O
metric O
, O
called O

NLI B-MetricName
Diversity I-MetricName
. O
We O
additionally O
explore O
two O
modifications O
of O
NLI B-MetricName
Diversity. I-MetricName
First O
, O
because O
the O
neutral O
prediction O
may O
be O
indicative O
of O
diversity O
, O
we O
propose O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
where O
neutral O
predictions O
are O
weighted O
the O
same O
as O
contradiction O
predictions. O
Second O
, O
since O
our O
Baseline O
NLI B-MetricName
Diversity I-MetricName
method O
does O
not O
take O

into O
account O
the O
confidence B-MetricName
of O
the O
model O
's O
prediction O
, O
we O
propose O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
which O
aggregates O
the O
probability O
mass O
of O
the O
model O
's O
predicted O
class O
instead O
of O
aggregating O
the O
number O
of O
predictions O
for O
each O
class O
. O
We O
assess O
NLI B-MetricName
Diversity I-MetricName
using O
Tevet O
and O
Berant O
( O
2021 O
) O
's O

diversity O
metric O
evaluation O
framework O
, O
finding O
that O
NLI B-MetricName
Diversity I-MetricName
is O
highly O
correlated O
both O
with O
human O
judgments O
of O
diversity B-MetricName
and O
with O
the O
diversity B-MetricName
parameter O
, O
a O
gold O
standard O
diversity B-MetricName
value O
used O
to O
generate O
the O
set O
of O
responses. O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
state-of-the-art O
performance O
in O
terms O
of O
correlation O
with O
semantic B-MetricName
diversity. I-MetricName
Also O

, O
through O
an O
ablation O
study O
, O
we O
find O
positive O
, O
neutral O
, O
and O
negative O
correlations O
between O
human O
judgments O
and O
the O
number O
of O
contradiction O
, O
neutral O
, O
and O
entailment O
predictions O
, O
respectively O
. O
We O
next O
explore O
the O
use O
of O
a O
dialogue O
model O
to O
generate O
a O
set O
of O
candidate O
responses O
with O
a O

minimum O
target O
level O
of O
semantic B-MetricName
diversity I-MetricName
, O
such O
as O
10 B-MetricValue
Contradictions. O
Our O
new O
generation O
procedure O
, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
iteratively O
improves O
a O
set O
of O
model O
responses O
until O
this O
intended O
threshold O
is O
reached. O
If O
a O
set O
of O
sampled O
responses O
does O
not O
meet O
the O
intended O
threshold O
, O
the O
lowest-scoring O
response O
is O

thrown O
out O
and O
a O
new O
response O
is O
sampled O
until O
the O
diversity O
threshold O
is O
reached. O
We O
show O
this O
procedure O
results O
in O
a O
more O
diverse O
set O
of O
responses O
than O
the O
original O
sampled O
set O
, O
often O
with O
only O
a O
few O
resampled O
responses. O
Results O
of O
automated O
analysis O
shows O
relevancy O
is O
maintained O
from O
initial O
to O

final O
sets O
of O
responses O
. O
In O
summary O
, O
our O
contributions O
are O
: O
• O
A O
novel O
diversity O
metric O
, O
NLI B-MetricName
Diversity I-MetricName
, O
evaluated O
using O
Tevet O
and O
Berant O
( O
2021 O
) O
Measuring O
Model O
Response O
Diversity O
Traditionally O
, O
a O
model O
's O
diversity B-MetricName
has O
been O
measured O
in O
terms O
of O
its O
predictions O
over O
the O
test O

set O
( O
Li O
et O
al. O
, O
2016 O
) O
, O
which O
we O
call O
Test B-MetricName
Set I-MetricName
Diversity. I-MetricName
In O
this O
setup O
, O
the O
model O
predicts O
one O
response O
for O
each O
conversation O
in O
the O
test O
set O
( O
containing O
n O
conversations O
) O
, O
resulting O
in O
n O
predictions. O
The O
diversity B-MetricName
measure O
is O
computed O
over O
these O
n O
predictions O

, O
resulting O
in O
a O
score O
over O
the O
entire O
test O
set O
. O
The O
notion O
of O
diversity B-MetricName
we O
investigate O
, O
however O
, O
measures O
the O
model O
's O
ability O
to O
generate O
a O
set O
of O
responses O
for O
a O
single O
conversation O
Tevet O
and O
Berant O
, O
2021 O
) O
, O
which O
we O
call O
Multi-Response B-MetricName
Diversity. I-MetricName
Instead O
of O
generating O

one O
response O
for O
each O
of O
the O
conversations O
in O
the O
test O
set O
, O
we O
evaluate O
a O
model O
's O
ability O
to O
generate O
m O
responses O
for O
each O
of O
the O
n O
conversations O
. O
As O
shown O
by O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
metrics O
which O
have O
been O
proposed O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting O

can O
still O
be O
applied O
in O
the O
Multi-Response O
Diversity O
setting O
, O
however O
, O
by O
treating O
each O
set O
of O
m O
responses O
as O
its O
own O
" O
test O
set O
" O
and O
averaging O
over O
the O
n O
total O
sets O
. O
Diversity O
Metrics O
Lexical B-MetricName
diversity I-MetricName
metrics O
measure O
differences O
in O
word O
choice O
, O
as O
opposed O
to O
diversity O
of O

content. O
Li O
et O
al. O
( O
2016 O
) O
propose O
distinct-n O
, O
which O
measures O
the O
number O
of O
unique O
n-grams O
generated O
divided O
by O
the O
total O
number O
of O
n-grams O
generated O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting. O
Some O
past O
work O
has O
applied O
this O
metric O
to O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
( O
Tevet O
and O
Berant O
, O
2021 O

) O
. O
Cao O
and O
Clark O
( O
2017 O
) O
propose O
examining O
the O
percent O
of O
unique O
responses O
over O
the O
test O
set. O
Other O
past O
work O
has O
proposed O
using O
BLEU B-MetricName
score O
over O
a O
set O
of O
model O
responses O
in O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
setting O
( O
Zhu O
et O
al. O
, O
2018 O
) O
. O
Semantic B-MetricName
diversity I-MetricName
metrics O

, O
on O
the O
other O
hand O
, O
compare O
diversity B-MetricName
of O
the O
content O
present O
in O
each O
response. O
Many O
of O
these O
measures O
are O
adapted O
from O
semantic B-MetricName
similarity I-MetricName
scores I-MetricName
, O
since O
lower O
similarity B-MetricName
can O
indicate O
higher O
diversity B-MetricName
( O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
BERTScore B-MetricName
measures O
the O
similarity O
of O
BERT O
embeddings O
for O
each O

token O
in O
two O
sentences O
( O
Zhang O
et O
al. O
, O
2020a O
) O
. O
Bert-STS B-MetricName
assigns O
a O
score O
based O
on O
the O
semantic B-MetricName
similarity I-MetricName
of O
two O
sentences O
( O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
The O
Sent-BERT B-MetricName
metric O
computes O
cosine O
similarity O
between O
BERT O
sentence O
embeddings O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O

Larson O
et O
al. O
( O
2019 O
) O
propose O
identifying O
diverse O
paraphrases O
by O
identifying O
embedding O
outliers O
. O
Other O
past O
work O
has O
used O
human B-MetricName
evaluation I-MetricName
to O
measure O
a O
model O
's O
diversity. B-MetricName
Li O
et O
al. O
( O
2016 O
) O
ask O
humans O
to O
choose O
the O
better O
of O
two O
responses O
based O
on O
specificity O
to O
the O
past O
conversation. O

See O
et O
al. O
( O
2019 O
) O
ask O
humans O
to O
rank O
dialogue O
responses O
on O
a O
variety O
of O
factors O
, O
including O
interestingness B-MetricName
and O
inquisitiveness. B-MetricName
Tevet O
and O
Berant O
( O
2021 O
) O
compare O
participants O
' O
ability O
to O
judge O
diversity B-MetricName
of O
a O
set O
of O
responses O
in O
two O
ways O
: O
( O
i O
) O
by O
ranking O
one O

response O
as O
more O
diverse O
than O
a O
second O
response O
and O
( O
ii O
) O
by O
judging O
the O
diversity O
of O
a O
single O
response O
on O
a O
Likert B-MetricName
scale I-MetricName
, O
finding O
that O
participants O
were O
equally O
able O
to O
judge O
diversity B-MetricName
in O
both O
conditions. O
They O
also O
find O
that O
human O
judges O
are O
better O
at O
distinguishing O
semantic B-MetricName
diversity I-MetricName
than O

lexical B-MetricName
diversity I-MetricName
. O
Other O
past O
work O
has O
incorporated O
diversity B-MetricName
metrics O
into O
the O
dialogue B-MethodName
dataset I-MethodName
creation I-MethodName
pipeline. I-MethodName
Stasaski O
et O
al. O
( O
2020 O
) O
propose O
a O
method O
which O
measures O
the O
diversity B-MetricName
of O
a O
crowdworker O
's O
contributions O
compared O
to O
a O
corpus O
, O
using O
that O
information O
to O
determine O
when O
to O
stop O
collecting O
data O
from O

the O
worker. O
This O
results O
in O
a O
more O
diverse O
dataset O
. O
Evaluation O
of O
Diversity B-MetricName
Metrics I-MetricName
Tevet O
and O
Berant O
( O
2021 O
) O
propose O
a O
framework O
to O
examine O
the O
reliability O
of O
diversity O
metrics. O
They O
propose O
the O
notion O
of O
a O
diversity O
parameter O
, O
which O
is O
used O
to O
generate O
a O
set O
of O
model O
responses O
, O

e.g. O
, O
the O
p-value O
in O
nucleus O
sampling O
, O
which O
specifies O
the O
vocabulary O
probability O
distribution O
cutoff O
used O
to O
restrict O
sampling O
to O
the O
most-likely O
words O
whose O
combined O
likelihood O
≥ O
p. O
If O
p O
is O
higher O
, O
the O
set O
of O
responses O
should O
have O
higher O
diversity B-MetricName
, O
and O
viceversa. O
This O
diversity O
parameter O
is O
treated O
as O

a O
gold O
standard O
for O
a O
set O
of O
responses O
' O
diversity. B-MetricName
Diversity B-MetricName
metrics O
assign O
scores O
in O
the O
Multi-Response B-MetricName
Diversity I-MetricName
condition O
and O
are O
evaluated O
in O
terms O
of O
correlation O
to O
the O
diversity O
parameter. O
They O
further O
propose O
two O
datasets O
to O
evaluate O
diversity O
metrics O
: O
one O
which O
includes O
model O
responses O
and O
contains O
varying O
levels O
of O

lexical B-MetricName
diversity I-MetricName
and O
one O
which O
is O
human-created O
and O
maintains O
high O
lexical B-MetricName
diversity I-MetricName
to O
allow O
focused O
evaluation O
of O
semantic B-MetricName
diversity I-MetricName
. O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
is O
a O
task O
aimed O
at O
predicting O
whether O
one O
sentence O
contradicts O
, O
entails O
, O
or O
is O
neutral O
towards O
a O
second O
sentence. O
Models O
for O
NLI B-TaskName
are O

typically O
trained O
using O
one O
of O
two O
datasets O
: O
Stanford B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
SNLI B-DatasetName
) O
( O
Bowman O
et O
al. O
, O
2015 O
) O
or O
Multi-Genre B-DatasetName
NLI I-DatasetName
( O
MNLI B-DatasetName
) O
( O
Williams O
et O
al. O
, O
2018 O
) O
. O
More O
recent O
datasets O
include O
FEVER B-DatasetName
( O
Thorne O
et O
al. O
, O
2018 O
; O
Nie O
et O

al. O
, O
2019 O
) O
, O
adapted O
from O
a O
fact-checking O
dataset O
, O
and O
ANLI B-DatasetName
( O
Nie O
et O
al. O
, O
2020 O
) O
, O
collected O
in O
an O
adversarial O
human-in-the-loop O
procedure. O
With O
the O
rise O
of O
transformer O
architectures O
, O
models O
have O
achieved O
high O
performance O
on O
NLI B-TaskName
tasks O
( O
Liu O
et O
al. O
, O
2019 O
) O
. O

In O
a O
dialogue O
setting O
, O
NLI B-TaskName
has O
been O
used O
to O
improve O
consistency O
between O
a O
persona O
and O
model O
responses O
over O
the O
course O
of O
a O
conversation O
by O
integrating O
an O
NLI-based B-TaskName
reward O
into O
a O
reinforcement B-MethodName
learning I-MethodName
training I-MethodName
procedure O
( O
Song O
et O
al. O
, O
2020 O
) O
. O
To O
our O
knowledge O
, O
however O
, O
NLI B-TaskName

has O
not O
been O
used O
to O
measure O
the O
diversity B-MetricName
of O
model O
responses O
in O
either O
the O
Test B-MetricName
Set I-MetricName
Diversity I-MetricName
or O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
. O
Generating O
Diverse O
Sets O
of O
Hypotheses O
While O
work O
has O
only O
recently O
begun O
to O
explore O
the O
task O
of O
generating O
multiple O
dialogue O
responses O
to O
a O
conversation O
Tevet O
and O
Berant O
, O

2021 O
) O
, O
past O
work O
has O
explored O
generating O
diverse O
sets O
of O
hypotheses O
in O
some O
other O
application O
areas. O
Carbonell O
and O
Goldstein O
( O
1998 O
) O
explored O
using O
Maximal B-MethodName
Mutual I-MethodName
Relevance I-MethodName
to O
reduce O
redundancy O
without O
sacrificing O
relevancy O
in O
document B-TaskName
selection I-TaskName
for O
summarization. B-TaskName
Batra O
et O
al. O
( O
2012 O
) O
proposed O
a O
greedy O
iterative O
algorithm O

to O
generate O
diverse O
, O
probable O
hypotheses O
for O
multiple O
vision O
tasks. O
Most O
related O
to O
our O
work O
is O
Gimpel O
et O
al. O
( O
2013 O
) O
, O
which O
applied O
Batra O
et O
al. O
( O
2012 O
) O
's O
approach O
to O
machine B-TaskName
translation I-TaskName
, O
generating O
a O
set O
of O
translations O
instead O
of O
a O
single O
translation. O
In O
contrast O
to O

Gimpel O
et O
al. O
( O
2013 O
) O
, O
by O
holding O
the O
sampling O
procedure O
constant O
throughout O
the O
iterative O
process O
, O
our O
method O
can O
explore O
the O
extent O
to O
which O
diversity B-MetricName
can O
be O
increased O
without O
altering O
standard O
decoding O
practices O
. O
NLI B-MetricName
Diversity I-MetricName
Metric I-MetricName
We O
propose O
three O
diversity O
metrics O
in O
the O
Multi-Response B-MetricName
Diversity I-MetricName
setting O
which O

leverage O
the O
predictions O
of O
an O
NLI O
model. O
Two O
metrics O
( O
Baseline O
and O
Neutral O
) O
aggregate O
the O
NLI O
model O
's O
class O
predictions O
and O
one O
metric O
( O
Confidence O
) O
aggregates O
the O
weight O
of O
these O
predictions O
. O
Baseline O
NLI B-MetricName
Diversity I-MetricName
We O
propose O
a O
new O
metric O
, O
called O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
which O
uses O

an O
NLI B-TaskName
model O
's O
predictions O
to O
measure O
diversity. B-MetricName
More O
formally O
, O
for O
a O
given O
conversation O
, O
c O
, O
and O
a O
dialogue B-TaskName
generation I-TaskName
model O
M O
, O
a O
set O
of O
utterances O
u O
1 O
, O
... O
, O
u O
n O
is O
produced O
by O
the O
model. O
Each O
pair O
of O
utterances O
is O
compared O
in O
both O
directions O

using O
an O
NLI B-MetricName
model O
, O
N O
LI O
( O
u O
1 O
, O
u O
2 O
) O
, O
N O
LI O
( O
u O
2 O
, O
u O
1 O
) O
, O
... O
, O
N O
LI O
( O
u O
n O
, O
u O
n−1 O
) O
. O
The O
NLI B-TaskName
model O
predicts O
a O
distribution O
over O
the O
three O
potential O
classes O
: O
contradiction O

, O
neutral O
, O
and O
entailment. O
We O
take O
the O
argmax O
over O
these O
classes O
, O
resulting O
in O
a O
list O
of O
NLI B-TaskName
predictions O
, O
N O
LI O
preds O
( O
N O
LI O
( O
u O
1 O
, O
u O
2 O
) O
, O
... O
, O
N O
LI O
( O
u O
n−1 O
, O
u O
n O
) O
) O
of O
size O
n O

( O
n O
− O
1 O
) O
. O
To O
produce O
an O
overall O
diversity B-MetricName
score I-MetricName
for O
N O
LI O
preds O
( O
u O
1 O
, O
... O
, O
u O
n O
) O
, O
we O
assign O
each O
of O
these O
classes O
a O
value O
representing O
their O
diversity O
, O
denoted O
N B-MetricName
LI I-MetricName
score I-MetricName
( O
N O
LI O
preds O
( O
u O
1 O
, O

... O
, O
u O
n O
) O
) O
. O
We O
hypothesize O
that O
larger O
numbers O
of O
entailment O
predictions O
found O
in O
a O
set O
of O
model-generated O
utterances O
is O
indicative O
of O
a O
lack O
of O
diversity O
; O
similarly O
, O
larger O
number O
of O
contradiction O
predictions O
is O
indicative O
of O
a O
larger O
amount O
of O
diversity. O
Because O
we O
want O
a O
higher O

value O
of O
N B-MetricName
LI I-MetricName
score I-MetricName
to O
indicate O
higher O
diversity B-MetricName
, O
we O
assign O
values O
as O
: O
N B-MetricName
LI I-MetricName
score I-MetricName
= O
 O
 O
 O
 O
 O
1 O
if O
contradiction O
0 O
if O
neutral O
-1 O
if O
entailment O
The O
sum O
of O
the O
N B-MetricName
LI I-MetricName
score I-MetricName
values O
for O
the O
set O
of O
utterances O
results O
in O
the O
final O

NLI B-MetricName
Diversity I-MetricName
score I-MetricName
, O
formally O
defined O
as O
: O
Baseline B-MetricName
N I-MetricName
LIDiversity I-MetricName
= O
u O
i O
, O
u O
j O
∈u O
1 O
, O
... O
, O
un O
N B-MetricName
LI I-MetricName
score I-MetricName
( O
N O
LI O
pred O
( O
N O
LI O
( O
u O
i O
, O
u O
j O
) O
) O
While O
the O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
metric O
aggregates O
all O
classes O

, O
we O
also O
investigate O
the O
separate O
number O
of O
entailment O
, O
contradiction O
, O
and O
neutral O
predictions O
in O
N O
LI O
preds O
, O
denoted O
# O
Entailment O
, O
# O
Contradiction O
, O
and O
# O
Neutral O
, O
respectively O
. O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
Our O
primary O
hypothesis O
is O
that O
contradictions O
indicate O
diversity O
and O
entailments O
indicate O
lack O
of O
diversity. B-MetricName

Because O
it O
is O
unclear O
what O
the O
role O
of O
neutrals O
might O
be O
, O
we O
explore O
a O
version O
of O
NLI B-MetricName
Diversity I-MetricName
which O
weights O
neutral O
and O
contradiction O
predictions O
as O
equally O
diverse. O
This O
metric O
is O
the O
same O
as O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
except O
the O
N B-MetricName
LI I-MetricName
score I-MetricName
used O
to O
assign O
values O
is O
: O
N B-MetricName
LI I-MetricName

score_neutral I-MetricName
= O
 O
 O
 O
 O
 O
1 O
if O
contradiction O
1 O
if O
neutral O
-1 O
if O
entailment O
Confidence B-MetricName
NLI B-MetricName
Diversity I-MetricName
Because O
the O
prior O
two O
NLI B-MetricName
Diversity I-MetricName
metrics O
do O
not O
incorporate O
the O
confidence O
of O
the O
NLI O
model O
's O
class O
predictions O
, O
we O
explore O
an O
additional O
metric O
which O
incorporates O
this O
value. O
Letting O
conf O

class O
( O
u O
1 O
, O
u O
2 O
) O
represent O
the O
model O
's O
probability O
mass O
assigned O
to O
the O
predicted O
NLI O
class O
after O
sof O
tmax O
, O
the O
function O
is O
defined O
as O
: O
N B-MetricName
LI I-MetricName
score_conf I-MetricName
idence I-MetricName
= O
 O
 O
 O
 O
 O
1 O
× O
conf O
con O
( O
u O
1 O
, O
u O
2 O

) O
if O
contradiction O
0 O
if O
neutral O
-1 O
× O
conf O
ent O
( O
u O
1 O
, O
u O
2 O
) O
if O
entailment O
Intuitively O
, O
instead O
of O
assigning O
a O
1 O
value O
for O
a O
contradiction O
prediction O
, O
this O
metric O
assigns O
the O
probability O
of O
the O
contradiction O
class. O
Likewise O
, O
instead O
of O
a O
-1 O
for O
an O
entailment O

prediction O
, O
this O
metric O
assigns O
the O
negative O
probability O
mass O
of O
the O
entailment O
class O
. O
Evaluation O
of O
NLI B-MetricName
Diversity I-MetricName
We O
evaluate O
NLI B-MetricName
Diversity I-MetricName
by O
computing O
the O
correlation O
between O
the O
metric O
and O
both O
human O
labels O
and O
diversity B-MetricName
parameter O
labels. O
Below O
we O
first O
describe O
the O
models O
and O
data O
and O
then O
present O
the O
results O

of O
the O
evaluation O
. O
Models O
We O
explore O
two O
NLI O
models O
: O
a O
Roberta-large B-MethodName
model O
( O
Liu O
et O
al. O
, O
2019 O
) O
Tevet O
and O
Berant O
( O
2021 O
) O
. O
Corresponding O
temperature B-HyperparameterValue
parameter O
( O
higher O
is O
more O
diverse O
) O
or O
semantic B-MetricName
and O
lexical B-MetricName
diversity I-MetricName
levels O
accompany O
each O
example. O
containing O
300M O
parameters. O
We O

refer O
to O
these O
models O
as O
NLI B-MethodName
Diversity I-MethodName
-MNLI I-MethodName
and O
NLI B-MethodName
Diversity I-MethodName
-Combined I-MethodName
, O
respectively. O
We O
do O
not O
employ O
additional O
fine-tuning O
of O
these O
models O
. O
Data O
There O
are O
two O
different O
English O
datasets O
released O
to O
evaluate O
diversity O
metrics O
in O
Tevet O
and O
Berant O
( O
2021 O
) O
: O
conTest B-DatasetName
and O
decTest B-DatasetName
, O
described O
in O

Table O
1. O
The O
conTest B-DatasetName
dataset O
is O
human-created O
and O
captures O
content O
, O
or O
semantic O
, O
diversity B-MetricName
independent O
of O
lexical B-MetricName
diversity. I-MetricName
Low-diversity O
examples O
in O
this O
dataset O
have O
high O
lexical B-MetricName
diversity I-MetricName
but O
low O
semantic B-MetricName
diversity. I-MetricName
This O
dataset O
was O
created O
by O
asking O
crowdworkers O
to O
generate O
sets O
of O
utterances O
with O
either O
low O
or O
high O
semantic B-MetricName

diversity I-MetricName
using O
varied O
language O
, O
in O
order O
to O
keep O
a O
high O
level O
of O
lexical B-MetricName
diversity I-MetricName
constant O
across O
both O
conditions O
. O
The O
decTest B-DatasetName
dataset O
includes O
model-generated O
responses O
, O
with O
diversity O
controlled O
by O
a O
decoding B-HyperparameterName
parameter O
, O
such O
as O
a O
temperature B-HyperparameterName
parameter. O
The O
dataset O
can O
include O
duplicate O
responses O
, O
and O
does O
not O

attempt O
to O
mediate O
lexical B-MetricName
diversity I-MetricName
; O
therefore O
, O
low-diversity O
examples O
in O
this O
dataset O
may O
reflect O
low O
lexical B-MetricName
as O
well O
as O
low O
semantic B-MetricName
diversity I-MetricName
. O
While O
the O
original O
dataset O
includes O
multiple O
generation O
tasks O
, O
we O
evaluate O
on O
the O
dialogue O
task O
, O
respGen B-TaskName
, O
which O
is O
drawn O
from O
Reddit O
conversations O
( O
Hashimoto O

et O
al. O
, O
2019 O
) O
3 O
. O
There O
are O
200 O
conversations O
for O
each O
of O
conTest B-DatasetName
and O
decTest B-DatasetName
for O
the O
respGen B-TaskName
task O
, O
with O
multiple O
responses O
for O
each O
conversation O
( O
5 O
for O
conTest O
, O
10 O
for O
decTest O
) O
. O
Diversity B-HyperparameterName
Parameter I-HyperparameterName
Correlation O
The O
diversity B-HyperparameterName
parameter I-HyperparameterName
from O
Tevet O
and O
Berant O
( O
2021 O

) O
represents O
either O
a O
parameter O
directly O
used O
to O
generate O
responses O
via O
a O
dialogue O
model O
, O
such O
as O
p O
in O
nucleus O
sampling O
, O
or O
a O
binary O
value O
indicating O
whether O
crowdworkers O
were O
instructed O
to O
generate O
a O
high-or O
low-diversity O
set O
of O
responses. O
A O
measure O
which O
is O
able O
to O
capture O
diversity O
will O
be O
positively O

correlated O
with O
this O
diversity O
parameter O
. O
Table O
2 O
shows O
Spearman O
's O
correlations O
between O
NLI B-MetricName
Diversity I-MetricName
and O
the O
diversity B-HyperparameterName
parameter. I-HyperparameterName
On O
the O
conTest B-DatasetName
semantic B-MetricName
diversity I-MetricName
dataset O
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
the O
highest O
correlation O
of O
all O
metrics O
( O
0.62 O
) O
and O
approaches O
human O
performance. O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
performs O
comparably O
to O
the O

top-performing O
automatic O
metric O
from O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
at O
0.59 B-MetricValue
correlation. O
We O
note O
the O
95 B-MetricValue
% I-MetricValue
confidence B-MetricName
intervals O
overlaps O
between O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Sent-BERT B-MetricName
, O
and O
human B-MetricName
judgements I-MetricName
, O
indicating O
a O
lack O
of O
significant O
differences O
( O
see O
Appendix O
A O
) O
. O
Although O

Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
does O
relatively O
poorly O
on O
conTest B-DatasetName
( O
0.24 O
) O
, O
it O
is O
the O
highest-performing O
NLI B-TaskName
metric O
on O
decTest O
( O
0.72 O
) O
, O
suggesting O
that O
incorporating O
neutral O
predictions O
may O
capture O
lexical O
instead O
of O
semantic B-MetricName
diversity I-MetricName
. O
A O
histogram O
of O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
values O
for O
low O
and O
high O
semantic B-MetricName
diversity I-MetricName

sets O
of O
responses O
is O
shown O
in O
Figure O
2. O
We O
note O
the O
lack O
of O
large O
overlap O
between O
the O
distributions O
of O
low O
and O
high O
semantic B-MetricName
diversity I-MetricName
data. O
In O
addition O
to O
Human B-MetricName
Correlation I-MetricName
In O
this O
subsection O
, O
we O
examine O
the O
NLI B-MetricName
Diversity I-MetricName
metric O
's O
correlation B-MetricName
to O
the O
human O
annotations O
collected O
by O
Tevet O
and O

Berant O
( O
2021 O
) O
. O
Each O
set O
of O
responses O
in O
conTest B-DatasetName
and O
decTest B-DatasetName
is O
scored O
by O
10 O
annotators O
from O
1 O
( O
not O
diverse O
at O
all O
) O
to O
5 O
( O
very O
diverse O
) O
with O
half-point O
increments. O
We O
compute O
correlation O
with O
respect O
to O
the O
averaged O
rating O
. O
In O
addition O
to O
NLI B-MetricName
Diversity I-MetricName

, O
we O
explore O
the O
prediction O
counts O
for O
each O
category. O
We O
expect O
that O
a O
higher O
# O
Entailment O
value O
will O
be O
negatively O
correlated O
with O
diversity B-MetricName
because O
the O
more O
pairs O
of O
responses O
that O
entail O
each O
other O
, O
the O
more O
similar O
the O
set O
of O
responses O
is. O
Similarly O
, O
we O
expect O
that O
a O
higher O
# O

Contradiction O
value O
will O
be O
positively O
correlated O
with O
diversity. O
Since O
the O
NLI B-MetricName
Diversity I-MetricName
metric O
incorporates O
both O
# O
Entailment O
and O
# O
Contradiction O
, O
we O
would O
expect O
this O
metric O
to O
be O
highly O
correlated O
with O
human O
judgments O
as O
well. O
Spearmean B-MetricName
's I-MetricName
ρ I-MetricName
rank I-MetricName
correlation O
results O
between O
our O
metrics O
and O
the O
human B-MetricName
diversity I-MetricName
scores I-MetricName
are O

shown O
in O
Table O
3. O
The O
highest-performing O
correlation O
for O
lexical B-MetricName
diversity I-MetricName
is O
the O
Neutral B-MetricName
NLI I-MetricName
Diversity I-MetricName
( O
0.69 B-MetricValue
) O
. O
The O
highest-performing O
semantic B-MetricName
diversity I-MetricName
correlation O
is O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
( O
0.64 B-MetricValue
) O
. O
Additionally O
, O
Baseline O
and O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
correlations O
are O
stronger O
when O
evaluating O
with O
the O
conTest B-DatasetName
dataset O
than O
the O

decTest B-DatasetName
dataset O
( O
an O
increase O
of O
0.48 B-MetricValue
to O
0.63 B-MetricValue
for O
Baseline B-MetricName
MNLI I-MetricName
and O
0.41 B-MetricValue
to O
0.64 B-MetricValue
for O
Confidence B-MetricName
NLI I-MetricName
) O
, O
indicating O
these O
metrics O
are O
more O
correlated O
with O
human O
ratings O
of O
semantic B-MetricName
diversity I-MetricName
than O
lexical B-MetricName
diversity I-MetricName
. O
Across O
both O
datasets O
, O
# O
Entailment O
is O
negatively O
correlated O
with O
diversity B-MetricName
, O
# O

Neutral O
does O
not O
have O
a O
strong O
correlation O
, O
and O
# O
Contradiction O
is O
positively O
correlated O
, O
as O
hypothesized. O
This O
supports O
our O
motivation O
to O
use O
NLI B-TaskName
as O
a O
diversity O
metric O
. O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
We O
have O
verified O
that O
NLI B-MetricName
Diversity I-MetricName
is O
both O
able O
to O
capture O
semantic B-MetricValue
diversity I-MetricValue
and O
aligns O
with O
human B-MetricValue
judgements. I-MetricValue

We O
can O
additionally O
use O
NLI B-MetricName
Diversity I-MetricName
to O
define O
a O
straightforward O
desired O
diversity O
threshold O
, O
div O
thresh O
for O
a O
set O
of O
model-generated O
responses O
, O
u O
1 O
, O
... O
, O
u O
n O
. O
For O
example O
, O
we O
might O
intend O
there O
to O
be O
10 O
Contradictions O
within O
the O
set. O
We O
propose O
a O
generation O
procedure O

, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
designed O
to O
iteratively O
increase O
the O
diversity O
of O
a O
set O
of O
responses O
for O
a O
conversation O
. O
For O
a O
conversation O
, O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
begins O
by O
sampling O
n O
responses. O
We O
score O
the O
diversity B-MetricName
of O
these O
responses O
using O
a O
diversity B-MetricName
metric O
, O
div_metric O
( O
u O
1 O
, O
... O

, O
u O
n O
) O
. O
If O
the O
diversity O
score O
falls O
above O
div O
thresh O
, O
the O
process O
is O
finished O
. O
If O
, O
however O
, O
the O
score O
falls O
below O
div B-HyperparameterName
thresh I-HyperparameterName
, O
we O
identify O
the O
model O
response O
which O
contributes O
least O
to O
the O
diversity O
score O
by O
calculating O
div_metric O
( O
u O
1 O
, O
... O

, O
u O
n−1 O
) O
for O
each O
sub-group O
of O
model O
responses O
of O
size O
n O
− O
1. O
We O
discard O
the O
model O
response O
not O
present O
in O
the O
highestscoring O
subgroup O
and O
resample O
a O
new O
response. O
We O
re-calculate O
div_metric O
( O
u O
1 O
, O
... O
, O
u O
n O
) O
and O
if O
div_metric O
( O
u O
1 O
, O

... O
, O
u O
n O
) O
> O
div B-HyperparameterName
thresh I-HyperparameterName
, O
the O
process O
finishes. O
We O
continue O
resampling O
until O
the O
maximum O
cutoff O
of O
S O
is O
reached O
. O
Evaluation O
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
Method O
Models O
and O
Datasets O
We O
experiment O
with O
two O
neural O
dialogue O
models O
, O
DialoGPT B-MethodName
( O
700M O
parameters O
) O
( O
Zhang O
et O
al. O

, O
2020b O
) O
4 O
and O
BlenderBot B-MethodName
1.0 O
( O
300M O
parameters O
) O
( O
Roller O
et O
al. O
, O
2021 O
) O
5 O
. O
We O
use O
the O
default O
Transformers O
implementation O
for O
each O
model O
( O
Wolf O
et O
al. O
, O
2020 O
) O
and O
do O
not O
fine-tune O
them. O
Runtime O
was O
between O
3 O
and O
36 O
hours O
on O
one O

Titan-X O
GPU O
. O
All O
experiments O
involve O
the O
dialogue O
model O
M O
generating O
5 O
responses O
for O
each O
conversation. O
The O
maximum O
number O
of O
samples O
, O
S O
, O
is O
set O
to O
20. O
All O
experiments O
are O
averaged O
over O
10 O
trials O
for O
stability O
. O
We O
evaluate O
each O
model O
on O
the O
development O
set O
of O
two O
public O
English O

conversational O
datasets O
: O
Dai-lyDialog++ B-DatasetName
( O
1,028 O
conversations O
) O
( O
Sai O
et O
al. O
, O
2020 O
; O
Li O
et O
al. O
, O
2017 O
) O
and O
EmpatheticDialogues B-DatasetName
( O
2,763 O
conversations O
) O
( O
Rashkin O
et O
al. O
, O
2019 O
) O
. O
DailyDia-log++ B-DatasetName
includes O
5 O
human-written O
responses O
per O
conversation O
, O
allowing O
for O
multi-reference O
comparison. O
We O
split O
each O

EmpatheticDialogues B-DatasetName
conversation O
at O
a O
random O
turn O
( O
consistent O
for O
all O
experiments O
) O
for O
generation. O
Since O
BlenderBot O
supports O
up O
to O
128 O
positional O
embeddings O
, O
we O
pass O
in O
the O
last O
128 O
tokens O
of O
the O
conversation O
for O
this O
condition O
. O
Metrics O
We O
evaluate O
three O
diversity O
metrics O
: O
two O
semantic O
diversity O
metrics O
, O
Baseline B-MetricName

NLI I-MetricName
Diversity I-MetricName
( O
Section O
3 O
) O
and O
Sent-BERT B-MetricName
( O
Reimers O
and O
Gurevych O
, O
2019 O
; O
Tevet O
and O
Berant O
, O
2021 O
) O
, O
and O
one O
lexical O
diversity O
metric O
, O
distinct-n B-MetricName
( O
Li O
et O
al. O
, O
2016 O
; O
Tevet O
and O
Berant O
, O
2021 O
) O
. O
For O
Sent-BERT B-MetricName
, O
we O
compute O
the O
average B-MetricName

negative I-MetricName
cosine I-MetricName
similarity I-MetricName
between O
BERT O
sentence O
embeddings O
for O
each O
pair O
of O
responses. O
Like O
Tevet O
and O
Berant O
( O
2021 O
) O
, O
for O
distinct-n B-MetricName
, O
we O
compute O
the O
average O
distinct O
n-grams O
from O
n O
∈ O
1 O
, O
2 O
, O
3 O
, O
4 O
, O
5 O
. O
Because O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
is O
more O
humaninterpretable O
than O

Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
we O
use O
this O
version O
for O
experimentation. O
For O
all O
NLI B-TaskName
Diversity O
experiments O
, O
div B-HyperparameterName
thresh I-HyperparameterName
is O
achieved O
when O
# O
Contradictions O
is O
greater O
than O
10 O
out O
of O
a O
total O
of O
20 O
pair-wise O
comparisons. O
For O
both O
Sent-BERT B-MetricName
and O
distinct-n B-MetricName
, O
however O
, O
we O
do O
not O
have O
a O
humanspecifiable O
threshold. O

We O
use O
empirical O
thresholds O
measured O
from O
the O
sets O
of O
5 O
human O
responses O
for O
each O
conversation O
in O
DailyDialog++. B-DatasetName
We O
choose O
the O
90th B-HyperparameterValue
percentile I-HyperparameterValue
for O
div B-HyperparameterName
thresh I-HyperparameterName
( O
0.98 B-HyperparameterValue
and O
-0.179 B-HyperparameterValue
for O
distinct-n B-MetricName
and O
Sent-BERT B-MetricName
, O
respectively O
) O
. O
We O
decode O
using O
nucleus O
sampling O
( O
p O
= O
0.9 O
) O
, O
as O
it O

has O
been O
shown O
to O
increase O
response O
diversity O
( O
Holtzman O
et O
al. O
, O
2020 O
) O
. O
However O
our O
method O
could O
be O
applied O
with O
other O
decoding O
procedures O
. O
In O
order O
to O
robustly O
evaluate O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
we O
measure O
both O
( O
i O
) O
whether O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
is O
able O
to O
generate O
more O

diverse O
sets O
of O
responses O
than O
was O
originally O
sampled O
and O
( O
ii O
) O
whether O
the O
increased O
diversity O
comes O
at O
the O
expense O
of O
decreased O
relevancy O
of O
the O
responses O
. O
Diversity O
Results O
We O
aim O
to O
measure O
whether O
the O
diversity O
of O
the O
5 O
responses O
from O
M O
increases O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
, O
compared O
to O

the O
initial O
5 O
sampled O
responses. O
Diversity O
of O
the O
starting O
and O
ending O
sets O
of O
utterances O
is O
measured O
by O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
distinct-n B-MetricName
, O
or O
Sent-BERT. B-MetricName
We O
also O
report O
the O
number O
of O
sampled O
utterances O
required O
to O
reach O
div O
thresh O
. O
Results O
for O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
are O
shown O
in O
Table O
4. O
For O

every O
condition O
, O
we O
see O
an O
increase O
from O
starting O
to O
ending O
diversity O
; O
for O
NLI B-MetricName
Diversity I-MetricName
, O
this O
results O
in O
an O
average O
137 O
% O
increase. O
For O
most O
conditions O
, O
distinct-n B-MetricName
requires O
more O
samples O
than O
Sent-BERT B-MetricName
and O
Baseline O
NLI B-MetricName
Diversity I-MetricName
. O
We O
can O
use O
the O
results O
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
to O

probe O
differences O
in O
the O
models. O
In O
our O
experimental O
setup O
, O
DialoGPT B-MethodName
generates O
more O
diverse O
utterances O
across O
all O
conditions O
than O
BlenderBot. B-MethodName
The O
models O
change O
by O
similar O
proportions O
from O
starting O
to O
ending O
diversity O
using O
the O
NLI B-TaskName
metric. O
However O
, O
the O
starting O
diversity O
for O
BlenderBot B-MethodName
is O
far O
lower O
than O
DialoGPT B-MethodName
; O
the O
negative O

value O
for O
BlenderBot B-MethodName
indicates O
that O
a O
large O
number O
of O
entailment O
predictions O
were O
present O
in O
the O
starting O
response O
set O
. O
We O
can O
also O
examine O
differences O
between O
the O
datasets. O
For O
instance O
, O
we O
observe O
lower O
starting O
diversities O
for O
the O
Empathetic B-DatasetName
Dialogues I-DatasetName
dataset O
than O
for O
DailyDialog++ B-DatasetName
for O
both O
models. O
Additionally O
, O
the O
number O

of O
samples O
required O
for O
Em-patheticDialogues B-DatasetName
is O
consistently O
higher O
than O
for O
DailyDialog++. B-DatasetName
This O
is O
likely O
because O
div O
thresh O
for O
both O
datasets O
was O
calculated O
using O
human O
responses O
from O
DailyDialog++ B-DatasetName
, O
since O
EmpatheticDialogues B-DatasetName
does O
not O
include O
multiple O
human O
responses O
. O
Sampled O
responses O
can O
be O
seen O
in O
Appendix O
B O
and O
results O
reporting O
the O
average O

overlap O
from O
starting O
to O
ending O
sets O
of O
responses O
is O
in O
Appendix O
C. O
Appendix O
D O
includes O
results O
using O
beam O
search O
instead O
of O
nucleus O
sampling O
, O
and O
Appendix O
E O
reports O
the O
stability B-MetricName
of O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
. O
Relevance O
Results O
Since O
past O
work O
has O
documented O
a O
tradeoff O
between O
diversity B-MetricName
and O
relevancy B-MetricName
, O
we O

also O
report O
results O
for O
the O
relevancy O
of O
the O
starting O
and O
ending O
sets O
of O
responses O
for O
Diversity B-MethodName
Threshold I-MethodName
Generation. I-MethodName
We O
use O
two O
established O
relevancy O
metrics O
: O
BLEU B-MetricName
Score O
( O
Papineni O
et O
al. O
, O
2002 O
) O
6 O
and O
BERTScore B-MetricName
( O
Zhang O
et O
al. O
, O
2020a O
) O
7 O
. O
We O
show O
results O
on O

DailyDialog++ B-DatasetName
, O
which O
has O
multiple O
human-generated O
responses O
for O
comparison O
, O
which O
is O
more O
correlated O
to O
human O
judgements O
than O
single-reference O
evaluation O
( O
Gupta O
et O
al. O
, O
2019 O
) O
. O
Results O
are O
shown O
in O
Table O
5. O
The O
key O
takeaway O
is O
that O
the O
relevancy O
values O
remain O
virtually O
unchanged O
when O
using O
the O
Diversity B-MethodName
Threshold I-MethodName

Generation I-MethodName
procedure O
, O
according O
to O
both O
BLEU B-MetricName
score O
and O
BERTScore. B-MetricName
The O
average O
percent O
difference O
is O
0.08 O
% O
for O
BertScore B-MetricName
and O
1.1 O
% O
for O
BLEU B-MetricName
. O
Conclusion O
We O
propose O
a O
novel O
semantic O
diversity O
metric O
, O
NLI B-MetricName
Diversity I-MetricName
, O
which O
is O
highly O
correlated O
to O
human O
judgments. O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
achieves O
state-ofthe-art O
results O

on O
measuring O
semantic O
diversity. O
We O
propose O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
to O
incentivize O
production O
of O
diverse O
sets O
of O
responses O
for O
a O
conversation. O
This O
results O
in O
more O
diverse O
sets O
of O
responses O
than O
originally O
sampled O
for O
multiple O
models O
, O
datasets O
, O
and O
metrics O
while O
maintaining O
relevancy O
, O
and O
can O
also O
be O
used O
to O
investigate O

a O
model O
's O
ability O
to O
produce O
diverse O
responses O
. O
A O
Confidence O
Interval O
Analysis O
We O
perform O
experimentation O
using O
bootstrapping O
to O
determine O
confidence B-MetricName
intervals O
for O
conTest B-DatasetName
correlations O
to O
the O
diversity O
parameter. O
We O
sample O
a O
dataset O
of O
110 O
elements O
( O
50 O
% O
of O
the O
original O
con-Test B-DatasetName
dataset O
's O
size O
) O
from O
conTest B-DatasetName
with O

replacement O
and O
compute O
corresponding O
Spearman B-MetricName
's I-MetricName
correlation I-MetricName
values O
using O
the O
sampled O
dataset O
for O
Sent-BERT B-MetricName
, O
Baseline B-MetricName
NLI I-MetricName
Diversity I-MetricName
, O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
, I-MetricName
and O
human B-MetricName
judgements. I-MetricName
We O
repeat O
this O
process O
1,000 O
times O
for O
stability B-MetricName
and O
calculate O
95 B-MetricValue
% I-MetricValue
Confidence B-MetricName
Intervals. O
The O
full O
conTest B-DatasetName
correlation O
value O
plotted O
with O
these O
intervals O
can O

be O
seen O
in O
Figure O
3. O
While O
the O
Confidence O
Interval O
values O
overlap O
between O
all O
4 O
conditions O
, O
the O
Confidence B-MetricName
NLI I-MetricName
Diversity I-MetricName
distribution O
closely O
matches O
the O
human O
distribution O
. O
B O
Sampled O
Responses O
Table O
7 O
shows O
randomly-sampled O
examples O
from O
the O
DailyDialog++ B-DatasetName
dataset O
, O
created O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
with O
the O
DialoGPT B-MethodName
model O
and O

NLI B-MetricName
Diversity O
as O
the O
intended O
div_metric O
. O
C O
Average B-MetricName
Utterance I-MetricName
Overlap I-MetricName
We O
measure O
the O
number O
of O
utterances O
which O
occur O
in O
both O
the O
starting O
and O
ending O
sets O
of O
responses O
, O
called O
utterance B-MetricName
overlap. I-MetricName
A O
high O
utterance O
overlap O
represents O
a O
set O
of O
responses O
which O
did O
not O
need O
to O
be O
significantly O
changed O
to O

reach O
div B-HyperparameterName
thresh I-HyperparameterName
. O
For O
example O
, O
an O
utterance B-MetricName
overlap I-MetricName
of O
4 B-MetricValue
indicates O
that O
only O
1 O
response O
needed O
to O
be O
resampled O
( O
potentially O
multiple O
times O
) O
from O
the O
starting O
set O
to O
reach O
div B-HyperparameterName
thresh I-HyperparameterName
. O
Results O
are O
seen O
in O
Table O
6. O
Keeping O
in O
mind O
that O
higher O
Average O
Overlap O
indicates O
less O

resampling O
was O
needed O
, O
we O
note O
higher O
overlap O
for O
DialoGPT B-MethodName
than O
BlenderBot B-MethodName
1.0 I-MethodName
( O
with O
the O
exception O
of O
distinct-n B-MetricName
and O
EmpatheticDialogues B-DatasetName
) O
. O
D O
Beam O
Search O
We O
evaluate O
beam O
search O
's O
ability O
to O
generate O
diverse O
utterances O
using O
Diversity B-MethodName
Threshold I-MethodName
Generation I-MethodName
for O
DailyDialog++ B-DatasetName
and O
NLI B-TaskName
Diversity. B-MetricName
To O
compare O
nucleus O
sampling O
to O

beam O
search O
, O
we O
generate O
25 O
beams O
and O
consider O
these O
responses O
from O
most O
to O
least O
probable O
, O
i.e. O
if O
the O
5 O
most O
likely O
beams O
do O
not O
satisfy O
the O
diversity O
threshold O
, O
we O
remove O
the O
lowest-scoring O
beam O
and O
replace O
it O
with O
the O
6th O
most O
likely O
beam. O
We O
find O
the O
starting O
NLI B-MetricName

Diversity I-MetricName
for O
beam O
search O
is O
-5.05 B-MetricValue
, O
the O
ending O
diversity O
is O
5.35 B-MetricValue
, O
and O
an O
average O
of O
10.97 O
sampled O
utterances O
is O
required. O
While O
the O
NLI B-TaskName
Diversity B-MetricName
does O
improve O
from O
the O
starting O
to O
ending O
set O
of O
responses O
, O
beam O
search O
has O
a O
much O
lower O
ending O
diversity O
than O
nucleus O
sampling. O
While O
past O

work O
has O
confirmed O
that O
nucleus O
sampling O
is O
more O
lexically O
diverse O
than O
beam O
search O
using O
Self-BLEU B-MetricName
( O
Holtzman O
et O
al. O
, O
2020 O
) O
, O
our O
results O
confirm O
that O
nucleus O
sampling O
is O
also O
able O
to O
generate O
more O
semantically O
diverse O
utterances O
. O

-DOCSTART- O
Learning O
Natural O
Language O
Generation O
with O
Truncated B-MethodName
Reinforcement I-MethodName
Learning I-MethodName
Learning B-MethodName
for I-MethodName
Language I-MethodName
( I-MethodName
TrufLL I-MethodName
) I-MethodName
, O
an O
original O
approach O
to O
train O
conditional O
language O
models O
without O
a O
supervised O
learning O
phase O
, O
by O
only O
using O
reinforcement O
learning O
( O
RL O
) O
. O
As O
RL O
methods O
unsuccessfully O
scale O
to O
large O
action O
spaces O
, O
we O

dynamically O
truncate O
the O
vocabulary O
space O
using O
a O
generic O
language O
model. O
TrufLL B-MethodName
thus O
enables O
to O
train O
a O
language O
agent O
by O
solely O
interacting O
with O
its O
environment O
without O
any O
task-specific O
prior O
knowledge O
; O
it O
is O
only O
guided O
with O
a O
task-agnostic O
language O
model. O
Interestingly O
, O
this O
approach O
avoids O
the O
dependency O
to O
labelled O
datasets O
and O

inherently O
reduces O
pretrained O
policy O
flaws O
such O
as O
language O
or O
exposure O
biases. O
We O
evaluate O
TrufLL B-MethodName
on O
two O
visual O
question O
generation O
tasks O
, O
for O
which O
we O
report O
positive O
results O
over O
performance O
and O
language O
metrics O
, O
which O
we O
then O
corroborate O
with O
a O
human O
evaluation. O
To O
our O
knowledge O
, O
it O
is O
the O
first O
approach O

that O
successfully O
learns O
a O
language O
generation O
policy O
without O
pre-training O
, O
using O
only O
reinforcement O
learning. O
1 O
Introduction O
Since O
the O
development O
of O
generic O
language B-MethodName
models I-MethodName
trained O
on O
massive O
unlabelled O
text O
corpora O
Brown O
et O
al. O
, O
2020 O
) O
, O
state-of-the O
art O
language O
processing O
systems O
rely O
on O
sequential B-MethodName
transfer I-MethodName
learning I-MethodName
( O
Ruder O
, O
2019 O

) O
. O
The O
pretrained O
Language B-MethodName
Model I-MethodName
( O
LM B-MethodName
) O
is O
fine-tuned O
on O
the O
downstream O
task O
using O
a O
standard B-MethodName
supervised I-MethodName
learning I-MethodName
( I-MethodName
SL I-MethodName
) I-MethodName
1 O
Code O
is O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
AMDonati O
/ O
RL-NLP O
VQG O
, O
TrufLL B-MethodName
truncates O
the O
vocabulary O
space O
by O
using O
a O
language O
model. O
Here O

, O
'run O
, O
' O
and O
'the O
' O
are O
syntactically O
incorrect O
and O
thus O
truncated. O
Yet O
, O
'car O
' O
is O
not O
trimmed O
as O
the O
LM B-MethodName
is O
not O
visually O
grounded. O
( O
right O
) O
In O
a O
VQG B-MethodName
training O
loop O
, O
the O
agent O
generates O
a O
question O
given O
an O
image-answer O
pair O
, O
which O
is O
then O
fed O

to O
a O
VQA B-MethodName
model O
predicting O
an O
expected O
answer. O
If O
both O
answers O
match O
, O
the O
agent O
is O
rewarded O
. O
objective O
Peters O
et O
al. O
, O
2019 O
) O
. O
Yet O
, O
such O
an O
approach O
suffers O
from O
several O
issues O
: O
( O
i O
) O
catastrophic O
forgetting O
when O
a O
model O
forgets O
previously O
learned O
knowledge O
and O
overfits O

to O
target O
domains O
, O
( O
ii O
) O
computational O
inefficiency O
from O
fine-tuning O
billion-parameters O
networks O
, O
and O
( O
iii O
) O
the O
need O
of O
supervised O
datasets. O
Moreover O
, O
task-specific O
language B-MethodName
models I-MethodName
learned O
with O
SL B-MethodName
suffer O
from O
well-studied O
text B-TaskName
degeneration I-TaskName
issues O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
such O
as O
the O
exposure O
bias O

, O
language O
biases O
( O
Saleh O
et O
al. O
, O
2020 O
; O
, O
or O
a O
lack O
of O
diversity O
( O
Li O
et O
al. O
, O
2015 O
) O
. O
On O
the O
other O
hand O
, O
text B-TaskName
generation I-TaskName
can O
be O
naturally O
framed O
as O
a O
sequential B-TaskName
decision I-TaskName
making I-TaskName
problem O
, O
with O
the O
sequence O
of O
words O
seen O
as O
successive O

actions O
over O
a O
vocabulary. O
Thus O
, O
some O
researchers O
have O
recently O
focused O
on O
learning O
language O
models O
using O
instead O
Reinforcement B-MethodName
Learning I-MethodName
( I-MethodName
RL I-MethodName
) I-MethodName
Das O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
. O
RL B-MethodName
methods O
allow O
acquiring O
language O
through O
interactions O
within O
rich O
and O
diverse O
environments O
( O
Luketina O
et O

al. O
, O
2019 O
) O
, O
help O
understanding O
language O
acquisition O
and O
language O
pragmatics O
( O
Lazaridou O
et O
al. O
, O
2016 O
; O
Bisk O
et O
al. O
, O
2020 O
) O
. O
" O
Reward O
is O
enough O
" O
( O
Silver O
et O
al. O
, O
2021 O
) O
highlights O
the O
necessity O
of O
using O
RL B-MethodName
for O
AI O
systems O
to O
acquire O
language O

in O
its O
full O
richness. O
Indeed O
, O
( O
i O
) O
language O
may O
be O
intertwined O
with O
other O
modalities O
of O
action O
and O
observation O
, O
( O
ii O
) O
the O
utility O
of O
language O
varies O
according O
to O
situations O
and O
behaviours O
, O
( O
iii O
) O
it O
is O
consequential O
and O
purposeful O
, O
and O
( O
iv O
) O
some O
linguistic O

problems O
are O
better O
solved O
dynamically O
, O
through O
experience O
( O
such O
as O
using O
a O
diplomatic O
tone O
in O
a O
speech. O
) O
In O
addition O
, O
RL B-MethodName
allows O
optimizing O
a O
non-differentiable O
learning O
signal O
, O
hence O
handles O
more O
diverse O
objective O
functions O
, O
and O
also O
avoids O
some O
of O
the O
text B-TaskName
degeneration I-TaskName
issues O
previously O
mentioned. O
So O
far O

, O
RL-based B-MethodName
text-generation B-TaskName
tasks O
have O
relied O
on O
a O
pre-training O
phase O
to O
ease O
learning O
: O
the O
policy O
language O
model O
is O
trained O
with O
SL B-MethodName
on O
the O
task O
dataset O
, O
before O
being O
fine-tuned O
with O
policy O
gradient O
methods O
( O
Sutton O
et O
al. O
, O
1999 O
) O
on O
the O
task O
at O
hand. O
Those O
approaches O
often O
require O

human-labelled O
datasets. O
Besides O
, O
combining O
pre-training O
and O
fine-tuning O
phases O
either O
barely O
change O
the O
policy O
distribution O
, O
or O
induces O
language O
drift O
( O
Lazaridou O
et O
al. O
, O
2020 O
; O
Lu O
et O
al. O
, O
2020b O
) O
, O
i.e O
the O
generated O
language O
drifts O
semantically O
or O
syntactically O
from O
natural O
language O
. O
In O
this O
paper O
, O

we O
aim O
at O
learning O
a O
conditional O
language O
model O
using O
RL B-MethodName
without O
a O
pre-training O
phase O
, O
so O
that O
( O
i O
) O
we O
get O
free O
from O
datasets O
with O
human O
annotations O
, O
and O
( O
ii O
) O
we O
avoid O
the O
text B-TaskName
generation I-TaskName
flaws O
induced O
by O
the O
common O
methods. O
While O
appealing O
, O
such O
an O
approach O

requires O
overcoming O
the O
hurdle O
of O
the O
combinatorial O
language O
action O
space O
, O
a O
vocabulary O
usually O
containing O
more O
than O
10,000 O
words. O
Yet O
, O
while O
large O
and O
discrete O
, O
a O
language O
action O
space O
contains O
a O
specific O
structure O
, O
made O
of O
all O
the O
syntactical O
and O
semantics O
rules O
of O
a O
given O
language. O
TrufLL B-MethodName
leverages O
such O

structure O
to O
drive O
the O
exploration O
of O
the O
RL-based B-MethodName
language O
agent O
during O
training. O
At O
each O
time O
step O
of O
the O
text B-TaskName
generation I-TaskName
process O
, O
TrufLL B-MethodName
truncates O
its O
effective O
action O
space O
to O
a O
small O
subset O
of O
words O
provided O
by O
a O
pretrained O
task-agnostic O
language O
model. O
Such O
an O
approach O
injects O
a O
generic O
prior O
linguistic O
knowledge O

into O
the O
RL B-MethodName
algorithm O
, O
is O
usable O
on O
tasks O
lacking O
in-domain O
labeled O
data O
, O
and O
can O
be O
easily O
transferred O
to O
new O
RL-based B-MethodName
text O
generation O
tasks. O
Thus O
, O
TrufLL B-MethodName
can O
be O
applied O
to O
any O
language O
generation O
task O
given O
a O
generic O
LM O
and O
a O
reward. O
We O
here O
evaluate O
it O
on O
two O
Visual O

Question O
Generation O
( O
VQG O
) O
tasks O
, O
the O
synthetic O
CLEVR B-DatasetName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
, O
and O
the O
natural O
language O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
. O
Unlike O
alternative O
RL B-MethodName
without O
pre-training O
approaches O
, O
TrufLL B-MethodName
manages O
to O
ask O
meaningful O
and O
valid O
questions O
on O
large O

vocabularies O
, O
exhibiting O
success B-MetricName
rate I-MetricName
and O
language B-MetricName
metrics I-MetricName
close O
to O
pretrain O
models O
with O
labeled O
data O
, O
while O
producing O
more O
original O
language O
. O
TrufLL B-MethodName
We O
here O
aim O
at O
making O
RL B-MethodName
methods O
feasible O
in O
the O
language O
setting O
by O
dynamically O
reducing O
the O
action O
space O
, O
i.e. O
, O
by O
restricting O
the O
language O
agent O
to O

select O
a O
word O
within O
a O
subset O
of O
the O
vocabulary O
at O
each O
time O
step. O
We O
detail O
below O
the O
action O
space O
's O
truncation O
model O
and O
the O
associated O
RL B-MethodName
algorithm O
to O
learn O
the O
language O
agent O
. O
Dynamic B-MethodName
Vocabulary I-MethodName
Truncation I-MethodName
TrufLL I-MethodName
combines O
two O
distinct O
language O
models O
, O
which O
share O
the O
same O
vocabulary O
V O
: O

a O
RL O
language O
agent O
π O
θ O
and O
a O
pretrained O
language O
model O
f O
LM O
. O
At O
each O
timestep O
t O
, O
TrufLL B-MethodName
restricts O
the O
vocabulary O
space O
of O
the O
RL B-MethodName
language O
agent O
with O
: O
V O
− O
t O
= O
{ O
w|w O
∈V O
, O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
} O
, O
where O

g O
trunc O
is O
a O
truncation O
function O
based O
on O
f O
LM O
which O
either O
associates O
0 O
or O
1 O
with O
each O
word O
in O
the O
vocabulary O
given O
the O
past O
words O
w O
< O
t O
. O
From O
a O
language O
modelling O
perspective O
, O
the O
vocabulary O
space O
of O
the O
language O
agent O
is O
reduced O
from O
V O
to O
V O
− O

where O
|V O
− O
|≪|V| O
, O
with O
|•| O
the O
cardinal O
of O
a O
finite O
set. O
From O
a O
RL B-MethodName
perspective O
, O
the O
RL B-MethodName
agent O
follows O
a O
truncated O
policy O
π O
− O
θ O
which O
only O
samples O
actions O
over O
the O
subset O
V O
− O
. O
In O
practice O
, O
such O
a O
policy O
is O
computed O
using O
a O
masked O
softmax O

function O
over O
the O
truncated O
vocabulary O
V O
− O
t O
: O
π O
− O
θ O
( O
.|w O
< O
t O
, O
c O
) O
= O
softmax O
( O
m O
* O
logits O
π O
θ O
( O
w O
< O
t O
, O
c O
) O
) O
where O
m=1 O
when O
g O
trunc O
( O
w|w O
< O
t O
) O
=1 O
otherwise O
m=−∞ O
. O
Truncation O

Functions O
We O
here O
list O
the O
different O
truncation O
functions O
g O
trunc O
explored O
through O
the O
paper O
. O
Top-k O
words O
: O
This O
function O
selects O
the O
k O
words O
with O
the O
highest O
probability O
given O
by O
f O
LM O
( O
.|w O
< O
t O
) O
: O
g O
top O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O

k O
) O
=1 O
wt∈top O
( O
k O
) O
( O
f O
LM O
( O
.|w O
< O
t O
) O
) O
. O
Probability O
threshold O
( O
α O
) O
: O
This O
function O
only O
keeps O
words O
having O
a O
probability O
f O
LM O
( O
.|w O
< O
t O
) O
greater O
than O
α O
: O
g O
p O
th O
( O
α O
) O
( O
w O

t O
|w O
< O
t O
; O
α O
) O
=1 O
f O
LM O
( O
wt|w O
< O
t O
) O
> O
α O
. O
Top-p B-HyperparameterName
: O
This O
function O
is O
based O
on O
nucleus O
sampling O
( O
Holtzman O
et O
al. O
, O
2019 O
) O
, O
and O
it O
keeps O
the O
most O
likely O
words O
contained O
in O
a O
probability O
mass O
p O
of O
f O

LM O
( O
.|w O
< O
t O
) O
. O
Formally O
, O
we O
define O
V O
p O
t O
as O
: O
V O
p O
t O
= O
g O
sample O
( O
k O
) O
( O
w O
t O
|w O
< O
t O
; O
k O
) O
=1 O
wt∈ O
{ O
w O
i O
∼f O
LM O
( O
.|w O
< O
t O
) O
i∈ O
1 O
, O
... O

, O
k O
} O
. O
Only O
top B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
provides O
a O
fixed O
number O
of O
words O
at O
each O
time O
step. O
p B-HyperparameterName
th I-HyperparameterName
( I-HyperparameterName
α I-HyperparameterName
) I-HyperparameterName
, O
top B-HyperparameterName
( I-HyperparameterName
p I-HyperparameterName
) I-HyperparameterName
, O
and O
sample B-HyperparameterName
( I-HyperparameterName
k I-HyperparameterName
) I-HyperparameterName
have O
a O
dynamic O
truncation O
, O
whose O
size O
at O
t O
depends O
on O
the O
language O
model O
entropy O

. O
Task-Specific O
vs. O
Generic O
LM O
We O
benchmark O
two O
types O
of O
language O
models O
for O
truncation. O
On O
the O
one O
hand O
, O
we O
use O
an O
external O
language O
model O
pretrained O
on O
a O
large O
task-agnostic O
language O
corpora. O
Such O
a O
model O
provides O
a O
generic O
linguistic O
prior O
to O
the O
RL B-MethodName
agent O
exploration O
process O
, O
solely O
encoding O
syntactic O

and O
semantic O
information. O
On O
the O
other O
hand O
, O
we O
use O
a O
task-related O
language O
model O
pretrained O
on O
the O
supervised O
dataset O
associated O
with O
the O
task. O
Such O
a O
model O
provides O
a O
task-specific O
linguistic O
prior O
to O
the O
RL B-MethodName
language O
agent O
, O
and O
captures O
language O
pragmatics. O
We O
emphasize O
that O
this O
paper O
aims O
at O
leveraging O
taskagnostic O

language O
models O
as O
they O
discard O
the O
need O
for O
task-specific O
data. O
For O
the O
sake O
of O
completeness O
, O
we O
also O
study O
the O
truncation O
with O
the O
task-related O
LM O
as O
an O
additional O
benchmark O
to O
assess O
our O
approach O
. O
Experimental O
Setting O
We O
here O
list O
the O
experimental O
setting O
and O
detail O
the O
network O
and O
hyperparameters O
in O
Appendix O

A.4 O
. O
Visual O
Question O
Generation O
We O
showcase O
TrufLL B-MethodName
on O
the O
task O
of O
Visual B-MethodName
Question I-MethodName
Generation I-MethodName
( O
VQG B-MethodName
) O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
, O
which O
is O
a O
form O
of O
Visual O
Jeopardy O
! O
™ O
( O
Ferrucci O
, O
2012 O
) O
. O
There O
, O
the O
language O
agent O
observes O
an O
image-answer O
pair O

and O
has O
to O
generate O
a O
question O
that O
results O
in O
a O
similar O
answer O
, O
as O
illustrated O
in O
Figure O
1. O
Such O
a O
task O
presents O
multiple O
advantages. O
First O
, O
by O
combining O
vision O
, O
scene O
understanding O
and O
language O
generation O
, O
it O
requires O
high-level O
reasoning O
and O
exhibits O
a O
large O
spectrum O
of O
language O
difficulties. O
Secondly O
, O

the O
success O
criterion O
is O
naturally O
non-differentiable O
, O
hence O
a O
natural O
fit O
for O
RL B-MethodName
methods. O
Such O
a O
criterion O
, O
unlike O
metrics O
based O
on O
ground-truth O
sentences O
, O
allows O
generating O
diverse O
grounded O
questions O
given O
an O
image-answer O
pair O
. O
Formally O
, O
the O
initial O
context O
c O
is O
composed O
of O
the O
image-answer O
pair O
( O
I O
, O

A O
) O
. O
The O
RL B-MethodName
agent O
then O
generates O
a O
sequence O
of O
words O
w O
< O
t O
of O
maximum O
length O
T O
. O
We O
then O
provide O
the O
generated O
question O
to O
a O
pretrained O
VQA B-MethodName
model. O
This O
model O
takes O
as O
inputs O
the O
image O
I O
, O
the O
generated O
question O
w O
< O
t O
and O
outputs O
a O
predicted O

answerÂ. O
Finally O
, O
the O
agent O
receives O
a O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
based O
on O
A O
andÂ O
. O
Datasets O
We O
evaluate O
TrufLL B-MethodName
on O
the O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
datasets O
to O
simulate O
large-scale O
VQG B-DatasetName
datasets. O
The O
two O
datasets O
have O
been O
originally O
created O
for O
the O
task O
of O

Visual B-MethodName
Question I-MethodName
Answering I-MethodName
( I-MethodName
VQA I-MethodName
) I-MethodName
, O
i.e. O
for O
multi-modal O
classification O
algorithms O
predicting O
an O
answer O
given O
an O
image-question O
pair O
. O
CLEVR B-DatasetName
The O
CLEVR B-DatasetName
VQA B-MethodName
dataset O
( O
Johnson O
et O
al. O
, O
2017 O
) O
is O
made O
of O
template O
questions O
on O
synthetic O
images O
, O
which O
contain O
simple O
objects O
with O
four O
distinct O
properties O
( O

shape O
, O
material O
, O
color O
, O
size O
) O
. O
The O
vocabulary O
contains O
86 O
words O
and O
28 O
potential O
answers O
, O
making O
it O
a O
valuable O
proof O
of O
concept O
for O
assessing O
TrufLL. B-MethodName
Both O
language O
models O
are O
single-layer O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
512 O
units O
, O
and O
512 O
word O
embedding O

dimension. O
The O
task-specific O
LM O
is O
trained O
over O
the O
full O
train O
dataset O
of O
CLEVR B-DatasetName
questions. O
The O
external O
language O
model O
is O
trained O
on O
the O
mixture O
of O
CLOSURE B-DatasetName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
and O
CLEVR-Dialog B-DatasetName
( O
Kottur O
et O
al. O
, O
2019 O
) O
datasets. O
Although O
those O
two O
datasets O
share O
the O
CLEVR B-DatasetName
vocabulary O

, O
their O
language O
distribution O
differs O
from O
vanilla B-DatasetName
CLEVR. I-DatasetName
Finally O
, O
we O
use O
a O
pretrained O
GT-Vector-NMN B-MethodName
( O
Bahdanau O
et O
al. O
, O
2019 O
) O
to O
compute O
the O
reward O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O
A=Â O
, O
t=T O
−1 O
, O
where O
1 O
is O
the O
indicator O
function O

. O
VQAv2 B-DatasetName
The O
VQAv2 B-DatasetName
dataset O
( O
Goyal O
et O
al. O
, O
2017 O
) O
is O
made O
of O
natural O
language O
and O
open-formed O
questions O
on O
images O
from O
the O
MS-Coco B-DatasetName
Dataset O
( O
Lin O
et O
al. O
, O
2014 O
) O
. O
It O
has O
a O
vocabulary O
of O
14,810 O
words O
and O
3,149 O
answers. O
The O
task-specific O
language O
model O
is O
a O

one-layer B-MethodName
LSTM I-MethodName
with O
512 O
units O
and O
a O
512 O
word O
embedding O
dimension O
, O
pretrained O
over O
the O
full O
training O
dataset O
of O
VQAv2 B-DatasetName
questions. O
The O
External O
Language O
Model O
is O
Open-AI B-MethodName
's I-MethodName
GPT-2 I-MethodName
. O
The O
original O
language O
model O
outputs O
a O
probability O
distribution O
over O
50,257 O
tokens O
, O
but O
we O
use O
a O
masked O
softmax O
function O
to O

restrict O
the O
probability O
distribution O
to O
the O
14,810 O
tokens O
of O
the O
VQAv2 B-DatasetName
dataset. O
Unlike O
most O
NLP B-MethodName
tasks O
relying O
on O
pretrained O
generic O
language O
models O
, O
we O
do O
not O
fine-tune O
it O
on O
the O
task O
dataset. O
Instead O
, O
we O
leverage O
the O
few-shot O
generalization O
capabilities O
of O
GPT-2 B-MethodName
, O
by O
feeding O
the O
language O
model O
with O
the O

prompt O
" O
Here O
are O
a O
few O
examples O
: O
" O
followed O
by O
100 O
random O
questions O
q O
< O
100 O
from O
the O
dataset. O
The O
truncation O
is O
then O
based O
on O
the O
probability O
distribution O
f O
gpt2 O
LM O
( O
.|q O
< O
100 O
, O
w O
< O
t O
) O
. O
Finally O
, O
we O
used O
a O
pretrained O
Vil-BERT B-MethodName
to O

compute O
the O
reward O
( O
Lu O
et O
al. O
, O
2020a O
) O
. O
Given O
the O
large O
number O
of O
answers O
, O
we O
use O
as O
reward O
a O
decreasing O
function O
of O
the O
rank O
of O
the O
reference O
answer O
rk O
( O
A O
) O
: O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
= O
1 O

rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
as O
further O
explained O
in O
Appendix O
A.5 O
. O
In O
these O
two O
settings O
, O
we O
acknowledge O
that O
the O
task O
dataset O
is O
still O
used O
to O
train O
the O
VQA B-MethodName
models. O
Please O
note O
that O
the O
VQA B-MethodName
modules O
are O
only O

used O
to O
model O
the O
environment O
, O
i.e. O
to O
provide O
a O
positive O
/ O
negative O
feedback O
to O
the O
agent. O
In O
other O
settings O
, O
TrufLL B-MethodName
would O
still O
work O
if O
we O
replace O
the O
VQA B-MethodName
model O
by O
any O
language O
interface O
: O
text-game O
( O
e.g. O
Zork O
) O
, O
expert-systems O
, O
or O
humans. O
Here O
, O
we O
only O

use O
the O
VQG O
framework O
as O
a O
proof O
of O
concept O
that O
natural O
language O
can O
be O
learned O
through O
pure O
interaction O
given O
any O
task O
reward. O
Other O
language O
generation O
applications O
are O
discussed O
in O
Section O
5.3 O
. O
Baselines O
In O
this O
paper O
, O
we O
aim O
to O
show O
that O
a O
RL B-MethodName
language O
agent O
can O
be O
trained O
from O

scratch O
, O
i.e. O
without O
the O
usual O
pre-training O
phase O
by O
solely O
interacting O
with O
another O
language O
system O
, O
the O
VQA B-MethodName
model O
, O
when O
supported O
by O
truncation O
methods. O
The O
truncation O
with O
the O
task-related O
LM O
is O
referred O
to O
as O
TrufLL B-MethodName
( O
Task-LM B-MethodName
) O
, O
while O
the O
one O
with O
the O
External O
LM O
is O
referred O
as O

TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
. O
We O
first O
emphasize O
the O
difficulty O
of O
training O
an O
RL B-MethodName
language O
agent O
without O
a O
supervised O
pre-training O
phase O
through O
two O
baselines. O
We O
trained O
a O
simple O
on-policy O
PPO O
algorithm O
without O
any O
action O
space O
pruning O
, O
and O
refer O
to O
it O
as O
scratch. O
Then O
, O
we O
added O
a O
Kullback-Leibler B-HyperparameterName
( I-HyperparameterName

KL I-HyperparameterName
) I-HyperparameterName
regularization I-HyperparameterName
term O
to O
the O
loss O
, O
λ O
KL B-HyperparameterName
KL B-HyperparameterName
( O
π O
θ O
||f O
LM O
) O
, O
with O
λ O
KL B-HyperparameterName
> O
0 O
, O
to O
incorporate O
language O
prior O
to O
the O
agent O
as O
in O
( O
Jaques O
et O
al. O
, O
2017 O
( O
Jaques O
et O
al. O
, O
, O
2019. O
We O
refer O
to O
it O

as O
scratch O
+ O
KL-task B-HyperparameterName
when O
distilling O
the O
task-specific O
language O
model O
, O
and O
scratch O
+ O
KL-ext B-HyperparameterName
with O
the O
external O
language O
model. O
Finally O
, O
we O
include O
two O
baselines O
with O
a O
pre-training O
phase. O
We O
trained O
a O
language O
agent O
on O
the O
task-dataset O
with O
a O
log-likelihood O
objective O
, O
and O
refer O
to O
it O
as O
pretrain. O
Then O

, O
we O
fine-tune O
the O
pretrained O
language O
agent O
with O
PPO O
without O
truncation O
, O
and O
refer O
to O
it O
as O
pretrain O
+ O
RL B-MethodName
fine-tune. O
These O
two O
baselines O
should O
be O
viewed O
as O
gold O
standards O
as O
they O
rely O
on O
task-related O
data O
; O
additionally O
, O
pretrain O
+ O
RL B-MethodName
fine-tune O
is O
today O
the O
state-of-the-art O
method O
for O
learning O

RL-based B-MethodName
LM O
. O
Metrics O
and O
Evaluation O
Methods O
Evaluating O
text O
generation O
is O
an O
open-research O
problem O
in O
language O
literature. O
We O
decompose O
automatic O
language O
evaluation O
into O
three O
categories O
to O
assess O
different O
facets O
of O
language O
, O
and O
perform O
as O
well O
a O
human O
evaluation O
study O
. O
Performance O
metrics. O
We O
measure O
the O
taskcompletion B-MetricName
score I-MetricName
or O
recall B-MetricName

@ O
1 B-MetricValue
which O
states O
whether O
the O
target O
answer O
A O
is O
the O
top O
answer O
of O
the O
VQA B-MethodName
models O
, O
and O
the O
recall B-MetricName
@ O
5 B-MetricValue
( O
R O
@ O
5 O
) O
, O
which O
assesses O
whether O
A O
is O
in O
the O
5 O
top O
answers. O
These O
scores O
measure O
the O
task-solving O
abilities O
of O
the O
agent O
, O
but O

they O
are O
also O
conditioned O
by O
the O
VQA B-MethodName
model O
abilities O
. O
Language O
Metrics. O
First O
, O
we O
used O
n-grams B-MetricName
metrics O
, O
BLEU B-MetricName
( O
Papineni O
et O
al. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
and O
CIDEr B-MetricName
( O
Vedantam O
et O
al. O
, O
2015 O
) O
, O
to O
measure O
the O
similarity O

between O
the O
generated O
question O
and O
the O
reference O
questions O
in O
the O
evaluation O
set. O
While O
those O
scores O
can O
capture O
syntactic O
and O
semantic O
properties O
of O
language O
, O
they O
also O
fall O
short O
when O
dealing O
with O
open-form O
language O
, O
e.g. O
an O
identical O
answer O
may O
arise O
from O
two O
non-overlapping O
but O
syntactically O
correct O
questions. O
Thus O
, O
we O

also O
compute O
two O
metrics O
assessing O
the O
quality O
of O
the O
language O
independently O
of O
reference O
questions O
, O
the O
perplexity B-MetricName
of I-MetricName
the I-MetricName
question I-MetricName
given I-MetricName
an I-MetricName
external I-MetricName
LM I-MetricName
( I-MetricName
ppl-e I-MetricName
) I-MetricName
, O
and O
its O
perplexity B-MetricName
given I-MetricName
the I-MetricName
task-related I-MetricName
LM I-MetricName
( I-MetricName
ppl-t I-MetricName
) I-MetricName
. O
Diversity O
Metrics. O
We O
here O
estimate O
a O
self-BLEU B-MetricName
( I-MetricName
sBLEU I-MetricName
) I-MetricName
score O

( O
Zhang O
et O
al. O
, O
2017 O
) O
over O
10 O
questions O
generated O
on O
the O
same O
image-answer O
pair. O
Although O
such O
score O
detects O
potential O
mode O
collapse O
, O
i.e. O
, O
when O
the O
language O
utters O
identical O
sequences O
of O
words O
, O
it O
also O
values O
babbling O
, O
i.e. O
, O
outputting O
random O
words. O
We O
thus O
also O
measure O
the O

probability O
mass O
of O
the O
ten O
most O
frequent O
words O
( O
Choshen O
et O
al. O
, O
2020 O
) O
, O
and O
refer O
to O
it O
as O
peakiness B-MetricName
( I-MetricName
peak I-MetricName
) I-MetricName
. O
Human O
Evaluation. O
On O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
performed O
human O
evaluation O
by O
surveying O
53 O
participants O
on O
the O
first O
50 O
questions O
produced O
by O
some O

of O
the O
models O
at O
test O
time. O
The O
study O
( O
further O
detailed O
in O
Appendix O
C O
) O
is O
based O
on O
pairwise O
comparison O
of O
question O
samples O
produced O
by O
the O
concurrent O
algorithms O
according O
to O
four O
criteria. O
First O
, O
we O
evaluated O
the O
language O
quality O
of O
the O
question O
samples O
, O
by O
asking O
the O
participants O
to O
select O

the O
most O
syntactically O
and O
semantically O
correct O
question O
among O
the O
two O
samples O
of O
the O
questions O
pair. O
Secondly O
, O
we O
evaluated O
language O
grounding O
, O
i.e O
adequacy O
of O
the O
sample O
to O
the O
image-answer O
pair O
, O
by O
asking O
the O
participants O
to O
select O
the O
question O
most O
suitable O
given O
the O
two O
elements. O
Thirdly O
, O
we O
evaluated O

the O
language O
originality O
and O
diversity O
, O
by O
asking O
participants O
to O
select O
the O
question O
the O
most O
different O
from O
the O
dataset O
reference O
question. O
Finally O
, O
we O
evaluated O
the O
number O
of O
syntax O
errors O
by O
asking O
participants O
to O
tick O
the O
question O
if O
it O
is O
grammatically O
incorrect. O
Examples O
of O
questions O
asked O
during O
the O
study O
are O

included O
in O
the O
Appendix O
C O
. O
Sampling O
methods O
for O
text B-TaskName
generation I-TaskName
When O
generating O
text O
from O
a O
trained O
language O
model O
, O
the O
quality O
and O
diversity O
of O
samples O
depend O
on O
the O
decoding O
algorithm O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O
We O
consider O
three O
text B-TaskName
generation I-TaskName
methods. O
greedy O
uses O
the O
argmax O
of O

the O
policy O
, O
while O
sampling O
uses O
the O
multinomial O
distribution. O
Finally O
, O
we O
sampled O
ten O
text O
sequences O
from O
the O
policy O
, O
and O
selected O
the O
one O
with O
the O
lowest O
perplexity B-MetricName
according O
to O
the O
external O
language O
model O
, O
and O
refer O
to O
it O
as O
lm-ranking. O
This O
process O
has O
been O
used O
recently O
in O
Text-to-Image B-TaskName
Generation O

tasks O
( O
Ramesh O
et O
al. O
, O
2021 O
5 O
Results O
CLEVR B-DatasetName
results O
Quantitative O
performance O
: O
In O
Table O
1 O
, O
vanilla O
RL B-MethodName
from O
scratch O
fails O
to O
have O
a O
decent O
performance O
even O
with O
synthetic O
language. O
Besides O
, O
adding O
a O
KL B-HyperparameterName
regularisation I-HyperparameterName
term O
does O
kick-start O
the O
learning O
process. O
Yet O
, O
as O
soon O
as O
we O

apply O
the O
dynamic O
truncation O
, O
TrufLL B-MethodName
matches O
the O
pretrained O
baselines O
performance O
when O
using O
the O
external O
LM O
, O
and O
even O
outperforms O
them O
with O
the O
task-specific O
LM. O
In O
this O
synthetic O
VQG B-MethodName
setting O
, O
TrufLL B-MethodName
seems O
to O
be O
a O
viable O
and O
promising O
procedure O
to O
learn O
a O
RL B-MethodName
language O
agent O
without O
a O
supervised O
training O

phase. O
Pretrained O
baselines O
have O
high O
language O
scores O
when O
assessed O
with O
datasetbased O
metrics O
, O
e.g O
BLEU B-MetricName
or O
task-perplexity. B-MetricName
Yet O
, O
they O
also O
remain O
close O
to O
the O
original O
dataset O
distribution O
with O
a O
medium O
external O
perplexity. O
Noticeably O
, O
TrufLL B-MethodName
with O
the O
task-specific O
LM O
follows O
the O
same O
pattern. O
On O
the O
other O
hand O
, O
TrufLL B-MethodName

with O
the O
external O
LM O
reports O
poor O
dataset-based O
language O
scores O
, O
while O
maintaining O
a O
low O
external O
perplexity. O
Therefore O
, O
TrufLL B-MethodName
seems O
to O
correctly O
capture O
the O
language O
distribution O
of O
the O
initial O
LM. O
As O
the O
performance O
score O
is O
high O
when O
using O
an O
external O
LM O
, O
it O
suggests O
that O
our O
approach O
can O
learn O
a O

policy O
on O
a O
language O
task O
with-out O
the O
need O
of O
a O
task-related O
dataset. O
Less O
positively O
, O
TrufLL B-MethodName
diversity O
metrics O
suggest O
potential O
mode O
collapse O
, O
with O
a O
high O
peakiness B-MetricName
and O
self-BLEU B-MetricName
score O
. O
Qualitative O
performance O
: O
We O
display O
qualitative O
samples O
in O
Figure O
2 O
and O
Appendix O
D. O
On O
the O
one O
hand O
, O
the O

pretrained O
baselines O
generate O
either O
a O
question O
inconsistent O
with O
the O
visual O
context O
, O
or O
which O
fails O
to O
answer O
the O
expected O
answer. O
action O
space O
, O
while O
having O
a O
lower O
performance O
, O
yields O
to O
the O
most O
correct O
and O
diverse O
language O
, O
with O
higher O
language B-MetricName
scores I-MetricName
and O
a O
lower O
self-BLEU. B-MetricName
A O
stochastic O
action O
space O

might O
be O
harder O
to O
explore O
efficiently O
for O
reaching O
good O
task-solving O
abilities O
, O
but O
might O
strengthen O
the O
agent O
language O
generation O
properties O
. O
VQAv2 B-DatasetName
task O
In O
CLEVR B-DatasetName
, O
we O
observe O
that O
TrufLL B-MethodName
seems O
a O
promising O
approach O
to O
learn O
a O
language O
policy O
without O
a O
supervised O
training O
phase O
, O
by O
solely O
interacting O
with O
another O

language O
system. O
We O
scale O
our O
approach O
to O
natural O
language O
with O
large O
vocabulary O
( O
15k O
tokens O
) O
through O
the O
VQAv2 B-DatasetName
dataset O
. O
Quantitative O
performance O
: O
Table O
3 O
reports O
the O
VQAv2 B-DatasetName
results O
, O
for O
which O
TrufLL B-MethodName
and O
the O
baselines O
present O
a O
similar O
trend O
than O
on O
CLEVR. B-DatasetName
First O
, O
the O
scratch O
baselines O
keep O

failing O
to O
learn O
a O
valuable O
policy O
, O
with O
performance B-MetricName
scores I-MetricName
and O
n-grams B-MetricName
metrics O
close O
to O
zero. O
Although O
TrufLL B-MethodName
does O
not O
outperform O
the O
performance O
of O
the O
pretrained O
baselines O
anymore O
, O
it O
still O
leads O
to O
similar O
performances O
, O
and O
satisfactory O
language B-MetricName
scores. I-MetricName
The O
similarity O
between O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName

Ext-LM I-MethodName
) I-MethodName
results O
suggests O
that O
the O
truncation O
approach O
is O
viable O
when O
using O
a O
generic O
LM O
whose O
original O
vocabulary O
distribution O
differs O
from O
the O
task. O
Interestingly O
, O
TrufLL B-MethodName
displays O
a O
self-BLEU B-MetricName
score O
similar O
to O
the O
pretrained O
baselines. O
This O
suggests O
that O
the O
poor O
diversity O
behavior O
observed O
on O
CLEVR B-DatasetName
is O
likely O
attributable O
to O
the O

small O
vocabulary O
and O
synthetic O
language O
distribution O
. O
Qualitative O
performance O
: O
In O
Figure O
2 O
and O
Appendix O
D O
, O
we O
display O
question O
samples O
for O
all O
models O
. O
TrufLL B-MethodName
and O
the O
pretrained O
baselines O
successfully O
generate O
a O
question O
giving O
the O
expected O
answer O
( O
" O
Black O
" O
) O
, O
while O
the O
RL B-MethodName
from O
scratch O
baselines O

fail O
, O
and O
even O
showcase O
degenerated O
language. O
Pretrained O
baselines O
tend O
to O
output O
a O
question O
closer O
to O
the O
reference O
question O
whereas O
TrufLL B-MethodName
outputs O
original O
questions O
which O
differs O
from O
the O
VQA B-MethodName
distribution O
, O
yet O
consistent O
with O
the O
context O
. O
Human O
Evaluation O
: O
Figure O
3 O
details O
the O
Human O
Evaluation O
results. O
Among O
the O
RL B-MethodName

from O
scratch O
baselines O
, O
we O
selected O
scratch+KL-task O
as O
the O
only O
model O
producing O
sometimes O
meaningful O
questions. O
Yet O
, O
it O
fails O
to O
generate O
correct O
and O
grounded O
language O
; O
it O
is O
thus O
not O
a O
viable O
approach O
despite O
its O
diverse O
output. O
In O
line O
with O
the O
automatic O
metrics O
, O
the O
supervised O
baselines O
produce O
the O
best O

language O
, O
while O
being O
accurately O
grounded. O
Yet O
, O
they O
exhibit O
significantly O
less O
diversity O
with O
the O
reference O
language O
; O
this O
suggests O
in O
particular O
that O
pretrain+RL O
fails O
to O
go O
beyond O
the O
initial O
task-data O
distribution. O
Finally O
, O
unlike O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
which O
suffers O
from O
syntactic O
errors O
, O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
produces O

language O
that O
qualitatively O
competes O
with O
pretrain O
models O
( O
53 B-MetricValue
% I-MetricValue
) O
, O
with O
a O
similar O
ratio O
of O
syntactic O
uncorrect O
samples. O
Although O
its O
questions O
are O
less O
grounded O
, O
they O
are O
diverse O
, O
which O
suggests O
that O
they O
follow O
a O
different O
distribution O
from O
the O
initial O
VQA B-MethodName
dataset. O
It O
confirms O
that O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName

) I-MethodName
could O
be O
an O
alternative O
approach O
as O
it O
has O
an O
excellent O
trade-off O
between O
language O
quality O
, O
diversity O
, O
and O
grounding O
. O
Decoding O
procedure O
: O
In O
Table O
4 O
, O
we O
evaluate O
the O
text O
sampling O
procedures O
described O
in O
Section O
4.5. O
While O
greedy O
decoding O
produces O
the O
best O
outcome O
for O
pretrained O
models O
, O
lm-ranking O

provides O
an O
excellent O
trade-off O
between O
task O
performance O
and O
language O
quality O
with O
RL-based B-MethodName
methods. O
As O
PG O
solely O
optimizes O
the O
task B-MetricName
success I-MetricName
ratio I-MetricName
, O
this O
may O
reduce O
overall O
language O
quality O
, O
the O
re-ranking O
thus O
retrieves O
the O
best O
syntactically O
sentences O
a O
posteriori O
. O
Discussion O
Removing O
the O
truncation O
at O
evaluation O
with O
offpolicy O
RL. B-MethodName
So O

far O
, O
TrufLL B-MethodName
directly O
learns O
the O
truncated O
policy O
over O
the O
truncated O
vocabulary O
V O
− O
each O
cell O
displays O
the O
proportion O
of O
questions O
chosen O
for O
the O
models O
in O
the O
row O
( O
bold O
) O
when O
compared O
to O
the O
concurrent O
model O
in O
the O
column. O
The O
table O
at O
the O
bottom O
displays O
the O
proportion O
of O
incorrect O

questions O
coming O
from O
each O
model O
among O
all O
incorrect O
samples. O
In O
all O
figures O
, O
bracket O
numbers O
indicates O
the O
model O
rank O
per O
criteria O
, O
from O
1= O
" O
best O
" O
to O
5= O
" O
worst O
" O
. O
2012 O
) O
. O
Formally O
, O
the O
off-policy O
PPO B-MetricName
loss I-MetricName
is O
defined O
by O
: O
L B-MetricName
off I-MetricName
ppo I-MetricName
( I-MetricName

θ I-MetricName
) I-MetricName
=E O
π O
− O
θ O
min O
( O
ρ O
θ O
t O
A O
t O
, O
clip O
( O
1−ϵ O
, O
ρ O
θ O
t O
,1+ϵ O
) O
A O
t O
) O
, O
whereρ O
θ O
t O
= O
π O
θ O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
θ O
old O
( O
at|st O
) O
π O
− O

θ O
old O
( O
at|st O
) O
is O
the O
new O
ratio. O
4 O
Table O
5 O
displays O
the O
on-policy O
and O
off-policy O
results O
on O
both O
VQG B-MethodName
tasks O
for O
TrufLL B-MethodName
( O
task-LM O
) O
, O
and O
is O
further O
detailed O
in O
Appendix O
B.3. O
We O
also O
monitor O
the O
probability B-MetricName
mass I-MetricName
of O
the O
policy O
attributed O
to O
the O
truncated O
action O
space O

( O
sumVA B-MetricName
) O
. O
The O
policy O
only O
samples O
words O
within O
the O
truncated O
action O
space O
when O
sumVA B-MetricName
= O
1 O
, O
without O
needing O
the O
truncation. O
On O
CLEVR B-DatasetName
, O
the O
TrufLL B-MethodName
off O
has O
lower O
-yet O
close O
-performance O
on O
language O
and O
task O
scores O
than O
TrufLL. B-MethodName
As O
its O
sumVA B-MetricName
ratios O
are O
very O
close O
to O
1 O

, O
the O
agent O
has O
learned O
to O
generalize O
over O
the O
full O
vocabulary. O
However O
, O
the O
approach O
does O
not O
manage O
to O
sufficiently O
scale O
to O
VQAv2. B-DatasetName
It O
could O
be O
improved O
with O
regularisation O
techniques O
and O
the O
use O
of O
TruFLL B-MethodName
within O
state-of-the-art O
off-policy O
RL B-MethodName
algorithms. O
We O
leave O
such O
possibilities O
to O
future O
works. O
Additional O
experiments. O
We O

sweep O
over O
truncation O
hyper-parameters O
in O
Table O
6 O
of O
Appendix O
B. O
In O
Table O
8 O
, O
we O
observe O
that O
rewarding O
an O
agent O
with O
a O
BLEU B-MetricName
score O
is O
sub-optimal O
in O
both O
language O
and O
task O
scores O
on O
CLEVR. B-DatasetName
In O
VQA B-MethodName
, O
we O
apply O
temperature O
scheduling O
on O
the O
LM O
to O
perform O
fine-grained O
truncations O
in O
Table O

9 O
of O
B.2. O
Finally O
, O
we O
explore O
TrufLL B-MethodName
with O
a O
pre-training O
phase O
in O
Table O
10 O
. O
Generalization O
of O
the O
approach. O
TrufLL B-MethodName
learns O
conditional O
language O
models O
able O
to O
solve O
specific O
Natural B-MethodName
Language I-MethodName
Generation I-MethodName
tasks O
given O
a O
context O
c. O
For O
solving O
such O
tasks O
, O
it O
only O
requires O
the O
context O
, O
a O
reward O

function O
that O
scores O
the O
language O
generated O
by O
the O
RL B-MethodName
agent O
with O
respect O
to O
the O
task O
, O
and O
eventually O
a O
few O
natural O
language O
demonstrations O
fed O
as O
input O
prompt O
to O
the O
generic O
language O
model O
used O
in O
the O
truncation O
algorithm. O
Hence O
, O
the O
method O
is O
transferable O
to O
a O
wide O
variety O
of O
NLG B-MethodName
tasks O

, O
without O
requiring O
upfront O
large-scale O
labelled O
datasets. O
Additionally O
, O
the O
RL B-MethodName
framework O
allows O
to O
optimize O
non-differentiable O
objectives O
, O
making O
TrufLL B-MethodName
a O
natural O
choice O
to O
learn O
end-to-end O
task-oriented O
dialogs O
, O
such O
as O
Das O
et O
al. O
, O
2017 O
) O
. O
Other O
interesting O
tasks O
for O
TrufLL B-MethodName
include O
the O
ones O
typically O
found O
in O
Vision O

and O
Language O
Representation O
Learning O
( O
Lu O
et O
al. O
, O
2020a O
) O
, O
such O
as O
Image B-TaskName
Captioning I-TaskName
, O
Grounding B-TaskName
Referring I-TaskName
Expressions I-TaskName
( O
generation O
of O
a O
referring O
expression O
over O
a O
specific O
bounding O
box O
of O
an O
image O
) O
, O
Captionbased B-TaskName
Image I-TaskName
Retrieval I-TaskName
( O
generation O
of O
a O
caption O
that O
discriminates O
an O
image O
between O
a O

set O
of O
images O
) O
. O
Reward O
functions O
for O
such O
tasks O
can O
be O
based O
on O
similarity O
scores O
between O
the O
generated O
language O
and O
the O
associated O
image O
or O
image O
region O
, O
which O
can O
be O
computed O
using O
pretrained O
language O
representations O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
or O
multi-modal O
pretrained O
systems O

such O
as O
ViLBERT B-MethodName
( O
Lu O
et O
al. O
, O
2019 O
) O
. O
The O
context O
can O
be O
any O
kind O
of O
data O
structure O
( O
natural O
language O
, O
database O
, O
video O
, O
etc O
) O
: O
if O
it O
is O
a O
linguistic O
input O
, O
TrufLL B-MethodName
can O
be O
applied O
for O
instance O
to O
text O
summarization O
, O
paraphrase O
generation O

( O
with O
reward O
functions O
based O
on O
similarity O
scores O
between O
the O
context O
and O
the O
generated O
language O
) O
or O
text-based O
games O
( O
Ammanabrolu O
and O
Riedl O
, O
2018 O
) O
. O
6 O
Related O
work O
RL B-MethodName
and O
NLP B-MethodName
Tasks. O
Following O
( O
Singh O
et O
al. O
, O
2002 O
; O
Lemon O
and O
Pietquin O
, O
2007 O
) O
, O
recent O

RL-based O
taskoriented O
dialogues O
Das O
et O
al. O
, O
2017 O
; O
Lewis O
et O
al. O
, O
2017 O
; O
Narasimhan O
et O
al. O
, O
2015 O
) O
have O
been O
developed O
, O
where O
the O
policy O
language O
model O
is O
generally O
pretrained O
with O
SL B-MethodName
followed O
RL B-MethodName
( O
Yao O
et O
al. O
, O
2020 O
) O
combines O
a O
pretrained O
language O
model O
to O

prune O
the O
action O
space O
with O
a O
Deep-Q B-MethodName
network O
, O
aka O
DRNN O
) O
. O
Yet O
, O
its O
truncation O
language O
model O
remains O
fine-tuned O
on O
the O
RL B-MethodName
dataset. O
Besides O
, O
CALM B-MethodName
is O
only O
evaluated O
on O
a O
vocabulary O
of O
697 O
tokens O
, O
and O
on O
4-words O
action O
sequences O
. O
Learning O
Language O
Models O
from O
scratch. O
( O

Ziegler O
et O
al. O
, O
2019 O
; O
Garg O
et O
al. O
, O
2021 O
) O
finetune O
pretrained O
GPT-2 B-MethodName
models O
with O
RL B-MethodName
for O
language O
generation O
tasks O
without O
task-related O
data O
, O
only O
using O
reward O
signals. O
Yet O
, O
they O
still O
face O
optimization O
and O
computational O
challenges O
( O
Parisotto O
et O
al. O
, O
2020 O
) O
. O
Conclusion O
We O
proposed O

TrufLL B-MethodName
, O
an O
original O
approach O
to O
learn O
a O
natural B-MethodName
language I-MethodName
generation I-MethodName
( O
NLG B-MethodName
) O
task O
using O
RL B-MethodName
, O
without O
the O
usual O
pre-training O
phase O
requiring O
supervised O
datasets. O
To O
our O
knowledge O
, O
this O
is O
the O
first O
RL-based B-MethodName
algorithm O
dedicated O
to O
learning O
a O
word-based O
text-generation O
task O
, O
which O
does O
not O
rely O
on O
a O

pre-training O
phase O
while O
scaling O
to O
large O
vocabularies. O
Although O
it O
comes O
with O
its O
limitations O
, O
the O
truncated O
RL B-MethodName
algorithm O
provided O
by O
TrufLL B-MethodName
gets O
free O
from O
labelled O
data O
in O
task-oriented O
language O
models O
, O
presents O
interesting O
language O
generation O
properties O
, O
and O
provides O
a O
generic O
and O
transferable O
method O
to O
learn O
any O
NLG B-MethodName
problem O
. O

A O
Dataset O
and O
training O
details O
A.1 O
Evaluation O
Metrics O
For O
the O
BLEU B-MetricName
and O
METEOR B-MetricName
scores O
, O
we O
used O
the O
NLTK B-MethodName
5 I-MethodName
implementations O
with O
the O
smoothing O
function O
number O
2 O
for O
the O
BLEU B-MetricName
score. O
For O
the O
CIDEr B-MetricName
score O
, O
we O
used O
the O
nlg-eval B-MethodName
implementation O
6 O
. O
A.2 O
Answer O
filtering O
For O
each O
dataset O
, O

we O
remove O
yes O
and O
no O
question-answer O
pairs O
which O
frequency O
largely O
exceeds O
other O
answers O
, O
to O
avoid O
any O
bias O
in O
the O
question O
generation O
process O
, O
as O
usually O
done O
in O
the O
VQG B-MethodName
litterature O
( O
Mostafazadeh O
et O
al. O
, O
2016 O
) O
. O
A.3 O
Dataset O
split O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
the O

RL B-MethodName
language O
agent O
is O
trained O
for O
50k O
( O
resp. O
100k O
) O
episodes O
over O
the O
first O
20k O
images O
( O
resp. O
all O
the O
images O
) O
of O
the O
training O
dataset O
, O
and O
is O
then O
evaluated O
on O
the O
first O
5k O
( O
resp. O
20k O
) O
images O
of O
the O
validation O
set. O
Besides O
, O
we O
uniformly O
sample O

the O
answer O
in O
the O
set O
of O
reference O
answers O
for O
each O
image O
to O
reduce O
the O
bias O
in O
the O
distribution O
of O
answers. O
Finally O
, O
questions O
are O
limited O
to O
20 O
( O
resp. O
10 O
) O
words O
. O
A.4 O
Language O
Agent O
Networks O
and O
Training O
For O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
we O
used O
a O
single-layer O

LSTM B-MethodName
with O
64 B-HyperparameterValue
( O
resp. O
256 O
) O
units O
for O
the O
policy B-HyperparameterName
network. I-HyperparameterName
At O
every O
time O
step O
, O
the O
LSTM B-MethodName
input O
is O
then O
the O
concatenation O
of O
the O
word B-HyperparameterName
embedding I-HyperparameterName
of I-HyperparameterName
dimension I-HyperparameterName
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
the O
answer B-HyperparameterName
embedding I-HyperparameterName
of O
dimension O
32 B-HyperparameterValue
( O
resp. O
128 O
) O
, O
and O
the O
image O

representation. O
For O
CLEVR B-DatasetName
, O
the O
image O
representation O
is O
extracted O
from O
a O
pretrained O
ResNet50 B-MethodName
and O
projected O
into O
a O
tensor B-HyperparameterName
of O
size O
( O
32,7,7 B-HyperparameterValue
) O
before O
being O
flattened. O
For O
VQAv2 B-DatasetName
, O
the O
image O
representation O
is O
the O
average O
of O
200 O
bounding O
box O
features O
of O
dimension O
1048 B-HyperparameterValue
, O
extracted O
from O
a O
faster O
R- B-MethodName
CNN I-MethodName

( O
Ren O
et O
al. O
, O
2015 O
) O
. O
We O
optimize O
the O
full O
loss B-MetricName
L=L O
P O
P O
O O
+αL O
V O
F O
+βL O
E O
with O
α=0.5 B-HyperparameterName
, O
β B-HyperparameterName
=0.01 B-HyperparameterValue
and O
a O
PPO B-HyperparameterName
clipping I-HyperparameterName
ratio I-HyperparameterName
ϵ=0.02 I-HyperparameterName
( O
resp. O
0.01 O
) O
for O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
. O
We O
use O
Adam O
optimizer O
( O
Kingma O

and O
Ba O
, O
2014 O
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
) O
of O
10 B-MetricName
−3 I-MetricName
for O
TrufLL B-MethodName
and O
the O
scratch O
baseline O
, O
10 B-HyperparameterValue
−5 I-HyperparameterValue
( O
resp. O
10 O
−6 O
) O
for O
RL B-MethodName
algorithms O
with O
a O
pre-training O
phase O
on O
CLEVR B-DatasetName
( O
resp. O
VQAv2 B-DatasetName
) O
, O
and O
5 B-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
for O
models O

including O
a O
KL B-MethodName
regularization I-MethodName
term. O
We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
( O
bs O
) O
of O
128 B-HyperparameterValue
for O
all O
models O
except O
the O
ones O
with O
KL O
regularization O
, O
for O
which O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64. B-HyperparameterValue
Finally O
, O
for O
the O
RL B-MethodName
from O
scratch O
baselines O
, O
we O
perform O
gradient B-HyperparameterName
clipping I-HyperparameterName
( O
gladclip B-HyperparameterName
) O

of O
1 B-HyperparameterValue
( O
resp. O
5 O
) O
for O
CLEVR B-DatasetName
and O
VQAv2 B-DatasetName
. O
Such O
hyper-parameters O
were O
selected O
, O
after O
conducting O
an O
extensive O
hyper-parameter O
search. O
The O
following O
values O
were O
tested O
: O
β B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue
} O
, O
ϵ B-HyperparameterName
∈ O
{ O
0.01 B-HyperparameterValue
, I-HyperparameterValue
0.02 I-HyperparameterValue
, I-HyperparameterValue
0.05 I-HyperparameterValue
, I-HyperparameterValue
0.1 I-HyperparameterValue

, I-HyperparameterValue
0.5 I-HyperparameterValue
, I-HyperparameterValue
0.9 I-HyperparameterValue
} O
, O
lr B-HyperparameterName
∈ O
{ O
10 B-HyperparameterValue
−6 I-HyperparameterValue
,10 I-HyperparameterValue
−5 I-HyperparameterValue
,10 I-HyperparameterValue
−4 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−4 I-HyperparameterValue
,10 I-HyperparameterValue
−3 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−3 I-HyperparameterValue
,10 I-HyperparameterValue
−2 I-HyperparameterValue
,5 I-HyperparameterValue
* I-HyperparameterValue
10 I-HyperparameterValue
−2 I-HyperparameterValue
} O
, O
gradclip B-HyperparameterName
∈ O
{ O
None,1,5,10,100 B-HyperparameterValue
} O
, O
bs B-HyperparameterName
∈ O
{ O
32,64,128 B-HyperparameterValue
} O
. O
Additionally O
, O
we O
also O
tested O

for O
VQAv2 B-DatasetName
policy B-HyperparameterName
networks I-HyperparameterName
with O
64 B-HyperparameterValue
, O
256 B-HyperparameterValue
and O
1024 B-HyperparameterValue
units O
, O
with O
respectively O
32 B-HyperparameterValue
, O
128 B-HyperparameterValue
and O
512 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimensions. I-HyperparameterName
We O
kept O
the O
network B-HyperparameterName
size I-HyperparameterName
giving O
the O
best O
performances O
, O
i.e. O
policy B-HyperparameterName
network I-HyperparameterName
of O
256 B-HyperparameterValue
units O
and O
128 B-HyperparameterValue
word O
embedding B-HyperparameterName
dimension I-HyperparameterName
. O
A.5 O
Reward O
formula O
for O
VQAv2 B-DatasetName
In O

this O
section O
, O
we O
detail O
the O
reward O
function O
used O
for O
the O
VQAv2 B-DatasetName
task. O
r O
( O
w O
t O
, O
w O
< O
t O
, O
c O
) O
=1 O
rk O
( O
A O
) O
≤10 O
, O
t=T O
−1 O
e O
−rk O
( O
A O
) O
/ O
2 O
, O
with O
rk O
( O
A O
) O
the O
rank O
of O
the O

ground-truth O
answer O
given O
by O
the O
VQA B-MethodName
model O
, O
when O
predicting O
the O
actual O
answer O
from O
the O
terminal O
state O
( O
c O
, O
w O
< O
T O
) O
. O
Formally O
, O
it O
is O
defined O
as O
: O
rk O
( O
A O
) O
=rank O
( O
VQA O
( O
c O
, O
w O
< O
T O
) O
[ O
A O
] O
) O

, O
with O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
the O
probability O
distribution O
given O
by O
the O
VQA B-MethodName
model O
over O
the O
set O
of O
answers O
, O
and O
rank O
the O
function O
which O
ranks O
the O
probability O
of O
answer O
A O
within O
VQA B-MethodName
( O
c O
, O
w O
< O
T O
) O
probability O
distribution O
. O
B O
Additional O
experiments O

B.1 O
CLEVR B-DatasetName
Table O
6 O
displays O
the O
complete O
ablation O
on O
the O
truncation O
functions O
with O
parameters O
sweep. O
The O
'sizeVA O
' O
variable O
indicates O
the O
average O
size O
of O
the O
truncated O
action O
space O
for O
each O
truncation O
function. O
Table O
7 O
displays O
the O
ablation O
over O
the O
three O
decoding O
procedures O
defined O
in O
Section O
4.5. O
Such O
an O
ablation O
presents O

a O
similar O
pattern O
than O
VQAv2 B-DatasetName
results O
described O
in O
section O
5.2 O
. O
Finally O
, O
Table O
8 O
reports O
CLEVR B-DatasetName
metrics O
when O
using O
the O
BLEU B-MetricName
score O
as O
the O
reward. O
While O
on O
such O
a O
task O
TrufLL B-MethodName
still O
exhibits O
promising O
language B-MetricName
scores I-MetricName
, O
the O
n-grams B-MetricName
metrics O
remain O
lower O
than O
the O
pretrained O
baselines. O
This O
illustrates O
that O

using O
a O
language O
similarity B-MetricName
score I-MetricName
as O
a O
reward O
signal O
is O
much O
less O
interesting O
than O
a O
reward O
based O
on O
a O
task O
completion O
score O
. O
B.2 O
VQAv2 B-DatasetName
Temperature O
scheduling O
: O
On O
the O
CLEVR B-DatasetName
task O
, O
we O
observed O
that O
dynamic O
truncations O
outperform O
static O
ones O
such O
as O
top B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
: O
indeed O
, O

they O
better O
take O
into O
account O
the O
inherent O
variability O
of O
the O
language O
structure O
at O
the O
sentence-level. O
When O
scaling O
up O
to O
the O
15k O
words O
of O
the O
VQAv2 B-DatasetName
task O
, O
we O
also O
dynamically O
decrease O
the O
truncation O
size O
through O
training O
, O
by O
applying O
a O
decreasing O
temperature O
schedule O
on O
the O
language O
model. O
While O
temperature O
scaling O

( O
Bahdanau O
et O
al. O
, O
2015 O
) O
is O
usually O
used O
at O
test O
time O
to O
control O
the O
smoothness O
of O
the O
language O
model O
distribution O
, O
temperature O
schedules O
during O
training O
of O
language O
models O
have O
been O
used O
in O
several O
settings O
( O
Jang O
et O
al. O
, O
2016 O
; O
Zhang O
et O
al. O
, O
2018 O
; O
Wang O

et O
al. O
, O
2020 O
) O
. O
Formally O
, O
f O
LM O
( O
w O
i O
|w O
< O
t O
) O
distribution O
is O
computed O
as O
softmax B-MetricName
( I-MetricName
x I-MetricName
i I-MetricName
) I-MetricName
=e O
−x O
i O
/ O
τ O
/ O
j O
e O
−x O
j O
/ O
τ O
, O
with O
x O
j O
the O
LM O
logits O
and O
τ O
the O
temperature O
, O

which O
decreases O
from O
τ O
max O
to O
τ O
min O
by O
a O
factor O
T O
F O
every O
T O
u O
training O
step. O
In O
Table O
9 O
, O
both O
TrufLL B-MethodName
( I-MethodName
Task-LM I-MethodName
) I-MethodName
and O
TrufLL B-MethodName
( I-MethodName
Ext-LM I-MethodName
) I-MethodName
benefit O
slightly O
from O
truncation O
with O
a O
temperature O
schedule O
compared O
to O
a O
vanilla O
truncation. O
The O
former O
displays O
the O
best O

performance B-MetricName
/ I-MetricName
language I-MetricName
scores I-MetricName
trade-off O
for O
the O
schedule O
" O
τ O
: O
3 O
> O
1. O
& O
T O
u O
=5,000 O
" O
, O
while O
the O
latter O
has O
the O
best O
metrics O
trade-off O
for O
" O
τ O
: O
1.5 O
> O
1. O
& O
T O
u O
=5,000 O
" O
. O
Finally O
, O
Figure O
4 O
displays O
the O
evolution O
of O
the O

training O
return O
for O
TrufLL B-MethodName
and O
the O
baselines. O
As O
expected O
, O
the O
pretrain+RL O
fine-tune O
baseline O
return O
does O
not O
evolve O
much O
, O
confirming O
that O
the O
policy O
distribution O
almost O
does O
not O
shift O
through O
the O
fine-tuning O
phase. O
The O
training O
curves O
of O
TrufLL B-MethodName
present O
a O
steady O
increase O
in O
the O
return O
until O
reaching O
convergence O
, O
confirming O

that O
our O
approach O
, O
by O
guiding O
the O
exploration O
of O
the O
action O
space O
, O
provides O
a O
sufficient O
learning O
signal. O
On O
the O
other O
hand O
, O
the O
scratch+KL O
baselines O
stay O
stuck O
to O
a O
low O
training O
return. O
This O
suggests O
that O
the O
KL B-MethodName
regularization I-MethodName
term O
, O
while O
encouraging O
the O
policy O
distribution O
to O
resemble O
the O
language O

model O
distribution O
, O
fails O
to O
capture O
the O
task O
pragmatics O
, O
which O
requires O
generating O
a O
language O
that O
is O
visually O
grounded O
. O

-DOCSTART- O
Language B-MethodName
Model I-MethodName
Augmented I-MethodName
Monotonic I-MethodName
Attention I-MethodName
for I-MethodName
Simultaneous I-MethodName
Translation I-MethodName
The B-MethodName
state-of-the-art I-MethodName
adaptive I-MethodName
policies I-MethodName
for I-MethodName
simultaneous I-MethodName
neural I-MethodName
machine I-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
use O
monotonic O
attention O
to O
perform O
read O
/ O
write O
decisions O
based O
on O
the O
partial O
source O
and O
target O
sequences. O
The O
lack O
of O
sufficient O
information O
might O
cause O
the O
monotonic O
attention O
to O
take O

poor O
read O
/ O
write O
decisions O
, O
which O
in O
turn O
negatively O
affects O
the O
performance O
of O
the O
SNMT B-MethodName
model. O
On O
the O
other O
hand O
, O
human O
translators O
make O
better O
read O
/ O
write O
decisions O
since O
they O
can O
anticipate O
the O
immediate O
future O
words O
using O
linguistic O
information O
and O
domain O
knowledge. O
In O
this O
work O
, O
we O
propose O

a O
framework O
to O
aid O
monotonic B-MethodName
attention I-MethodName
with O
an O
external O
language O
model O
to O
improve O
its O
decisions. O
Experiments O
on O
MuST-C B-DatasetName
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
show O
the O
future O
information O
from O
language O
model O
improves O
the O
state-of-the-art O
monotonic O
multi-head O
attention O
model O
further O
. O
Introduction O
A O
typical O
application O
of O
simultaneous O
neural B-TaskName
machine I-TaskName
translation I-TaskName
( I-TaskName

SNMT I-TaskName
) I-TaskName
is O
conversational O
speech O
or O
live O
video O
caption O
translation. O
In O
order O
to O
achieve O
live O
translation O
, O
an O
SNMT B-TaskName
model O
alternates O
between O
performing O
read O
from O
source O
sequence O
and O
write O
to O
target O
sequence. O
For O
a O
model O
to O
decide O
whether O
to O
read O
or O
write O
at O
certain O
moment O
, O
either O
a O
fixed O
or O

an O
adaptive O
read O
/ O
write O
policy O
can O
be O
used O
. O
Earlier O
approaches O
in O
simultaneous O
translation O
such O
as O
Ma O
et O
al. O
( O
2019a O
) O
and O
Dalvi O
et O
al. O
( O
2018 O
) O
employ O
a O
fixed O
policy O
that O
alternate O
between O
read O
and O
write O
after O
the O
waiting O
period O
of O
k O
tokens. O
To O
alleviate O
possible O

long O
delay O
of O
fixed O
polices O
, O
recent O
works O
such O
as O
monotonic B-MethodName
infinite I-MethodName
lookback I-MethodName
attention I-MethodName
( I-MethodName
MILk I-MethodName
) I-MethodName
( O
Arivazhagan O
et O
al. O
, O
2019 O
) O
, O
and O
monotonic B-MethodName
multihead I-MethodName
attention I-MethodName
( I-MethodName
MMA I-MethodName
) I-MethodName
( O
Ma O
et O
al. O
, O
2019c O
) O
developed O
flexible O
policies O
using O
monotonic B-MethodName
attention I-MethodName
( O
Raffel O
et O
al. O
, O

2017 O
) O
. O
* O
⋆ O
Work O
done O
while O
at O
Samsung O
Research O
† O
Equal O
contribution O
Figure O
1 O
: O
The O
finetuned O
XLM-RoBERTa B-MethodName
language O
model O
predicts O
German O
words O
using O
the O
prefix O
as O
input. O
( O
Green O
: O
Correct O
, O
Red O
: O
Incorrect O
, O
Black O
: O
Neutral O
) O
. O
While O
these O
monotonic B-MethodName
attention I-MethodName
anticipates O
target O

words O
using O
only O
available O
prefix O
source O
and O
target O
sequence O
, O
human O
translators O
anticipate O
the O
target O
words O
using O
their O
language O
expertise O
( O
linguistic O
anticipation O
) O
as O
well O
as O
contextual O
information O
( O
extra-linguistic O
anticipation O
) O
( O
Vandepitte O
, O
2001 O
) O
. O
Inspired O
by O
human B-MethodName
translation I-MethodName
experts O
, O
we O
aim O
to O
augment O
monotonic B-MethodName

attention I-MethodName
with O
future O
information O
using O
language O
models O
( O
LM O
) O
( O
Devlin O
et O
al. O
, O
2019 O
; O
Conneau O
et O
al. O
, O
2019 O
) O
. O
Integrating O
the O
external O
information O
effectively O
into O
text-to-text B-TaskName
machine I-TaskName
translation I-TaskName
( B-MethodName
MT I-MethodName
) I-MethodName
systems I-MethodName
has O
been O
explored O
by O
several O
works O
( O
Khandelwal O
et O
al. O
, O
2020 O
; O

Gulcehre O
et O
al. O
, O
2015Gulcehre O
et O
al. O
, O
, O
2017Stahlberg O
et O
al. O
, O
2018 O
) O
. O
Also O
, O
integrating O
future O
information O
implicitly O
into O
SNMT B-MethodName
system O
during O
training O
is O
explored O
in O
Wu O
et O
al. O
( O
2020 O
) O
by O
simultaneously O
training O
different O
wait-k O
SNMT B-MethodName
systems. O
However O
, O
no O
previous O
works O
make O
use O

of O
explicit O
future O
information O
both O
during O
training O
and O
inference. O
To O
utilize O
explicit O
future O
information O
, O
we O
explored O
to O
integrate O
future O
information O
from O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
However O
, O
it O
did O
not O
provide O
any O
improvements O
( O
refer O
to O
Appendix O
A O
) O
, O
thus O
motivating O
us O

to O
explore O
a O
tighter O
integration O
of O
the O
LM B-MethodName
information O
into O
SNMT B-MethodName
model O
. O
In O
this O
work O
, O
we O
explicitly O
use O
plausible O
future O
information O
from O
LM O
during O
training O
by O
transforming O
the O
monotonic O
attention O
mechanism. O
As O
shown O
in O
Figure O
1 O
, O
at O
each O
step O
, O
the O
LM O
takes O
the O
prefix O
target O
( O

and O
source O
, O
for O
cross-lingual O
LM O
) O
sequence O
and O
predicts O
the O
probable O
future O
information. O
We O
hypothesize O
that O
aiding O
the O
monotonic B-TaskName
attention I-TaskName
with O
this O
future O
information O
can O
improve O
MMA B-MethodName
model O
's O
read O
/ O
write O
policy O
, O
eventually O
leading O
to O
better O
translation O
with O
less O
delay. O
Several O
experiments O
on O
MuST-C O
( O
Di O
Gangi O

et O
al. O
, O
2019 O
) O
English-German O
and O
English-French O
speech-to-text B-TaskName
translation I-TaskName
tasks O
with O
our O
proposed O
approach O
show O
clear O
improvements O
of O
latency-quality B-MetricName
trade-offs O
over O
the O
state-of-the-art O
MMA B-MethodName
models O
. O
2 O
Monotonic B-MethodName
Attention I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
Model I-MethodName
Monotonic B-MethodName
Attention I-MethodName
In O
simultaneous O
machine B-MethodName
translation I-MethodName
( I-MethodName
SNMT I-MethodName
) I-MethodName
models I-MethodName
, O
the O
probability O
of O
predicting O
the O

target O
token O
y O
i O
∈ O
y O
depends O
on O
the O
partial O
source O
and O
target O
sequences O
( O
x O
≤j O
∈ O
x O
, O
y O
< O
i O
∈ O
y O
) O
. O
In O
sequence-tosequence B-MethodName
based I-MethodName
SNMT I-MethodName
model I-MethodName
, O
each O
target O
token O
y O
i O
is O
generated O
as O
follows O
: O
h O
j O
= O
E O
( O
x O
≤j O

) O
( O
1 O
) O
s O
i O
= O
D O
( O
y O
< O
i O
, O
c O
i O
= O
A O
( O
s O
i−1 O
, O
h O
≤j O
) O
) O
( O
2 O
) O
y O
i O
= O
Output O
( O
s O
i O
) O
( O
3 O
) O
where O
E O
( O
. O
) O
and O
D O
( O
. O
) O
are O

the O
encoder O
and O
decoder O
layers O
, O
and O
c O
i O
is O
a O
context O
vector. O
In O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
SNMT I-MethodName
, O
the O
context O
vector O
is O
computed O
as O
follows O
: O
e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i−1 O
, O
h O
j O
) O
( O
4 O
) O
p O
i O
, O
j O
= O
Sigmoid O

( O
e O
i O
, O
j O
) O
( O
5 O
) O
z O
i O
, O
j O
∼ O
Bernoulli O
( O
p O
i O
, O
j O
) O
( O
6 O
) O
When O
generating O
a O
target O
token O
y O
i O
, O
the O
decoder O
chooses O
whether O
to O
read O
/ O
write O
based O
on O
Bernoulli O
selection O
probability O
p O
i O
, O
j O
. O

When O
z O
i O
, O
j O
= O
1 O
( O
write O
) O
, O
model O
sets O
t O
i O
= O
j O
, O
c O
i O
= O
h O
j O
and O
generates O
the O
target O
token O
y O
i O
. O
For O
z O
i O
, O
j O
= O
0 O
( O
read O
) O
, O
it O
sets O
t O
i O
= O
j O
+ O
1 O

and O
repeats O
Eq. O
4 O
to O
6. O
Here O
t O
i O
refers O
to O
the O
index O
of O
the O
encoder O
when O
decoder O
needs O
to O
produce O
the O
i O
th O
target O
token. O
Instead O
of O
hard O
alignment O
of O
c O
i O
= O
h O
j O
, O
Raffel O
et O
al. O
( O
2017 O
) O
compute O
an O
expected O
alignment O
in O
a O
recurrent O

manner O
and O
propose O
a O
closed-form O
parallel O
solution. O
Arivazhagan O
et O
al. O
( O
2019 O
) O
adopt O
monotonic B-MethodName
attention I-MethodName
into O
SNMT B-MethodName
and O
later O
, O
Ma O
et O
al. O
( O
2019c O
) O
extend O
it O
to O
MMA B-MethodName
to O
integrate O
it O
into O
the O
Transformer O
model O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O
Monotonic B-MethodName
Attention I-MethodName
with B-MethodName
Future I-MethodName

Information I-MethodName
The O
monotonic O
attention O
described O
in O
Section O
2.1 O
performs O
anticipation O
based O
only O
on O
the O
currently O
available O
source O
and O
target O
information. O
To O
augment O
this O
anticipation O
process O
using O
future O
information O
extracted O
using O
LMs B-MethodName
, O
we O
propose O
the O
following O
modifications O
to O
the O
monotonic B-MethodName
attention I-MethodName
. O
Future O
Representation O
Layer O
: O
At O
every O
decoding O
step O

i O
, O
the O
previous O
target O
token O
y O
i−1 O
is O
equipped O
with O
a O
plausible O
future O
tokenŷ O
i O
as O
shown O
in O
the O
Figure O
2. O
Since O
the O
tokenŷ O
i O
comes O
from O
an O
LM B-MethodName
possibly O
with O
a O
different O
tokenizer O
and O
vocabulary O
set O
, O
applying O
the O
model O
's O
tokenizer O
and O
vocabulary O
might O
split O
the O
tokenŷ O

i O
further O
into O
multiple O
sub-tokens O
{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
. O
To O
get O
a O
single O
future O
token O
representations O
i O
∈ O
R O
d O
from O
all O
the O
sub-tokens O
, O
we O
apply O
a O
sub-token O
summary O
layer O
: O
s O
i O
= O
Γ O

( O
{ O
ŷ O
1 O
i O
, O
ŷ O
2 O
i O
, O
• O
• O
• O
, O
ŷ O
m O
i O
} O
) O
( O
7 O
) O
The O
Γ B-HyperparameterName
represents O
a O
general O
sequence O
representation O
layer O
such O
as O
a O
Transformer B-HyperparameterName
encoder I-HyperparameterName
layer I-HyperparameterName
or O
a O
simple O
normalized O
sum O
of O
sub-token O
representations O
. O
We O
enrichs O
i O
at O
every O

layer B-HyperparameterName
l B-HyperparameterName
of O
the O
decoder O
block O
by O
applying O
a O
residual O
feed-forward O
network O
. O
s O
l O
i O
= O
F O
F O
N O
( O
ỹ O
l−1 O
i O
) O
( O
8 O
) O
Monotonic B-MethodName
Energy I-MethodName
Layer I-MethodName
with I-MethodName
Future I-MethodName
Information I-MethodName
: O
Despite O
the O
fact O
that O
we O
can O
add O
the O
plausible O
future O
information O
to O
the O
output O
layer O

( O
Appendix O
A O
) O
or O
append O
it O
to O
the O
target O
token O
representation O
y O
i−1 O
, O
the O
MMA B-MethodName
read O
/ O
write O
decisions O
happen O
in O
Eq. O
4. O
Therefore O
, O
we O
integrates O
i O
into O
the O
Eq. O
4 O
instead O
. O
The O
integration O
is O
carried O
out O
by O
modifying O
Eq. O
4 O
-Eq. O
5. O
We O
compute O
the O

monotonic O
energy O
for O
future O
information O
using O
the O
enriched O
future O
token O
representations O
i O
available O
at O
each O
layer O
: O
e O
i O
, O
j O
= O
M O
onotonicEnergy O
( O
s O
i O
, O
h O
j O
) O
( O
9 O
) O
We O
integrate O
the O
future O
monotonic O
energy O
function O
into O
Eq. O
5 O
as O
follows O
: O
p O
i O
, O

j O
= O
Sigmoid O
( O
e O
i O
, O
j O
+ẽ O
i O
, O
j O
) O
( O
10 O
) O
After O
computingp O
i O
, O
j O
, O
we O
compute O
c O
i O
similar O
to O
MMA O
model. O
This O
way O
of O
integration O
of O
future O
information O
allows O
the O
model O
to O
condition O
the O
LM O
output O
usage O
on O
the O
input O
sequence. O

The O
model O
can O
control O
the O
relative O
weightage O
given O
to O
the O
LM O
output O
by O
varying O
theẽ O
i O
, O
j O
. O
In O
case O
of O
insufficient O
source O
information O
in O
the O
low O
latency B-MetricName
regime O
, O
we O
expect O
the O
model O
's O
decision O
policy O
to O
rely O
more O
onẽ O
i O
, O
j O
. O
Inference O
: O
During O
inference O

, O
the O
start O
token O
does O
not O
contain O
any O
plausible O
information. O
After O
predicting O
the O
first O
target O
token O
, O
for O
every O
subsequent O
prediction O
of O
target O
token O
y O
i O
, O
we O
invoke O
the O
LM O
to O
predict O
the O
next O
plausible O
future O
token O
and O
integrate O
this O
new O
information O
into O
Eq. O
10 O
. O
Experiments O
and O
Results O

Experimental O
Settings O
Datasets O
and O
Metrics O
: O
We O
conduct O
our O
experiments O
on O
the O
MuST-C B-DatasetName
English O
( O
En O
) O
-German O
( O
De O
) O
and O
English O
( O
En O
) O
-French O
( O
Fr O
) O
speech-to-text B-TaskName
( I-TaskName
ST I-TaskName
) I-TaskName
translation I-TaskName
task. O
The O
speech O
sequence O
is O
represented O
using O
80-dimensional O
log-mel O
filter O
bank O
features. O
The O
target O
sequence O

is O
represented O
as O
subwords O
using O
a O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
model O
with O
a O
unigram O
vocabulary O
of O
size O
10,000. O
We O
evaluate O
the O
performance O
of O
the O
models O
on O
both O
the O
latency B-MetricName
and O
quality B-MetricName
aspects. O
We O
use O
Average B-MetricName
Lagging I-MetricName
( I-MetricName
AL I-MetricName
) I-MetricName
as O
our O
latency B-MetricName
metric O
and O
case-sensitive O
detokenized O

SacreBLEU B-MetricName
( O
Post O
, O
2018 O
) O
to O
measure O
the O
translation O
quality B-MetricName
, O
similar O
to O
( O
Ma O
et O
al. O
, O
2020 O
) O
. O
The O
best O
models O
are O
chosen O
based O
on O
the O
dev O
set O
results O
and O
reported O
results O
are O
from O
the O
MuST-C B-DatasetName
test O
( O
tst-COMMON O
) O
sets O
. O
Language O
Models O
We O
use O

two O
language O
models O
to O
train O
our O
proposed O
modified O
MMA B-MethodName
model. O
Firstly O
, O
we O
use O
the O
pretrained O
XLM-RoBERTa B-MethodName
( O
Conneau O
et O
al. O
, O
2019 O
) O
model O
from O
Huggingface O
Transformers O
1 O
model O
repository. O
Since O
the O
LM O
output O
can O
be O
very O
open-ended O
and O
might O
not O
directly O
suit O
/ O
cater O
to O
our O
task O
and O

dataset O
, O
we O
finetune O
the O
head O
of O
the O
model O
using O
the O
MuST-C B-DatasetName
target O
text O
data O
for O
each O
task O
. O
We O
also O
train O
a O
smaller B-MethodName
language I-MethodName
model I-MethodName
( O
SLM B-MethodName
) O
, O
which O
contains O
6 B-HyperparameterValue
Transformer B-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
512 B-HyperparameterValue
hidden-states B-HyperparameterName
and O
24M O
parameters. O
We O
use O
the O
MuST-C B-DatasetName
data O
along O
with O
additional O

data O
augmentation O
to O
reduce O
overfitting. O
The O
SLM B-MethodName
helps O
to O
remove O
the O
issues O
related O
to O
vocabulary O
mismatch O
as O
discussed O
in O
the O
Section O
2.2 O
. O
Implementation O
Details O
: O
Our O
base O
model O
is O
adopted O
from O
Ma O
et O
al. O
( O
2020 O
) O
. O
We O
use O
a O
predecision B-MetricName
ratio I-MetricName
of O
7 B-MetricValue
, O
which O
means O
that O

the O
simultaneous O
read O
/ O
write O
decisions O
are O
made O
after O
every O
seven O
encoder O
states. O
We O
use O
λ B-HyperparameterName
or O
λ B-HyperparameterName
latency B-HyperparameterName
to O
refer O
to O
the O
hyperparameter O
corresponding O
to O
the O
weighted O
average O
( O
λ B-HyperparameterName
avg I-HyperparameterName
) O
in O
MMA. B-MethodName
The O
values O
of O
this O
hyperparameter O
λ B-HyperparameterName
are O
chosen O
from O
the O
set O
{ O
0.01 B-HyperparameterValue
, O
0.05 B-HyperparameterValue

, O
0.1 B-HyperparameterValue
} O
. O
The O
Γ O
layer O
in O
Eq. O
7 O
computes O
the O
normalized O
sum O
of O
the O
sub-token O
representations. O
For O
SLM B-MethodName
, O
it O
simply O
finds O
the O
embedding O
since O
it O
shares O
the O
same O
vocabulary O
set. O
All O
the O
models O
are O
trained O
on O
a O
NVIDIA O
v100 O
GPU O
with O
update_f B-MetricName
req I-MetricName
set O
to O
8 B-MetricValue
. O

Simultaneous O
Translation O
Models O
: O
Even O
though O
future O
information O
can O
be O
integrated O
explicitly O
into O
the O
fixed O
policy O
approaches O
such O
as O
Wait-K O
( O
Ma O
et O
al. O
, O
2019b O
) O
, O
we O
choose O
monotonic B-MethodName
attention I-MethodName
as O
our O
baseline O
due O
to O
its O
superior O
performance O
( O
Arivazhagan O
et O
al. O
, O
2019 O
; O
Ma O
et O
al. O

, O
2019c O
) O
. O
We O
train O
a O
baseline O
based O
on O
Ma O
et O
al. O
( O
2020 O
) O
work O
, O
called O
as O
MMA B-MethodName
model. O
The O
MMA B-MethodName
model O
encoder O
and O
decoder O
embedding B-HyperparameterName
dimensions I-HyperparameterName
are O
set O
to O
392 B-HyperparameterValue
, O
whereas O
our O
proposed O
model O
's O
encoder B-HyperparameterName
and I-HyperparameterName
decoder I-HyperparameterName
embeddings I-HyperparameterName
are O
set O
to O
256 B-HyperparameterValue
to O
have O

similar O
parameters O
( O
≈ O
39M O
) O
for O
a O
fair O
comparison. O
We O
train O
two O
models O
using O
the O
Results O
We O
first O
analyze O
how O
the O
LM O
predictions O
are O
being O
utilized O
by O
the O
our O
model. O
In O
order O
to O
measure O
the O
relative O
weight O
given O
to O
model O
's O
internal O
states O
versus O
the O
predictions O
from O
the O
LM O

, O
we O
compare O
the O
norm B-MetricName
of I-MetricName
the I-MetricName
monotonic I-MetricName
energies I-MetricName
corresponding O
to O
the O
LM O
predictions O
e O
pred O
( O
Eq. O
9 O
) O
and O
the O
previous O
output O
tokens O
e O
output O
( O
Eq. O
4 O
) O
. O
Let O
us O
define O
LM O
prediction O
weight O
as O
follows O
: O
LM B-MethodName
pw B-HyperparameterName
= O
∥e O
pred O
∥ O
∥e O
output O
∥ O

( O
11 O
) O
In O
Figure O
3 O
, O
we O
plot O
the O
variation O
of O
LM O
pw B-HyperparameterName
( O
averaged O
) O
vs. O
λ. B-HyperparameterName
We O
use O
two O
additional O
values O
of O
λ B-HyperparameterName
∈ O
{ O
0.005 B-HyperparameterValue
, O
0.001 B-HyperparameterValue
} O
to O
obtain O
this O
plot. O
We O
can O
observe O
that O
as O
the O
latency B-MetricName
requirements O
become O
more O
and O
more O
strict O
, O

the O
model O
starts O
to O
give O
more O
weightage O
to O
the O
predictions O
coming O
from O
the O
LM. O
This O
shows O
that O
the O
model O
learns O
to O
utilize O
the O
information O
coming O
from O
LM O
predictions O
based O
on O
latency B-MetricName
requirements. O
Next O
, O
we O
discuss O
the O
performance O
improvements O
obtained O
from O
our O
proposed O
approach. O
By O
varying O
the O
λ B-HyperparameterName
, O
we O

train O
separate O
models O
for O
different O
latency B-MetricName
regimes. O
Moreover O
, O
the O
quality B-MetricName
and O
latency B-MetricName
for O
a O
particular O
model O
can O
also O
be O
varied O
by O
controlling O
the O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
the O
inference. O
Speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
or O
step B-HyperparameterName
size I-HyperparameterName
refers O
to O
the O
duration O
of O
speech O
( O
in O
ms O
) O
processed O
corresponding O
to O
each O

read O
decision. O
We O
vary O
these O
hyperparameters O
for O
all O
the O
three O
models O
, O
namely O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
. O
The O
BLEU-AL B-MetricName
curves O
for O
all O
the O
models O
have O
been O
provided O
in O
Figure O
4 O
and O
BLEU-AL B-MetricName
numbers O
for O
all O
models O
are O
included O
in O
Appendix O
F O
for O
reference. O
We O
vary O
the O
step B-HyperparameterName
sizes I-HyperparameterName

in O
intervals O
of O
80ms B-HyperparameterValue
from O
120 B-HyperparameterValue
ms I-HyperparameterValue
to O
520 B-HyperparameterValue
ms I-HyperparameterValue
in O
order O
to O
get O
performances O
corresponding O
to O
different O
latency B-MetricName
regimes. O
We O
can O
observe O
that O
the O
LM-based O
models O
using O
both O
XLM B-MethodName
and O
SLM B-MethodName
provide O
a O
significant O
performance O
improvement O
over O
the O
baseline O
MMA O
model. O
We O
observe O
improvements O
in O
the O
range O
of O
1-2 B-MetricValue

BLEU B-MetricName
scores O
consistently O
across O
all O
the O
latency O
regimes O
( O
λ B-HyperparameterName
= O
0.1 B-HyperparameterValue
, O
0.05 B-HyperparameterValue
, O
0.01 B-HyperparameterValue
) O
. O
The O
MMA B-MethodName
using O
SLM B-MethodName
language O
model O
performs O
slightly O
better O
than O
MMA B-MethodName
using O
XLM B-MethodName
language O
model. O
This O
is O
due O
to O
SLM B-MethodName
's I-MethodName
higher O
accuracy B-MetricName
on O
the O
next O
token O
prediction O
task O
as O
compared O
to O

XLM B-MethodName
, O
30.15 B-MetricValue
% O
vs. O
A O
LM B-MethodName
at O
MMA B-MethodName
Output O
Layer O
We O
explored O
a O
naive O
approach O
of O
integrating O
LM O
information O
into O
the O
MMA. B-MethodName
In O
this O
approach O
, O
we O
integrate O
the O
future O
information O
obtained O
from O
the O
LM O
directly O
into O
the O
output O
layer O
of O
the O
MMA B-MethodName
model. O
We O
refer O
to O
this O
experiment O

as O
'LM B-MethodName
Rescoring I-MethodName
( I-MethodName
LMR I-MethodName
) I-MethodName
' O
, O
and O
the O
corresponding O
model O
is O
called O
MMA-LMR. B-MethodName
As O
observed O
in O
Figure O
5 O
, O
MMA-LMR B-MethodName
has O
inferior O
performance O
compared O
to O
the O
MMA B-MethodName
model. O
Since O
the O
LM O
information O
integration O
is O
only O
done O
at O
the O
output O
layer O
of O
the O
model O
, O
the O
MMA O
model O
can O

not O
easily O
discard O
the O
incorrect O
information O
from O
LM. O
This O
motivates O
us O
to O
tightly O
integrate O
the O
LM O
information O
into O
the O
simultaneous O
model O
. O
B O
Language O
Models O
As O
mentioned O
earlier O
, O
we O
train O
two O
different O
language O
models O
( O
LMs O
) O
and O
use O
them O
to O
improve O
the O
anticipation O
in O
monotonic B-MethodName
attention I-MethodName
based I-MethodName
Simultaneous I-MethodName

models I-MethodName
. O
B.1 O
XLM-Roberta B-MethodName
( I-MethodName
XLM-R I-MethodName
) I-MethodName
XLM-R B-MethodName
Large I-MethodName
model I-MethodName
2 O
was O
trained O
on O
the O
100 O
languages O
CommonCrawl B-DatasetName
corpora O
total O
size O
of O
2.5TB O
with O
550M O
parameters O
from O
24 B-HyperparameterValue
layers B-HyperparameterName
, O
1024 B-HyperparameterValue
hidden B-HyperparameterName
states I-HyperparameterName
, O
4096 B-HyperparameterValue
feed-forward B-HyperparameterName
hidden-states I-HyperparameterName
, O
and O
16 B-HyperparameterValue
heads. B-HyperparameterName
Total O
number O
of O
parameters O
is O
558M. O
We O
finetune O
the O

head O
of O
the O
XLM-R O
LM O
model O
using O
the O
Masked O
Language O
Modeling O
objective O
which O
accounts O
for O
0.23 O
% O
of O
the O
total O
model O
parameters O
, O
i.e. O
, O
1.3M O
parameters O
. O
B.2 O
Smaller O
Language O
Model O
Since O
the O
LM B-MethodName
predictions O
are O
computed O
serially O
during O
inference O
, O
the O
time O
taken O
to O
compute O
the O
2 O
https O

: O
/ O
/ O
huggingface.co O
/ O
xlm-roberta-large O
LM O
token O
serves O
as O
a O
bottleneck O
to O
the O
latency B-MetricName
requirements. O
To O
reduce O
the O
LM O
computation B-MetricName
time I-MetricName
, O
we O
train O
a O
smaller B-MethodName
Language I-MethodName
Model I-MethodName
( I-MethodName
SLM I-MethodName
) I-MethodName
from O
scratch O
using O
the O
Causal B-MethodName
Language I-MethodName
Modeling I-MethodName
objective. O
SLM B-MethodName
is O
composed O
of O
6 O
Transformer O
decoder O
blocks O
, O
512 O

hidden-states O
, O
2048 O
feed-forward O
hidden-states O
& O
8 O
attention O
heads. O
It O
alleviates O
the O
need O
for O
the O
sub-token O
summary O
layer O
since O
it O
shares O
the O
vocabulary O
and O
tokenization O
with O
the O
MMA B-MethodName
models. O
The O
train O
examples O
are O
at O
the O
sentence O
level O
, O
rather O
than O
forming O
a O
block O
out O
of O
multiple O
sentences O
( O
which O
is O

the O
usual O
case O
for O
Language O
Models O
) O
. O
Since O
the O
target O
texts O
contain O
lesser O
than O
250k O
examples O
, O
we O
use O
additional O
data O
augmentation O
techniques O
to O
upsample O
the O
target O
data. O
We O
also O
use O
additional O
data O
to O
avoid O
overfitting O
on O
the O
MuST-C B-DatasetName
target O
text. O
Details O
have O
been O
provided O
in O
B.2.1 O
. O
B.2.1 O

Data O
Augmentation O
Up-Sampling O
: O
To O
boost O
the O
LM O
performance O
and O
mitigate O
overfitting O
, O
we O
use O
contextual O
data O
augmentation O
( O
Kobayashi O
, O
2018 O
) O
to O
upsample O
the O
MuST-C B-DatasetName
target O
text O
data O
by O
substituting O
and O
inserting O
words O
based O
on O
LM O
predictions. O
We O
use O
the O
NLPAUG O
3 O
package O
to O
get O
similar O
words O
based O

on O
contextual O
embeddings. O
From O
the O
Hugging O
Face O
Repository O
, O
we O
use O
two O
different O
pretrained O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
We O
observe O
that O
both O
upsampling O
and O
data O
augmentation O
help O
us O
to O
reduce O
the O
overfitting O
on O
the O
MuST-C B-DatasetName
dev O
set O
. O
B.3 O
Token O
Prediction O
For O
each O
output O
token O
, O

the O
LM O
prediction O
is O
obtained O
by O
feeding O
the O
prefix O
upto O
that O
token O
to O
the O
LM O
model. O
These O
predictions O
are O
pre-computed O
for O
training O
and O
validation O
sets. O
This O
ensures O
parallelization O
and O
avoids O
the O
overhead O
to O
run O
the O
LM O
simultaneously O
during O
the O
training O
process. O
During O
inference O
, O
the O
LM O
model O
is O
called O
every O

time O
a O
new O
output O
token O
is O
written O
. O
C O
Dataset O
The O
MuST-C B-DatasetName
dataset O
comprises O
of O
English O
TED O
talks O
, O
the O
translations O
and O
transcriptions O
have O
been O
aligned O
with O
the O
speech O
at O
sentence O
level. O
Dataset O
statistics O
have O
been O
provided O
in O
the O
Table O
1 O
. O
D O
Effect O
of O
LM B-HyperparameterName
Size I-HyperparameterName
on O
Latency-Quality B-MetricName
We O

train O
several O
SLM O
models O
with O
varying O
sizes O
in O
our O
experiments O
and O
choose O
the O
best O
model O
based O
on O
the O
top-1 B-MetricName
accuracy. I-MetricName
As O
we O
increase O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
model O
from O
2 B-HyperparameterValue
to O
4 B-HyperparameterValue
to O
6 B-HyperparameterValue
layers O
, O
the O
SLM B-MethodName
and O
the O
proposed O
MMA B-MethodName
with O
future O
information O
models O
have O

shown O
performance O
improvements. O
However O
, O
increasing O
the O
number O
of O
layers O
greater O
than O
6 B-HyperparameterValue
does O
not O
yield O
any O
performance O
improvements. O
We O
also O
notice O
this O
degradation O
of O
performance O
with O
the O
XLM B-MethodName
model O
while O
varying O
the O
number B-HyperparameterName
of I-HyperparameterName
hidden I-HyperparameterName
layers I-HyperparameterName
in O
the O
LM O
head O
. O
E O
Training O
Details O
We O
follow O
the O
training O
process O

similar O
to O
Ma O
et O
al. O
( O
2020 O
) O
training O
process. O
We O
train O
an O
English O
ASR B-MethodName
model O
using O
the O
source O
speech O
data. O
Next O
, O
we O
train O
a O
simultaneous O
model O
without O
the O
latency B-HyperparameterName
loss I-HyperparameterName
( O
setting O
λ B-HyperparameterName
latency B-HyperparameterName
= O
0 B-HyperparameterValue
) O
after O
initializing O
the O
encoder O
from O
the O
English O
ASR O
model. O
After O
this O

step O
, O
we O
finetune O
the O
simultaneous O
model O
for O
different O
λs. B-HyperparameterName
This O
training O
process O
is O
repeated O
for O
all O
the O
reported O
models O
and O
for O
each O
task. O
The O
details O
regarding O
the O
hyperparameters O
for O
the O
model O
have O
been O
provided O
in O
Table O
2 O
. O
F O
BLEU-AL O
Numbers O
As O
mentioned O
in O
the O
results O
section O
of O
the O

main O
paper O
, O
we O
vary O
the O
latency O
weight O
hyperparameter O
( O
λ O
) O
to O
train O
different O
models O
to O
obtain O
different O
latency O
regimes. O
We O
also O
vary O
the O
step-size B-HyperparameterName
/ O
speech B-HyperparameterName
segment I-HyperparameterName
size I-HyperparameterName
during O
inference. O
In O
total O
, O
we O
obtain O
18 O
different O
data O
points O
corresponding O
to O
each O
model. O
In O
Table O
3 O
, O
we O

compare O
the O
results O
obtained O
using O
MMA B-MethodName
, O
MMA-XLM B-MethodName
and O
MMA-SLM B-MethodName
under O
similar O
hyperparameter O
settings. O
It O
will O
help O
the O
reader O
to O
quantify O
the O
benefits O
obtained O
from O
our O
proposed O
approach O
. O

-DOCSTART- O
TSTR B-MethodName
: O
Too B-MethodName
Short I-MethodName
to I-MethodName
Represent I-MethodName
, O
Summarize O
with O
Details O
! O
Intro-Guided O
Extended O
Summary O
Generation O
Many O
scientific O
papers O
such O
as O
those O
in O
arXiv O
and O
PubMed O
data O
collections O
have O
abstracts O
with O
varying O
lengths O
of O
50-1000 O
words O
and O
average O
length O
of O
approximately O
200 O
words O
, O
where O
longer O
abstracts O
typically O
convey O
more O

information O
about O
the O
source O
paper. O
Up O
to O
recently O
, O
scientific O
summarization O
research O
has O
typically O
focused O
on O
generating O
short O
, O
abstractlike O
summaries O
following O
the O
existing O
datasets O
used O
for O
scientific O
summarization. O
In O
domains O
where O
the O
source O
text O
is O
relatively O
long-form O
, O
such O
as O
in O
scientific O
documents O
, O
such O
summary O
is O
not O
able O

to O
go O
beyond O
the O
general O
and O
coarse O
overview O
and O
provide O
salient O
information O
from O
the O
source O
document. O
The O
recent O
interest O
to O
tackle O
this O
problem O
motivated O
curation O
of O
scientific O
datasets O
, O
arXiv-Long O
and O
PubMed-Long O
, O
containing O
human-written O
summaries O
of O
400-600 O
words O
, O
hence O
, O
providing O
a O
venue O
for O
research O
in O
generating O
long O

/ O
extended O
summaries. O
Extended O
summaries O
facilitate O
a O
faster O
read O
while O
providing O
details O
beyond O
coarse O
information. O
In O
this O
paper O
, O
we O
propose O
TSTR B-MethodName
, O
an O
extractive O
summarizer O
that O
utilizes O
the O
introductory O
information O
of O
documents O
as O
pointers O
to O
their O
salient O
information. O
The O
evaluations O
on O
two O
existing O
large-scale O
extended O
summarization O
datasets O
indicate O
statistically O

significant O
improvement O
in O
terms O
of O
ROUGE B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( I-MetricName
F1 I-MetricName
) I-MetricName
scores O
( O
except O
in O
one O
case O
) O
as O
compared O
to O
strong O
baselines O
and O
state-of-theart. O
Comprehensive O
human O
evaluations O
favor O
our O
generated O
extended O
summaries O
in O
terms O
of O
cohesion O
and O
completeness O
. O
Introduction O
Over O
the O
past O
few O
years O
, O
summarization O
task O

has O
witnessed O
a O
huge O
deal O
of O
progress O
in O
extractive B-TaskName
( O
Nallapati O
et O
al. O
, O
2017 O
; O
Liu O
and O
Lapata O
, O
2019 O
; O
Yuan O
et O
al. O
, O
2020 O
; O
Cui O
et O
al. O
, O
2020 O
; O
Jia O
et O
al. O
, O
2020 O
; O
Feng O
et O
al. O
, O
2018 O
) O
and O
abstractive B-TaskName
( O
See O

et O
al. O
, O
2017 O
; O
Gehrmann O
et O
al. O
, O
2018 O
; O
Zhang O
et O
al. O
, O
2019 O
; O
Tian O
et O
al. O
, O
2019 O
; O
Zou O
et O
al. O
, O
2020 O
) O
[ O
Introductory O
] O
Neural B-TaskName
machine I-TaskName
translation I-TaskName
( O
@ O
xcite O
) O
, O
directly O
applying O
a O
single O
neural O
network O
to O
transform O
the O
source O

sentence O
into O
the O
target O
sentence O
, O
has O
now O
reached O
impressive O
performance O
( O
@ O
xcite O
[ O
… O
] O
Motivated O
by O
recent O
success O
in O
unsupervised O
cross-lingual O
embeddings O
( O
@ O
xcite O
) O
, O
the O
models O
proposed O
for O
unsupervised O
NMT B-TaskName
often O
assume O
that O
a O
pair O
of O
sentences O
from O
two O
different O
languages O
can O
be O
mapped O

to O
a O
same O
latent O
representation O
in O
a O
shared-latent O
space O
( O
@ O
xcite O
) O
[ O
… O
] O
Although O
the O
shared O
encoder O
is O
vital O
for O
mapping O
sentences O
from O
different O
languages O
into O
the O
shared-latent O
space O
, O
it O
is O
weak O
in O
keeping O
the O
uniqueness O
and O
internal O
characteristics O
of O
each O
language O
, O
such O
as O
the O

style O
, O
terminology O
and O
sentence O
structure. O
[ O
… O
] O
For O
each O
language O
, O
the O
encoder O
and O
its O
corresponding O
decoder O
perform O
an O
AE O
, O
where O
the O
encoder O
generates O
the O
latent O
representations O
from O
the O
perturbed O
input O
sentences O
and O
the O
decoder O
reconstructs O
the O
sentences O
from O
the O
latent O
representations. O
Experimental O
results O
show O
that O
the O

proposed O
approach O
consistently O
achieves O
great O
success O
. O
Related O
Work O
Summarizing O
scientific O
documents O
has O
gained O
a O
huge O
deal O
of O
attention O
from O
researchers O
, O
although O
it O
has O
been O
studied O
for O
decades. O
Neural O
efforts O
in O
scientific O
text O
have O
used O
specific O
characteristics O
of O
papers O
such O
as O
discourse O
structure O
Xiao O
and O
Carenini O
, O
2019 O
) O

and O
citation O
information O
( O
Qazvinian O
and O
Radev O
, O
2008 O
; O
Goharian O
, O
2015 O
, O
2018 O
) O
to O
aid O
summarization O
model. O
While O
prior O
work O
has O
mostly O
covered O
the O
generation O
of O
shorter-form O
summaries O
( O
approx. O
200 O
terms O
) O
, O
generating O
extended O
summaries O
of O
roughly O
600 O
terms O
for O
long-form O
source O
documents O
such O
as O

scientific O
papers O
has O
been O
motivated O
very O
recently O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
. O
The O
proposed O
models O
for O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task I-TaskName
include O
jointly O
learning O
to O
predict O
sentence O
importance O
and O
sentence O
section O
to O
extract O
top O
sentences O
( O
Sotudeh O
et O
al. O
, O
2020 O
) O
; O
utilizing O
section-contribution O
computations O
to O

pick O
sentences O
from O
important O
section O
for O
forming O
the O
final O
summary O
( O
Ghosh O
Roy O
et O
al. O
, O
2020 O
) O
; O
identifying O
salient O
sections O
for O
generating B-TaskName
abstractive I-TaskName
summaries I-TaskName
( O
Gidiotis O
et O
al. O
, O
2020 O
) O
; O
ensembling O
of O
extraction O
and O
abstraction O
models O
to O
form O
final O
summary O
( O
Ying O
et O
al. O
, O
2021 O

) O
; O
an O
extractive O
model O
with O
TextRank O
algorithm O
equipped O
with O
BM25 O
as O
similarity O
function O
( O
Kaushik O
et O
al. O
, O
2021 O
) O
; O
and O
incorporating O
sentences O
embeddings O
into O
graph-based O
extractive O
summarizer O
in O
an O
unsupervised O
manner O
( O
Ramirez-Orta O
and O
Milios O
, O
2021 O
) O
. O
Unlike O
these O
works O
, O
we O
do O
not O
exploit O

any O
sectional O
nor O
citation O
information O
in O
this O
work. O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
at O
proposing O
the O
novel O
method O
of O
utilizing O
introductory O
information O
of O
the O
scientific O
paper O
to O
guide O
the O
model O
to O
learn O
to O
generate O
summary O
from O
the O
salient O
and O
related O
information O
. O
3 O
Background O

: O
Contextualized O
language O
models O
for O
summarization O
Contextualized O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
and O
ROBERTA B-MethodName
have O
achieved O
state-of-the-art O
performance O
on O
a O
variety O
of O
downstream O
NLP O
tasks O
including O
text O
summarization. O
Liu O
and O
Lapata O
( O
2019 O
) O
were O
the O
first O
to O
fine-tune O
a O
contextualized O
language O

model O
( O
i.e. O
, O
BERT B-MethodName
) O
for O
the O
summarization O
task. O
They O
proposed O
BERTSUM B-MethodName
-a O
fine-tuning O
scheme O
for O
text O
summarization-that O
outputs O
the O
sentence O
representations O
of O
the O
source O
document O
( O
we O
use O
the O
term O
source O
and O
source O
document O
interchangeably O
, O
referring O
to O
the O
entire O
document O
) O
. O
The O
BERT-SUMEXT B-MethodName
model O
, O
which O

is O
built O
based O
on O
BERT-SUM B-MethodName
, O
was O
proposed O
for O
the O
extractive B-TaskName
summarization I-TaskName
task. O
It O
utilizes O
the O
representations O
produced O
by O
BERTSUM B-MethodName
, O
passes O
them O
through O
Transformers O
encoder O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
, O
and O
finally O
uses O
a O
linear O
layer O
with O
Sigmoid O
function O
to O
compute O
copying O
probabilities O
for O
each O
input O

sentence. O
Formally O
, O
let O
l O
1 O
, O
l O
2 O
, O
... O
, O
l O
n O
be O
the O
binary O
tags O
over O
the O
source O
sentences O
x O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
n O
} O
of O
a O
long O
document O
, O
in O
which O
n O
is O
the O
number O
of O
sentences O
in O

the O
paper. O
The O
BERTSUMEXT B-MethodName
network O
runs O
over O
the O
source O
documents O
as O
follows O
( O
Eq. O
1 O
) O
, O
h O
b O
= O
BertSum B-MethodName
( O
x O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
o O
h O
+ O
b O
o O
) O
( O
1 O
) O
where O
h O
b O
and O

h O
are O
the O
representations O
of O
source O
sentences O
encoded O
by O
BERTSUM B-MethodName
and O
Trasformers O
encoder O
, O
respectively. O
W O
o O
and O
b O
o O
are O
trainable O
parameters O
, O
and O
p O
is O
the O
probability O
distribution O
over O
the O
source O
sentences O
, O
signifying O
extraction O
copy O
likelihood. O
The O
goal O
of O
this O
network O
is O
to O
train O
a O
network O
that O

can O
identify O
the O
positive O
sets O
of O
sentences O
as O
the O
summary. O
To O
prevent O
the O
network O
from O
selecting O
redundant O
sentences O
, O
BERTSUM B-MethodName
uses O
Trigram O
Blocking O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
for O
sentence O
selection O
in O
inference O
time. O
We O
refer O
the O
reader O
to O
the O
main O
paper O
for O
more O
details O
. O
TSTR B-MethodName
: O

Intro-guided O
Summarization O
In O
this O
section O
, O
we O
describe O
our O
methodology O
to O
tackle O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task. O
Our O
approach O
exploits O
the O
introductory O
information O
3 O
. O
3 O
Introductory O
information O
is O
defined O
in O
Section O
1 O
of O
the O
paper O
as O
pointers O
to O
salient O
sentences O
within O
it O
, O
as O
shown O
in O
Figure O
2. O
It O

is O
ultimately O
expected O
that O
the O
extractive O
summarizer O
is O
guided O
to O
pick O
salient O
sentences O
across O
the O
entire O
paper O
. O
The O
detailed O
illustration O
of O
our O
model O
is O
shown O
in O
Figure O
3. O
To O
aid O
the O
extractive B-TaskName
summarization I-TaskName
model O
( O
i.e. O
, O
right-hand O
box O
in O
Figure O
3 O
) O
which O
takes O
in O
source O
sentences O
of O

a O
scientific O
paper O
, O
we O
utilize O
an O
additional O
BERTSUM B-MethodName
encoder O
called O
Introductory O
encoder O
( O
left-hand O
box O
in O
Fig. O
3 O
) O
that O
receives O
x O
intro O
= O
{ O
sent O
1 O
, O
sent O
2 O
, O
... O
, O
sent O
m O
} O
, O
with O
m O
being O
the O
number O
of O
sentences O
in O
introductory O
section. O
The O
aim O

of O
adding O
second O
encoder O
in O
this O
framework O
is O
to O
identify O
the O
clues O
in O
the O
introductory O
section O
which O
point O
to O
the O
salient O
supplementary O
sentences O
4 O
. O
The O
BERTSUM B-MethodName
network O
computes O
the O
extraction O
probabilities O
for O
introductory O
sentences O
as O
follow O
( O
same O
way O
as O
in O
Eq. O
1 O
) O
, O
h O
b O
= O
BertSum B-MethodName

( O
x O
intro O
) O
h O
= O
Encoder O
t O
( O
h O
b O
) O
p O
= O
σ O
( O
W O
jh O
+ O
b O
j O
) O
( O
2 O
) O
in O
whichh O
b O
, O
andh O
are O
the O
introductory O
sentence O
representations O
by O
BERTSUM B-MethodName
, O
Transformers O
encoder O
, O
respectively.p O
is O
the O
introductory O
sentence O
extraction O
probabilities. O
W O
j O

and O
b O
j O
are O
trainable O
matrices O
. O
After O
identifying O
salient O
introductory O
sentences O
, O
the O
representations O
associated O
with O
them O
are O
retrieved O
using O
a O
pooling O
function O
and O
further O
used O
to O
guide O
the O
first O
task O
( O
i.e. O
, O
right-hand O
side O
in O
Figure O
3 O
) O
as O
follows O
, O
where O
Select O
( O
• O
) O
is O

a O
function O
that O
takes O
in O
all O
introductory O
sentence O
representations O
( O
i.e. O
, O
h O
) O
, O
and O
introductory O
sentence O
probabilitiesp. O
It O
then O
outputs O
the O
representations O
associated O
with O
top O
k O
introductory O
sentences O
, O
sorted O
byp. O
To O
extract O
top O
introductory O
sentences O
, O
we O
first O
sorth O
vectors O
based O
on O
their O
computed O
probabilitiesp O
and O
then O

we O
pick O
up O
top B-HyperparameterName
k I-HyperparameterName
hidden I-HyperparameterName
vectors I-HyperparameterName
( O
i.e. O
, O
h B-HyperparameterName
top I-HyperparameterName
) O
that O
has O
the O
highest O
probability. O
MLP O
1 O
is O
a O
multi-layer O
perceptron O
that O
takes O
in O
concatenated O
vector O
of O
top O
introductory O
sentences O
and O
projects O
it O
into O
a O
new O
vector O
calledĥ O
. O
h B-HyperparameterName
top I-HyperparameterName
= O
Select O
( O
h O
, O
p O

, O
k O
) O
h O
= O
MLP O
1 O
( O
h O
top O
) O
( O
3 O
) O
At O
the O
final O
stage O
, O
we O
concatenate O
the O
transformed O
introductory O
top O
sentence O
representations O
( O
i.e. O
, O
ĥ O
) O
with O
each O
source O
sentence O
representations O
from O
Eq. O
1 O
( O
i.e. O
, O
h O
i O
where O
i O
shows O
the O
ith O

paper O
sentence O
) O
and O
process O
them O
to O
produce O
a O
resulting O
vector O
r O
which O
is O
intro-aware O
source O
sentence O
hidden O
representations. O
After O
processing O
the O
resulting O
vector O
through O
a O
linear O
output O
layer O
( O
with O
W O
z O
and O
b O
z O
as O
trainable O
parameters O
) O
, O
we O
obtain O
final O
introaware O
sentence O
extraction O
probabilities O
( O
i.e. O

, O
p O
) O
as O
follows O
, O
r O
= O
MLP O
2 O
( O
h O
i O
; O
ĥ O
) O
p O
= O
σ O
( O
W O
z O
r O
+ O
b O
z O
) O
( O
4 O
) O
in O
which O
MLP O
2 O
is O
a O
multi-layer O
perceptron O
, O
influencing O
the O
knowledge O
from O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O

t O
2 O
) O
into O
the O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
task O
( O
i.e. O
, O
t O
1 O
) O
. O
We O
train O
both O
tasks O
through O
our O
end-to-end O
system O
jointly O
as O
follows O
, O
ℓ O
total O
= O
( O
α O
) O
ℓ O
t O
1 O
+ O
( O
1 O
− O
α O
) O
ℓ O
t O
2 O
( O
5 O
) O
where O

ℓ B-MetricName
t I-MetricName
1 I-MetricName
, O
and O
ℓ B-MetricName
t I-MetricName
2 I-MetricName
are O
the O
losses B-MetricName
computed O
for O
introductory B-TaskName
sentence I-TaskName
extraction I-TaskName
and O
source B-TaskName
sentence I-TaskName
extraction I-TaskName
tasks O
, O
α O
is O
the O
regularizing O
parameter O
that O
balances O
the O
learning O
process O
between O
two O
tasks O
, O
and O
ℓ B-MetricName
total I-MetricName
is O
the O
total O
computed O
loss B-MetricName
that O
is O
optimized O
during O
the O
training O

. O
Experimental O
Setup O
In O
this O
section O
, O
we O
explain O
the O
datasets O
, O
baselines O
, O
and O
preprocessing O
and O
training O
parameters O
. O
Dataset O
We O
use O
two O
publicly O
available O
scientific O
extended O
summarization O
datasets O
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
. O
-arXiv-Long O
: O
A O
set O
of O
arXiv O
scientific O
papers O
containing O
papers O
from O
various O

scientific O
domains O
such O
as O
physics O
, O
mathematics O
, O
computer O
science O
, O
quantitative O
biology. O
arXiv-Long B-DatasetName
is O
intended O
for O
extended B-TaskName
summarization I-TaskName
task O
and O
was O
filtered O
from O
a O
larger O
dataset O
i.e. O
, O
arXiv O
for O
the O
summaries O
of O
more O
than O
350 O
tokens. O
The O
ground-truth O
summaries O
( O
i.e. O
, O
abstract O
) O
are O
long O
, O
with O

the O
average O
length O
of O
574 O
tokens. O
It O
contains O
7816 O
( O
train O
) O
, O
1381 O
( O
validation O
) O
, O
and O
1952 O
( O
test O
) O
papers O
. O
-PubMed-Long B-DatasetName
: O
A O
set O
of O
biomedical O
scientific O
papers O
from O
PubMed O
with O
average O
summary O
length O
of O
403 O
tokens. O
This O
dataset O
contains O
79893 O
( O
train O
) O
, O

4406 O
( O
validation O
) O
, O
and O
4402 O
( O
test O
) O
scientific O
papers O
. O
Baselines O
We O
compare O
our O
model O
with O
two O
strong O
non-neural O
systems O
, O
and O
four O
state-of-the-art O
neural O
summarizers. O
We O
use O
all O
of O
these O
baselines O
for O
the O
purpose O
of O
extended O
summary O
generation O
whose O
documents O
hold O
different O
characteristics O
in O
length O
, O

writing O
style O
, O
and O
discourse O
structure O
as O
compared O
to O
documents O
in O
the O
other O
domains O
of O
summarization O
. O
-LSA B-MethodName
( O
Steinberger O
and O
Jezek O
, O
2004 O
) O
: O
an O
extractive O
vector-based O
model O
that O
utilizes O
Singular B-MethodName
Value I-MethodName
Decomposition I-MethodName
( O
SVD B-MethodName
) O
to O
find O
the O
semantically O
important O
sentences O
. O
-LEXRANK B-MethodName
( O
Erkan O
and O
Radev O

, O
2004 O
) O
: O
a O
widely O
adopted O
extractive O
summarization O
baseline O
that O
utilizes O
a O
graph-based O
approach O
based O
on O
eigenvector O
centrality O
to O
identify O
the O
most O
salient O
sentences O
. O
-BERTSUMEXT B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
: O
a O
contextualized O
summarizer O
fine-tuned O
for O
summarization O
task O
, O
which O
encodes O
input O
sentence O
representations O
, O
and O

then O
processes O
them O
through O
a O
multi-layer O
Transformers O
encoder O
to O
obtain O
document-level O
sentence O
representation. O
Finally O
, O
a O
linear O
output O
layer O
with O
Sigmoid O
activation O
function O
outputs O
a O
probability O
distribution O
over O
each O
input O
sentence O
, O
denoting O
the O
extent O
to O
which O
they O
are O
probable O
to O
be O
extracted O
. O
-BERTSUMEXT-INTRO B-MethodName
( O
Liu O
and O
Lapata O
, O

2019 O
) O
: O
a O
BERTSUMEXT B-MethodName
model O
that O
only O
runs O
on O
the O
introductory O
sentences O
as O
the O
input O
, O
and O
extracts O
the O
salient O
introductory O
sentences O
as O
the O
summary O
. O
-BERTSUMEXTMULTI B-MethodName
( O
Sotudeh O
et O
al. O
, O
2021 O
) O
: O
an O
extension O
of O
the O
BERTSUMEXT B-MethodName
model O
that O
incorporates O
an O
additional O
linear O
layer O
with O
Sigmoid O

classifier O
to O
output O
a O
probability O
distribution O
over O
a O
fixed O
number O
of O
pre-defined O
sections O
that O
an O
input O
sentence O
might O
belong O
to. O
The O
additional O
network O
is O
expected O
to O
predict O
a O
single O
section O
for O
an O
input O
sentence O
and O
is O
trained O
jointly O
with O
BERTSUMEXT B-MethodName
module O
( O
i.e. O
, O
sentence O
extractor O
) O
. O
-BART B-MethodName
( O

Lewis O
et O
al. O
, O
2020 O
) O
: O
a O
state-of-the-art O
abstractive O
summarization O
model O
that O
makes O
use O
of O
pretrained O
encoder O
and O
decoder. O
BART B-MethodName
can O
be O
thought O
of O
as O
an O
extension O
of O
BERTSUM B-MethodName
in O
which O
merely O
encoder O
is O
pre-trained O
, O
but O
decoder O
is O
trained O
from O
scratch. O
While O
our O
model O
is O
an O
extractive O
one O

, O
at O
the O
same O
time O
, O
we O
find O
it O
of O
value O
to O
measure O
the O
abstractive O
model O
performance O
in O
the O
extended B-TaskName
summary I-TaskName
generation I-TaskName
task O
. O
Preprocessing O
, O
parameters O
, O
labeling O
, O
and O
implementation O
details O
We O
used O
the O
open O
implementation O
of O
BERT-SUMEXT B-MethodName
with O
default O
parameters O
5 B-HyperparameterValue
. O
To O
implement O
the O
non-neural O
baseline O

models O
, O
we O
utilized O
Sumy O
python O
package O
6 O
. O
Longformer B-MethodName
model O
( O
Beltagy O
et O
al. O
, O
2020 O
) O
is O
utilized O
as O
our O
contextualized O
language O
model O
for O
running O
all O
the O
models O
due O
to O
its O
efficacy O
at O
processing O
long O
documents. O
For O
our O
model O
, O
the O
cross-entropy O
loss O
function O
is O
set O
for O
two O

tasks O
( O
i.e. O
, O
t O
1 O
: O
source O
sentence O
extraction O
and O
t O
2 O
: O
introductory O
sentences O
extraction O
in O
Figure O
3 O
) O
and O
the O
model O
is O
optimized O
through O
multi-tasking O
approach O
as O
discussed O
in O
Section O
3. O
The O
model O
with O
the O
highest O
ROUGE-2 B-MetricName
on O
validation O
set O
is O
selected O
for O
inference. O
The O
validation O
is O

performed O
every O
2k B-HyperparameterValue
training B-HyperparameterName
steps. I-HyperparameterName
α B-HyperparameterName
( O
in O
Eq. O
5 O
) O
is O
set O
to O
be O
0.5 B-HyperparameterValue
( O
empirically O
determined O
) O
. O
Our O
model O
includes O
474M B-HyperparameterValue
trainable B-HyperparameterName
parameters I-HyperparameterName
, O
trained O
on O
dual O
GeForce O
GTX O
1080Ti O
GPUs O
for O
approximately O
a O
week. O
We O
use O
k B-HyperparameterName
= O
5 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
k B-HyperparameterName
= O
8 B-HyperparameterValue

for O
PubMed-Long B-DatasetName
datasets O
( O
Eq. O
3 O
) O
. O
We O
make O
our O
model O
implementation O
as O
well O
as O
sample O
summaries O
publicly O
available O
to O
expedite O
ongoing O
research O
in O
this O
direction O
7 O
. O
A O
two-stage O
labeling O
approach O
was O
employed O
to O
identify O
ground-truth O
introductory O
and O
nonintroductory O
sentences. O
In O
the O
first O
stage O
, O
we O
used O
a O

greedy O
labeling O
approach O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
to O
label O
sentences O
within O
the O
first O
section O
of O
a O
given O
paper O
( O
i.e. O
, O
labeling O
introductory O
sentences O
) O
with O
respect O
to O
their O
ROUGE B-MetricName
overlap O
8 O
with O
the O
groundtruth O
summary O
( O
i.e. O
, O
abstract O
) O
. O
In O
the O
second O
stage O
, O

the O
same O
greedy O
approach O
was O
exploited O
over O
the O
rest O
of O
sentences O
( O
i.e. O
, O
non-introductory O
) O
9 O
with O
regard O
to O
their O
ROUGE B-MetricName
overlap O
with O
the O
identified O
introductory O
sentences O
in O
the O
first O
stage. O
Our O
choice O
of O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
is O
based O
on O
the O
fact O
that O
these O
express O
higher O
similarity O
with O
human O

judgments O
( O
Cohan O
and O
Goharian O
, O
2016 O
) O
. O
We O
continued O
the O
second O
stage O
until O
a O
fixed O
length O
of O
the O
summary O
was O
reached. O
Specifically O
, O
the O
fixed B-HyperparameterName
length I-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
labels I-HyperparameterName
is O
set O
to O
be O
15 B-HyperparameterValue
for O
arXiv-Long B-DatasetName
, O
and O
20 B-HyperparameterValue
for O
PubMed-Long B-DatasetName
datasets O
as O
these O
achieved O
the O
highest O
oracle O

ROUGE B-MetricName
scores O
in O
our O
experiments O
. O
Results O
Experimental O
evaluation O
The O
recent O
effort O
in O
extended O
summarization O
and O
its O
shared O
task O
of O
LongSumm O
( O
Chandrasekaran O
et O
al. O
, O
2020 O
) O
used O
average O
ROUGE B-MetricName
( O
F1 B-MetricName
) O
to O
rank O
the O
participating O
systems O
, O
in O
addition O
to O
commonly-used O
ROUGE-N B-MetricName
scores. O
Table O
2 O
shows O
the O

performance O
of O
the O
participated O
systems O
on O
the O
blind O
test O
set. O
As O
shown O
, O
BERTSUMEXTMULTI B-MethodName
model O
outperforms O
other O
models O
by O
a O
large O
margin O
( O
i.e. O
, O
with O
relative O
improvements O
of O
6 B-MetricValue
% I-MetricValue
and O
3 B-MetricValue
% I-MetricValue
on O
ROUGE-1 B-MetricName
and O
average B-MetricName
ROUGE I-MetricName
( O
F1 B-MetricName
) O
, O
respectively O
) O
; O
hence O
, O
we O
use O

the O
best-performing O
in O
terms O
of O
F1 B-MetricName
( O
i.e. O
, O
BERTSUMEXTMULTI B-MethodName
model O
) O
in O
our O
experiments. O
Tables. O
1 O
presents O
our O
results O
on O
the O
test O
sets O
of O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
datasets O
, O
respectively. O
As O
observed O
, O
our O
model O
statistically O
significantly O
outperforms O
the O
state-of-the-art O
systems O
on O
both O
datasets O
across O
most O
of O
the O
ROUGE B-MetricName

vari-ants O
, O
except O
ROUGE-L B-MetricName
on O
PubMed-Long. B-DatasetName
The O
improvements O
gained O
by O
our O
model O
validates O
our O
hypothesis O
that O
incorporating O
the O
salient O
introductory O
sentence O
representations O
into O
the O
extractive O
summarizer O
yields O
a O
promising O
improvement. O
Two O
nonneural O
models O
( O
i.e. O
, O
LSA B-MethodName
and O
LEXRANK B-MethodName
) O
underperform O
the O
neural O
models O
, O
as O
expected. O
Comparing O
the O
abstractive O

model O
( O
i.e. O
, O
BART B-MethodName
) O
with O
extractive O
neural O
ones O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
, O
we O
see O
that O
while O
there O
is O
relatively O
a O
smaller O
gap O
in O
terms O
of O
ROUGE-1 B-MetricName
, O
the O
gap O
is O
larger O
for O
ROUGE-2 B-MetricName
, O
and O
ROUGE-L. B-MetricName
Interestingly O
, O
in O
the O
case O
of O
BART B-MethodName
, O

we O
found O
that O
generating O
extended O
summaries O
is O
rather O
challenging O
for O
abstractive O
summarizers. O
Current O
abstractive O
summarizers O
including O
BART B-MethodName
have O
difficulty O
in O
abstracting O
very O
detailed O
information O
, O
such O
as O
numbers O
, O
and O
quantities O
, O
which O
hurts O
the O
faithfulness O
of O
the O
generated O
summaries O
to O
the O
source. O
This O
behavior O
has O
a O
detrimental O
effect O
, O

specifically O
, O
on O
ROUGE-2 B-MetricName
and O
ROUGE-L B-MetricName
as O
their O
high O
correlation O
with O
human O
judgments O
in O
terms O
of O
faithfulness O
has O
been O
shown O
( O
Pagnoni O
et O
al. O
, O
2021 O
) O
. O
Comparing O
the O
extractive O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
models O
, O
while O
BERTSUMMULTIEXT B-MethodName
is O
expected O
to O
outperfom O
BERTSUMEXT B-MethodName
, O
it O
is O
observed O
that O
they O
perform O

almost O
similarly O
, O
with O
small O
( O
i.e. O
, O
insignificant O
) O
improved O
metrics. O
This O
might O
be O
due O
to O
the O
fact O
that O
BERTSUMEXTMULTI B-MethodName
works O
out-of-the-box O
when O
a O
handful O
amount O
of O
sentences O
are O
sampled O
from O
diverse O
sections O
to O
form O
the O
oracle O
summary O
as O
also O
reported O
by O
its O
authors. O
However O
, O
when O
labeling O
oracle O

sentences O
in O
our O
framework O
( O
i.e. O
, O
Intro-guided O
labeling O
) O
, O
there O
is O
no O
guarantee O
that O
the O
final O
set O
of O
oracle O
sentences O
are O
labeled O
from O
diverse O
sections. O
Overall O
, O
our O
model O
achieves O
about O
1.4 B-MetricValue
% I-MetricValue
, O
2.4 B-MetricValue
% I-MetricValue
, O
3.5 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
and O
1.0 B-MetricValue
% I-MetricValue
, O
2.5 B-MetricValue

% I-MetricValue
, O
1.3 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
across O
ROUGE B-MetricName
score O
variants O
; O
and O
2.2 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
, O
1.4 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
improvements O
over O
F1 B-MetricName
, O
compared O
to O
the O
neural O
baselines O
( O
i.e. O
, O
BERTSUMEXT B-MethodName
and O
BERT-SUMEXTMULTI B-MethodName
) O
. O
While O
comparing O
our O
model O
with O
BERTSUMEXT-INTRO B-MethodName
, O
we O

see O
the O
vital O
effect O
of O
adding O
second O
encoder O
at O
finding O
supplementary O
sentences O
across O
non-introductory O
sections O
, O
where O
our O
model O
gains O
relative O
improvements O
of O
9.62 B-MetricValue
% I-MetricValue
-26.26 B-MetricValue
% I-MetricValue
-16.09 B-MetricValue
% I-MetricValue
and O
9.40 B-MetricValue
% I-MetricValue
-5.27 B-MetricValue
% I-MetricValue
-9.99 B-MetricValue
% I-MetricValue
for O
ROUGE-1 B-MetricName
, O
ROUGE-2 B-MetricName
, O
ROUGE-L B-MetricName
on O
arXiv-Long B-DatasetName
and O
PubMed-Long B-DatasetName
, O
respectively. O
In O
fact O

, O
the O
sentences O
that O
are O
picked O
as O
summary O
from O
the O
in- O
troduction O
section O
are O
not O
comprehensive O
as O
such O
they O
are O
clues O
to O
the O
main O
points O
of O
the O
paper. O
The O
other O
important O
sentences O
are O
picked O
from O
the O
supplementary O
parts O
( O
i.e. O
, O
non-introductory O
) O
of O
the O
paper O
. O
Human O
evaluation O
While O

our O
model O
statistically O
significantly O
improves O
upon O
the O
state-of-the-art O
baselines O
in O
terms O
of O
ROUGE B-MetricName
scores O
, O
a O
few O
works O
have O
reported O
the O
low O
correlation O
of O
ROUGE B-MetricName
with O
human O
judgments O
( O
Liu O
and O
Liu O
, O
2008 O
; O
Cohan O
and O
Goharian O
, O
2016 O
; O
Fabbri O
et O
al. O
, O
2021 O
) O
. O
In O
order O

to O
provide O
insights O
into O
why O
and O
how O
our O
model O
outperforms O
the O
bestperforming O
baselines O
, O
we O
perform O
a O
manual O
analysis O
of O
our O
system O
's O
generated O
summaries O
, O
BERT-SUMEXT B-MethodName
's I-MethodName
, O
and O
BERTSUMEXTMULTI's. B-MethodName
For O
the O
sake O
of O
evaluation O
, O
two O
annotators O
were O
asked O
to O
manually O
evaluate O
two O
sets O
of O
40 O
papers O
' O

groundtruth O
abstracts O
( O
40 O
for O
arXiv-Long B-DatasetName
, O
and O
40 O
for O
PubMed-Long B-DatasetName
) O
with O
their O
generated B-TaskName
extended I-TaskName
summaries I-TaskName
( O
baselines O
' O
and O
ours O
) O
to O
gain O
insights O
into O
qualities O
of O
each O
model. O
Annotators O
were O
Electrical O
Engineering O
and O
Computer O
Science O
PhD O
students O
and O
familiar O
with O
principles O
of O
reading O
scientific O
papers. O
Samples O
were O

randomly O
selected O
from O
the O
test O
set O
, O
one O
from O
each O
40 O
evenly-spaced O
bins O
sorted O
by O
the O
difference O
of O
ROUGE-L B-MetricName
between O
two O
experimented O
systems O
. O
The O
evaluations O
were O
performed O
according O
to O
two O
metrics O
: O
( O
1 O
) O
Cohesion B-MetricName
: O
whether O
the O
ordering O
of O
sentences O
in O
summary O
is O
cohesive O
, O
namely O
sentences O

entail O
each O
other. O
( O
2 O
) O
Completeness B-MetricName
: O
whether O
the O
summary O
covers O
all O
salient O
information O
provided O
in O
the O
ground-truth O
summary. O
To O
prevent O
bias O
in O
selecting O
summaries O
, O
the O
ordering O
of O
systemgenerated O
summaries O
were O
shuffled O
such O
that O
it O
could O
not O
be O
guessed O
by O
the O
annotators. O
Annotators O
were O
asked O
to O
specify O
if O

the O
first O
system-generated O
summary O
wins O
/ O
loses O
or O
ties O
with O
the O
second O
systemgenerated O
summary O
in O
terms O
of O
qualitative O
metrics. O
It O
has O
to O
be O
mentioned O
that O
since O
our O
model O
is O
purely O
extractive O
, O
it O
does O
not O
introduce O
any O
fact O
that O
is O
unfaithful O
to O
the O
source O
. O
Our O
human O
evaluation O
results O
along O

with O
Cohen O
's O
kappa O
( O
Cohen O
, O
1960 O
) O
inter-rater O
agreements O
are O
shown O
in O
Table O
3 O
( O
agr. O
column O
) O
. O
As O
shown O
, O
our O
system O
's O
generated O
summaries O
improve O
completeness B-MetricName
and O
cohesion B-MetricName
in O
over O
40 B-MetricValue
% I-MetricValue
for O
most O
of O
the O
cases O
( O
6 O
out O
of O
8 O
for O
win O
cases O

10 O
) O
. O
Specifically O
, O
when O
comparing O
with O
BERTSUMEXT B-MethodName
, O
we O
see O
that O
68 B-MetricValue
% I-MetricValue
, O
80 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
60 B-MetricValue
% I-MetricValue
, O
66 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
sampled O
summaries O
are O
at O
least O
as O
good O
as O
or O
better O
than O
the O
corresponding O
baseline O
's O
generated O
summaries O
in O

terms O
of O
cohesion O
and O
completeness O
, O
respectively. O
Overall O
, O
across O
two O
metrics O
for O
BERTSUMEXT B-MethodName
and O
BERTSUMEXT-MULTI B-MethodName
, O
we O
gain O
relative O
improvements O
over O
the O
baselines O
: O
25.6 B-MetricValue
% I-MetricValue
, O
19.0 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
56.5 B-MetricValue
% I-MetricValue
, O
[ O
Introductory O
] O
The O
objective O
of O
the O
work O
presented O
here O
is O
to O

study O
the O
mechanism O
of O
radiative O
line O
driving O
and O
the O
corresponding O
properties O
of O
the O
winds O
of O
possible O
generations O
of O
very O
massive O
stars O
at O
extremely O
low O
metallicities O
and O
to O
investigate O
the O
principal O
influence O
of O
these O
winds O
on O
ionizing O
fluxes O
and O
observable O
ultraviolet O
spectra O
. O
[ O
" O
# O
] O
The O
basic O
new O
element O

of O
this O
approach O
, O
needed O
in O
the O
domain O
of O
The O
purpose O
of O
this O
first O
study O
is O
to O
provide O
an O
estimate O
about O
the O
strengths O
of O
stellar O
winds O
at O
very O
low O
metallicity O
for O
very O
massive O
hot O
stars O
in O
a O
mass O
range O
roughly O
between O
100 O
to O
300 O
m O
@ O
xmath3 O
. O
[ O
" O

) O
] O
With O
our O
new O
approach O
to O
describe O
line O
driven O
stellar O
winds O
at O
extremely O
low O
metallicity O
we O
were O
able O
to O
make O
first O
predictions O
of O
stellar O
wind O
properties O
, O
ionizing O
fluxes O
and O
synthetic O
spectra O
of O
a O
possible O
population O
of O
very O
massive O
stars O
in O
this O
range O
of O
metallicity. O
46.7 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName

) O
on O
arXiv-Long B-DatasetName
; O
and O
23.1 B-MetricValue
% I-MetricValue
, O
13.5 B-MetricValue
% I-MetricValue
( O
cohesion B-MetricName
) O
, O
and O
27.7 B-MetricValue
% I-MetricValue
, O
21.9 B-MetricValue
% I-MetricValue
( O
completeness B-MetricName
) O
on O
PubMed-Long. B-DatasetName
11 O
These O
improvements O
, O
qualitatively O
evaluated O
by O
the O
human O
annotators O
, O
show O
the O
promising O
capability O
of O
our O
purposed O
model O
in O
generating O
improved O
extended O
summaries O
which O

are O
more O
preferable O
than O
the O
baselines'. O
We O
observe O
a O
similar O
improvement O
trend O
when O
comparing O
our O
summaries O
with O
BERTSUMEXTMULTI B-MethodName
, O
where O
66 B-MetricValue
% I-MetricValue
, O
77 B-MetricValue
% I-MetricValue
( O
arXiv-Long B-DatasetName
) O
; O
and O
58 B-MetricValue
% I-MetricValue
, O
58 B-MetricValue
% I-MetricValue
( O
PubMed-Long B-DatasetName
) O
of O
our O
summaries O
are O
as O
good O
as O
or O
better O
than O
the O
baseline O

's O
in O
terms O
of O
cohesion O
and O
completeness. O
Looking O
at O
the O
Cohen O
's O
inter-rater O
agreement O
, O
the O
correlation B-MetricName
scores O
fall O
into O
" B-MetricValue
moder- I-MetricValue
11 I-MetricValue
Relative O
improvement O
of O
win O
rate O
over O
lose O
rate O
. O
ate O
" O
agreement O
range O
according O
to O
the O
interpretation O
of O
Cohen O
's O
kappa O
range O
( O
McHugh O
, O
2012 O
) O

. O

-DOCSTART- O
CERES B-MethodName
: O
Pretraining O
of O
Graph-Conditioned O
Transformer O
for O
Semi-Structured O
Session O
Data O
User O
sessions O
empower O
many O
search O
and O
recommendation O
tasks O
on O
a O
daily O
basis. O
Such O
session O
data O
are O
semi-structured O
, O
which O
encode O
heterogeneous O
relations O
between O
queries O
and O
products O
, O
and O
each O
item O
is O
described O
by O
the O
unstructured O
text. O
Despite O
recent O
advances O

in O
self-supervised O
learning O
for O
text O
or O
graphs O
, O
there O
lack O
of O
self-supervised O
learning O
models O
that O
can O
effectively O
capture O
both O
intra-item O
semantics O
and O
inter-item O
interactions O
for O
semi-structured O
sessions. O
To O
fill O
this O
gap O
, O
we O
propose O
CERES B-MethodName
, O
a O
graph-based O
transformer O
model O
for O
semi-structured O
session O
data. O
CERES B-MethodName
learns O
representations O
that O
capture O
both O

inter-and O
intra-item O
semantics O
with O
( O
1 O
) O
a O
graph-conditioned O
masked O
language O
pretraining O
task O
that O
jointly O
learns O
from O
item O
text O
and O
item-item O
relations O
; O
and O
( O
2 O
) O
a O
graph-conditioned O
transformer O
architecture O
that O
propagates O
inter-item O
contexts O
to O
itemlevel O
representations. O
We O
pretrained O
CERES B-MethodName
using O
∼468 O
million O
Amazon O
sessions O
and O
find O
that O
CERES B-MethodName

outperforms O
strong O
pretraining O
baselines O
by O
up O
to O
9 O
% O
in O
three O
session O
search O
and O
entity O
linking O
tasks O
. O
Introduction O
User O
sessions O
are O
ubiquitous O
in O
online O
e-commerce O
stores. O
An O
e-commerce O
session O
contains O
customer O
interactions O
with O
the O
platform O
in O
a O
continuous O
period. O
Within O
one O
session O
, O
the O
customer O
can O
issue O
multiple O
queries O

and O
take O
various O
actions O
on O
the O
retrieved O
products O
for O
these O
queries O
, O
such O
as O
clicking O
, O
adding O
to O
cart O
, O
and O
purchasing. O
Sessions O
are O
important O
in O
many O
e-commerce O
applications O
, O
e.g. O
, O
product O
recommendation O
( O
Wu O
et O
al. O
, O
2019a O
) O
, O
query O
recommendation O
( O
Cucerzan O
and O
White O
, O
2007 O

) O
, O
and O
query O
understanding O
( O
Zhang O
et O
al. O
, O
2020 O
) O
. O
This O
paper O
considers O
sessions O
as O
semi-structured O
data O
, O
as O
illustrated O
in O
Figure O
1. O
At O
the O
higher O
level O
, O
sessions O
are O
heterogeneous O
graphs O
that O
contain O
interactions O
between O
items. O
At O
the O
lower O
level O
, O
each O
graph O
node O
has O
unstructured O

text O
descriptions O
: O
we O
can O
describe O
queries O
by O
search O
keywords O
and O
products O
by O
titles O
, O
attributes O
, O
customer O
reviews O
, O
and O
other O
descriptors. O
Our O
goal O
is O
to O
simultaneously O
encode O
both O
the O
graph O
and O
text O
aspects O
of O
the O
session O
data O
to O
understand O
customer O
preferences O
and O
intents O
in O
a O
session O
context O
. O

Pretraining O
on O
semi-structured O
session O
data O
remains O
an O
open O
problem. O
First O
, O
existing O
works O
on O
learning O
from O
session O
data O
usually O
treat O
a O
session O
as O
a O
sequence O
or O
a O
graph O
( O
Xu O
et O
al. O
, O
2019 O
; O
You O
et O
al. O
, O
2019 O
; O
Qiu O
et O
al. O
, O
2020b O
) O
. O
While O
they O

can O
model O
inter-item O
relations O
, O
they O
do O
not O
capture O
the O
rich O
intra-item O
semantics O
when O
text O
descriptions O
are O
available. O
Furthermore O
, O
these O
models O
are O
usually O
large O
neural O
networks O
that O
require O
massive O
labeled O
data O
to O
train O
from O
scratch. O
Another O
line O
of O
research O
utilizes O
large-scale O
pretrained O
language O
models O
( O
Lan O
et O
al. O
, O

2019 O
; O
Clark O
et O
al. O
, O
2020 O
) O
as O
text O
encoders O
for O
session O
items. O
However O
, O
they O
fail O
to O
model O
the O
relational O
graph O
structure. O
Several O
works O
attempt O
to O
improve O
language O
models O
with O
a O
graph-structured O
knowledge O
base O
, O
such O
as O
in O
( O
Liu O
et O
al. O
, O
2020 O
; O
Yao O
et O
al. O

, O
2019 O
; O
. O
While O
adjusting O
the O
semantics O
of O
entities O
according O
to O
the O
knowledge O
graph O
, O
they O
fail O
to O
encode O
general O
graph O
structures O
in O
sessions O
. O
We O
propose O
CERES B-MethodName
( O
Graph B-MethodName
Conditioned I-MethodName
Encoder I-MethodName
Representations I-MethodName
for I-MethodName
Session I-MethodName
Data I-MethodName
) O
, O
a O
pretraining O
model O
for O
semi-structured O
e-commerce O
session O
data O
, O
which O
can O

serve O
as O
a O
generic O
session O
encoder O
that O
simultaneously O
captures O
both O
intra-item O
semantics O
and O
inter-item O
relations. O
Beyond O
training O
a O
potent O
language O
model O
for O
intra-item O
semantics O
, O
our O
model O
also O
conditions O
the O
language O
modeling O
task O
on O
graph-level O
session O
information O
, O
thus O
encouraging O
the O
pretrained O
model O
to O
learn O
how O
to O
utilize O
inter-item O
signals. O

Our O
model O
architecture O
tightly O
integrates O
two O
key O
components O
: O
( O
1 O
) O
an O
item O
Transformer O
encoder O
, O
which O
captures O
text O
semantics O
of O
session O
items O
; O
and O
( O
2 O
) O
a O
graph O
conditioned O
Transformer O
, O
which O
aggregates O
and O
propagates O
inter-item O
relations O
for O
cross-item O
prediction. O
As O
a O
result O
, O
CERES B-MethodName
models O
the O

higher-level O
interactions O
between O
items O
. O
We O
have O
pretrained O
CERES B-MethodName
using O
468,199,822 O
sessions O
and O
performed O
experiments O
on O
three O
session-based O
tasks O
: O
product O
search O
, O
query O
search O
, O
and O
entity O
linking. O
By O
comparing O
with O
publicly O
available O
state-of-the-art O
language O
models O
and O
domain-specific O
language O
models O
trained O
on O
alternative O
representations O
of O
session O
data O
, O
we O

show O
that O
CERES B-MethodName
outperforms O
strong O
baselines O
on O
various O
session-based O
tasks O
by O
large O
margins. O
Experiments O
show O
that O
CERES B-MethodName
can O
effectively O
utilize O
sessionlevel O
information O
for O
downstream O
tasks O
, O
better O
capture O
text O
semantics O
for O
session O
items O
, O
and O
perform O
well O
even O
with O
very O
scarce O
training O
examples O
. O
We O
summarize O
our O
contributions O
as O
follows O

: O
1 O
) O
We O
propose O
CERES B-MethodName
, O
a O
pretrained O
model O
for O
semistructured O
e-commerce O
session O
data. O
CERES B-MethodName
can O
effectively O
encode O
both O
e-commerce O
items O
and O
sessions O
and O
generically O
support O
various O
sessionbased O
downstream O
tasks. O
2 O
) O
We O
propose O
a O
new O
graph-conditioned O
transformer O
model O
for O
pretraining O
on O
general O
relational O
structures O
on O
text O
data. O
3 O

) O
We O
conducted O
extensive O
experiments O
on O
a O
largescale O
e-commerce O
benchmark O
for O
three O
sessionrelated O
tasks. O
The O
results O
show O
the O
superiority O
of O
CERES B-MethodName
over O
strong O
baselines O
, O
including O
mainstream O
pretrained O
language O
models O
and O
state-ofthe-art O
deep O
session O
recommendation O
models O
. O
Customer O
Sessions O
A O
customer O
session O
is O
the O
search O
log O
before O
a O
final O
purchase O

action. O
It O
consists O
of O
customer-queryproduct O
interactions O
: O
a O
customer O
submits O
search O
queries O
obtains O
a O
list O
of O
products. O
The O
customer O
may O
take O
specific O
actions O
, O
including O
view O
and O
purchase O
on O
the O
retrieved O
products. O
Hence O
, O
a O
session O
contains O
two O
types O
of O
items O
: O
queries O
and O
products O
, O
and O
various O
relations O
between O

them O
established O
by O
customer O
actions O
. O
We O
define O
each O
session O
as O
a O
relational O
graph O
G O
= O
( O
V O
, O
E O
) O
that O
contains O
all O
queries O
and O
products O
in O
a O
session O
and O
their O
relations. O
The O
vertex O
set O
V O
= O
( O
Q O
, O
P O
) O
is O
partitioned O
into O
ordered B-HyperparameterName
query I-HyperparameterName
set I-HyperparameterName
Q I-HyperparameterName

and O
unordered B-HyperparameterName
product I-HyperparameterName
set I-HyperparameterName
P. I-HyperparameterName
The O
queries O
Q B-HyperparameterName
= O
( O
q O
1 O
, O
. O
. O
. O
, O
q O
n O
) O
are O
indexed O
by O
order O
of O
the O
customer O
's O
searches. O
The O
edge B-HyperparameterName
set I-HyperparameterName
E I-HyperparameterName
contains O
two O
types O
of O
edges O
: O
{ O
( O
q O
i O
, O
q O
j O
) O
, O
i O
< O

j O
} O
are O
one-directional O
edges O
that O
connect O
each O
query O
to O
its O
previous O
queries O
; O
and O
{ O
q O
i O
, O
p O
j O
, O
a O
ij O
} O
are O
bidirectional O
edges O
that O
connects O
the O
ith O
query O
and O
jth O
product O
, O
if O
the O
customer O
took O
action O
a O
ij O
on O
product O
p O
j O
retrieved O
by O

query O
q O
j O
. O
The O
queries O
and O
products O
are O
represented O
by O
textual O
descriptions. O
Specifically O
, O
each O
query O
is O
represented O
by O
customer-generated O
search O
keywords. O
Each O
product O
is O
represented O
with O
a O
table O
of O
textual O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
description. O
In O
this O
paper O
, O
we O
call O

" O
product O
sequence O
" O
as O
the O
concatenation O
of O
title O
and O
description. O
A O
product O
may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
their O
specific O
categories O
. O
Our O
Method O
In O
this O
section O
we O
present O
the O
details O
of O
CERES. B-MethodName
We O
first O
describe O

our O
designed O
session O
pretraining O
task O
in O
Section O
3.1 O
, O
and O
then O
describe O
the O
model O
architecture O
of O
CERES B-MethodName
in O
Section O
3.2 O
. O
Graph-Conditioned O
Masked O
Language O
Modeling O
Task O
Suppose O
G O
= O
( O
V O
, O
E O
) O
is O
a O
graph O
on O
T O
text O
items O
as O
vertices O
, O
v O
1 O
, O
. O
. O
. O

, O
v O
T O
, O
each O
of O
which O
is O
a O
sequence O
of O
text O
tokens O
: O
v O
i O
= O
[ O
v O
i1 O
, O
. O
. O
. O
, O
v O
iT O
i O
] O
, O
i O
= O
1 O
, O
. O
. O
. O
, O
T O
. O
We O
propose O
graph-conditioned B-MethodName
masked I-MethodName
language I-MethodName
modeling I-MethodName
( O
GMLM B-MethodName
) O
, O

where O
masked O
tokens O
are O
predicted O
with O
both O
intra-item O
context O
and O
inter-item O
context O
: O
p O
GMLM B-MethodName
( O
v O
masked O
) O
= O
jth O
masked O
P O
( O
vij|G O
, O
{ O
v O
ik O
} O
kth O
unmasked O
) O
, O
( O
1 O
) O
which O
encourages O
the O
model O
to O
leverage O
information O
graph-level O
inter-item O
semantics O
efficiently O
in O
order O

to O
predict O
masked O
tokens. O
To O
optimize O
( O
1 O
) O
, O
we O
need O
to O
learn O
token-level O
embeddings O
that O
are O
infused O
with O
session-level O
information O
, O
which O
we O
introduce O
in O
Section O
3.2.2. O
Suppose O
certain O
tokens O
in O
the O
input O
sequence O
of O
items O
as O
masked O
( O
detailed O
below O
) O
, O
we O
optimize O
the O
predictions O
of O

the O
masked O
tokens O
with O
cross O
entropy O
loss. O
The O
pretraining O
framework O
is O
illustrated O
in O
Figure O
3. O
Token O
Masking O
Strategy O
. O
To O
mask O
tokens O
in O
long O
sequences O
, O
including O
product O
titles O
and O
descriptions O
, O
we O
follow O
( O
Devlin O
et O
al. O
, O
2018 O
) O
and O
choose O
15 O
% O
of O
the O
tokens O
for O
masking. O

For O
short O
sequences O
, O
including O
queries O
and O
product O
attributes O
, O
there O
is O
a O
50 O
% O
probability O
that O
a O
short O
sequence O
will O
be O
masked O
, O
and O
for O
those O
sequences O
50 O
% O
of O
their O
tokens O
are O
randomly O
selected O
for O
masking O
. O
Model O
Architecture O
To O
model O
the O
probability O
in O
( O
1 O
) O
, O

we O
design O
two O
key O
components O
in O
the O
CERES B-MethodName
model O
: O
1 O
) O
a O
Transformer-based O
item O
encoder O
, O
which O
produces O
token-level O
intra-item O
embeddings O
that O
contain O
context O
information O
within O
a O
single O
item O
; O
and O
2 O
) O
a O
graph-conditioned O
Transformer O
for O
session O
encoding O
, O
which O
produces O
session-level O
embeddings O
that O
encodes O
inter-item O
relations O
, O

and O
propagates O
the O
session O
information O
back O
to O
the O
token-level. O
We O
illustrate O
our O
model O
architecture O
in O
Figure O
2 O
. O
Item O
Transformer O
Encoder O
The O
session O
item O
encoder O
aims O
to O
encode O
intra-item O
textual O
information O
for O
each O
item O
in O
a O
session. O
We O
design O
the O
item O
encoder O
based O
on O
Transformers O
, O
which O
allows O
CERES B-MethodName
to O

leverage O
the O
expressive O
power O
of O
the O
self-attention O
mechanism O
for O
modeling O
domain-specific O
language O
in O
e-commerce O
sessions. O
Given O
an O
item O
i O
, O
the O
transformer-based O
item O
encoder O
compute O
its O
token O
embeddings O
as O
follows O
: O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
= O
Transformeritem O
( O
[ O
vi1 O
, O
. O
. O
. O

, O
viT O
i O
] O
) O
vi O
= O
Pool O
( O
[ O
vi1 O
, O
. O
. O
. O
, O
viT O
i O
] O
) O
, O
( O
2 O
) O
where O
v O
ij O
is O
the O
embedding O
of O
the O
jth O
token O
in O
the O
ith O
item O
, O
and O
v O
i O
is O
the O
pooled O
embedding O
of O
the O
ith O
item. O

At O
this O
stage O
, O
{ O
v O
ij O
} O
, O
{ O
v O
i O
} O
are O
embeddings O
that O
only O
encode O
the O
intra-item O
information. O
Details O
of O
Item O
Encoding. O
We O
detail O
the O
encoding O
method O
for O
the O
two O
types O
of O
items O
, O
queries O
and O
products O
, O
in O
the O
following O
paragraphs. O
Each O
query O
q O
i O
= O

[ O
q O
i1 O
, O
. O
. O
. O
, O
q O
iT O
i O
] O
is O
a O
sequence O
of O
tokens O
generated O
by O
customers O
as O
search O
keywords. O
We O
add O
a O
special O
token O
at O
the O
beginning O
of O
the O
queries O
, O
[ O
SEARCH O
] O
, O
to O
indicate O
that O
the O
sequence O
represents O
a O
customer O
's O
search O
keywords. O

Then O
, O
to O
obtain O
the O
token-level O
embedding O
of O
the O
queries O
and O
the O
pooled O
query O
embedding O
by O
taking O
the O
embedding O
of O
the O
special O
token O
[ O
SEARCH O
] O
. O
Each O
product O
p O
i O
is O
a O
table O
of O
K O
attributes O
: O
p O
1 O
, O
. O
. O
. O
, O
p O
K O
, O
where O
p O

1 O
is O
always O
the O
product O
sequence O
, O
which O
is O
the O
concatenation O
of O
product O
title O
and O
bullet O
description. O
Each O
attribute O
p O
k O
i O
= O
[ O
p O
k O
i1 O
, O
p O
k O
i2 O
, O
. O
. O
. O
] O
starts O
with O
a O
special O
token O
[ O
ATTRTYPE O
] O
, O
where O
ATTRTYPE O
is O
replaced O
with O

the O
language O
descriptor O
of O
the O
attribtue. O
Then O
, O
the O
Transformer O
is O
used O
to O
compute O
token O
and O
sentence O
embeddings O
for O
all O
attributes. O
The O
product O
embedding O
is O
obtained O
by O
average O
pooling O
of O
all O
attribute O
's O
sentence O
embeddings O
. O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
The O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
aims O
to O
infuse O
intra-item O
and O
inter-item O
information O

to O
produce O
item O
and O
token O
embeddings. O
For O
this O
purpose O
, O
we O
first O
design O
a O
position-aware B-MethodName
graph I-MethodName
neural I-MethodName
network I-MethodName
( O
PGNN B-MethodName
) O
to O
capture O
the O
interitem O
dependencies O
in O
a O
session O
graph O
to O
produce O
Item B-MethodName
Token I-MethodName
Embeddings I-MethodName
Latent B-MethodName
Conditioning I-MethodName
Tokens I-MethodName
Figure O
4 O
: O
Illustration O
of O
cross-attention O
over O
latent O
conditioning O
tokens. O
The O
item O

token O
embeddings O
perform O
self-attention O
as O
well O
as O
cross-attention O
over O
latent O
conditioning O
tokens O
, O
thus O
incorporating O
session-level O
information. O
Latent O
conditioning O
tokens O
perform O
selfattention O
to O
update O
their O
embeddings O
, O
but O
do O
not O
attend O
to O
item O
tokens O
to O
preserve O
session-level O
information O
. O
item O
embeddings. O
The O
effect O
of O
PGNN B-MethodName
is O
analyzed O
in O
Section O
4.4. O

Then O
conditioned O
on O
the O
PGNN-learned B-MethodName
item O
embedding O
, O
we O
propose O
a O
cross-attention O
Transformer O
, O
which O
produces O
infused O
item O
and O
token O
embeddings O
for O
the O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
task I-MethodName
. O
Position-Aware B-MethodName
Graph I-MethodName
Neural I-MethodName
Network. I-MethodName
We O
use O
a O
GNN B-MethodName
to O
capture O
inter-item O
relations. O
This O
will O
allow O
CERES B-MethodName
to O
obtain O
item O
embeddings O
that O

encode O
the O
information O
from O
other O
locally O
correlated O
items O
in O
the O
session. O
Let O
[ O
v O
1 O
, O
. O
. O
. O
, O
v O
N O
] O
denote O
the O
item O
embeddings O
produced O
by O
the O
intra-item O
transformer O
encoder. O
We O
treat O
them O
as O
hidden O
states O
of O
nodes O
in O
the O
session O
graph O
G O
and O
feed O
them O
to O

the O
GNN B-MethodName
model O
, O
obtaining O
session-level O
item O
embeddings O
[ O
v O
h O
1 O
, O
. O
. O
. O
, O
v O
h O
N O
] O
. O
The O
items O
in O
a O
session O
graph O
are O
sequential O
according O
to O
the O
order O
the O
customers O
generated O
them O
. O
To O
let O
the O
GNN B-MethodName
model O
learn O
of O
the O
positional O
information O
of O

items O
, O
we O
train O
an O
item O
positional O
embedding O
in O
the O
same O
way O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
trains O
positional O
embeddings O
of O
tokens. O
Before O
feeding O
the O
item O
embeddings O
to O
GNN B-MethodName
, O
the O
pooled O
item O
embeddings O
are O
added O
item O
positional O
embeddings O
according O
to O
their O
positions O
in O
the O
session O
's O

item O
sequence. O
In O
this O
way O
, O
the O
item O
embeddings O
{ O
v O
i O
} O
i∈V O
are O
encoded O
their O
positional O
information O
as O
well O
. O
Cross-Attention B-MethodName
Transformer. I-MethodName
Conditioned O
on O
PGNN B-MethodName
, O
we O
design O
a O
cross-attention O
transformer O
which O
propagates O
session-level O
information O
in O
PGNN-produced B-MethodName
item O
embeddings O
to O
all O
tokens O
to O
produce O
token O
embeddings O
that O
are O

infused O
with O
both O
intra-item O
and O
inter-item O
information O
. O
In O
order O
to O
propagate O
item O
embeddings O
to O
tokens O
, O
we O
treat O
item O
embeddings O
as O
latent O
tokens O
that O
can O
be O
treated O
as O
a O
" O
part O
" O
of O
item O
texts. O
for O
each O
item O
i O
, O
we O
first O
expand O
v O
h O
i O
to O
K O
latent O

conditioning O
tokens O
by O
using O
a O
multilayer O
perceptron O
module O
to O
map O
v O
h O
i O
to O
K O
embedding O
vectors O
[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
] O
of O
the O
same O
size. O
For O
each O
item O
i O
, O
we O
compute O
its O
latent O
conditioning O
tokens O
by O
averaging O
all O
latent O
tokens O

in O
its O
neighborhood. O
Suppose O
N O
( O
i O
) O
is O
the O
set O
of O
all O
neighboring O
items O
in O
the O
session O
graph O
, O
itself O
included. O
In O
each O
position O
, O
we O
take O
the O
average O
of O
the O
latent O
token O
embeddings O
in O
N O
( O
i O
) O
as O
the O
kth O
latent O
conditioning O
token O
, O
v O
h O
ik O

, O
for O
the O
ith O
item. O
Then O
, O
we O
concatenate O
the O
latent O
conditioning O
token O
embeddings O
and O
the O
item O
token O
embeddings O
obtained O
by O
the O
session O
item O
encoder O
: O
[ O
v O
h O
i1 O
, O
. O
. O
. O
, O
v O
h O
iK O
, O
v O
i1 O
, O
. O
. O
. O
, O
v O
iN O
i O
] O

. O
( O
3 O
) O
Finally O
, O
we O
compute O
the O
token-level O
embeddings O
with O
session O
information O
by O
feeding O
the O
concatenated O
sequence O
to O
a O
shallow O
cross-attention O
Transformer. O
The O
cross-attention O
Transformer O
is O
of O
the O
same O
structure O
as O
normal O
Transformers. O
The O
difference O
is O
that O
we O
prohibit O
the O
latent O
conditioning O
tokens O
from O
attending O
over O
original O
item O

tokens O
to O
prevent O
the O
influx O
of O
intra-item O
information O
potentially O
diluating O
session-level O
information O
stored O
in O
latent O
conditioning O
tokens. O
Illustration O
of O
crossattention O
Transformer O
is O
provided O
in O
Figrue O
4 O
. O
We O
use O
the O
embeddings O
produced O
by O
this O
crossattention O
Transformer O
as O
the O
final O
embeddings O
for O
modeling O
the O
token O
probabilities O
in O
Equation O
( O
1 O
) O

and O
learning O
the O
masked O
language O
modeling O
tasks O
. O
During O
training O
, O
the O
model O
is O
encouraged O
to O
learn O
good O
token O
embeddings O
with O
the O
Item O
Transformer O
Encoder O
, O
as O
better O
embeddings O
{ O
v O
ij O
} O
N O
i O
j=1 O
is O
necessary O
to O
improve O
the O
quality O
of O
{ O
v O
c O
ij O
} O
N O
i O

j=1 O
. O
The O
Graph-Conditioned B-MethodName
Transformer I-MethodName
will O
be O
encouraged O
to O
produce O
high-quality O
session-level O
embeddings O
for O
the O
GMLM B-MethodName
task. O
Hence O
, O
CERES B-MethodName
is O
encouraged O
to O
produce O
high-quality O
embeddings O
that O
unify O
both O
intra-item O
and O
inter-item O
information O
. O
Finetuning O
When O
finetuning O
CERES B-MethodName
for O
downstream O
tasks O
, O
we O
first O
obtain O
session-level O
item O
embeddings. O
The O
session O

embedding O
is O
computed O
as O
the O
average O
of O
all O
item O
embeddings. O
To O
obtain O
embedding O
for O
a O
single O
item O
without O
session O
context O
, O
such O
as O
for O
retrieved O
items O
in O
recommendation O
tasks O
, O
only O
the O
Item O
Transformer O
Encoder O
is O
used O
. O
To O
measure O
the O
relevance O
of O
an O
item O
to O
a O
given O
session O
, O

we O
first O
transform O
the O
obtained O
embeddings O
by O
separate O
linear O
maps. O
Denote O
the O
transformed O
session O
embeddings O
as O
s O
and O
item O
embeddings O
as O
y. O
The O
similarity O
between O
them O
is O
computed O
by O
cosine O
similarity O
d O
cos O
( O
s O
, O
y O
) O
. O
To O
finetune O
the O
model O
, O
we O
optimize O
a O
hinge O
loss O
on O

the O
cosine O
similarity O
between O
sessions O
and O
items O
. O
Experiments O
Experiment O
Setup O
Dataset. O
We O
collected O
customer B-DatasetName
sessions I-DatasetName
from O
Amazon O
for O
pretraining O
and O
finetuning O
on O
downstream O
tasks. O
468,199,822 O
customer B-DatasetName
sessions I-DatasetName
are O
collected O
from O
August O
1 O
2020 O
to O
August O
31 O
2020 O
for O
pretraining. O
30,000 O
sessions O
are O
collected O
from O
September O
2020 O
to O
September O
7 O

2020 O
for O
downstream O
tasks. O
The O
pretraining O
and O
downstreaming O
datasets O
are O
from O
disjoint O
time O
spans O
to O
prevent O
data O
leakage. O
All O
data O
are O
cleaned O
and O
anonymized O
so O
that O
no O
personal O
information O
about O
customers O
was O
used. O
Each O
session O
is O
collected O
as O
follows O
: O
when O
a O
customer O
perform O
a O
purchase O
action O
, O
we O
backtrace O

all O
actions O
by O
the O
customer O
in O
600 O
seconds O
before O
the O
purchase O
until O
a O
previous O
purchase O
is O
encountered. O
The O
actions O
of O
customers O
include O
: O
1 O
) O
search O
, O
2 O
) O
view O
, O
3 O
) O
, O
add-to-cart O
, O
and O
4 O
) O
purchase. O
Search O
action O
is O
associated O
with O
customer O
generated O
query O
keywords. O
View O

, O
add-to-cart O
, O
and O
purchase O
are O
associated O
with O
the O
target O
products. O
All O
the O
products O
in O
the O
these O
sessions O
are O
gathered O
with O
their O
product O
title O
, O
bullet O
description O
, O
and O
various O
other O
attributes O
, O
including O
color O
, O
manufacturer O
, O
product O
type O
, O
size O
, O
etc. O
In O
total O
, O
we O
have O
37,580,637 O

products. O
The O
sessions O
have O
an O
average O
of O
3.24 O
queries O
and O
4.36 O
products. O
Queries O
have O
on O
average O
5.63 O
tokens O
, O
while O
product O
titles O
and O
bullet O
descriptions O
have O
averagely O
17.42 O
and O
96.01 O
tokens O
. O
Evaluation O
Tasks O
and O
Metrics. O
We O
evaluate O
all O
the O
compared O
models O
on O
the O
following O
tasks O
: O
1 O
) O
Product B-TaskName

Search. I-TaskName
In O
this O
task O
, O
given O
observed O
customer O
behaviors O
in O
a O
session O
, O
the O
model O
is O
asked O
to O
predict O
which O
product O
will O
be O
purchased O
from O
a O
pool O
of O
candidate O
products. O
The O
purchased O
products O
are O
removed O
from O
sessions O
to O
avoid O
trivial O
inference. O
The O
candidate O
product O
pool O
is O
the O
union O
of O
all O

purchased O
products O
in O
the O
test O
set O
and O
the O
first O
10 O
products O
returned O
by O
the O
search O
engine O
of O
all O
sessions O
in O
the O
test O
set O
. O
2 O
) O
Query B-TaskName
Search. I-TaskName
Query O
Search O
is O
a O
recommendation O
task O
where O
the O
model O
retrieves O
next O
queries O
for O
customers O
which O
will O
lead O
to O
a O
purchase. O
Given O
a O

session O
, O
we O
hide O
the O
last O
query O
along O
with O
products O
associated O
with O
it O
, O
i.e. O
viewed O
or O
purchased O
with O
the O
removed O
query. O
Then O
, O
we O
ask O
the O
model O
to O
predict O
the O
last O
query O
from O
a O
pool O
of O
candidate O
queries. O
The O
candidate O
query O
pool O
consists O
of O
all O
last O
queries O
in O
the O

test O
set O
. O
3 O
) O
Entity B-TaskName
Linking. I-TaskName
In O
this O
task O
we O
try O
to O
understand O
the O
deeper O
semantics O
of O
customer O
sessions. O
Specifically O
, O
if O
customer O
purchases O
a O
product O
in O
a O
session O
, O
the O
task O
is O
to O
predict O
the O
attributes O
of O
the O
purchased O
product O
from O
the O
rest O
contexts O
in O
the O
session. O
In O

total O
, O
we O
have O
60K O
possible O
product O
attributes O
. O
Baselines. O
The O
compared O
baselines O
can O
be O
categorized O
into O
three O
groups O
: O
1 O
) O
General-domain O
pretrained O
language O
models O
which O
include O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
, O
RoBERTa B-MethodName
, O
and O
ELECTRA B-MethodName
( O
Clark O
et O
al. O
, O
2020 O
) O
. O
These O

models O
are O
state-of-the-art O
pretrained O
language O
models O
, O
which O
can O
serve O
as O
general-purpose O
language O
encoders O
for O
items O
and O
enable O
downstream O
session-related O
tasks. O
Specifically O
, O
the O
language O
encoders O
produce O
item O
embeddings O
first O
, O
and O
compose O
session O
embeddings O
by O
pooling O
the O
items O
in O
sessions. O
To O
retrieve O
items O
for O
sessions O
, O
one O
can O
compare O

the O
cosine O
similarity O
between O
sessions O
and O
retrieved O
items O
. O
2 O
) O
Pretrained O
session O
models O
which O
are O
pretrained O
models O
on O
e-commerce O
session O
data. O
Specifically O
, O
we O
pretrain O
the O
following O
language O
models O
using O
our O
session O
data O
: O
a O
) O
Product-BERT B-MethodName
, O
which O
is O
a O
domain-specific O
BERT B-MethodName
model O
pretrained O
with O
product O
information O
; O

b O
) O
SQSP-BERT B-MethodName
, O
where O
SQSP B-MethodName
is O
short O
for O
Single-query O
Single-Product. O
SQSP-BERT B-MethodName
is O
pretrained O
on O
query-product O
interaction O
pairs O
with O
language O
modeling O
and O
contrastive O
learning O
objectives. O
They O
are O
used O
in O
the O
same O
manner O
in O
downstream O
tasks O
as O
general-domain O
pretrained O
language O
models. O
The O
detailed O
configurations O
are O
provided O
in O
the O
Appendix O
. O
3 O

) O
Session-based O
recommendation O
methods O
including O
SR-GNN B-MethodName
( O
Wu O
et O
al. O
, O
2019b O
) O
and O
NISER+ B-MethodName
( O
Gupta O
et O
al. O
, O
2019 O
) O
, O
which O
are O
state-ofthe-art O
models O
for O
session-based O
product O
recommendation O
on O
traditional O
benchmarks O
, O
including O
YOOCHOOSE B-DatasetName
and O
DIGINETICA B-DatasetName
; O
and O
Nvidia B-MethodName
's I-MethodName
MERLIN I-MethodName
( O
Mobasher O
et O
al. O
, O
2001 O

) O
, O
which O
is O
the O
bestperforming O
model O
in O
the O
recent O
SIGIR O
Next O
Items O
Prediction O
challenge O
( O
Kallumadi O
et O
al. O
, O
2021 O
) O
To O
evaluate O
the O
performance O
on O
these O
tasks O
, O
we O
employ O
standard O
metrics O
for O
recommendation O
systems O
, O
including O
MAP B-MetricName
@ I-MetricName
K I-MetricName
, I-MetricName
and O
Recall B-MetricName
@ I-MetricName
K I-MetricName
. O
Implementation O
Details O

The O
implementation O
details O
for O
pretraining O
and O
finetuning O
stages O
are O
described O
as O
follows O
. O
Pretraining O
details. O
We O
developed O
our O
model O
based O
on O
Megatron-LM B-MethodName
( O
Shoeybi O
et O
al. O
, O
2019 O
) O
. O
We O
used O
768 B-HyperparameterValue
as O
the O
hidden B-HyperparameterName
size I-HyperparameterName
, O
a O
12-layer O
transformer O
blocks O
as O
the O
backbone O
language O
model O
, O
a O
twolayer O

Graph B-HyperparameterName
Attention I-HyperparameterName
Network I-HyperparameterName
and O
three-layer O
Transformer O
as O
the O
conditioned O
language O
model O
layers. O
In O
total O
, O
our O
model O
has O
141M O
parameters. O
The O
model O
is O
trained O
for O
300,000 O
steps O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
sessions. O
The O
parameters O
are O
updated O
with O
Adam O
, O
with O
peak O
learning B-HyperparameterName
rate I-HyperparameterName
as O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
, O

1 B-HyperparameterValue
% I-HyperparameterValue
steps B-HyperparameterName
for O
linear O
warm-up O
, O
and O
linear O
learning B-HyperparameterName
rate I-HyperparameterName
decay O
after O
warm-up O
until O
the O
learning B-HyperparameterName
rate I-HyperparameterName
reaches O
the O
minimum O
1e B-HyperparameterValue
− I-HyperparameterValue
5. I-HyperparameterValue
We O
trained O
our O
model O
on O
16 O
A400 O
GPUs O
on O
Amazon O
AWS O
for O
one O
week. O
Finetuning O
details. O
For O
each O
downstream O
task O
, O
we O
collected O
30,000 O
sessions O
for O

training O
, O
3000 O
for O
validation O
and O
5000 O
for O
testing. O
For O
each O
of O
the O
pretrained O
model O
, O
we O
finetune O
them O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
maximal O
learning B-HyperparameterName
rate I-HyperparameterName
chosen O
from O
[ O
1e-4 O
, O
1e-5 O
, O
5e-5 O
, O
5e-6 O
] O
to O
maximize O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
on O
the O
validation O
set. O
The O
rest O
of O

the O
configuration O
of O
optimizers O
is O
the O
same O
as O
in O
pretraining O
. O
Main O
Results O
Product B-TaskName
Search I-TaskName
Table O
1 O
shows O
the O
performance O
of O
different O
methods O
for O
the O
product B-TaskName
search I-TaskName
task. O
We O
observe O
that O
CERES B-MethodName
outperforms O
domain-specific O
methods O
by O
more O
than O
1 B-MetricValue
% I-MetricValue
and O
general-domain O
methods O
by O
over O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName

1. O
The O
second O
best O
performing O
model O
is O
Product-BERT B-MethodName
, O
which O
is O
pretrained O
on O
product O
information O
alone O
. O
We O
also O
compared O
with O
session-based O
recommendation O
systems. O
SR-GNN B-MethodName
and O
NISER+ B-MethodName
model O
only O
session O
graph O
structure O
but O
not O
text O
semantics O
; O
hence O
they O
have O
limited O
performance O
because O
of O
the O
suboptimal O
representation O
of O
session O
items. O

While O
MERLIN B-MethodName
can O
capture O
better O
text O
semantics O
, O
its O
text O
encoder O
is O
not O
trained O
on O
domain-specific O
e-commerce O
data. O
While O
it O
can O
outperform O
generaldomain O
methods O
, O
its O
performance O
is O
lower O
than O
Product-BERT B-MethodName
and O
CERES. B-MethodName
The O
benefits O
of O
joint O
modeling O
of O
text O
and O
graph O
data O
and O
the O
Graph-Conditioned B-MethodName
MLM I-MethodName
allow O
CERES B-MethodName
to O

outperform O
existing O
session O
recommendation O
models O
. O
Query B-TaskName
Search I-TaskName
Table O
2 O
shows O
the O
performance O
of O
different O
methods O
on O
Query B-TaskName
Search. I-TaskName
Query B-TaskName
Search I-TaskName
is O
a O
more O
difficult O
task O
than O
Product B-TaskName
Search I-TaskName
because O
customergenerated O
next O
queries O
are O
of O
higher O
variance. B-MetricName
In O
this O
challenging O
task O
, O
CERES B-MethodName
outperforms O
the O
best O
domain-specific O
model O
by O
over O

7 B-MetricValue
% I-MetricValue
and O
generaldomain O
model O
by O
12 B-MetricValue
% I-MetricValue
in O
all O
metrics O
. O
Entity B-TaskName
Linking I-TaskName
Table O
3 O
shows O
the O
results O
on O
Entity B-TaskName
Linking. I-TaskName
Similar O
to O
Query B-DatasetName
Search I-DatasetName
, O
this O
task O
also O
requires O
the O
models O
to O
tie O
text O
semantics O
( O
queries O
/ O
product O
attributes O
) O
to O
a O
customer B-DatasetName
session I-DatasetName
, O
which O
requires O

a O
deeper O
understanding O
of O
customer O
preferences. O
It O
is O
easier O
than O
Query B-TaskName
Search I-TaskName
as O
product O
attributes O
are O
of O
lower O
variance. O
However O
, O
the O
product O
attributes O
that O
the O
customer O
prefer O
rely O
more O
on O
session O
information O
, O
as O
they O
may O
have O
been O
reflected O
in O
the O
past O
search O
queries O
and O
viewed O
products. O
In O
this O

task O
, O
CERES B-MethodName
outperforms O
domain-specific O
models O
and O
general-domain O
models O
by O
averagely O
9 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
1 I-MetricName
and O
6 B-MetricValue
% I-MetricValue
in O
MAP B-MetricName
@ I-MetricName
32 I-MetricName
and O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
. O
Further O
Analysis O
and O
Ablation O
Studies O
In O
this O
section O
we O
present O
further O
studies O
to O
understand O
: O
1 O
) O
the O
effect O
of O
training O

data O
sizes O
in O
the O
downstream O
task O
; O
2 O
) O
the O
effects O
of O
different O
components O
in O
CERES B-MethodName
for O
both O
the O
pretraining O
and O
finetuning O
stages. O
following O
observations O
: O
CERES B-MethodName
is O
highly O
effective O
when O
training O
data O
are O
scarce O
. O
We O
compare O
CERES B-MethodName
with O
two O
strongest O
baselines O
( O
BERT B-MethodName
, O
and O
Product-BERT B-MethodName
) O
when O

the O
training O
sample O
size O
varies. O
Figure O
5 O
shows O
the O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
these O
methods O
on O
Product B-TaskName
Search I-TaskName
and O
Query B-TaskName
Search I-TaskName
when O
training O
size O
varies. O
Clearly O
, O
the O
advantage O
of O
CERES B-MethodName
is O
greater O
when O
training O
data O
is O
extremely O
small. O
With O
a O
training O
size O
of O
300 O
, O
CERES B-MethodName
can O
achieve O

a O
decent O
performance O
of O
about O
37.55 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
and O
36.37 B-MetricValue
% O
in O
Query B-TaskName
Search I-TaskName
, O
while O
the O
baseline O
models O
can O
not O
be O
trained O
sufficiently O
with O
such O
small-sized O
data. O
This O
shows O
that O
the O
efficient O
utilization O
of O
session-level O
information O
in O
pretraining O
and O
fine-tuning O
stages O
make O
the O
model O
more O
data O
efficient O

than O
other O
pretrained O
models O
. O
Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
Vital O
to O
Pretraining. O
Without O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
pretraining O
, O
our O
model O
is O
essentially O
the O
same O
as O
domain-specific O
baselines O
, O
such O
as O
Product-BERT B-MethodName
, O
which O
are O
trained O
on O
session O
data O
but O
only O
with O
intra-item O
text O
signals. O
While O
SQSP-BERT B-MethodName
has O
access O
to O
session-level O

information O
when O
maximizing O
the O
masked O
language O
modeling O
objective O
, O
the O
lack O
of O
a O
dedicated O
module O
for O
GMLM B-MethodName
results O
in O
worse O
performance O
, O
as O
shown O
in O
the O
main O
experiment O
results. O
We O
could O
train O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
from O
scratch O
in O
the O
finetuning O
stage. O
We O
present O
a O
model O
called O
CERES B-MethodName
w O
/ O
o O

Pretrain O
, O
which O
attaches O
the O
Graph-Conditioned B-MethodName
Session I-MethodName
Transformer I-MethodName
to O
Product-BERT B-MethodName
as O
the O
Item O
Transformer O
Encoder. O
As O
shown O
in O
Figure O
6 O
, O
this O
ablation O
method O
achieves O
MAP B-MetricName
@ I-MetricName
64 I-MetricName
scores O
of O
89.341 B-MetricValue
% I-MetricValue
in O
Product B-TaskName
Search I-TaskName
, O
64.890 B-MetricValue
% I-MetricValue
in O
Query B-TaskName
Search I-TaskName
, O
and O
74.031 B-MetricValue
% I-MetricValue
in O
Entity B-TaskName
Linking I-TaskName
, O
which O

are O
below O
Product-BERT. B-MethodName
This O
shows O
that O
the O
pretraining O
stage O
of O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
is O
necessary O
to O
facilitate O
its O
ability O
to O
aggregate O
and O
propagate O
session-level O
information O
for O
downstream O
tasks O
. O
Graph-Conditioned B-MethodName
Transformer I-MethodName
Improves O
Item-level O
Embeddings. O
We O
also O
present O
CERES B-MethodName
w O
/ O
o O
Cond O
, O
which O
has O
the O
same O
pretrained O
model O
as O

CERES B-MethodName
, O
but O
only O
uses O
the O
Item O
Transformer O
Encoder O
in O
the O
finetuning O
stage. O
The O
Item O
Transformer O
Encoder O
is O
used O
to O
compute O
session O
item O
embeddings O
that O
contain O
only O
item-level O
information O
, O
and O
then O
takes O
the O
average O
of O
these O
embeddings O
as O
session O
embedding. O
As O
shown O
in O
Figure O
6 O
, O
CERES B-MethodName
w O
/ O

o O
Cond O
acheives O
94.741 B-MetricValue
% I-MetricValue
, O
72.175 B-MetricValue
% I-MetricValue
, O
and O
81.03 B-MetricValue
% I-MetricValue
respectively O
in O
Product B-TaskName
Search I-TaskName
, O
Query B-TaskName
Search I-TaskName
, O
and O
Entity B-TaskName
Linking I-TaskName
, O
observing O
a O
drop B-MetricName
of O
0.1 B-MetricValue
% I-MetricValue
to O
0.2 B-MetricValue
% I-MetricValue
in O
performance O
compared O
with O
CERES. B-MethodName
The O
performance O
drop O
is O
minor O
and O
CERES B-MethodName
w O
/ O
o O
Cond O
still O

outperforms O
baseline O
pretrained O
language O
models. O
Hence O
, O
the O
Graph-Conditioned B-MethodName
Transformer I-MethodName
in O
the O
pretraining O
stage O
helps O
the O
Item O
Transformer O
Encoder O
to O
learn O
better O
item-level O
embeddings O
that O
can O
be O
used O
for O
more O
effective O
leveraging O
of O
session O
information O
in O
the O
downstream O
tasks O
. O
Graph B-MethodName
Neural I-MethodName
Networks I-MethodName
Improve O
Representation O
of O
Sessions. O
In O
CERES B-MethodName
w O

/ O
o O
GNN B-MethodName
, O
we O
pretrain O
a O
CERES B-MethodName
model O
without O
a O
Graph B-MethodName
Neural I-MethodName
Network. I-MethodName
Specifically O
, O
CERES B-MethodName
w O
/ O
o O
GNN B-MethodName
skips O
the O
neighborhood O
information O
aggregation O
for O
items O
, O
and O
uses O
item-level O
embeddings O
obtained O
by O
the O
Item O
Transformer O
Encoder O
directly O
as O
latent O
conditioning O
tokens. O
We O
train O
and O
finetune O
this O
model O

with O
the O
same O
setup O
as O
CERES. B-MethodName
Without O
GNN B-MethodName
, O
the O
model O
's O
performance O
is O
consistently O
lower O
than O
CERES B-MethodName
, O
achieving O
93.453 B-MetricValue
% I-MetricValue
, O
71.231 B-MetricValue
% I-MetricValue
, O
80.26 B-MetricValue
% I-MetricValue
MAP B-MetricName
@ I-MetricName
64 I-MetricName
in O
three O
downstream O
tasks O
, O
observing O
a O
1.13 B-MetricValue
% I-MetricValue
performance O
drop. B-MetricName
This O
shows O
that O
GNN B-MethodName
's I-MethodName
aggregation O
of O
information O

can O
help O
item-level O
embeddings O
encode O
more O
session-level O
information O
, O
improving O
performance O
in O
downstream O
tasks O
. O
Model O
Efficiency. O
CERES B-MethodName
has O
additional O
few O
GNN B-MethodName
and O
Transformer O
layers O
attached O
to O
the O
end O
of O
the O
model. O
The O
additional O
layers O
bring O
∼20 O
% O
additional O
inference O
time O
compared O
to O
standard O
BERT B-MethodName
with O
12 B-HyperparameterValue
layers B-HyperparameterName
and O
768 B-HyperparameterValue

hidden B-HyperparameterName
size I-HyperparameterName
. O
Conclusion O
We O
proposed O
a O
pretraining O
framework O
, O
CERES B-MethodName
, O
for O
learning O
representations O
for O
semi-structured O
ecommerce O
sessions. O
We O
are O
the O
first O
to O
jointly O
model O
intra-item O
text O
and O
inter-item O
relations O
in O
session O
graphs O
with O
an O
end-to-end O
pretraining O
framework. O
By O
modeling O
Graph-Conditioned B-MethodName
Masked I-MethodName
Language I-MethodName
Modeling I-MethodName
, O
our O
model O
is O
encouraged O

to O
learn O
high-quality O
representations O
for O
both O
intraitem O
and O
inter-item O
information O
during O
its O
pretraining O
on O
massive O
unlabeled O
session O
graphs. O
Furthermore O
, O
as O
a O
generic O
session O
encoder O
, O
our O
model O
enabled O
effective O
leverage O
of O
session O
information O
in O
downstream O
tasks. O
We O
conducted O
extensive O
experiments O
and O
ablation O
studies O
on O
CERES B-MethodName
in O
comparison O
to O
state-of-the-art O

pretrained O
models O
and O
recommendation O
systems. O
Experiments O
show O
that O
CERES B-MethodName
can O
produce O
higher O
quality O
text O
representations O
as O
well O
as O
better O
leverage O
of O
session O
graph O
structure O
, O
which O
are O
important O
to O
many O
ecommerce O
related O
tasks O
, O
including O
product B-TaskName
search I-TaskName
, O
query B-TaskName
search I-TaskName
, O
and O
query B-TaskName
understanding I-TaskName
. O
A O
Details O
on O
Session O
Data O

A.1 O
Product O
Attributes O
. O
A O
product O
is O
represented O
with O
a O
table O
of O
attributes. O
Each O
product O
is O
guaranteed O
to O
have O
a O
product O
title O
and O
bullet O
description. O
In O
this O
paper O
, O
we O
regard O
the O
product O
title O
as O
the O
representative O
sequence O
of O
the O
product O
, O
called O
" O
product O
sequence O
" O
. O
A O
product O

may O
have O
additional O
attributes O
, O
such O
as O
product O
type O
, O
color O
, O
brand O
, O
and O
manufacturer O
, O
depending O
on O
specific O
products O
. O
A.2 O
Alternative O
Pretraining O
Corpora O
In O
this O
section O
we O
introduce O
alternative O
pretraining O
corpora O
that O
encode O
information O
in O
a O
session O
, O
including O
products O
and O
queries O
, O
but O
not O
treating O
sessions O

as O
a O
whole O
. O
A.2.1 O
Product B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
gathered O
all O
product O
information O
that O
appeared O
in O
the O
sessions O
from O
August O
2020 O
to O
September O
2020. O
Each O
product O
will O
have O
descriptions O
such O
as O
product O
title O
and O
bullet O
description O
, O
and O
other O
attributes O
like O
entity O
type O
, O
product O
type O
, O

manufacturer O
, O
etc. O
Particularly O
, O
bullet O
description O
is O
composed O
of O
several O
lines O
of O
descriptive O
facts O
about O
the O
product. O
All O
products O
without O
titles O
are O
removed. O
Each O
of O
the O
remaining O
product O
forms O
a O
paragraph O
, O
where O
the O
product O
title O
comes O
as O
the O
first O
sentence O
, O
followed O
by O
the O
entries O
of O
bullet O
descriptions O

each O
as O
a O
sentence O
, O
and O
product O
attributes. O
An O
example O
document O
in O
this O
corpora O
is O
as O
follows O
: O
[ O
Title O
] O
product O
title O
[ O
Description O
] O
A.2.3 O
Session B-DatasetName
Corpus I-DatasetName
In O
this O
corpus O
, O
we O
treat O
each O
session O
as O
a O
document O
and O
sequentially O
put O
text O
representations O
of O
items O
in O
a O
session O

to O
the O
document O
with O
special O
tokens O
indicating O
the O
fields O
of O
items. O
An O
example O
document O
looks O
like O
the O
follows O
: O
[ O
SEARCH O
] O
keywords O
1 O
[ O
SEARCH O
] O
keywords O
2 O
[ O
CLICK O
] O
[ O
TITLE O
] O
product O
1 O
[ O
SEARCH O
] O
keywords O
3 O
[ O
PURCHASE O
] O
[ O
TITLE O
] O
product O
2 O

In O
this O
example O
, O
the O
customer O
first O
attempted O
to O
search O
with O
keywords O
1 O
and O
then O
modified O
the O
keywords O
to O
keywords O
2. O
The O
customer O
then O
clicked O
on O
product O
1. O
At O
last O
, O
the O
customer O
modified O
his O
search O
to O
keywords O
3 O
and O
purchased O
product O
2. O
In O
this O
corpus O
, O
session O
information O
is O

present O
in O
a O
document O
, O
but O
the O
specific O
relations O
between O
elements O
are O
not O
specified. O
The O
comparison O
of O
different O
datasets O
are O
in O
Table O
5 O
. O
A.3 O
Alternative O
Pretraining O
Methods O
We O
introduce O
the O
alternative O
pretraining O
models O
. O
• O
Product-Bert. B-MethodName
It O
is O
pretrained O
on O
the O
Product B-DatasetName
Corpus. I-DatasetName
Specifically O
, O
we O
treat O
each O
product O

in O
the O
Product B-DatasetName
Corpus I-DatasetName
as O
an O
article. O
Product O
titles O
is O
always O
the O
first O
sentence O
, O
followed O
by O
paragraphs O
of O
bullet O
descriptions O
, O
which O
can O
contain O
multiple O
sentences. O
Then O
, O
each O
additional O
product O
attribute O
is O
a O
sentence O
added O
after O
the O
bullet O
descriptions. O
5 O
: O
Comparision O
of O
different O
pretraining O
dataset. O
Product B-DatasetName
Corpus I-DatasetName

has O
access O
only O
to O
product O
information. O
SQSP B-MethodName
models O
on O
the O
queries O
and O
query-product O
relations O
, O
without O
access O
to O
session O
context. O
Session B-DatasetName
Corpus I-DatasetName
has O
access O
to O
contextual O
information O
in O
a O
session O
, O
but O
does O
not O
model O
on O
relations O
between O
objects. O
Session-Graph O
has O
access O
to O
all O
information O
and O
models O
on O
the O
relational O

nature O
of O
nodes O
in O
the O
session O
graph O
. O
Product B-MethodName
Bert I-MethodName
is O
trained O
for O
300,000 B-HyperparameterValue
steps B-HyperparameterName
, O
with O
a O
12-layer B-HyperparameterValue
transformer O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
6144 B-HyperparameterValue
and O
peak B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-HyperparameterValue
, O
1 B-HyperparameterValue
% I-HyperparameterValue
linear B-HyperparameterName
warm-up I-HyperparameterName
steps I-HyperparameterName
, O
and O
1e−2 B-HyperparameterValue
linear B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
to O
a O
minimum B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O

1e-5 B-HyperparameterValue
. O
• O
SQSP-Bert. B-MethodName
It O
is O
pretrained O
on O
SQSP B-MethodName
Corpus. O
The O
SQSP B-MethodName
Bert I-MethodName
uses O
the O
same O
Transformer O
backbone O
as O
Product B-MethodName
Bert. I-MethodName
Given O
each O
query-product O
pair O
, O
SQSP B-MethodName
feeds O
the O
text O
pair O
sequence O
to O
the O
Transformer O
for O
token O
embeddings O
for O
masked O
language O
modeling O
loss. O
In O
addition O
to O
language O
modeling O
, O
for O

each O
queryproduct O
pair O
, O
we O
sample O
a O
random O
product O
for O
the O
query O
as O
a O
negative O
query-product O
pair O
. O
The O
text O
pair O
sequence O
of O
the O
negative O
sample O
is O
also O
fed O
to O
the O
Transformer. O
Then O
, O
a O
discriminator O
is O
trained O
in O
the O
pretraining O
stage O
to O
distinguish O
the O
ground-truth O
query-product O
pairs O
and O
randomly O

sampled O
pairs. O
The O
discriminator O
's O
classification O
loss O
should O
serve O
as O
a O
contrastive O
loss O
. O
SQSP B-MethodName
Bert I-MethodName
is O
trained O
with O
the O
same O
configuration O
of O
Product B-MethodName
Bert I-MethodName
. O
B O
Details O
on O
Evaluation O
Metrics O
Mean B-MetricName
Average I-MetricName
Precision. I-MetricName
Suppose O
that O
for O
a O
session O
, O
m O
items O
are O
relevant O
and O
N O
items O
are O
retrieved O
by O

the O
model O
, O
the O
Average B-MetricName
Precision I-MetricName
( O
AP B-MetricName
) O
of O
a O
session O
is O
defined O
as O
AP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
min O
( O
m O
, O
N O
) O
N O
k=1 O
P O
( O
k O
) O
rel O
( O
k O
) O
, O
( O
4 O
) O
where O
P B-MetricName
( I-MetricName
k I-MetricName
) I-MetricName
is O
the O
precision O
of O
the O

top O
k O
retrieved O
items O
, O
and O
rel O
( O
k O
) O
is O
an O
indicator O
function O
of O
whether O
the O
kth O
item O
is O
relevant. O
As O
we O
have O
at O
most O
one O
relevant O
item O
for O
each O
session O
, O
the O
above O
metric O
reduces O
to O
1 O
r O
, O
where O
r O
is O
the O
rank O
of O
the O
relevant O
item O

in O
the O
retrieved O
list O
, O
and O
k O
= O
∞ O
when O
the O
relevant O
item O
is O
not O
retrieved. O
MAP B-MetricName
@ I-MetricName
N I-MetricName
averages O
AP B-MetricName
@ I-MetricName
N I-MetricName
over O
all O
sessions O
, O
MAP B-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|S| O
s∈S O
1 O
r O
s O
( O
5 O
) O
where O
r O
s O
is O
the O
rank O
of O
the O
relevant O
item O

for O
a O
specific O
session O
s. O
MAP B-MetricName
in O
this O
case O
is O
equivalent O
to O
MRR B-MetricName
. O
Mean B-MetricName
Average I-MetricName
Precision I-MetricName
by I-MetricName
Queries I-MetricName
( O
MAPQ B-MetricName
) O
. O
Different O
from O
MAP B-MetricName
, O
MAPQ B-MetricName
averages O
AP B-MetricName
over O
last O
queries O
instead O
of O
sessions. O
Suppose O
Q O
is O
the O
set O
of O
unique O
last O
queries O
, O
and O
S O
( O
q O

) O
, O
q O
∈ O
Q O
is O
the O
set O
of O
sessions O
whose O
last O
queries O
are O
q O
, O
then O
the O
average B-MetricName
precision I-MetricName
for O
one O
query O
q O
is O
1 O
k O
i=1 O
rel O
( O
k O
) O
N O
k=1 O
min O
( O
1 O
, O
rs≤k O
rel O
( O
k O
) O
k O
) O
( O
6 O
) O
then O
we O

sum O
over O
all O
queries O
to O
obtain O
MAPQ B-MetricName
@ I-MetricName
N I-MetricName
. O
Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
by I-MetricName
Queries I-MetricName
( O
MRRQ B-MetricName
) O
. O
MRRQ B-MetricName
averages O
MRR B-MetricName
over O
session O
last O
queries O
instead O
of O
sessions O
. O
M B-MetricName
RRQ I-MetricName
@ I-MetricName
N I-MetricName
= O
1 O
|Q| O
q∈Q O
max O
s∈S O
( O
q O
) O
( O
r O
s O
) O
( O
7 O
) O

Recall. O
Recall B-MetricName
@ I-MetricName
N I-MetricName
calculates O
the O
percentage O
of O
sessions O
whose O
relevant O
items O
were O
retrieved O
among O
the O
top O
N O
predictions O
. O

-DOCSTART- O
Putting O
the O
Con O
in O
Context O
: O
Identifying O
Deceptive O
Actors O
in O
the O
Game O
of O
Mafia O
While O
neural O
networks O
demonstrate O
a O
remarkable O
ability O
to O
model O
linguistic O
content O
, O
capturing O
contextual O
information O
related O
to O
a O
speaker O
's O
conversational O
role O
is O
an O
open O
area O
of O
research. O
In O
this O
work O
, O
we O
analyze O
the O

effect O
of O
speaker O
role O
on O
language O
use O
through O
the O
game O
of O
Mafia O
, O
in O
which O
participants O
are O
assigned O
either O
an O
honest O
or O
a O
deceptive O
role. O
In O
addition O
to O
building O
a O
framework O
to O
collect O
a O
dataset O
of O
Mafia O
game O
records O
, O
we O
demonstrate O
that O
there O
are O
differences O
in O
the O
language O
produced O

by O
players O
with O
different O
roles. O
We O
confirm O
that O
classification B-TaskName
models O
are O
able O
to O
rank O
deceptive O
players O
as O
more O
suspicious O
than O
honest O
ones O
based O
only O
on O
their O
use O
of O
language. O
Furthermore O
, O
we O
show O
that O
training O
models O
on O
two O
auxiliary O
tasks O
outperforms O
a O
standard O
BERT-based B-MethodName
text B-MethodName
classification I-MethodName
approach. O
We O
also O
present O

methods O
for O
using O
our O
trained O
models O
to O
identify O
features O
that O
distinguish O
between O
player O
roles O
, O
which O
could O
be O
used O
to O
assist O
players O
during O
the O
Mafia O
game O
. O
Introduction O
Correct O
interpretation O
of O
language O
must O
take O
into O
account O
not O
only O
the O
meaning O
of O
utterances O
, O
but O
also O
characteristics O
of O
the O
speaker O
and O

the O
context O
in O
which O
their O
utterances O
are O
produced. O
Modeling O
the O
impact O
of O
this O
context O
on O
language O
is O
still O
challenging O
for O
NLP B-TaskName
systems. O
For O
example O
, O
differences O
in O
language B-TaskName
identification I-TaskName
accuracy B-MetricName
, O
speech B-TaskName
recognition I-TaskName
word B-MetricName
error I-MetricName
rates I-MetricName
, O
and O
translation B-MetricName
quality I-MetricName
have O
been O
observed O
on O
the O
basis O
of O
attributes O
such O
as O

a O
speaker O
's O
gender O
, O
race O
, O
dialect O
, O
or O
role O
( O
Blodgett O
and O
O'Connor O
, O
2017 O
; O
Tatman O
and O
Kasten O
, O
2017 O
; O
Tatman O
, O
2017 O
; O
Stanovsky O
et O
al. O
, O
2019 O
) O
. O
Moreover O
, O
these O
systems O
systematically O
underperform O
on O
data O
generated O
by O
those O
in O
the O
minority O
, O

having O
implications O
for O
the O
ethics O
and O
fairness O
of O
using O
these O
technologies. O
* O
Equal O
contribution. O
This O
work O
explores O
language O
used O
for O
deception O
: O
a O
type O
of O
speaker O
context O
that O
is O
particularly O
challenging O
to O
model O
because O
it O
is O
intentionally O
hidden O
by O
the O
speaker. O
To O
do O
so O
, O
we O
collect O
and O
release O
a O

set O
of O
records O
for O
the O
game O
of O
Mafia O
, O
in O
which O
each O
player O
is O
assigned O
either O
an O
honest O
or O
a O
deceptive O
role. O
Then O
, O
we O
develop O
models O
that O
distinguish O
players O
' O
roles O
based O
only O
on O
the O
text O
of O
the O
players O
' O
dialog. O
We O
describe O
two O
auxiliary O
tasks O
that O
improve O
classification B-TaskName

accuracy B-MetricName
over O
a O
BERT-based B-MethodName
text B-MethodName
classifier I-MethodName
. O
The O
novel O
contributions O
of O
this O
paper O
include O
: O
1. O
A O
methodology O
for O
collecting O
records O
of O
online O
Mafia O
games O
and O
a O
dataset O
collected O
from O
460 O
human O
subjects O
, O
2. O
Three O
classification B-TaskName
models O
that O
can O
distinguish O
between O
honest O
and O
deceptive O
players O
, O
3. O
An O
approach O

for O
identifying O
features O
of O
the O
game O
dialog O
text O
that O
can O
be O
used O
to O
help O
identify O
deceptive O
players O
during O
the O
game O
. O
The O
task O
of O
identifying O
deception O
in O
dialog O
is O
far O
from O
solved. O
Our O
classification B-TaskName
methods O
, O
while O
not O
accurate O
enough O
to O
reliably O
identify O
deceptive O
players O
in O
a O
game O
, O
do O

show O
that O
the O
text O
of O
a O
dialog O
in O
the O
setting O
we O
study O
does O
contain O
information O
about O
the O
roles O
of O
the O
participants O
, O
even O
when O
those O
participants O
are O
motivated O
to O
hide O
those O
characteristics O
by O
deceiving O
the O
listener. O
Although O
the O
models O
and O
results O
described O
in O
this O
work O
only O
apply O
to O
a O
particular O

game O
setting O
rather O
than O
dialog O
in O
general O
, O
the O
approaches O
we O
describe O
are O
general O
in O
character O
and O
therefore O
may O
inform O
future O
work O
on O
determining O
speaker O
roles O
from O
the O
contents O
of O
dialog O
. O
Dataset O
A O
total O
of O
460 O
English-speaking O
participants O
based O
in O
the O
United O
States O
were O
recruited O
from O
Amazon B-MetricName
Mechanical I-MetricName
Turk I-MetricName

using O
the O
experiment O
platform O
Dallinger B-MetricName
1 O
. O
Between O
4 O
and O
10 O
participants O
were O
recruited O
for O
each O
Mafia O
game O
: O
1 O
to O
2 O
participants O
were O
designated O
mafia O
, O
and O
the O
rest O
were O
bystanders. O
Forty-four O
of O
these O
Mafia O
games O
are O
included O
in O
the O
final O
analysis. O
Participants O
were O
paid O
$ O
2.50 O
for O
completing O

the O
task O
, O
plus O
bonuses O
for O
time O
spent O
waiting O
for O
other O
participants O
to O
arrive O
in O
a O
chatroom O
to O
begin O
the O
experiment. O
Waiting O
was O
paid O
at O
$ O
5 O
/ O
hour O
. O
Upon O
recruitment O
, O
participants O
were O
shown O
a O
consent O
form O
, O
per O
IRB O
approval O
, O
followed O
by O
an O
instructional O
video O
and O

accompanying O
transcript O
describing O
how O
to O
play O
the O
text-based O
Mafia O
game O
using O
an O
interface O
we O
developed O
( O
see O
Appendix O
) O
. O
After O
they O
completed O
a O
quiz O
demonstrating O
they O
understood O
the O
information O
, O
they O
entered O
a O
waiting O
room O
until O
the O
desired O
number O
of O
participants O
was O
reached. O
Participants O
were O
then O
assigned O
a O
role O

( O
mafioso O
or O
bystander O
) O
and O
fake O
name O
, O
after O
which O
they O
began O
playing O
the O
game O
. O
The O
game O
dynamics O
were O
as O
follows. O
Each O
mafia O
member O
was O
aware O
of O
the O
roles O
of O
their O
fellow O
mafia O
members O
and O
thus O
, O
by O
process O
of O
elimination O
, O
knew O
the O
roles O
of O
the O
bystanders. O

However O
, O
the O
bystanders O
did O
not O
know O
the O
true O
role O
of O
anyone O
else O
in O
the O
game. O
The O
goal O
of O
the O
mafia O
was O
to O
eliminate O
bystanders O
until O
the O
number O
of O
mafia O
was O
greater O
than O
or O
equal O
to O
that O
of O
the O
bystanders. O
The O
goal O
of O
the O
bystanders O
was O
to O
identify O
and O
eliminate O

all O
of O
the O
mafia O
members. O
Since O
the O
incentive O
structure O
was O
set O
up O
such O
that O
bystanders O
benefited O
from O
true O
beliefs O
about O
who O
the O
mafia O
members O
were O
, O
whereas O
mafia O
members O
benefited O
from O
false O
beliefs O
, O
bystanders O
were O
thus O
motivated O
to O
be O
honest O
actors O
, O
whereas O
mafia O
members O
were O
motivated O
to O
M O

and O
B O
denote O
the O
mafioso O
and O
bystander O
classes O
, O
respectively O
, O
while O
T O
denotes O
the O
total O
number O
for O
both O
groups. O
The O
last O
row O
shows O
the O
distribution O
of O
roles O
among O
the O
players O
with O
no O
utterances O
throughout O
the O
game. O
Note O
that O
nearly O
all O
of O
the O
no-utterance O
players O
are O
bystanders O
. O
be O
deceptive O

actors O
in O
the O
Mafia O
game. O
The O
game O
proceeded O
in O
phases O
, O
alternating O
between O
nighttime O
and O
daytime O
( O
Figure O
1 O
) O
. O
During O
the O
nighttime O
, O
mafia O
members O
could O
secretly O
communicate O
to O
decide O
on O
who O
to O
eliminate O
, O
after O
which O
they O
discretely O
voted O
, O
and O
the O
person O
with O
the O
majority O
vote O

was O
eliminated O
from O
the O
game. O
If O
there O
was O
a O
tie O
, O
one O
of O
the O
people O
involved O
in O
the O
tie O
was O
randomly O
chosen O
to O
be O
eliminated. O
During O
the O
daytime O
, O
everyone O
was O
made O
aware O
of O
who O
was O
eliminated O
during O
the O
nighttime O
, O
and O
then O
all O
players O
could O
openly O
communicate O
to O
decide O

who O
to O
eliminate. O
All O
the O
players O
then O
voted O
publicly O
, O
and O
the O
person O
with O
the O
majority O
vote O
was O
eliminated O
and O
announced O
to O
be O
a O
bystander O
or O
mafioso. O
Thus O
, O
during O
the O
nighttime O
mafia O
could O
secretly O
communicate O
and O
eliminate O
anyone O
, O
whereas O
during O
the O
daytime O
mafia O
could O
participate O
in O
the O
voting O

and O
communication O
protocols O
in O
the O
same O
way O
as O
bystanders. O
The O
game O
proceeded O
until O
there O
was O
a O
winning O
faction O
according O
to O
the O
goals O
described O
above O
. O
From O
these O
experiments O
, O
we O
collected O
a O
dataset O
consisting O
of O
both O
mafia O
and O
bystander O
utterances O
over O
the O
course O
of O
each O
game O
, O
as O
well O
as O

the O
participants O
' O
voting O
behavior. O
Dataset O
statistics O
appear O
in O
Table O
1. O
Figure O
2 O
displays O
a O
snippet O
of O
the O
daytime O
dialog O
from O
one O
Mafia O
game. O
As O
shown O
, O
many O
utterances O
are O
either O
social O
interactions O
( O
eg. O
" O
hi O
erybody O
" O
) O
or O
discussions O
about O
what O
to O
do O
in O
the O
game O
, O

such O
as O
accusations O
or O
comments O
about O
voting O
( O
eg. O
" O
I O
bet O
it O
's O
Mandy O
... O
" O
) O
. O
Upon O
further O
inspection O
of O
the O
data O
, O
we O
can O
observe O
several O
strategies O
used O
by O
mafia O
members O
to O
deceive O
bystanders O
: O
Figure O
1 O
: O
Mafia O
experiment O
screenshot O
during O
( O
left O
) O
first O

nighttime O
phase O
, O
with O
participant O
as O
a O
mafioso O
, O
and O
( O
right O
) O
first O
daytime O
phase O
, O
with O
participant O
as O
a O
bystander O
( O
note O
that O
mafia O
messages O
are O
not O
visible O
to O
the O
bystander O
) O
. O
1. O
Mafia O
members O
may O
suggest O
that O
there O
is O
not O
enough O
information O
to O
decide O
on O
who O

to O
eliminate O
, O
despite O
their O
knowledge O
of O
everyone O
's O
roles O
( O
eg. O
" O
Should O
we O
wait O
to O
eliminate O
someone O
? O
" O
/ O
" O
It O
's O
a O
little O
early O
to O
tell. O
" O
/ O
" O
It O
's O
a O
shot O
in O
the O
dark. O
" O
) O
, O
2. O
Mafia O
members O
may O
raise O
suspicion O
about O

another O
player O
, O
despite O
knowing O
that O
said O
player O
is O
a O
bystander O
( O
eg. O
hmm O
ok O
analyzing O
this O
conversation O
... O
.I O
think O
bianca O
was O
a O
little O
to O
flippant O
in O
how O
she O
was O
like O
" O
sucks O
to O
be O
andrew O
" O
haha O
/ O
I O
'm O
going O
to O
vote O
bianca. O
she O
's O
so O
casual O

with O
life O
and O
death O
) O
, O
3. O
Mafia O
members O
may O
invent O
a O
false O
motive O
and O
assign O
that O
motive O
to O
another O
player O
, O
despite O
knowing O
that O
the O
player O
is O
a O
bystander O
( O
eg. O
It O
might O
be O
Jonathan O
Kim O
... O
killing O
off O
Erin O
who O
accused O
him O
" O
yesterday O
" O
) O
. O
Approach O

Given O
our O
mafia O
dataset O
, O
there O
are O
several O
tasks O
that O
one O
might O
address O
, O
for O
example O
, O
predicting O
participants O
' O
daytime O
voting O
behavior O
or O
generating O
mafia O
members O
' O
nighttime O
dialog. O
As O
our O
aim O
is O
to O
identify O
deceptive O
actors O
, O
however O
, O
we O
focus O
on O
predicting O
participants O
' O
roles O
, O
i.e. O

bystander O
or O
mafioso. O
Due O
to O
the O
asymmetry O
in O
the O
knowledge O
available O
to O
each O
group O
and O
the O
goals O
which O
incentivize O
bystanders O
to O
increase O
true O
belief O
and O
mafia O
members O
to O
reduce O
it O
, O
the O
bystanders O
are O
said O
to O
take O
on O
an O
honest O
role O
in O
the O
game O
, O
whereas O
the O
mafia O
members O
take O

on O
a O
deceptive O
role. O
To O
focus O
on O
the O
relationship O
between O
language O
and O
deception O
, O
we O
ignore O
voting O
behavior O
and O
consider O
just O
the O
daytime O
dialog O
in O
the O
game O
, O
as O
only O
the O
mafia O
members O
were O
able O
to O
converse O
during O
the O
nighttime. O
As O
shown O
in O
Table O
1 O
, O
since O
most O
of O
the O

players O
with O
no O
utterances O
are O
bystanders O
, O
we O
only O
consider O
players O
who O
make O
at O
least O
one O
utterance O
throughout O
the O
game O
. O
To O
investigate O
whether O
linguistic O
information O
can O
be O
used O
to O
identify O
players O
' O
roles O
, O
we O
train O
and O
evaluate O
classifiers O
that O
predict O
the O
role O
of O
a O
particular O
player. O
Since O
we O

have O
a O
small O
dataset O
, O
we O
chose O
to O
fine-tune B-MethodName
pre-trained I-MethodName
Transformer I-MethodName
models O
rather O
than O
train O
them O
from O
scratch O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O
To O
predict O
the O
role O
for O
a O
player O
p O
, O
we O
construct O
an O
input O
representation O
r O
( O
C O
, O
p O
) O
of O
the O
full O
game O

dialog O
C O
that O
encodes O
the O
player O
of O
interest O
p. O
We O
develop O
three O
approaches O
which O
differ O
in O
both O
the O
dialog B-TaskName
representation I-TaskName
function O
r O
and O
the O
modeling O
approach O
. O
Standard O
Classification B-TaskName
Our O
baseline O
approach O
uses O
a O
standard O
BERT-based B-MethodName
text B-MethodName
classifier I-MethodName
( O
Devlin O
et O
al. O
, O
2018 O
) O
. O
To O
classify O
player O
p O

via O
the O
full O
record O
of O
the O
game O
C O
, O
let O
boolean O
variable O
M O
p O
be O
true O
if O
p O
is O
a O
mafioso. O
Let O
T O
p O
be O
the O
concatenation O
2 O
of O
utterances O
made O
by O
p. O
We O
train O
BERT B-MethodName
parameters O
θ O
M O
to O
predict O
P O
( O
M O
p O
|T O
p O
; O
θ O
M O

) O
. O
This O
approach O
, O
which O
provides O
as O
input O
to O
the O
classifier O
only O
the O
utterances O
of O
the O
player O
to O
be O
classified O
, O
outperformed O
an O
alternative O
representation O
r O
( O
C O
, O
p O
) O
that O
included O
the O
entire O
record O
of O
all O
utterances O
by O
all O
players O
. O
Auxiliary O
Tasks O
Limiting O
the O
input O
representation O

r O
to O
contain O
only O
the O
speech O
of O
the O
player O
p O
being O
classified O
is O
not O
ideal O
; O
correctly O
interpreting O
a O
dialog O
requires O
considering O
all O
other O
players O
' O
statements O
as O
well. O
We O
introduce O
two O
auxiliary O
tasks O
that O
involve O
the O
entire O
game O
dialog O
C O
: O
1. O
Given O
all O
of O
the O
prior O
utterances O
, O

is O
a O
bystander O
or O
a O
mafia O
member O
more O
likely O
to O
have O
produced O
the O
current O
utterance O
? O
( O
Utterance O
Classification O
) O
2. O
Given O
all O
of O
the O
prior O
utterances O
, O
what O
current O
utterance O
would O
a O
player O
produce O
, O
given O
that O
they O
are O
a O
bystander O
or O
a O
mafia O
member O
? O
( O
Utterance B-TaskName
Generation I-TaskName

) O
We O
develop O
a O
BERT-based B-MethodName
classification B-MethodName
model O
for O
task O
1 O
and O
fine-tune O
the O
GPT-2 B-MethodName
language O
model O
for O
task O
2 O
( O
Radford O
et O
al. O
, O
2019 O
) O
. O
Then O
, O
we O
use O
each O
of O
these O
auxiliary O
models O
to O
classify O
the O
role O
of O
a O
particular O
player O
p O
in O
the O
game O
. O
Utterance B-TaskName

Classification I-TaskName
To O
classify O
player O
p O
using O
the O
auxiliary O
task O
of O
utterance B-TaskName
classification I-TaskName
, O
let O
boolean O
variable O
S O
i O
be O
true O
if O
utterance O
C O
i O
was O
made O
by O
a O
mafioso O
( O
rather O
than O
a O
bystander O
) O
. O
Let O
C O
be O
the O
full O
record O
of O
utterances O
in O
the O
game O
and O
C O
≤i O

be O
the O
concatenation O
of O
all O
utterances O
C O
1 O
. O
. O
. O
C O
i O
. O
We O
train O
BERT B-MethodName
parameters O
θ O
S O
to O
predict O
P O
( O
S O
i O
|C O
≤i O
; O
θ O
S O
) O
. O
Finally O
, O
let O
I O
p O
be O
the O
set O
of O
indices O
of O
utterances O
by O
player O
p. O
M O
relates O

to O
S O
in O
that O
if O
M O
p O
is O
true O
, O
then O
S O
i O
is O
true O
for O
all O
i O
∈ O
I O
p O
. O
We O
thus O
calculate O
P O
( O
M O
p O
|C O
; O
θ O
S O
) O
∝ O
i∈Ip O
P O
( O
S O
i O
|C O
≤i O
; O
θ O
S O
) O
N O
, O
where O
N O

= O
|I O
p O
|. O
The O
original O
data O
is O
shown O
on O
the O
left-hand O
side O
, O
while O
the O
right-hand O
side O
shows O
the O
processed O
data O
containing O
two O
versions O
of O
each O
utterance O
, O
one O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
and O
one O
assuming O
that O
they O
are O
a O
bystander O
, O
with O
the O
prior O

conversation O
context O
preceding O
each O
and O
labels O
corresponding O
to O
whether O
the O
assumed O
role O
matches O
the O
actual O
role O
of O
the O
player O
. O
Utterance B-TaskName
Generation I-TaskName
To O
classify O
player O
p O
using O
the O
auxiliary O
task O
of O
utterance B-TaskName
generation I-TaskName
, O
we O
fine-tune B-TaskName
GPT-2 B-MethodName
to O
generate O
utterance O
C O
i O
conditioned O
on O
prior O
utterances O
C O
< O
i O
and O

the O
role O
S O
i O
of O
the O
speaker O
that O
produced O
C O
i O
. O
From O
Bayes O
' O
rule O
, O
we O
have O
P O
( O
M O
p O
|C O
) O
∝ O
P O
( O
M O
p O
) O
P O
( O
C|M O
p O
) O
. O
To O
estimate O
P O
( O
C|M O
p O
) O
, O
let O
C O
p O
include O
all O

C O
i O
for O
i O
∈ O
I O
p O
. O
We O
make O
the O
simplifying O
assumption O
that O
P O
( O
C|M O
p O
) O
∝ O
P O
( O
C O
p O
|M O
p O
) O
, O
which O
assumes O
that O
the O
utterances O
made O
by O
players O
other O
than O
p O
are O
independent O
of O
the O
role O
of O
player O
p. O
Then O
, O
if O

M O
p O
is O
true O
, O
S O
i O
is O
true O
for O
all O
i O
∈ O
I O
p O
, O
and O
so O
, O
P O
( O
C O
p O
|M O
p O
; O
θ O
C O
) O
= O
i∈Ip O
P O
( O
C O
i O
|C O
< O
i O
, O
S O
i O
; O
θ O
C O
) O
. O
Using O
the O
full O
dialog O

C O
, O
the O
final O
probability O
of O
player O
p O
being O
mafioso O
is O
calculated O
as O
follows O
: O
P O
( O
M O
p O
|C O
) O
= O
P O
( O
M O
p O
) O
P O
( O
C O
p O
|M O
p O
; O
θ O
C O
) O
R∈ O
{ O
M O
, O
¬M O
} O
P O
( O
R O
p O
) O
P O
( O

C O
p O
|R O
p O
; O
θ O
C O
) O
( O
1 O
) O
Figure O
4 O
: O
Data B-TaskName
processing I-TaskName
for O
fine-tuning B-TaskName
GPT-2. B-MethodName
The O
original O
data O
is O
shown O
on O
the O
left-hand O
side O
, O
while O
the O
right-hand O
side O
shows O
the O
processed O
data O
containing O
a O
version O
of O
the O
corresponding O
utterance O
with O
the O
prior O
conversation O
context O
preceding O

. O
Figure O
5 O
: O
Prediction O
pipeline O
for O
our O
fine-tuned O
GPT-2 B-MethodName
model. O
Similar O
to O
the O
pipeline O
used O
to O
produce O
the O
training O
utterances O
, O
for O
prediction O
, O
there O
are O
now O
two O
versions O
of O
each O
, O
one O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
and O
one O
assuming O
that O
they O
are O
a O
bystander O

. O
The O
losses O
for O
each O
utterance O
of O
the O
target O
player O
are O
summed O
together O
in O
order O
to O
calculate O
the O
mafia O
and O
bystander O
probabilities O
as O
described O
in O
Equation O
1 O
. O
Data B-TaskName
Processing I-TaskName
To O
train O
models O
for O
utterance B-TaskName
classification I-TaskName
( O
using O
BERT B-MethodName
) O
and O
utterance B-TaskName
generation I-TaskName
( O
using O
GPT-2 B-MethodName
) O
, O
we O
perform O

data B-TaskName
processing I-TaskName
procedures O
on O
the O
games O
' O
original O
dataset O
to O
create O
input O
representations O
r O
( O
C O
, O
p O
) O
for O
each O
player O
p O
and O
obtain O
our O
training O
datasets O
as O
shown O
in O
Figures O
3 O
and O
4. O
The O
left O
side O
of O
each O
figure O
shows O
a O
snippet O
of O
a O
game O
's O
data O
, O

where O
" O
Mafioso O
" O
and O
" O
Bystander O
" O
denote O
the O
true O
roles O
of O
the O
players. O
The O
utterances O
to O
the O
right O
of O
each O
figure O
are O
training O
examples O
used O
for O
finetuning O
the O
BERT B-MethodName
and O
GPT-2 B-MethodName
models. O
Structuring O
the O
data O
in O
this O
way O
provides O
both O
the O
prior O
context O
of O
utterances O
and O
the O
current O

utterance O
that O
happened O
within O
this O
context. O
This O
not O
only O
gives O
us O
the O
information O
needed O
for O
the O
auxiliary O
tasks O
, O
but O
also O
provides O
us O
with O
more O
training O
examples O
, O
as O
we O
only O
have O
44 O
games O
and O
only O
421 O
players O
in O
total O
, O
with O
only O
2162 O
total O
utterances. O
Moreover O
, O
this O
mimics O

the O
real O
game O
scenario O
from O
the O
bystander O
view O
in O
that O
they O
can O
only O
confirm O
their O
own O
role O
, O
but O
no O
one O
else O
's O
, O
which O
is O
the O
appropriate O
setting O
for O
us O
in O
which O
to O
detect O
deception. O
Figure O
5 O
shows O
the O
pipeline O
for O
using O
the O
GPT-2 B-MethodName
model O
to O
predict O
players O
' O

roles. O
Let O
us O
assume O
that O
the O
target O
player O
for O
whom O
we O
want O
to O
predict O
their O
role O
is O
Mafioso O
1. O
From O
the O
original O
game O
log O
on O
the O
left O
, O
we O
first O
perform O
the O
data B-TaskName
processing I-TaskName
scheme O
from O
Figure O
4 O
twice O
, O
assuming O
that O
the O
target O
player O
is O
a O
mafioso O
( O
top O

of O
Figure O
5 O
) O
and O
a O
bystander O
( O
bottom O
of O
Figure O
5 O
) O
. O
Using O
our O
trained O
GPT-2 B-MethodName
model O
, O
we O
then O
obtain O
a O
loss O
for O
each O
utterance O
denoted O
by O
L1 O
through O
L4. O
Summing O
all O
the O
losses O
for O
each O
role O
, O
as O
they O
denote O
log O
probabilities O
, O
we O
calculate O
P B-MetricName

( O
M O
p O
|C O
) O
and O
P B-MetricName
( O
¬M O
p O
|C O
) O
via O
Equation O
1. O
The O
target O
player O
's O
role O
as O
predicted O
by O
the O
model O
is O
finally O
given O
by O
comparing O
the O
two O
probabilities. O
A O
similar O
process O
is O
used O
to O
calculate O
P B-MetricName
( I-MetricName
M I-MetricName
p I-MetricName
|C I-MetricName
) I-MetricName
and O
P B-MetricName
( I-MetricName
¬M I-MetricName

p I-MetricName
|C I-MetricName
) I-MetricName
for O
the O
utterance B-TaskName
classification I-TaskName
BERT B-MethodName
model I-MethodName
. O
Random O
Baseline O
This O
random O
classifier O
classifies O
each O
player O
as O
a O
mafioso O
or O
a O
bystander O
with O
probabilities O
equal O
to O
the O
prior O
distribution O
of O
each O
class O
, O
estimated O
as O
the O
ratio O
of O
roles O
across O
all O
training O
games. O
This O
serves O
as O
a O
baseline O

to O
be O
compared O
to O
for O
all O
other O
methods. O
In O
the O
game O
setting O
, O
this O
mimics O
a O
bystander O
player O
with O
only O
public O
information O
of O
how O
many O
mafia O
and O
bystanders O
are O
in O
the O
game O
. O
Standard O
Classification B-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained B-TaskName
BERT B-MethodName
Base O
model O
( O
12 B-HyperparameterValue
layers B-HyperparameterName
, O

768 B-HyperparameterValue
hidden B-HyperparameterName
dimension I-HyperparameterName
size I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
. O
We O
train O
with O
a O
maximum O
sequence B-HyperparameterName
length I-HyperparameterName
of O
256 B-HyperparameterValue
, O
which O
is O
sufficient O
for O
our O
post-processed O
dataset O
, O
setting O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
1e-5 B-HyperparameterValue
, O
and O
the O
maximum O
number O
of O
epochs B-HyperparameterName
to O
25 B-HyperparameterValue
. O

Utterance B-TaskName
Classification I-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained B-MethodName
BERT I-MethodName
Base I-MethodName
model I-MethodName
( O
12 B-HyperparameterValue
layers B-HyperparameterName
, O
768 B-HyperparameterValue
hidden B-HyperparameterName
dimension I-HyperparameterName
size I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
. O
We O
train O
with O
a O
maximum O
sequence B-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
, O
which O
is O
sufficient O
for O
our O
post-processed O
dataset O
, O
setting O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O

5 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
5e-5 B-HyperparameterValue
, O
and O
the O
maximum O
number O
of O
epochs B-HyperparameterName
to O
25. B-HyperparameterValue
Table O
2 O
: O
Experiment O
results O
on O
the O
validation O
set O
for O
random O
baseline O
( O
Random O
) O
, O
standard B-TaskName
classification I-TaskName
( O
Std O
Class O
) O
, O
utterance B-TaskName
classification I-TaskName
( O
Utt O
Class O
) O
, O
and O
utterance B-TaskName
generation I-TaskName
( O

Utt O
Gen O
) O
approaches. O
Methods O
that O
use O
auxiliary O
tasks O
( O
Utt O
Class O
and O
Utt O
Gen O
) O
outperform O
other O
methods O
in O
terms O
of O
average O
ranking O
overall O
and O
per O
game O
while O
also O
maintaining O
higher O
accuracy B-MetricName
and O
F1-score B-MetricName
for O
each O
class O
. O
Utterance B-TaskName
Generation I-TaskName
We O
initialize O
the O
model O
by O
loading O
a O
pre-trained O
12-layer B-HyperparameterValue

GPT-2 B-MethodName
model O
with O
an O
embedding B-HyperparameterName
size I-HyperparameterName
of O
768. B-HyperparameterValue
For O
the O
dataset O
, O
we O
set O
the O
maximum O
length B-HyperparameterName
of O
each O
sentence O
to O
be O
512 B-HyperparameterName
, O
which O
is O
sufficient O
for O
our O
dataset O
after O
post-processing. O
During O
training O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
be O
5 B-HyperparameterValue
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-5. B-HyperparameterValue

We O
train O
the O
model O
for O
a O
maximum O
of O
100 B-HyperparameterValue
epochs B-HyperparameterName
. O
Metrics O
These O
approaches O
each O
estimate O
a O
probability B-MetricName
P O
( O
M O
p O
|C O
) O
that O
a O
player O
p O
is O
a O
mafioso O
given O
the O
full O
record O
of O
game O
texts O
C. O
In O
Mafia O
, O
bystanders O
do O
not O
declare O
who O
is O
and O
is O

not O
a O
mafioso O
, O
but O
instead O
vote O
each O
day O
to O
eliminate O
one O
of O
the O
players. O
Because O
the O
act O
of O
voting O
involves O
choosing O
one O
player O
among O
them O
all O
, O
a O
natural O
metric O
for O
evaluating O
the O
usefulness O
of O
a O
model O
is O
to O
order O
all O
players O
p O
from O
greatest O
to O
least O
P O
( O

M O
p O
|C O
) O
, O
their O
probability B-MetricName
of O
being O
a O
mafioso O
under O
the O
model O
, O
and O
then O
to O
compute O
the O
average B-MetricName
rank I-MetricName
of O
the O
true O
mafia O
members. O
Therefore O
, O
the O
first O
metric O
in O
Table O
2 O
is O
the O
average O
ranking O
of O
all O
mafia O
members O
when O
each O
player O
is O
ranked O
by O
P O

( O
M O
p O
|C O
) O
across O
the O
entire O
validation O
set O
composed O
of O
5 O
games. O
It O
is O
also O
natural O
to O
consider O
player O
ranking O
within O
a O
single O
game O
, O
so O
we O
calculate O
the O
average B-MetricName
ranking I-MetricName
of O
mafia O
members O
within O
each O
game O
as O
a O
second O
metric. O
Smaller O
average O
ranking O
for O
mafia O
members O
means O

that O
the O
model O
is O
able O
to O
assign O
mafia O
players O
a O
high O
P O
( O
M O
p O
|C O
) O
relative O
to O
bystanders O
, O
which O
is O
desired O
. O
In O
addition O
, O
we O
evaluate O
the O
accuracy B-MetricName
of O
the O
classifiers O
and O
the O
F1-score B-MetricName
for O
each O
class. O
To O
calculate O
these O
metrics O
, O
we O
first O
assign O
the O

mafioso O
label O
to O
the O
top O
k O
players O
with O
the O
highest O
P O
( O
M O
p O
|C O
) O
and O
the O
rest O
of O
the O
players O
with O
the O
bystander O
label O
, O
where O
k O
is O
the O
known O
number O
of O
mafia O
among O
all O
validation O
games O
( O
k O
= O
10 O
in O
our O
case O
) O
. O
Aside O
from O

the O
ranking O
metrics O
, O
these O
give O
further O
information O
of O
the O
models O
' O
quality O
after O
utilizing O
available O
game O
information O
. O
Results O
and O
Analysis O
We O
trained O
all O
models O
on O
39 O
training O
games O
and O
evaluated O
on O
the O
remaining O
5 O
validation O
games. O
The O
evaluation O
results O
are O
shown O
in O
Table O
2. O
We O
have O
a O
total O

of O
49 O
players O
in O
the O
validation O
games O
, O
but O
only O
considered O
the O
39 O
players O
who O
had O
spoken O
at O
least O
one O
utterance O
throughout O
the O
game O
when O
calculating O
the O
metrics. O
Players O
with O
no O
utterances O
are O
almost O
exclusively O
bystanders O
and O
are O
therefore O
easy O
to O
classify O
without O
considering O
language O
. O
First O
, O
we O
see O

that O
it O
is O
possible O
to O
achieve O
an O
average O
rank O
that O
is O
smaller O
than O
the O
random O
baseline O
, O
which O
demonstrates O
that O
there O
is O
information O
in O
the O
dialog O
about O
the O
roles O
of O
players O
, O
despite O
the O
fact O
that O
mafia O
members O
seek O
to O
hide O
their O
role O
while O
conversing. O
However O
, O
standard O
classification B-TaskName
is O

comparable O
to O
random. O
Next O
, O
we O
observe O
that O
both O
models O
using O
auxiliary O
tasks O
outperform O
the O
standard O
classifier O
in O
rank-based O
metrics O
, O
which O
demonstrates O
that O
the O
auxiliary O
tasks O
provide O
useful O
inductive O
bias O
for O
the O
mafia O
classification B-TaskName
task. O
Additionally O
, O
the O
accuracy B-TaskName
is O
similar O
for O
all O
approaches O
, O
including O
random O
classification B-TaskName
, O

which O
indicates O
that O
there O
is O
not O
enough O
information O
in O
the O
text O
of O
a O
Mafia O
game O
for O
these O
models O
to O
determine O
players O
' O
roles O
reliably. O
If O
the O
goal O
of O
the O
game O
were O
to O
guess O
the O
role O
of O
each O
player O
individually O
, O
then O
always O
guessing O
bystander O
( O
i.e. O
the O
majority O
class O
) O

would O
be O
the O
best O
strategy. O
However O
, O
since O
the O
goal O
for O
the O
bystanders O
is O
to O
vote O
to O
eliminate O
a O
mafia O
member O
each O
round O
, O
the O
utterance B-TaskName
classification I-TaskName
and O
utterance B-TaskName
generation I-TaskName
approaches O
, O
which O
achieve O
the O
lowest O
average O
mafia O
ranking B-MetricName
per O
game O
and O
overall O
, O
respectively O
, O
are O
the O
most O
favorable O

. O
Note O
that O
the O
precision B-MetricName
for O
the O
mafia O
is O
much O
lower O
than O
that O
of O
the O
bystanders O
for O
all O
models. O
This O
is O
due O
to O
the O
usual O
lack O
of O
information O
avail- O
able O
to O
predict O
that O
any O
player O
is O
a O
mafioso O
, O
which O
makes O
finding O
the O
mafia O
a O
much O
harder O
task O
than O
finding O

bystanders O
. O

-DOCSTART- O
Enhancing B-MethodName
Self-Attention I-MethodName
with I-MethodName
Knowledge-Assisted I-MethodName
Attention I-MethodName
Maps I-MethodName
Large-scale B-MethodName
pre-trained I-MethodName
language I-MethodName
models I-MethodName
have O
attracted O
extensive O
attentions O
in O
the O
research O
community O
and O
shown O
promising O
results O
on O
various O
tasks O
of O
natural B-TaskName
language I-TaskName
processing. I-TaskName
However O
, O
the O
attention O
maps O
, O
which O
record O
the O
attention B-MetricName
scores I-MetricName
between O
tokens O
in O
self-attention B-MethodName
mechanism I-MethodName
, O
are O
sometimes O
ineffective O

as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantic O
knowledge. O
Thus O
, O
we O
aim O
to O
infuse O
explicit O
external O
knowledge O
into O
pretrained B-MethodName
language I-MethodName
models I-MethodName
to O
further O
boost O
their O
performance. O
Existing O
works O
of O
knowledge O
infusion O
largely O
depend O
on O
multi-task B-MethodName
learning I-MethodName
frameworks O
, O
which O
are O
inefficient O
and O
require O
large-scale O
re-training O
when O

new O
knowledge O
is O
considered. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
solution O
, O
KAM-BERT B-MethodName
, O
which O
directly O
incorporates O
knowledge-generated B-MethodName
attention I-MethodName
maps I-MethodName
into O
the O
self-attention B-MethodName
mechanism. I-MethodName
It O
requires O
only O
a O
few O
extra O
parameters O
and O
supports O
efficient O
fine-tuning B-TaskName
once O
new O
knowledge O
is O
added. O
KAM-BERT B-MethodName
achieves O
consistent O
improvements O
on O
various O

academic O
datasets O
for O
natural B-TaskName
language I-TaskName
understanding. I-TaskName
It O
also O
outperforms O
other O
state-of-the-art O
methods O
which O
conduct O
knowledge B-TaskName
infusion I-TaskName
into O
transformerbased O
architectures. O
Moreover O
, O
we O
apply O
our O
model O
to O
an O
industry-scale O
ad O
relevance O
application O
and O
show O
its O
advantages O
in O
the O
real-world O
scenario O
. O
Introduction O
Language O
models O
pre-trained B-TaskName
by O
a O
large O
text O
corpus O
have O

shown O
superior O
performances O
on O
a O
wide O
range O
of O
natural B-TaskName
language I-TaskName
processing I-TaskName
tasks. O
Many O
advanced O
models O
based O
on O
the O
transformer O
architectures O
achieve O
state-of-the-art O
results O
on O
various O
NLP B-TaskName
benchmarks. O
Existing O
literature O
( O
Jawahar O
et O
al. O
, O
2019 O
; O
Hewitt O
and O
Manning O
, O
2019 O
) O
shows O
that O
pre-training B-TaskName
enables O
a O
model O
to O
capture O

syntactic O
and O
semantic O
information O
in O
the O
self-attention B-MethodName
mechanism. I-MethodName
However O
, O
the O
attention O
maps O
, O
which O
* O
The O
work O
was O
done O
when O
the O
author O
visited O
Microsoft. O
record O
the O
attention O
scores O
between O
tokens O
in O
a O
selfattention O
mechanism O
, O
are O
sometimes O
ineffective O
as O
they O
are O
learned O
implicitly O
without O
the O
guidance O
of O
explicit O
semantics O

( O
Jain O
and O
Wallace O
, O
2019 O
) O
. O
If O
the O
knowledge O
can O
be O
leveraged O
in O
a O
reasonable O
way O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
we O
have O
a O
good O
chance O
to O
improve O
the O
quality O
of O
attention B-MetricName
scores I-MetricName
as O
well O
as O
the O
performance O
of O
downstream O
applications O
. O
Recently O
, O
there O
have O
been O

multiple O
attempts O
for O
incorporating O
knowledge O
into O
transformer O
architectures. O
ERNIE B-MethodName
( O
Zhang O
et O
al. O
, O
2019 O
) O
and O
KE-PLER B-MethodName
utilize O
both O
large-scale O
textual O
corpora O
and O
knowledge O
graphs O
to O
train O
a O
representation O
model O
in O
a O
multi-task B-MethodName
learning I-MethodName
framework. I-MethodName
They O
need O
to O
be O
retrained O
from O
scratch O
when O
injecting O
new O
knowledge O
, O
which O
is O

inefficient O
and O
can O
not O
benefit O
from O
existing O
pre-trained O
checkpoints. O
K-Adapter B-MethodName
( O
Wang O
et O
al. O
, O
2020 O
) O
integrates O
additional O
neural O
models O
to O
capture O
different O
kinds O
of O
knowledge. O
It O
enables O
adaptation O
based O
on O
pretrained B-TaskName
language B-MethodName
models. I-MethodName
However O
, O
it O
does O
not O
instruct O
the O
self-attention B-MethodName
mechanism I-MethodName
directly O
and O
introduces O
a O
relatively O
large O

number O
of O
parameters O
to O
the O
original O
model O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
and O
generic O
self-attention B-MethodName
mechanism I-MethodName
enhanced O
by O
explicit O
knowledge O
to O
address O
problems O
mentioned O
above. O
First O
, O
we O
show O
a O
failure O
case O
of O
query-ad B-TaskName
matching I-TaskName
, O
which O
motivates O
us O
to O
inject O
explicit O
knowledge O
into O
self-attention B-MethodName
mechanism. I-MethodName

In O
Figure O
1 O
, O
the O
attention O
map O
of O
a O
query-ad O
pair O
is O
visualized O
, O
and O
the O
goal O
is O
to O
judge O
if O
the O
query O
and O
ad O
text O
are O
semantically O
relevant. O
As O
shown O
in O
the O
figure O
, O
BERT B-MethodName
misclassifies O
this O
pair O
as O
irrelevant O
, O
probably O
because O
it O
does O
not O
understand O
the O
query O

word O
" O
glipizide O
" O
, O
which O
rarely O
appears O
in O
the O
pre-training O
corpus. O
In O
fact O
, O
" O
glipizide O
" O
is O
a O
kind O
of O
medicine O
and O
highly O
related O
to O
the O
word O
" O
Pharmacy O
" O
in O
ad O
text O
, O
so O
this O
case O
should O
be O
classified O
as O
relevant. O
In O
this O
case O
, O
if O
we O

know O
" O
glipizide O
" O
is O
semantically O
correlated O
to O
" O
Pharmacy O
" O
as O
prior O
knowledge O
, O
we O
can O
enrich O
the O
attention O
maps O
accordingly. O
In O
addition O
, O
terms O
" O
Fred O
" O
and O
" O
Meyer O
" O
are O
from O
the O
same O
entity O
, O
so O
the O
attention B-MetricName
scores I-MetricName
between O
these O
two O
terms O
should O
be O
relatively O

high. O
Based O
on O
this O
fact O
, O
we O
believe O
that O
simply O
using O
language O
models O
pre-trained B-TaskName
by O
a O
general O
corpus O
is O
not O
enough O
to O
meet O
the O
satisfaction O
of O
a O
specific O
application. O
Thus O
, O
our O
motivation O
is O
to O
inject O
explicit O
knowledge O
into O
the O
transformer O
architecture O
, O
which O
guides O
the O
pre-trained B-TaskName
language O
model O
to O

perform O
better O
adaptation O
in O
an O
efficient O
fine-tuning B-TaskName
procedure O
. O
To O
address O
the O
above O
motivation O
, O
we O
propose O
a O
novel O
architecture O
, O
namely O
KAM-BERT B-MethodName
( O
Knowledge-assisted B-MethodName
Attention I-MethodName
Maps I-MethodName
for I-MethodName
BERT I-MethodName
) O
. O
First O
, O
it O
constructs O
semantic O
attention O
maps O
based O
on O
corresponding O
relevance O
functions O
defined O
by O
various O
kinds O
of O
semantic O
knowledge. O

Specifically O
, O
we O
consider O
three O
kinds O
of O
semantic O
knowledge O
to O
guide O
the O
self-attention B-MethodName
mechanism I-MethodName
, O
i.e. O
, O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O
and O
term B-TaskName
correlation. I-TaskName
Entity O
and O
phrase B-TaskName
segmentation I-TaskName
indicate O
the O
cohesion O
of O
continuous O
terms O
, O
while O
term B-TaskName
correlation I-TaskName
can O
help O
to O
enrich O
the O
semantic O
representation O
of O
a O
sentence. O
Then O

, O
the O
knowledge B-TaskName
infusion I-TaskName
procedure O
is O
completed O
by O
concatenating O
these O
attention O
maps O
with O
vanilla O
self-attention O
and O
then O
performing O
2Dconvolution O
for O
integration. O
Finally O
, O
the O
result O
attention O
maps O
are O
served O
as O
inputs O
for O
value O
projection O
, O
and O
the O
rest O
part O
is O
the O
same O
as O
a O
standard O
transformer. O
The O
KAM-BERT B-MethodName
model O
can O

be O
fine-tuned O
on O
existing O
pre-trained B-TaskName
checkpoints O
in O
a O
plug O
and O
play O
mode O
, O
which O
is O
highly O
efficient O
in O
practice O
. O
As O
illustrated O
in O
Section O
4 O
, O
we O
compare O
KAM-BERT B-MethodName
with O
BERT B-MethodName
and O
other O
knowledge-enhanced O
SOTAs O
on O
various O
natural B-TaskName
language I-TaskName
understanding I-TaskName
tasks O
, O
where O
KAM-BERT B-MethodName
shows O
consistent O
superiority. O
Especially O
, O
we O

lift O
the O
average O
score O
of O
BERT-Base B-MetricName
from O
77.5 B-MetricValue
to O
78.7 B-MetricValue
on O
the O
GLUE B-MetricName
benchmark. O
We O
also O
demonstrate O
the O
advantage O
of O
KAM-BERT B-MethodName
for O
knowledge B-TaskName
injection I-TaskName
on O
LAMA B-DatasetName
, O
a O
probing O
benchmark O
to O
analyze O
the O
factual O
and O
commonsense O
knowledge O
contained O
in O
a O
model. O
Furthermore O
, O
KAM-BERT B-MethodName
is O
successfully O
applied O
to O
the O
query-ad B-TaskName

relevance I-TaskName
scenario O
in O
a O
commercial O
search O
engine O
and O
shows O
significant O
lift O
in O
AUC B-MetricName
score O
. O
The O
major O
contributions O
of O
this O
paper O
are O
summarized O
as O
follows O
: O
• O
First O
, O
we O
propose O
a O
novel O
self-attention O
mechanism O
enhanced O
by O
semantic O
attention O
maps O
, O
which O
incorporates O
knowledge O
from O
entity O
, O
phrase B-TaskName
segmentation I-TaskName
, O

and O
term B-TaskName
correlation. I-TaskName
Ablation O
study O
will O
demonstrate O
the O
effectiveness O
of O
these O
kinds O
of O
semantic O
attention O
maps O
. O
• O
Second O
, O
KAM-BERT B-MethodName
requires O
little O
extra O
memory O
and O
computation O
cost O
compared O
to O
vanilla O
BERT B-MethodName
and O
other O
SOTAs. O
It O
can O
be O
finetuned B-TaskName
efficiently O
on O
existing O
language O
models O
for O
a O
given O
application. O
We O
have O

successfully O
applied O
it O
to O
improve O
the O
performance O
of O
query-ad B-TaskName
relevance I-TaskName
in O
a O
commercial O
search O
engine. O
• O
Last O
but O
not O
least O
, O
the O
proposed O
framework O
is O
generic O
and O
flexible O
for O
infusing O
various O
kinds O
of O
knowledge O
into O
the O
transformer O
architecture. O
Except O
for O
the O
three O
kinds O
of O
knowledge O
considered O
in O
this O
paper O
, O

we O
will O
also O
showcase O
how O
to O
incorporate O
other O
kinds O
of O
information O
, O
such O
as O
a O
knowledge O
graph. O
It O
opens O
up O
new O
opportunities O
for O
further O
exploration O
. O
KAM-BERT B-MethodName
As O
illustrated O
in O
Figure O
2 O
, O
KAM-BERT B-MethodName
injects O
multiple O
kinds O
of O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models I-MethodName
in O
the O
form O
of O
multi-channel O
semantic O
attention O

maps. O
Different O
kinds O
of O
knowledge O
can O
be O
extracted O
independently O
and O
infused O
together O
into O
one O
self-attention O
map O
in O
the O
transformer O
architecture. O
KAM-BERT B-MethodName
can O
be O
fine-tuned B-TaskName
directly O
from O
an O
existing O
checkpoint O
of O
BERT B-MethodName
, O
while O
additional O
parameters O
are O
initialized O
randomly O
and O
learned O
in O
the O
fine-tuning B-TaskName
stage. O
Thus O
, O
it O
is O
quite O
efficient O

and O
flexible O
to O
incorporate O
new O
kinds O
of O
knowledge O
into O
the O
model O
. O
Below O
we O
first O
describe O
the O
standard O
selfattention B-MethodName
mechanism. I-MethodName
Then O
, O
we O
will O
introduce O
the O
generic O
definition O
of O
semantic O
attention O
maps O
, O
as O
well O
as O
the O
methodology O
of O
multi-channel B-MethodName
knowledge I-MethodName
infusion I-MethodName
which O
integrates O
semantic O
attention O
maps O
into O
transformer O
models. O

At O
last O
, O
the O
generation O
of O
different O
kinds O
of O
semantic O
attention O
maps O
will O
be O
presented. O
Note O
that O
the O
time O
complexity O
of O
KAM-BERT B-MethodName
is O
on-par O
with O
a O
vanilla O
BERT. B-MethodName
A O
detailed O
analysis O
can O
be O
found O
in O
the O
supplementary O
material O
. O
Self-Attention B-MethodName
The O
representation O
of O
a O
text O
sequence O
can O
be O
written O
as O

X O
∈ O
R O
N O
×C O
, O
where O
N O
denotes O
the O
sequence O
length O
and O
C O
is O
the O
word O
embedding O
dimension O
size. O
A O
standard O
Transformer O
block O
is O
composed O
of O
a O
self-attention O
layer O
and O
a O
position-wise O
feedforward O
layer O
, O
while O
each O
attention O
map O
is O
generated O
by O
a O
self-attention O
layer O
without O
any O
other O
prior O

knowledge O
introduced O
. O
The O
self-attention B-MethodName
mechanism I-MethodName
plays O
an O
important O
role O
in O
the O
transformer-based O
model. O
In O
a O
vanilla O
Transformer O
, O
the O
self-attention O
map O
A O
i O
sa O
of O
layer O
i O
is O
calculated O
by O
the O
dimension-normalized O
dotproduct O
operation O
. O
A O
i O
sa O
=Self-Attention B-MethodName
( O
X O
) O
= O
QK O
⊤ O
√ O
d O
( O
1 O

) O
where O
d O
is O
the O
dimension O
of O
representation O
vectors O
. O
In O
a O
vanilla O
transformer O
, O
A O
i O
sa O
is O
then O
normalized O
by O
softmax O
and O
fed O
into O
position-wise O
feed-forward O
layers. O
In O
KAM-BERT B-MethodName
, O
the O
self-attention O
map O
A O
i O
sa O
is O
infused O
with O
semantic O
attention O
maps O
to O
calculate O
the O
final O
attention O
matrix O

, O
which O
will O
be O
described O
in O
the O
following O
sub-sections O
. O
Semantic O
Attention O
Maps O
Given O
a O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
, O
the O
semantic O
attention O
map O
can O
be O
defined O
in O
a O
generic O
form O
M O
∈ O
R O
n×n O
, O
where O
M O
i O

, O
j O
∈ O
[ O
0 O
, O
1 O
] O
denotes O
the O
attention B-MetricName
score I-MetricName
from O
token O
i O
to O
token O
j O
, O
and O
n O
is O
the O
number O
of O
tokens O
in O
the O
current O
sentence. O
Then O
, O
for O
a O
specific O
kind O
of O
knowledge O
, O
we O
need O
a O
corresponding O
relevance O
function O
to O
calculate O
the O
attention B-MetricName
score I-MetricName

, O
i.e. O
, O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
x O
i O
, O
x O
j O
) O
. O
Note O
that O
if O
x O
i O
denotes O
a O
sub-word O
as O
in O
the O
BERT B-MethodName
model O
, O
we O
define O
M O
i O
, O
j O
= O
Relevance B-MetricName
( O
W O
( O
x O
i O
) O
, O
W O
( O
x O
j O

) O
) O
, O
where O
W O
( O
x O
i O
) O
denotes O
the O
entire O
word O
which O
contains O
the O
sub-word O
x O
i O
. O
For O
example O
, O
BERT B-MethodName
will O
convert O
a O
sequence O
" O
I O
like O
tacos O
! O
" O
into O
a O
sequence O
of O
sub-words O
, O
i.e. O
, O
{ O
I O
, O
like O
, O
ta O
, O
# O

# O
cos O
, O
! O
} O
, O
where O
" O
ta O
" O
and O
" O
# O
# O
cos O
" O
are O
sub-words O
from O
" O
tacos O
" O
, O
so O
both O
W O
( O
ta O
) O
and O
W O
( O
# O
# O
cos O
) O
denote O
the O
word O
" O
tacos O
" O
. O
In O
Section O
3.4 O
, O
we O
will O
introduce O

three O
kinds O
of O
semantic O
attention O
maps O
considered O
in O
this O
paper O
and O
the O
generation O
method O
for O
other O
knowledgeassisted O
attention O
maps O
. O
Multi-Channel B-MethodName
Knowledge I-MethodName
Infusion I-MethodName
In O
order O
to O
incorporate O
external O
knowledge O
into O
self-attention B-MethodName
, O
we O
concatenate O
semantic O
attention O
maps O
with O
vanilla O
self-attention B-MethodName
, O
and O
then O
infuse O
them O
into O
a O
single O
multi-head O
attention O

map O
using O
multi-channel O
2D-convolutions. O
Applying O
2Dconvolution O
to O
a O
self-attention O
map O
is O
first O
pro- O
posed O
by O
( O
Wang O
et O
al. O
, O
2021 O
) O
and O
shows O
advantages O
in O
both O
NLP B-TaskName
and O
CV B-TaskName
tasks. O
Here O
we O
use O
2D-convolution O
to O
infuse O
different O
kinds O
of O
knowledge O
. O
Q O
K O
V O
multi-channel O
convolution O
block O
0 O
X O

X O
X O
X O
X O
input O
tokens O
feed-forward O
concatenate O
Knowledge-assisted O
attention O
maps O
Q O
K O
V O
multi-channel O
convolution O
block O
1 O
X O
X O
X O
X O
X O
feed-forward O
linear O
fusion O
First O
, O
we O
perform O
Channel B-MethodName
Wise I-MethodName
Concatenation I-MethodName
( O
CWC B-MethodName
) O
: O
the O
vanilla O
self-attention O
map O
A O
i O
sa O
will O
be O
concatenated O
with O
each O
semantic O

attention O
map O
M O
i O
separately O
along O
the O
channel O
dimension. O
Then O
, O
a O
multi-channel O
2D-convolution O
is O
applied O
to O
generate O
an O
knowledge-infused O
attention O
map O
, O
denoted O
by O
A O
i O
sem O
. O
The O
entire O
process O
can O
be O
formulated O
as O
below O
. O
A O
i O
sem O
= O
Conv O
( O
CW O
C O
( O
A O
i O
sa O

|M O
1..k O
) O
) O
( O
2 O
) O
where O
M O
1..k O
is O
a O
set O
of O
semantic O
attention O
maps O
obtained O
by O
k O
different O
kinds O
of O
knowledge O
, O
including O
but O
not O
limited O
to O
the O
three O
ones O
considered O
in O
this O
paper O
; O
To O
infuse O
different O
types O
of O
knowledge O
, O
we O
apply O
a O
standard O
2D O

convolution O
operation O
, O
the O
output O
dimension O
of O
which O
is O
the O
same O
as O
that O
of O
A O
sa O
. O
If O
A O
sa O
has O
m O
attention O
heads O
, O
then O
A O
sem O
will O
also O
has O
m O
attention O
heads. O
We O
adopt O
3 O
× O
3 O
kernel O
for O
the O
convolution O
empirically O
as O
it O
performs O
better O
than O
1 O

× O
1 O
and O
5 O
× O
5 O
kernels O
according O
to O
( O
Wang O
et O
al. O
, O
2021 O
) O
. O
At O
last O
, O
we O
adopt O
a O
hyper-parameter O
α B-HyperparameterName
to O
balance O
the O
importance O
of O
A O
i O
sa O
and O
A O
i O
sem O
. O
A O
i O
= O
Sof O
tmax O
( O
α B-HyperparameterName
• O
A O
i O
sem O
+ O

( O
1 O
− O
α B-HyperparameterName
) O
• O
A O
i O
sa O
) O
( O
3 O
) O
After O
softmax O
activation O
, O
we O
get O
the O
final O
selfattention O
map O
A O
i O
with O
m O
heads O
for O
the O
i-th O
layer O
. O
Given O
the O
self-attention B-MethodName
map O
, O
the O
rest O
components O
are O
identical O
to O
a O
vanilla O
Transformer O
, O
which O
can O

be O
calculated O
as O
h O
i O
j O
= O
A O
i O
j O
V O
i O
, O
H O
i O
= O
( O
m O
j=1 O
h O
j O
) O
W O
O O
, O
j O
∈ O
m. O
( O
4 O
) O
In O
detail O
, O
we O
use O
the O
obtained O
attention O
map O
A O
i O
to O
multiply O
the O
value O
matrix O
V O
in O
the O

attention B-MethodName
mechanism I-MethodName
to O
get O
the O
representation O
h O
j O
of O
the O
j-th O
attention O
head. O
Next O
, O
the O
outputs O
of O
all O
attention O
heads O
from O
each O
layer O
will O
be O
concatenated O
along O
the O
embedding O
dimension. O
Finally O
, O
we O
multiply O
this O
result O
with O
a O
linear O
transformation O
matrix O
W O
O O
to O
get O
the O
output O
representation O
of O

the O
i-th O
KAM-BERT B-MethodName
layer. O
Besides O
, O
we O
add O
a O
skip O
connection O
from O
the O
result O
attention O
map O
in O
the O
i-th O
layer O
to O
the O
self-attention O
map O
of O
the O
i O
+ O
1 O
layer O
to O
enhance O
the O
flow O
of O
information O
between O
layers O
. O
Generation O
of O
Semantic O
Attention O
Maps O
The O
knowledge O
we O
inject O
into O
KAM-BERT B-MethodName

includes O
entity B-TaskName
information I-TaskName
, O
phrase B-TaskName
segmentation I-TaskName
information I-TaskName
, O
and O
term B-TaskName
correlation I-TaskName
information. I-TaskName
We O
consider O
these O
three O
types O
of O
knowledge O
because O
they O
reflect O
language O
semantics O
from O
different O
perspectives. O
Entity O
and O
phrase O
represent O
coherence O
between O
adjacent O
words O
while O
term O
correlations O
build O
a O
semantic O
bridge O
for O
related O
words O
which O
may O
be O
far O
away O

or O
even O
unseen O
in O
the O
current O
sentence. O
The O
first O
two O
kinds O
of O
knowledge O
are O
presented O
as O
labeled O
sequences O
, O
and O
the O
last O
one O
is O
presented O
as O
relationship O
between O
tokens. O
As O
defined O
in O
Section O
3.2 O
, O
a O
specific O
kind O
of O
knowledge O
can O
be O
transferred O
to O
semantic O
attention O
maps O
once O
the O
corresponding O

Relevance O
function O
is O
defined O
. O
In O
the O
following O
paragraphs O
, O
we O
will O
demonstrate O
how O
to O
define O
Relevance O
functions O
for O
the O
three O
types O
of O
knowledge O
used O
in O
this O
paper. O
Also O
, O
we O
need O
to O
emphasize O
that O
the O
proposed O
framework O
is O
generic O
and O
is O
feasible O
to O
incorporate O
other O
semantic O
information O
like O
a O

knowledge O
graph. O
Thus O
, O
we O
will O
discuss O
how O
to O
generate O
other O
knowledgeassisted O
attention O
maps O
as O
our O
future O
work. O
Entity B-TaskName
Attention I-TaskName
Map I-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
( O
Nadeau O
and O
Sekine O
, O
2007 O
) O
is O
a O
standard O
task O
for O
natural B-TaskName
language I-TaskName
processing I-TaskName
which O
has O
been O
studied O
for O
years. O
Mathematically O
, O

a O
entity O
extractor O
transforms O
the O
sequence O
of O
tokens O
{ O
x O
0 O
, O
x O
1 O
, O
... O
, O
x O
n−1 O
} O
into O
a O
sequence O
of O
labels O
{ O
label O
0 O
, O
label O
1 O
, O
... O
, O
label O
n−1 O
} O
, O
where O
label O
i O
falls O
into O
one O
of O
three O
classes O
, O
denoting O
non-entity O

words O
, O
starting O
words O
in O
entities O
and O
other O
words O
in O
entities. O
Based O
on O
the O
labeled O
sequence O
, O
the O
Relevance O
function O
for O
entity O
attention O
map O
can O
be O
defined O
as O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
E O
W O
j O
0 O
otherwise O
( O
5 O
) O
where O

A O
≡ O
E O
B O
denotes O
A O
and O
B O
belong O
to O
the O
same O
entity O
. O
Phrase B-TaskName
Segmentation I-TaskName
Attention O
Map O
Similar O
to O
the O
entity O
attention O
map O
, O
one O
can O
highlight O
the O
term O
correlations O
within O
the O
same O
phrase O
segmentation O
to O
emphasize O
the O
locality O
inductive O
bias. O
Syntax O
tree O
is O
a O
generic O
source O
to O
extract O

phrases O
in O
different O
semantic O
levels O
, O
which O
can O
be O
generated O
by O
a O
trained O
syntax O
parser. O
In O
a O
syntax O
tree O
, O
each O
internal O
node O
represents O
a O
phrase O
segment O
for O
a O
specific O
level. O
For O
example O
, O
we O
can O
select O
the O
parents O
of O
leaf O
nodes O
in O
the O
syntax O
tree O
as O
the O
root O
of O

each O
sub-tree O
which O
represents O
a O
phrase. O
We O
define O
the O
distance O
of O
an O
internal O
node O
i O
to O
the O
leaf O
node O
as O
level O
( O
i O
) O
. O
Thus O
, O
the O
relevance O
function O
can O
be O
computed O
by O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
W O
i O
≡ O
T O
W O
j O

0 O
otherwise O
( O
6 O
) O
where O
A O
≡ O
T O
B O
denotes O
that O
A O
and O
B O
belong O
to O
the O
same O
sub-tree O
at O
level O
( O
i O
) O
. O
Term B-TaskName
Correlation I-TaskName
Attention O
Map O
In O
computational O
linguistics O
, O
Pointwise B-MetricName
Mutual I-MetricName
Information I-MetricName
( O
PMI B-MetricName
) O
has O
been O
widely O
used O
for O
finding O
associations O
between O
words O
( O

Arora O
et O
al. O
, O
2016 O
) O
. O
In O
our O
work O
, O
we O
adopt O
PMI B-MetricName
to O
measure O
the O
semantic O
correlations O
between O
terms. O
The O
PMI B-MetricName
of O
a O
pair O
( O
x O
, O
y O
) O
from O
discrete O
random O
variables O
( O
X O
, O
Y O
) O
quantifies O
the O
discrepancy O
between O
the O
probability O
of O
their O
coincidence O
given O

joint O
distributions O
and O
individual O
distributions. O
We O
define O
the O
PMI-based B-MetricName
relevance I-MetricName
function O
as O
P O
M O
I O
( O
x O
; O
y O
) O
= O
log O
p O
( O
x O
, O
y O
) O
p O
( O
x O
) O
p O
( O
y O
) O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
=P O
M O
I O
( O
W O
i O

; O
W O
j O
) O
/ O
Z O
( O
7 O
) O
where O
Z O
denotes O
the O
normalized O
factor O
of O
PMI B-MetricName
matrix. O
In O
our O
experiments O
, O
PMI B-MetricName
is O
calculated O
using O
a O
large O
web O
corpus. O
We O
calculate O
the O
probability O
p O
( O
x O
, O
y O
) O
of O
a O
word O
pair O
appearing O
jointly O
, O
and O
the O
probability O

of O
single O
word O
appearance O
is O
denoted O
as O
p O
( O
x O
) O
and O
p O
( O
y O
) O
. O
Finally O
, O
we O
use O
log O
p O
( O
x O
, O
y O
) O
− O
log O
p O
( O
x O
) O
− O
log O
p O
( O
y O
) O
to O
compute O
the O
PMI B-MetricName
score O
. O
To O
better O
incorporate O
semantic O

knowledge O
into O
attention O
maps O
, O
we O
further O
enrich O
each O
attention O
map O
by O
adding O
top O
k O
terms O
which O
do O
not O
appear O
in O
the O
current O
sentence O
but O
hold O
the O
highest O
average O
PMI B-MetricName
scores O
with O
terms O
in O
the O
original O
sentence. O
Note O
that O
we O
should O
expand O
the O
selected O
k O
words O
to O
K O
subwords O
for O

BERT. B-MethodName
Then O
the O
subwords O
will O
be O
appended O
to O
the O
original O
sentence. O
After O
augmentation O
, O
the O
input O
sentence O
has O
N O
+ O
K O
words O
and O
the O
shape O
of O
an O
attention O
map O
becomes O
( O
N O
+ O
K O
) O
× O
( O
N O
+ O
K O
) O
, O
where O
N O
and O
K O
denote O
the O
number O
of O

original O
terms O
and O
auxiliary O
terms O
respectively O
( O
see O
an O
example O
in O
Fig. O
2 O
( O
d O
) O
) O
. O
In O
order O
to O
align O
the O
shapes O
of O
different O
attention O
maps O
( O
including O
the O
vanilla O
self-attention O
map O
) O
, O
we O
add O
zero-padding O
for O
smaller O
ones. O
After O
passing O
one O
transformer O
layer O
, O
the O
output O

sequence O
length O
is O
still O
N O
+ O
K. O
Note O
that O
the O
auxiliary O
words O
are O
only O
utilized O
to O
enrich O
the O
semantics O
of O
original O
word O
representations O
, O
which O
is O
done O
within O
each O
transformer O
layer. O
Thus O
, O
we O
trim O
the O
output O
sequence O
length O
to O
N O
before O
taking O
it O
as O
input O
to O
the O
next O
transformer O

layer O
( O
while O
a O
new O
round O
of O
augmentation O
will O
be O
done O
in O
the O
next O
layer O
) O
. O
Other O
Knowledge-assisted O
Attention O
Maps. O
The O
KAM-BERT B-MethodName
framework O
is O
generic O
and O
can O
be O
extended O
to O
other O
kinds O
of O
knowledge O
in O
future O
works. O
For O
each O
semantic O
type O
, O
we O
can O
define O
a O
specific O
Relevance B-MetricName
function O

to O
transfer O
the O
corresponding O
information O
into O
semantic O
attention O
maps. O
For O
example O
, O
we O
can O
define O
the O
relevance O
function O
for O
a O
Knowledge O
Graph O
( O
KG O
) O
as O
: O
Relevance B-MetricName
( O
W O
i O
, O
W O
j O
) O
= O
1 O
E O
( O
W O
i O
) O
≡ O
KG O
E O
( O
W O
j O
) O
0 O

otherwise O
( O
8 O
) O
where O
E O
( O
W O
i O
) O
is O
the O
corresponding O
entity O
of O
word O
or O
sub-word O
W O
i O
in O
a O
KG O
, O
and O
A O
≡ O
KG O
B O
represents O
that O
both O
A O
and O
B O
exist O
and O
are O
adjacent O
in O
a O
KG O
. O
Experiments O
We O
briefly O
introduced O
the O
extraction O
of O

semantic O
information O
in O
Section O
4.1. O
Then O
we O
report O
experimental O
results O
on O
natural B-TaskName
language I-TaskName
understand I-TaskName
and O
question B-TaskName
answering I-TaskName
tasks O
in O
Section O
4.2 O
and O
4.3 O
respectively. O
In O
Section O
4.4 O
, O
we O
show O
evaluation O
on O
LAMA B-DatasetName
, O
a O
benchmark O
especially O
designed O
to O
study O
how O
much O
semantic O
knowledge O
is O
contained O
in O
a O
language O
model. O

Experiments O
on O
query-ad B-TaskName
relevance I-TaskName
is O
described O
in O
Section O
4.5. O
At O
last O
, O
we O
present O
ablation O
study O
in O
Section O
4.6 O
. O
Semantic O
Information O
Extraction O
We O
use O
Stanza O
library O
( O
Qi O
et O
al. O
, O
2020 O
) O
to O
extract O
NER B-TaskName
information O
and O
syntax O
information. O
Stanza O
NER B-TaskName
takes O
one O
sentence O
as O
input O
and O
returns O

the O
start O
and O
end O
indices O
of O
the O
corresponding O
named O
entity O
in O
the O
sentence. O
While O
Stanza O
Parser O
can O
extract O
the O
corresponding O
syntax O
tree O
for O
each O
sentence. O
We O
use O
query-ad O
logs O
from O
a O
commercial O
search O
engine O
to O
calculate O
PMI B-MetricName
matrix. O
These O
steps O
gain O
the O
knowledge O
required O
to O
generate O
the O
semantic O
attention O
maps O

mentioned O
in O
Section O
3.4 O
. O
GLUE B-MetricName
Benchmark O
The O
GLUE B-MetricName
benchmark O
offers O
a O
collection O
of O
tools O
for O
evaluating O
the O
performance O
of O
models O
across O
a O
diverse O
set O
of O
NLP B-TaskName
applications. O
It O
contains O
singlesentence B-TaskName
classification I-TaskName
tasks O
( O
CoLA B-TaskName
and O
SST-2 B-TaskName
) O
, O
similarity O
and O
paraphrase O
tasks O
( O
MRPC B-TaskName
, O
QQP B-TaskName
and O
STS-B B-TaskName
) O

and O
pairwise O
inference O
tasks O
( O
MNLI B-TaskName
, O
RTE B-TaskName
and O
QNLI B-TaskName
) O
. O
We O
use O
the O
default O
train O
/ O
dev O
/ O
test O
split O
for O
each O
dataset. O
The O
hyper-parameters O
are O
chosen O
based O
on O
the O
validation O
set O
( O
refer O
to O
appendix O
for O
details O
) O
. O
After O
the O
model O
is O
trained O
, O
we O
make O

predictions O
on O
the O
test O
data O
and O
send O
the O
results O
to O
GLUE B-MetricName
online O
evaluation O
service O
1 O
to O
get O
testing O
scores. O
Note O
that O
the O
original O
WNLI B-DatasetName
dataset O
in O
the O
GLUE B-MetricValue
benchmark O
is O
problematic O
, O
which O
causes O
the O
evaluation O
results O
to O
be O
65.1. B-MetricValue
In O
order O
to O
make O
a O
fair O
comparison O
, O
most O
papers O

( O
Devlin O
et O
al. O
, O
2019 O
; O
choose O
to O
ignore O
the O
results O
of O
WNLI B-DatasetName
when O
calculating O
GLUE B-MetricName
average B-MetricName
score I-MetricName
. O
The O
scores O
on O
all O
datasets O
in O
GLUE B-MetricName
benchmark O
are O
listed O
in O
Table O
1. O
We O
report O
test O
scores O
on O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
, O
RoBERTa B-MethodName
related O
models O
and O
their O
corresponding O
enhanced O
models. O

The O
performances O
of O
BERT-Base B-MethodName
, O
BERT-Large B-MethodName
and O
1 O
https O
: O
/ O
/ O
gluebenchmark.com O
RoBERTa-Large B-MethodName
are O
reproduced O
using O
the O
official O
checkpoints O
provided O
by O
corresponding O
authors O
. O
As O
shown O
in O
the O
table O
, O
our O
models O
outperform O
all O
corresponding O
baselines. O
KAM-BERT-Base B-MethodName
achieves O
an O
average O
GLUE B-MetricName
score O
of O
78.7 B-MetricValue
, O
lifting O
1.2 B-MetricValue
scores O
from O

standard O
BERT-Base B-MethodName
model O
with O
only O
a O
few O
extra O
parameters O
introduced O
to O
the O
baseline O
model. O
Particularly O
, O
the O
improvements O
on O
CoLA B-DatasetName
datasets O
are O
fairly O
large O
, O
showing O
that O
our O
knowledge O
integration O
method O
has O
good O
generalization O
performance O
for O
natural B-TaskName
language I-TaskName
inference I-TaskName
and O
understanding. O
ERNIE B-MethodName
have O
also O
added O
external O
information O
such O
as O
entity O

and O
knowledge O
graph O
, O
but O
it O
needs O
much O
more O
time O
for O
a O
joint O
re-training. O
As O
for O
BERT-Large B-MethodName
and O
its O
counterpart O
KAM-BERT-Large B-MethodName
, O
the O
average O
improvement O
on O
GLUE B-MetricName
benchmark O
is O
0.9. B-MetricValue
We O
can O
see O
that O
the O
improvement O
becomes O
smaller O
when O
the O
model O
grows O
larger O
, O
because O
larger O
models O
often O
capture O
more O

semantic O
knowledge O
in O
the O
pre-training B-TaskName
phrase. O
But O
incorporating O
explicit O
knowledge O
is O
still O
indispensable O
for O
achieving O
a O
superior O
performance O
. O
Question B-TaskName
Answering I-TaskName
We O
conduct O
experiments O
on O
two O
kinds O
of O
question O
answering O
tasks O
, O
i.e. O
, O
commonsense B-TaskName
QA I-TaskName
and O
open-domain B-TaskName
QA. I-TaskName
Commonsense B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
with O
commonsense. O
We O
adopt O
CosmosQA B-DatasetName

for O
evaluation O
, O
which O
requires O
commonsense-based O
reading O
comprehension O
formulated O
as O
multiple O
answer O
selection. O
Opendomain B-DatasetName
QA I-DatasetName
aims O
to O
answer O
questions O
using O
external O
resources O
such O
as O
collections O
of O
documents O
and O
webpages. O
We O
consider O
two O
public O
datasets O
for O
this O
task O
, O
i.e. O
, O
Quasar-T B-DatasetName
and O
SearchQA B-DatasetName
. O
The O
results O
of O
CosmosQA B-DatasetName
are O
shown O

in O
Table O
2. O
Compared O
with O
BERT-Large B-MethodName
, O
KAM-BERT-Large B-MethodName
achieves O
10.6 B-MetricValue
% O
improvement O
in O
accuracy. O
KAM-RoBERTa-Large B-MethodName
further O
improves O
the O
accuracy O
of O
RoBERTa-Large B-MethodName
by O
5.4 B-MetricValue
% O
, O
which O
indicates O
that O
our O
models O
has O
better O
knowledge O
inference O
ability. O
For O
open-domain B-DatasetName
QA I-DatasetName
, O
our O
model O
also O
achieves O
better O
results O
compared O
to O
corresponding O
baselines. O
This O

because O
that O
KAM-based B-MethodName
models I-MethodName
can O
make O
full O
use O
of O
the O
infused O
knowledge. O
At O
the O
same O
time O
, O
one O
can O
notice O
that O
KAM-based B-MethodName
models I-MethodName
have O
fewer O
parameters O
than O
K-Adaptor B-MethodName
, O
demonstrating O
its O
effectiveness O
for O
knowledge O
infusion. O
WKLM B-MethodName
( O
Xiong O
et O
al. O
, O
2019 O
) O
forces O
the O
pre-trained O
language O
model O
to O
incorporate O

knowledge O
from O
a O
knowledge O
graph. O
This O
makes O
WKLM B-MethodName
to O
achieve O
a O
better O
score O
on O
QA B-TaskName
tasks O
, O
but O
KAM-BERT B-MethodName
performs O
even O
better O
than O
WKLM B-MethodName
. O
LAMA B-DatasetName
Benchmark O
To O
further O
verify O
whether O
KAM-BERT B-MethodName
better O
integrate O
internal O
knowledge O
into O
pre-trained B-MetricName
language O
models O
, O
we O
conduct O
experiments O
on O
LAMA B-DatasetName
, O
a O
widely O
used O

benchmark O
for O
knowledge B-TaskName
probing I-TaskName
. O
LAMA B-DatasetName
examines O
models O
' O
abilities O
on O
recalling O
relational O
facts O
by O
cloze-style O
questions. O
The O
first O
place O
micro-averaged B-MetricName
accuracy I-MetricName
is O
used O
as O
evaluation O
metrics. O
The O
evaluation O
results O
are O
shown O
in O
Table O
3. O
KAM-BERT B-MethodName
consistently O
outperforms O
corresponding O
baselines O
on O
all O
tasks. O
It O
indicates O
that O
KAM-BERT B-MethodName
can O
generate O
better O

attention O
maps O
with O
semantic O
guidance O
. O
Query-Ad B-TaskName
Relevance I-TaskName
Query-ad O
relevance O
measures O
how O
relevant O
a O
search O
ad O
matches O
with O
a O
user O
's O
search O
query. O
Very O
often O
queries O
and O
ads O
have O
words O
with O
special O
meanings O
, O
which O
are O
not O
easily O
understood O
well O
by O
traditional O
NLP B-TaskName
techniques O
but O
can O
benefit O
from O
the O
knowledge-assisted O

mechanism O
proposed O
in O
this O
work. O
Besides O
, O
user O
queries O
and O
ads O
text O
often O
contain O
noises O
, O
so O
evaluation O
on O
query-ad O
relevance O
task O
would O
test O
our O
model O
's O
robustness O
and O
resistance O
of O
noise. O
We O
compare O
BERT B-MethodName
and O
KAM-BERT B-MethodName
on O
a O
large-scale O
internal O
dataset O
of O
a O
commercial O
search O
engine. O
As O
shown O
in O

Table O
5 O
, O
our O
model O
outperforms O
corresponding O
baselines O
by O
a O
large O
margin O
, O
which O
is O
statistically O
significant O
under O
95 B-HyperparameterValue
% I-HyperparameterValue
confidence B-HyperparameterName
interval. O
One O
thing O
to O
call O
out O
is O
that O
, O
although O
NER B-TaskName
and O
syntax B-TaskName
parsing I-TaskName
results O
are O
nosier O
comparing O
to O
the O
ones O
in O
academic O
datasets O
, O
we O
still O
have O
good O

improvements O
on O
this O
dataset. O
This O
indicates O
the O
way O
we O
combine O
those O
knowledge O
together O
makes O
our O
model O
more O
robust O
to O
noisy O
inputs O
. O
Model O
Analysis O
In O
this O
section O
, O
we O
explore O
the O
sensitivity O
of O
hyperparameter O
α B-HyperparameterName
, O
and O
then O
conduct O
ablation O
experiments O
on O
three O
types O
of O
added O
knowledge O
. O
Hyper-parameter O
Analysis O

The O
optimal O
α B-HyperparameterName
value O
after O
grid O
search O
is O
0.2 B-HyperparameterValue
, O
which O
means O
that O
the O
original O
attention O
maps O
still O
dominate O
the O
token O
relationships. O
We O
chose O
three O
tasks O
from O
different O
fields O
to O
do O
ablation O
study O
for O
α. B-HyperparameterName
Our O
model O
is O
KAM-BERT-Base B-MethodName
, O
and O
its O
performance O
is O
shown O
in O
Table O
4. O
In O
three O

different O
tasks O
, O
setting O
α B-HyperparameterName
to O
0.2 B-HyperparameterValue
achieves O
the O
best O
results. O
An O
intuitive O
understanding O
is O
that O
when O
α B-HyperparameterName
is O
small O
, O
external O
knowledge O
plays O
an O
unimportant O
role O
and O
can O
not O
participate O
in O
the O
entire O
training O
process O
well. O
With O
the O
gradual O
increase O
of O
α B-HyperparameterName
, O
the O
intervention O
of O
external O
knowledge O
on O

the O
attention O
map O
will O
increase O
, O
and O
the O
attention O
relationship O
in O
the O
original O
sequence O
will O
be O
gradually O
lost O
, O
resulting O
in O
the O
decline O
of O
model O
performance O
. O
Ablation O
Study O
For O
a O
comprehensive O
understanding O
of O
our O
model O
design O
, O
we O
conduct O
ablation O
study O
with O
the O
following O
settings O
in O
Table O
6. O
The O

average O
scores B-MetricValue
of O
all O
ablation O
experiments O
are O
better O
than O
BERT B-MethodName
, O
but O
are O
relatively O
worse O
than O
KAM-BERT B-MethodName
, O
demonstrating O
all O
the O
components O
are O
beneficial O
for O
the O
final O
performance. O
At O
the O
same O
time O
, O
we O
observe O
that O
after O
deleting O
the O
entity O
attention O
map O
, O
the O
score B-MetricName
of O
KAM-BERT B-MethodName
drops O
drastically O
from O

78.7 B-MetricValue
to O
77.7. B-MetricValue
This O
shows O
that O
the O
gain O
brought O
by O
entity O
information O
is O
the O
greatest. O
In O
addition O
, O
the O
convolution O
layer O
is O
indispensable O
for O
achieving O
a O
superior O
performance O
. O
Case O
Study O
In O
Figure O
3 O
, O
we O
visualize O
an O
example O
of O
query-ad B-TaskName
relevance I-TaskName
, O
where O
the O
query O
is O
" O
buy O
glipizide O

" O
and O
ad O
text O
is O
" O
Fred O
Meyer O
Pharmacy O
Near O
Me O
" O
. O
The O
darker O
color O
in O
the O
figure O
represents O
a O
higher O
attention B-MetricName
score. O
Figure O
3 O
( O
a O
) O
is O
the O
attention O
map O
of O
vanilla O
BERT B-MethodName
without O
adding O
explicit O
knowledge. O
When O
encountering O
rare O
words O
like O
" O
glipizide O
" O
, O
the O

self-attention O
mechanism O
can O
not O
do O
a O
good O
job O
to O
decide O
which O
terms O
should O
" O
glipizide O
" O
attend O
to. O
But O
in O
Figure O
3 O
( O
b O
) O
, O
the O
attention O
map O
of O
KAM-BERT B-MethodName
uses O
term O
correlations O
to O
learn O
that O
" O
glipizide O
" O
is O
a O
medicine O
, O
so O
it O
focuses O
on O
the O
medicinerelated O

tokens O
like O
" O
Pharmacy O
" O
and O
" O
antidiabetic O
" O
. O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
KAM-BERT B-MethodName
, O
a O
flexible O
and O
efficient O
approach O
to O
inject O
knowledge O
into O
transformer-based B-MethodName
pre-trained I-MethodName
models. I-MethodName
Extensive O
experiments O
on O
GLUE B-MetricName
and O
LAMA B-DatasetName
benchmark O
show O
that O
our O
approach O
outperforms O
all O
BERT-Style B-MethodName
baselines O
and O
achieves O
new O
SOTA O

on O
QA B-TaskName
tasks O
, O
suggesting O
that O
our O
models O
indeed O
integrate O
knowledge O
in O
an O
effective O
manner O
and O
have O
good O
generalization O
ability. O
In O
future O
work O
, O
we O
hope O
to O
investigate O
more O
types O
of O
knowledge O
which O
can O
be O
effectively O
integrated O
in O
our O
framework O
. O

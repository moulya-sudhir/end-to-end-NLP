-DOCSTART-
Zero-Shot B-TaskName
Text I-TaskName
Classification I-TaskName
with O
Self-Training O

Recent O
advances O
in O
large O
pretrained O
language O
models O
have O
increased O
attention O
to O
zero-shot B-TaskName
text I-TaskName
classification. I-TaskName
In O
particular O
, O
models O
finetuned O
on O
natural O
language O
inference O
datasets O
have O
been O
widely O
adopted O
as O
zero-shot O
classifiers O
due O
to O
their O
promising O
results O
and O
offthe-shelf O
availability. O
However O
, O
the O
fact O
that O
such O
models O
are O
unfamiliar O
with O
the O
target O
task O
can O
lead O
to O
instability O
and O
performance O
issues. O
We O
propose O
a O
plug-and-play O
method O
to O
bridge O
this O
gap O
using O
a O
simple O
self-training O
approach O
, O
requiring O
only O
the O
class O
names O
along O
with O
an O
unlabeled O
dataset O
, O
and O
without O
the O
need O
for O
domain O
expertise O
or O
trial O
and O
error. O
We O
show O
that O
fine-tuning O
the O
zero-shot O
classifier O
on O
its O
most O
confident O
predictions O
leads O
to O
significant O
performance O
gains O
across O
a O
wide O
range O
of O
text O
classification O
tasks O
, O
presumably O
since O
self-training O
adapts O
the O
zero-shot O
model O
to O
the O
task O
at O
hand O
. O

Introduction O
Large O
language O
models O
have O
revolutionized O
the O
field O
of O
natural O
language O
processing O
, O
leading O
to O
great O
leaps O
in O
performance O
across O
the O
NLP O
task O
landscape O
( O
Devlin O
et O
al. O
, O
2018 O
; O
Raffel O
et O
al. O
, O
2020 O
; O
Brown O
et O
al. O
, O
2020 O
) O
. O
The O
pretrain-finetune O
paradigm O
has O
led O
to O
a O
significant O
reduction O
in O
the O
amount O
of O
labeled O
data O
required O
for O
obtaining O
high O
performance O
on O
downstream O
tasks. O
However O
, O
the O
need O
to O
collect O
labeled O
examples O
for O
each O
target O
task O
remains O
an O
obstacle O
, O
limiting O
the O
usage O
of O
language O
models O
in O
practice O
, O
at O
scale. O
Thus O
, O
the O
more O
ambitious O
vision O
of O
a O
generalpurpose O
zero-shot O
model O
-one O
that O
can O
tackle O
many O
different O
tasks O
without O
requiring O
labeled O
data O
-has O
become O
an O
enticing O
goal O
for O
the O
community. O
This O
notion O
is O
increasingly O
gaining O
attention O
, O
with O
recent O
works O
suggesting O
new O
paradigms O
that O
aim O
to O
utilize O
the O
language O
understanding O
capabilities O
of O
large O
models O
for O
the O
zero-shot O
scenario. O
* O
These O
authors O
contributed O
equally O
to O
this O
work O
. O

In O
their O
pioneering O
work O
on O
more O
generalpurpose O
zero-shot O
models O
, O
Yin O
et O
al. O
( O
2019 O
) O
propose O
to O
formulate O
text O
classification O
tasks O
as O
a O
textual O
entailment O
problem O
( O
Dagan O
et O
al. O
, O
2005 O
) O
. O
This O
mapping O
enables O
using O
a O
model O
trained O
on O
natural O
language O
inference O
( O
NLI O
) O
as O
a O
zero-shot O
text O
classifier O
for O
a O
wide O
variety O
of O
unseen O
downstream O
tasks. O
The O
underlying O
idea O
is O
fairly O
intuitive. O
To O
determine O
if O
a O
particular O
text O
should O
be O
assigned O
to O
, O
e.g. O
, O
the O
" O
sports O
" O
class O
or O
the O
" O
politics O
" O
class O
, O
one O
constructs O
sentences O
such O
as O
" O
This O
text O
is O
about O
sports O
" O
and O
" O
This O
text O
is O
about O
politics O
" O
, O
respectively O
; O
the O
model O
prediction O
as O
to O
which O
one O
is O
most O
entailed O
by O
the O
original O
text O
can O
then O
be O
used O
to O
determine O
the O
predicted O
class O
label. O
Similarly O
, O
some O
recent O
works O
have O
tried O
to O
map O
even O
more O
varied O
types O
of O
NLP O
tasks O
into O
a O
unified O
cross-task O
format O
( O
Wei O
et O
al. O
, O
2022 O
; O
Zhong O
et O
al. O
, O
2021 O
; O
Bragg O
et O
al. O
, O
2021 O
; O
Sanh O
et O
al. O
, O
2022 O
) O
. O
Such O
unified O
task O
formats O
enable O
" O
meta-tuning O
" O
a O
model O
using O
existing O
labeled O
data O
from O
different O
tasks. O
By O
teaching O
the O
model O
to O
solve O
the O
broader O
" O
meta-task O
" O
, O
it O
is O
then O
able O
to O
cope O
with O
a O
wide O
variety O
of O
unseen O
tasks O
at O
inference O
time O
. O

While O
zero-shot O
models O
hold O
great O
promise O
by O
eliminating O
the O
burden O
of O
collecting O
task-specific O
labeled O
data O
, O
they O
often O
still O
come O
at O
the O
cost O
of O
providing O
mediocre O
performance O
compared O
to O
models O
trained O
in O
the O
conventional O
supervised O
learning O
paradigm. O
Thus O
, O
improving O
the O
prediction O
performance O
of O
zero-shot O
models O
is O
of O
great O
practical O
importance. O
One O
of O
the O
simplest O
and O
most O
effective O
approaches O
for O
improving O
performance O
of O
classifiers O
is O
self-training O
( O
Scudder O
, O
1965 O
) O
. O
In O
this O
paradigm O
, O
a O
model O
's O
own O
predictions O
on O
unlabelled O
data O
are O
leveraged O
for O
creating O
pseudo-labels O
, O
which O
are O
then O
used O
for O
further O
training O
the O
model O
. O

In O
the O
original O
setting O
of O
self-training O
, O
some O
labeled O
data O
is O
available O
for O
training O
an O
initial O
classifier O
, O
and O
the O
predictions O
of O
the O
classifier O
on O
unlabeled O
data O
are O
used O
for O
data O
augmentation O
( O
Van O
En- O
gelen O
and O
Hoos O
, O
2020 O
) O
. O
More O
recently O
, O
the O
use O
of O
self-training O
has O
been O
extended O
to O
the O
scenario O
of O
unsupervised O
domain O
adaptation O
, O
where O
labeled O
data O
is O
available O
only O
for O
a O
source O
domain O
, O
and O
only O
unlabeled O
data O
is O
available O
for O
the O
target O
domain O
( O
e.g. O
, O
Du O
et O
al. O
, O
2021 O
; O
Zou O
et O
al. O
, O
2019 O
) O
. O

Here O
, O
we O
aim O
to O
study O
self-training O
as O
a O
method O
for O
improving O
general-purpose O
zero-shot O
models O
, O
by O
adapting O
them O
to O
the O
task O
at O
hand. O
Given O
the O
distinct O
properties O
of O
such O
models O
, O
applying O
selftraining O
in O
this O
scenario O
is O
not O
trivial O
and O
poses O
unique O
challenges. O
Our O
approach O
can O
be O
viewed O
as O
a O
further O
extension O
of O
self-training O
-from O
unsupervised O
domain-adaptation O
to O
unsupervised O
taskadaptation O
, O
where O
only O
unlabeled O
data O
is O
available O
for O
the O
target O
task O
. O

As O
prominent O
representatives O
of O
general-purpose O
zero-shot O
models O
, O
in O
this O
work O
we O
focus O
on O
NLIbased O
models O
( O
Yin O
et O
al. O
, O
2019 O
) O
, O
which O
are O
increasingly O
being O
utilized O
for O
zero-shot B-TaskName
classification I-TaskName
( O
Davison O
, O
2020 O
; O
Sainz O
and O
Rigau O
, O
2021 O
; O
Basile O
et O
al. O
, O
2021 O
) O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
work O
that O
explores O
self-training O
in O
the O
context O
of O
general-purpose O
zero-shot O
models. O
We O
release O
our O
code O
1 O
, O
including O
access O
to O
all O
datasets O
, O
and O
an O
associated O
automatic O
evaluation O
framework O
, O
aiming O
to O
facilitate O
further O
research O
along O
the O
lines O
explored O
here. O
. O
This O
procedure O
can O
be O
repeated O
to O
obtain O
M O
′′ O
, O
and O
so O
on O
, O
in O
an O
iterative O
manner O
. O

Next O
, O
we O
describe O
the O
motivation O
of O
applying O
self-training O
to O
zero-shot O
text O
classifiers O
, O
and O
the O
details O
of O
our O
approach O
. O

Motivation O
We O
hypothesize O
that O
self-training O
brings O
forth O
unique O
benefits O
to O
general-purpose O
zero-shot O
models O
, O
going O
beyond O
data O
augmentation O
and O
the O
exposure O
to O
the O
target O
domain O
. O

A O
zero-shot O
model O
, O
as O
implied O
by O
its O
name O
, O
has O
never O
been O
directly O
exposed O
to O
the O
task O
it O
should O
perform. O
Moreover O
, O
one O
should O
expect O
significant O
differences O
between O
the O
characteristics O
and O
distributions O
of O
the O
task O
( O
s O
) O
used O
to O
create O
the O
generalpurpose O
model O
, O
and O
those O
of O
the O
downstream O
task. O
Self-training O
may O
help O
bridge O
this O
gap O
, O
by O
adapting O
the O
model O
to O
the O
properties O
of O
the O
target O
task O
. O

Specifically O
, O
for O
NLI-based O
classification O
models O
( O
Yin O
et O
al. O
, O
2019 O
) O
, O
which O
are O
at O
the O
focus O
of O
this O
work O
, O
self-training O
may O
provide O
two O
important O
benefits O
, O
discussed O
next O
. O

Exposure O
to O
class O
names. O
As O
a O
language O
model O
, O
the O
zero-shot O
model O
presumably O
embodies O
some O
knowledge O
about O
the O
meaning O
of O
the O
target O
class O
names O
, O
considering O
each O
class O
name O
independently O
; O
however O
, O
chances O
are O
it O
has O
never O
been O
trained O
to O
consider O
their O
potential O
interactions. O
Pseudolabeled O
examples O
, O
obtained O
via O
self-training O
, O
can O
force O
the O
zero-shot O
model O
to O
contrast O
the O
class O
names O
with O
one O
another O
, O
and O
to O
learn O
more O
subtle O
distinctions O
that O
will O
be O
required O
at O
test O
time. O
As O
a O
simple O
example O
, O
'guilt O
' O
and O
'shame O
' O
may O
often O
be O
considered O
synonyms O
, O
but O
represent O
two O
distinct O
classes O
in O
one O
of O
our O
datasets. O
Explicit O
exposure O
to O
even O
weakly O
labeled O
data O
is O
presumably O
essential O
to O
learn O
such O
distinctions O
. O

Exposure O
to O
the O
task O
and O
template O
/ O
prompt. O
Entailment-based O
models O
are O
originally O
trained O
on O
general O
NLI O
datasets O
, O
which O
aim O
to O
capture O
a O
broad O
and O
diverse O
range O
of O
textual O
entailment O
instances. O
Utilizing O
these O
models O
for O
text O
classification O
implies O
that O
they O
should O
focus O
on O
a O
narrower O
set O
of O
entailment O
types O
, O
namely O
those O
that O
map O
to O
the O
text O
classification O
problem O
under O
consideration. O
Moreover O
, O
the O
application O
of O
these O
models O
as O
zero-shot O
classifiers O
involves O
the O
use O
of O
generic O
hypothesis O
templates O
that O
aim O
to O
formulate O
the O
downstream O
classification O
task O
in O
terms O
of O
textual O
entailmente.g. O
, O
" O
This O
text O
is O
about O
X O
" O
. O
Both O
the O
relevant O
entailment O
sub-types O
, O
and O
the O
generic O
templates O
used O
at O
test O
time O
, O
are O
presumably O
not O
common O
in O
the O
data O
used O
to O
train O
the O
model. O
Thus O
, O
self-training O
exposes O
the O
model O
to O
the O
specific O
hypothesis O
template O
that O
will O
be O
used O
for O
text O
classification O
, O
as O
well O
as O
to O
the O
underlying O
distribution O
of O
text O
classification O
entailment O
problems O
it O
will O
need O
to O
face O
. O

Our O
approach O
We O
consider O
an O
entailment-based O
zero-shot O
model O
, O
M O
, O
for O
a O
multi-class O
classification O
task O
, O
with O
a O
set O
of O
target O
class O
names O
C. O
Yin O
et O
al. O
( O
2019 O
) O
proposed O
to O
map O
the O
text O
classification O
task O
into O
an O
entailment O
task O
, O
as O
depicted O
in O
Fig. O
1. O
Specifically O
, O
a O
target O
text O
, O
t O
, O
is O
taken O
as O
the O
premise. O
For O
every O
class O
c O
∈ O
C O
, O
a O
hypothesis O
is O
constructed O
from O
a O
template O
such O
as O
" O
This O
example O
is O
c O
" O
( O
e.g. O
, O
" O
This O
example O
is O
joy O
" O
, O
or O
" O
This O
example O
is O
anger O
" O
) O
. O
The O
entailment O
model O
is O
presented O
with O
t O
and O
a O
set O
of O
hypotheses O
that O
correspond O
to O
the O
different O
classes. O
The O
class O
whose O
hypothesis O
receives O
the O
top O
entailment O
score O
is O
predicted O
as O
the O
label O
for O
t O
( O
see O
Fig. O
1 O
) O
. O

We O
further O
assume O
a O
collection O
of O
unlabeled O
examples O
U O
is O
available. O
Following O
the O
entailment O
approach O
, O
we O
generate O
pseudo-labeled O
examples O
from O
U O
based O
on O
the O
predictions O
given O
by O
M O
. O

First O
, O
for O
each O
u O
∈ O
U O
and O
each O
class O
name O
c O
∈ O
C O
we O
obtain O
S O
uc O
, O
the O
confidence O
score O
for O
u O
entailing O
the O
hypothesis O
constructed O
for O
c O
( O
entailment O
score O
in O
Fig. O
1 O
) O
. O
In O
other O
words O
, O
S O
uc O
represents O
the O
confidence O
of O
assigning O
u O
to O
c O
. O

Selecting O
positive O
examples O
Our O
goal O
is O
to O
collect O
for O
each O
class O
c O
, O
a O
set O
of O
n O
pseudo-labeled O
positive O
examples O
in O
U O
. O
As O
common O
in O
self-training O
, O
we O
aim O
to O
focus O
on O
the O
most O
confident O
predictions. O
We O
follow O
a O
" O
Best-versus-Second-Best O
" O
approach O
, O
as O
in O
Slonim O
et O
al. O
( O
2011 O
) O
. O

To O
that O
end O
, O
we O
first O
consider O
all O
examples O
in O
U O
for O
which O
c O
obtained O
the O
top O
entailment O
score O
, O
i.e. O
, O
S O
uc O
> O
S O
uc O
′ O
, O
∀c O
′ O
̸ O
= O
c. O
Next O
, O
we O
focus O
our O
attention O
on O
examples O
that O
maximize O
the O
delta O
between O
the O
top O
ranked O
class O
and O
the O
second O
highest O
class O
( O
in O
Fig. O
1 O
, O
the O
delta O
is O
between O
the O
entailment O
score O
for O
joy O
and O
guilt O
) O
. O
Loosely O
speaking O
, O
such O
examples O
correspond O
to O
points O
farthest O
from O
the O
decision O
boundary O
, O
and O
thus O
the O
points O
that O
the O
model O
is O
most O
certain O
about. O
Assuming O
c O
and O
c O
′ O
are O
the O
top-ranked O
and O
second-ranked O
classes O
for O
u O
, O
respectively O
, O
let O
δ O
uc O
= O
S O
uc O
− O
S O
uc O
′ O
. O

Next O
, O
for O
a O
given O
class O
c O
, O
we O
sort O
all O
examples O
in O
U O
for O
which O
c O
was O
the O
top-ranked O
class O
by O
δ O
uc O
in O
a O
decreasing O
order O
, O
and O
select O
the O
top O
n O
examples O
as O
the O
positive O
examples O
for O
class O
c O
. O

In O
order O
to O
utilize O
these O
pseudo-labeled O
examples O
as O
training O
examples O
for O
the O
entailment O
model O
M O
, O
we O
use O
a O
similar O
transformation O
to O
the O
one O
described O
above O
-the O
example O
is O
taken O
as O
the O
premise O
, O
the O
class O
name O
is O
incorporated O
into O
the O
hypothesis O
template O
, O
and O
the O
premise-hypothesis O
pair O
is O
assigned O
the O
entail O
pseudo-label O
. O

Selecting O
negative O
examples O
To O
train O
the O
entailment O
model O
to O
contrast O
between O
classes O
we O
need O
to O
generate O
negative O
entailment O
examples O
, O
with O
the O
contradict O
pseudo-label. O
For O
that O
, O
we O
examine O
four O
approaches O
: O

Contrast-random O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O
a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
at O
random O
with O
another O
class O
. O

Contrast-closest O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O
a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
the O
class O
receiving O
the O
second-highest O
entailment O
score O
for O
this O
premise O
( O
guilt O
in O
the O
example O
of O
Fig. O
1 O
) O
. O

Contrast-furthest O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O
a O
pair O
with O
the O
contradict O
label O
, O
which O
is O
composed O
of O
the O
same O
premise O
, O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
the O
class O
receiving O
the O
lowest O
entailment O
score O
for O
this O
premise O
( O
anger O
in O
the O
example O
of O
Fig. O
1 O
) O
. O

Contrast-all O
For O
each O
entail O
pair O
for O
a O
hypothesis O
based O
on O
c O
, O
add O
|C| O
− O
1 O
pairs O
with O
the O
contradict O
label O
, O
all O
with O
same O
premise O
and O
a O
hypothesis O
in O
which O
c O
is O
replaced O
with O
each O
of O
the O
other O
target O
class O
c O
′ O
̸ O
= O
c. O
Note O
that O
for O
datasets O
with O
a O
large O
number O
of O
classes O
, O
this O
setting O
significantly O
increases O
the O
size O
of O
the O
training O
set O
, O
and O
correspondingly O
the O
run O
time O
. O

The O
full O
training O
data O
, O
including O
both O
entail O
and O
contradict O
pseudo-labeled O
examples O
, O
is O
used O
to O
fine-tune O
the O
general O
entailment O
model O
M O
, O
yielding O
an O
entailment O
zero-shot O
model O
M O
′ O
that O
has O
been O
adapted O
to O
the O
target O
task. O
We O
continue O
this O
procedure O
in O
iterations O
: O
we O
generate O
a O
new O
pseudolabeled O
dataset O
based O
on O
the O
predictions O
of O
M O
′ O
, O
which O
is O
then O
fine-tuned O
to O
generate O
M O
′′ O
, O
and O
so O
forth O
. O

Balancing O
noise O
and O
informativeness O
with O
token O
masking O
Self-training O
relies O
on O
a O
delicate O
balance. O
On O
the O
one O
hand O
, O
the O
pseudo-labels O
are O
noisy. O
Training O
on O
noisy O
data O
may O
lead O
to O
overconfidence O
and O
propagation O
of O
errors O
( O
Zou O
et O
al. O
, O
2019 O
) O
. O
Therefore O
, O
a O
standard O
self-training O
practice O
is O
to O
take O
the O
most O
confident O
predictions O
, O
which O
are O
presumed O
to O
be O
less O
noisy. O
On O
the O
other O
hand O
, O
the O
most O
confident O
examples O
are O
more O
likely O
to O
be O
the O
easy O
and O
less O
informative O
ones O
, O
and O
thus O
less O
useful O
for O
training O
( O
Hajmohammadi O
et O
al. O
, O
2015 O
; O
Mukherjee O
and O
Awadallah O
, O
2020 O
) O
. O

With O
zero-shot O
models O
, O
this O
trade-off O
becomes O
even O
more O
pronounced. O
As O
these O
models O
were O
not O
trained O
on O
the O
target O
task O
, O
the O
pseudo-labels O
that O
are O
obtained O
from O
their O
predictions O
are O
likely O
to O
be O
noisier O
than O
those O
obtained O
from O
a O
model O
trained O
on O
some O
labeled O
data. O
Thus O
, O
with O
zero-shot O
models O
we O
are O
compelled O
to O
raise O
the O
confidence O
bar O
in O
order O
to O
obtain O
pseudo-labels O
of O
reasonable O
quality O
, O
which O
in O
turn O
may O
focus O
the O
training O
on O
the O
easy O
and O
thus O
less O
informative O
examples O
. O

To O
increase O
the O
informativeness O
of O
the O
selected O
examples O
, O
we O
apply O
the O
following O
heuristic O
: O
in O
each O
example O
we O
identify O
the O
token O
which O
is O
the O
most O
similar O
to O
the O
positive O
class O
name O
assigned O
to O
this O
example O
, O
and O
mask O
it. O
In O
the O
example O
of O
Fig. O
1 O
, O
the O
word O
thrilled O
will O
be O
masked O
when O
this O
example O
is O
used O
as O
a O
positive O
or O
as O
a O
negative O
example O
for O
the O
class O
" O
joy O
" O
. O
By O
masking O
those O
most O
similar O
tokens O
, O
the O
selected O
examples O
become O
more O
challenging O
, O
and O
the O
model O
is O
forced O
to O
rely O
on O
other O
signalse.g. O
, O
in O
Fig. O
1 O
, O
on O
the O
understanding O
that O
the O
event O
of O
a O
paper O
getting O
accepted O
to O
a O
conference O
is O
a O
joyful O
one O
. O

Datasets O
and O
Tasks O
We O
experiment O
with O
8 O
datasets O
representing O
a O
variety O
of O
text O
classification O
tasks O
: O
20 B-DatasetName
newsgroup I-DatasetName
( O
Lang O
, O
1995 O
) O
, O
AG B-DatasetName
's I-DatasetName
news I-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
) O
, O
Amazon B-DatasetName
reviews I-DatasetName
( O
McAuley O
and O
Leskovec O
, O
2013 O
) O
, O
DBPedia B-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
) O
, O
GoEmotions B-DatasetName
( O
Demszky O
et O
al. O
, O
2020 O
) O
, O
IMDB B-DatasetName
reviews I-DatasetName
( O
Maas O
et O
al. O
, O
2011 O
) O
, O
ISEAR B-DatasetName
( O
Shao O
et O
al. O
, O
2015 O
) O
, O
and O
Yahoo B-DatasetName
! I-DatasetName
Answers I-DatasetName
( O
Zhang O
et O
al. O
, O
2015 O
names O
were O
used O
to O
describe O
the O
target O
labels O
for O
zero-shot O
inference O
2 O
. O
We O
report O
results O
on O
the O
test O
set O
of O
each O
dataset O
( O
the O
labels O
of O
the O
train O
sets O
were O
not O
used O
as O
there O
is O
no O
training O
in O
our O
method O
) O
; O
preliminary O
experiments O
were O
conducted O
on O
separate O
development O
sets. O
Since O
we O
aim O
for O
a O
practical O
setting O
with O
lower O
computational O
costs O
, O
we O
limit O
the O
size O
of O
our O
unlabeled O
set O
U O
to O
a O
maximum O
of O
10K O
examples O
sampled O
from O
the O
full O
training O
set O
of O
each O
dataset. O
For O
details O
on O
the O
dataset O
sizes O
, O
task O
types O
, O
and O
label O
names O
, O
see O
App. O
A O
. O

Zero-Shot O
Models O
We O
evaluate O
3 O
off-the-shelf O
entailment O
models O
, O
trained O
on O
the O
MNLI B-DatasetName
( O
Williams O
et O
al. O
, O
2018 O
) O
dataset O
: O
roberta-large-mnli B-MethodName
, O
deberta-large-mnlizero-cls B-MethodName
, O
and O
bart-large-mnli. B-MethodName
To O
infer O
zero-shot O
predictions O
from O
these O
models O
with O
respect O
to O
the O
target O
labels O
we O
rely O
on O
the O
dedicated O
zero-shot B-TaskName
classification I-TaskName
pipeline I-TaskName
from O
the O
Hugging O
Face O
Transformers O
library O
3 O
, O
using O
the O
default O
hypothesis O
template O
" O
This O
example O
is O
[ O
] O
. O
" O
. O

Implementation O
Details O
Each O
experiment O
is O
repeated O
5 O
times O
, O
with O
each O
repetition O
using O
a O
different O
random O
seed. O
All O
models O
are O
fine-tuned O
for O
one B-HyperparameterValue
epoch B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2 B-HyperparameterValue
× I-HyperparameterValue
10 I-HyperparameterValue
−5 I-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
using O
the O
AdamW O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
and O
cross O
entropy O
loss O
to O
optimize O
the O
models. O
A O
single O
NVIDIA O
A100 O
GPU O
was O
used O
for O
fine-tuning O
and O
inference. O
We O
base O
our O
implementation O
on O
Hugging O
Face O
Transformers O
( O
Wolf O
et O
al. O
, O
2019 O
) O
mask O
a O
token O
in O
the O
example O
texts O
based O
on O
similarity O
to O
the O
predicted O
class O
names. O
For O
each O
example O
we O
extract O
the O
GloVe O
( O
Pennington O
et O
al. O
, O
2014 O
) O
representations O
for O
each O
token O
in O
the O
example O
text O
, O
and O
for O
the O
predicted O
class O
name. O
Where O
the O
class O
name O
is O
an O
ngram O
, O
we O
average O
over O
its O
unigrams. O
Representations O
are O
extracted O
using O
the O
en-core-web-lg O
model O
from O
the O
spacy O
library O
, O
after O
removing O
punctuation O
and O
stopwords O
. O

As O
illustrated O
in O
Fig. O
2 O
, O
for O
each O
example O
, O
we O
select O
the O
token O
with O
the O
largest O
GloVe O
similarity O
to O
the O
class O
name. O
This O
token O
is O
then O
masked O
from O
the O
text O
by O
replacing O
it O
with O
the O
model O
's O
special O
unknown O
token O
( O
< O
unk O
> O
for O
RoBERTa O
and O
BART O
, O
[ O
UNK O
] O
for O
DeBERTa O
) O
. O

Experimental O
Results O
We O
set O
n B-HyperparameterName
, O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
examples I-HyperparameterName
per I-HyperparameterName
class I-HyperparameterName
, O
to O
be O
1 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
unlabeled I-HyperparameterValue
set I-HyperparameterValue
U I-HyperparameterValue
( O
i.e. O
, O
n O
= O
100 O
for O
a O
U O
of O
size O
10k O
) O
. O
For O
each O
dataset O
and O
zero-shot O
model O
, O
we O
perform O
two B-HyperparameterValue
iterations I-HyperparameterValue
of O
self-training. B-HyperparameterName
4 O
We O
test O
4 O
settings O
of O
adding O
contradict O
examples O
as O
described O
in O
Section O
2.2.2 O
. O

Classification B-MetricName
accuracy I-MetricName
before O
and O
after O
selftraining O
for O
all O
models O
using O
the O
Contrast-random O
setting O
is O
shown O
in O
Table O
1. O
The O
results O
demonstrate O
a O
clear O
and O
significant O
benefit O
to O
the O
self-training O
process O
, O
across O
all O
models O
and O
datasets. O
Significance O
was O
tested O
with O
paired O
t-tests O
to O
compare O
accuracy O
with O
and O
without O
self-training O
, O
pooling O
together O
all O
datasets O
and O
seeds O
for O
each O
of O
the O
three O
models O
. O

Fig. O
3 O
compares O
the O
4 O
settings O
of O
selecting O
negative O
examples. O
As O
can O
be O
seen O
, O
among O
the O
four O
settings O
, O
Contrast-furthest O
yields O
the O
poorest O
re-sults. O
A O
possible O
explanation O
is O
that O
in O
this O
setting O
the O
negative O
examples O
are O
too O
trivial O
for O
the O
model. O
Contrast-closest O
yields O
better O
results O
, O
probably O
because O
the O
negative O
examples O
in O
this O
setting O
are O
more O
difficult O
and O
thus O
more O
informative O
for O
the O
model. O
However O
, O
for O
this O
same O
reason O
, O
these O
pseudo O
labeled O
examples O
are O
expected O
to O
suffer O
from O
the O
largest O
noise. O
The O
best O
performing O
settings O
are O
Contrast-all O
and O
Contrast-random O
, O
which O
represent O
a O
better O
balance O
between O
the O
informativeness O
of O
the O
negative O
examples O
and O
their O
noise O
level. O
Taking O
the O
computational O
costs O
into O
account O
, O
Contrast-random O
emerges O
as O
the O
preferred O
setting O
. O

The O
contribution O
of O
token O
masking O
One O
component O
of O
our O
approach O
, O
which O
aims O
at O
increasing O
the O
informativeness O
of O
pseudo-labeled O
examples O
, O
is O
the O
masking O
of O
the O
token O
closest O
to O
the O
class O
name O
( O
§3.4 O
) O
. O

We O
examine O
the O
performance O
of O
self-training O
without O
the O
masking O
procedure. O
A O
comparison O
of O
classification O
accuracy O
with O
and O
without O
applying O
token O
masking O
is O
shown O
in O
Table O
2. O
Overall O
, O
applying O
token O
masking O
does O
provide O
a O
performance O
gainas O
confirmed O
by O
a O
paired O
t-test O
( O
p O
= O
3×10 O
−4 O
, O
pooling O
together O
all O
models O
, O
datasets O
and O
seeds O
) O
. O
However O
, O
as O
can O
be O
seen O
in O
Table O
2 O
, O
results O
do O
differ O
across O
models O
and O
datasets. O
For O
instance O
, O
masking O
affords O
more O
consistent O
benefits O
in O
the O
case O
of O
the O
RoBERTa B-MethodName
entailment I-MethodName
model I-MethodName
, O
and O
has O
a O
more O
pronounced O
effect O
in O
the O
case O
of O
the O
ISEAR B-DatasetName
dataset O
. O

The O
beneficial O
effect O
of O
masking O
raises O
the O
question O
of O
whether O
the O
pseudo-labeled O
train O
set O
, O
i.e. O
, O
the O
model O
's O
most O
confident O
predictions O
, O
are O
trivial O
examples O
that O
could O
just O
as O
easily O
have O
been O
obtained O
using O
a O
simple O
heuristic. O
To O
test O
this O
, O
we O
construct O
an O
alternative O
pseudo-labeled O
set O
that O
is O
based O
on O
a O
token-level O
heuristic O
rather O
than O
the O
model O
predictions. O
This O
example O
selection O
method O
had O
a O
substantial O
negative O
effect O
on O
performance O
( O
see O
App. O
B O
for O
details O
) O
. O

Cross-task O
effects O
A O
recurring O
question O
in O
transfer O
learning O
is O
whether O
fine-tuning O
on O
a O
task O
T O
i O
can O
translate O
to O
benefits O
on O
another O
task O
, O
T O
j O
( O
e.g. O
, O
Phang O
et O
al. O
, O
2018 O
; O
Aghajanyan O
et O
al. O
, O
2021 O
) O
. O
It O
is O
thus O
interesting O
to O
study O
this O
question O
in O
the O
present O
context O
of O
self-training O
zero-shot O
classifiers. O
In O
other O
words O
, O
can O
exposure O
to O
pseudo-labels O
for O
task O
T O
i O
improve O
zero-shot O
performance O
on O
task O
T O
j O
. O
One O
aspect O
that O
is O
specific O
to O
our O
scenario O
is O
that O
fine-tuning O
with O
pseudo-labels O
on O
T O
i O
exposes O
the O
model O
to O
the O
task O
template O
which O
is O
also O
used O
for O
T O
j O
. O

To O
explore O
this O
question O
, O
each O
model O
that O
was O
self-trained O
on O
T O
i O
is O
evaluated O
over O
all O
the O
other O
datasets O
as O
well. O
Fig. O
4 O
depicts O
the O
cross-task O
effects O
of O
self-training O
on O
performance. O
This O
analysis O
reveals O
that O
self-training O
on O
a O
different O
task O
can O
be O
beneficial O
or O
harmful. O
The O
topical O
datasets O
( O
20 B-DatasetName
newsgroup I-DatasetName
, O
AG B-DatasetName
's I-DatasetName
news I-DatasetName
, O
DBPedia B-DatasetName
, O
and O
Yahoo B-DatasetName
! I-DatasetName
answers I-DatasetName
) O
appear O
to O
be O
beneficial O
to O
each O
other O
; O
as O
do O
the O
two O
emotion O
classification O
datasets O
( O
GoEmotions B-DatasetName
and O
ISEAR B-DatasetName
) O
. O
In O
contrast O
, O
self-training O
on O
sentiment O
data O
( O
Amazon B-DatasetName
, O
IMDB B-DatasetName
) O
leads O
to O
significant O
degradation O
in O
results O
on O
the O
emotion O
datasets. O
Possibly O
, O
this O
is O
related O
to O
particular O
characteristics O
of O
the O
reviews O
domain O
, O
along O
with O
the O
sharp O
binary O
distinction O
between O
positive O
and O
negative O
sentiment O
, O
as O
opposed O
to O
the O
subtle O
nuances O
that O
are O
necessary O
to O
distinguish O
between O
different O
types O
of O
emotions. O
6 O
Related O
Work O

Self-training O
( O
Scudder O
, O
1965 O
) O
has O
a O
long O
history O
as O
a O
method O
for O
semi-supervised O
learning O
, O
where O
predictions O
of O
a O
supervised O
classifier O
on O
unlabeled O
examples O
are O
used O
as O
pseudo-labels O
to O
augment O
the O
amount O
of O
training O
data. O
This O
approach O
has O
successfully O
been O
applied O
to O
a O
wide O
range O
of O
machine O
learning O
problems O
( O
Van O
Engelen O
and O
Hoos O
, O
2020 O
) O
. O
Many O
variations O
of O
self-training O
have O
been O
put O
forth O
, O
varying O
in O
terms O
of O
the O
selection O
strategy O
of O
samples O
to O
pseudo-label O
, O
the O
amount O
-and O
characteristicsof O
the O
models O
involved O
in O
the O
procedure O
, O
and O
other O
specific O
design O
decisions O
( O
Triguero O
et O
al. O
, O
2015 O
) O
. O
From O
a O
more O
theoretical O
standpoint O
, O
previous O
works O
( O
Lee O
et O
al. O
, O
2013 O
) O
have O
described O
selftraining O
as O
somewhat O
equivalent O
to O
entropy O
minimization O
( O
Grandvalet O
and O
Bengio O
, O
2004 O
) O
, O
in O
that O
it O
modifies O
the O
model O
's O
decision O
boundaries O
by O
driving O
the O
model O
to O
make O
more O
confident O
predictions O
. O

Aiming O
for O
more O
general-purpose O
models O
, O
that O
can O
achieve O
cross-task O
generalization O
and O
perform O
in O
a O
zero-shot O
scenario O
, O
recent O
works O
have O
proposed O
different O
strategies O
for O
mapping O
a O
range O
of O
NLP O
tasks O
into O
a O
generic O
and O
unified O
framework. O
Yin O
et O
al. O
( O
2019 O
) O
suggest O
the O
textual O
entailment O
paradigm O
as O
one O
that O
can O
encompass O
different O
types O
of O
text O
classification O
tasks. O
Zhong O
et O
al. O
( O
2021 O
) O
map O
classification O
tasks O
to O
a O
question-answering O
format O
, O
where O
each O
class O
is O
formulated O
as O
a O
question O
and O
given O
as O
a O
prompt O
, O
and O
the O
decoder O
probabilities O
of O
the O
Yes O
and O
No O
tokens O
correspond O
to O
a O
positive O
or O
negative O
prediction O
for O
the O
class. O
They O
also O
propose O
a O
" O
meta-tuning O
" O
paradigm O
, O
where O
labeled O
data O
for O
different O
tasks O
-formulated O
in O
terms O
of O
the O
unified O
task O
format O
-is O
utilized O
in O
order O
to O
teach O
the O
model O
how O
to O
solve O
the O
generic O
" O
meta-task O
" O
, O
and O
thus O
better O
cope O
with O
unseen O
tasks O
at O
test O
time. O
By O
opting O
for O
a O
generic O
cross-task O
format O
of O
natural O
language O
instructions O
, O
Wei O
et O
al. O
( O
2022 O
) O
and O
Sanh O
et O
al. O
( O
2022 O
) O
extend O
this O
notion O
even O
further O
, O
where O
meta-tuning O
on O
multiple O
types O
of O
NLP O
tasks O
enables O
zero-shot O
prediction O
even O
on O
tasks O
of O
a O
very O
different O
nature O
from O
those O
seen O
during O
training O
. O

In O
the O
present O
work O
we O
explore O
the O
intersection O
of O
these O
two O
threads O
-namely O
, O
self-training O
and O
general O
purpose O
zero-shot O
models O
, O
while O
focusing O
on O
zero-shot O
text O
classifiers O
that O
use O
the O
entailment O
paradigm O
, O
and O
on O
a O
scenario O
where O
only O
unlabeled O
data O
is O
available. O
Ye O
et O
al. O
( O
2020 O
) O
apply O
self-training O
to O
text O
classification O
in O
order O
to O
transfer O
to O
unseen O
classes O
for O
which O
there O
is O
no O
labeled O
data O
, O
and O
propose O
a O
reinforcement O
learning O
method O
for O
selecting O
examples O
to O
pseudo-label. O
This O
scenario O
differs O
substantially O
from O
ours O
in O
that O
self-training O
is O
not O
applied O
to O
an O
existing O
general-purpose O
zero-shot O
model. O
In O
addition O
, O
they O
deal O
with O
a O
setting O
where O
labeled O
data O
for O
some O
of O
the O
target O
classes O
is O
available O
. O

Like O
the O
present O
work O
, O
Zhou O
et O
al. O
( O
2022 O
) O
also O
aim O
to O
improve O
existing O
general-purpose O
zero-shot O
learners O
by O
utilizing O
unlabeled O
data. O
Starting O
from O
T0 O
, O
the O
prompt-based O
zero-shot O
learner O
from O
Sanh O
et O
al. O
( O
2022 O
) O
, O
they O
use O
unlabeled O
texts O
to O
apply O
a O
prompt O
consistency O
loss O
: O
an O
example O
is O
fed O
into O
the O
model O
multiple O
times O
, O
each O
time O
in O
the O
context O
of O
a O
different O
-but O
synonymous O
-task O
prompt O
; O
then O
, O
the O
model O
is O
trained O
to O
assign O
similar O
predictions O
across O
differently-phrased O
prompts O
( O
Zhou O
et O
al. O
, O
2022 O
) O
. O
Thus O
, O
whereas O
we O
explore O
improving O
a O
generalpurpose O
model O
using O
a O
form O
of O
self-training O
, O
they O
do O
so O
using O
a O
variation O
on O
the O
paradigm O
of O
consistency O
training O
( O
Xie O
et O
al. O
, O
2020 O
) O
. O
Some O
works O
attempt O
to O
improve O
the O
performance O
of O
general-purpose O
models O
within O
a O
few-shot O
scenario. O
For O
example O
, O
Basile O
et O
al. O
( O
2021 O
) O
experiment O
with O
entailment-based O
classifiers. O
They O
show O
that O
compared O
to O
a O
standard O
pre-trained O
language O
model O
, O
off-the-shelf O
entailment O
models O
require O
less O
labeled O
examples O
for O
fine-tuning O
to O
reach O
reasonable O
performance O
on O
an O
emotion O
classification O
task O
. O

Discussion O
In O
this O
paper O
we O
look O
at O
the O
applicability O
of O
selftraining O
for O
adapting O
a O
general-purpose O
zero-shot O
model O
, O
focusing O
on O
the O
scenario O
of O
entailmentbased O
models. O
We O
opted O
for O
this O
specific O
setting O
due O
to O
the O
high O
accessibility O
of O
these O
off-the-shelf O
models. O
In O
other O
words O
, O
given O
that O
these O
models O
are O
readily O
available O
for O
use O
, O
we O
ask O
whether O
selftraining O
provides O
a O
straightforward O
way O
for O
practitioners O
to O
adapt O
the O
general O
model O
for O
their O
downstream O
task O
, O
using O
only O
a O
modest O
collection O
of O
unlabeled O
data. O
We O
show O
that O
in O
this O
setting O
self-training O
does O
indeed O
provide O
value O
, O
delivering O
significant O
performance O
gains O
for O
text O
classification O
. O

The O
notion O
of O
using O
self-training O
as O
a O
tool O
to O
adapt O
a O
general-purpose O
zero-shot O
model O
is O
not O
specific O
to O
entailment O
models O
, O
nor O
is O
it O
limited O
to O
classification O
tasks. O
Thus O
, O
a O
major O
avenue O
for O
future O
work O
would O
be O
to O
explore O
this O
combination O
on O
models O
that O
rely O
on O
different O
kinds O
of O
mapping O
functions O
or O
" O
meta-tasks O
" O
for O
formulating O
downstream O
tasks O
within O
a O
generic O
cross-task O
framework O
( O
Wei O
et O
al. O
, O
2022 O
; O
Zhong O
et O
al. O
, O
2021 O
; O
Bragg O
et O
al. O
, O
2021 O
; O
Sanh O
et O
al. O
, O
2022 O
) O
. O

Zero-shot B-TaskName
text I-TaskName
classification I-TaskName
is O
recently O
drawing O
much O
attention O
, O
with O
prominent O
recent O
works O
showing O
promising O
results O
using O
different O
kinds O
of O
iterative O
approaches O
( O
Meng O
et O
al. O
, O
2020 O
; O
. O
Such O
approaches O
build O
their O
zero-shot O
classi-fiers O
from O
scratch O
-and O
therefore O
typically O
require O
larger O
unlabeled O
datasets O
to O
perform O
well O
-whereas O
we O
aim O
to O
utilize O
and O
build O
on O
the O
knowledge O
contained O
in O
general-purpose O
zero-shot O
classifiers. O
Exploring O
ways O
to O
combine O
these O
differing O
approaches O
is O
left O
for O
future O
work O
. O

Importantly O
, O
while O
our O
method O
does O
assume O
the O
existence O
of O
a O
collection O
of O
unlabeled O
examples O
, O
our O
results O
show O
that O
an O
order O
of O
10K O
examples O
is O
sufficient O
to O
benefit O
from O
self-training. O
Moreover O
, O
the O
cross-task O
effects O
in O
section O
5.2 O
demonstrate O
that O
even O
unlabeled O
examples O
from O
a O
similar O
domain O
and O
/ O
or O
task O
may O
be O
useful O
in O
adapting O
the O
generalpurpose O
model O
for O
the O
downstream O
task. O
Determining O
the O
exact O
conditions O
under O
which O
self-training O
is O
useful O
for O
adaptation O
across O
tasks O
is O
a O
matter O
for O
future O
study. O
Moreover O
, O
it O
would O
be O
interesting O
to O
explore O
the O
effects O
of O
self-training O
on O
multiple O
datasets O
, O
akin O
to O
works O
on O
supervised O
multi-task O
fine-tuning O
( O
e.g. O
, O
Aghajanyan O
et O
al. O
, O
2021 O
) O
. O

In O
our O
work O
, O
we O
select O
examples O
for O
pseudolabeling O
based O
solely O
on O
model O
confidence O
( O
see O
§2.2 O
) O
. O
Some O
self-training O
works O
opt O
for O
more O
balanced O
approaches O
for O
example O
selection O
, O
aiming O
for O
a O
more O
diverse O
and O
/ O
or O
more O
informative O
set O
of O
examples O
( O
e.g. O
, O
Hajmohammadi O
et O
al. O
, O
2015 O
; O
Mukherjee O
and O
Awadallah O
, O
2020 O
) O
. O
It O
would O
be O
interesting O
to O
explore O
such O
questions O
in O
the O
zero-shot O
scenario. O
In O
addition O
, O
in O
Section O
2.2 O
we O
describe O
our O
method O
to O
select O
confident O
examples O
, O
namely O
by O
looking O
at O
the O
maximal O
delta O
between O
the O
highest O
and O
second O
highest O
prediction O
scores. O
Other O
alternatives O
for O
choosing O
confident O
examples O
, O
e.g. O
, O
by O
looking O
at O
the O
entropy O
across O
classes O
, O
could O
be O
tested O
as O
well O
. O

To O
conclude O
, O
the O
method O
we O
proposed O
in O
this O
paper O
can O
boost O
the O
performance O
of O
entailmentbased B-TaskName
zero-shot I-TaskName
text I-TaskName
classifiers I-TaskName
, O
with O
little O
effort O
and O
a O
modest O
amount O
of O
domain O
data. O
This O
can O
prove O
useful O
to O
the O
many O
practitioners O
who O
benefit O
from O
the O
practicality O
and O
accessibility O
of O
these O
models O
. O

Limitations O
Our O
focus O
here O
is O
on O
off-the-shelf O
models O
that O
are O
highly O
accessible O
-and O
thus O
potentially O
usefulfor O
practitioners. O
Nevertheless O
, O
these O
models O
are O
quite O
large O
, O
and O
thus O
carry O
a O
non-negligible O
computational O
footprint. O
For O
instance O
, O
inferring O
on O
10K O
unlabeled O
samples O
does O
require O
a O
GPU O
, O
limiting O
access O
to O
such O
approaches O
and O
models O
in O
practice O
. O

Our O
work O
is O
empirical O
in O
nature. O
As O
such O
, O
we O
report O
experimental O
results O
, O
with O
no O
theoretical O
guar-antees O
, O
and O
one O
should O
recognize O
the O
existence O
of O
exceptions. O
In O
addition O
, O
our O
experimental O
results O
are O
for O
relatively O
standard O
academic O
benchmarks O
for O
text O
classification. O
Real-world O
datasets O
, O
especially O
in O
specific O
domains O
such O
as O
legal O
and O
healthcare O
, O
may O
pose O
additional O
challenges. O
The O
practical O
value O
of O
our O
approach O
in O
these O
cases O
is O
yet O
to O
be O
seen O
. O

We O
formulate O
and O
test O
our O
approach O
in O
the O
scenario O
where O
each O
example O
should O
be O
assigned O
to O
exactly O
one O
class. O
Applying O
our O
method O
to O
the O
multi-label O
classification O
scenario O
might O
not O
be O
straightforward O
, O
and O
may O
require O
different O
ways O
of O
selecting O
examples O
for O
the O
pseudo-labeled O
set O
. O

Finally O
, O
the O
large O
scale O
of O
our O
experiments O
places O
a O
non-trivial O
burden O
on O
trying O
to O
replicate O
our O
results. O
Moreover O
, O
the O
off-the-shelf O
models O
used O
in O
our O
experiments O
are O
not O
guaranteed O
to O
be O
hosted O
publicly O
in O
the O
future O
. O

B O
Heuristic-based O
selection O
As O
stated O
in O
section O
5.1 O
, O
we O
experiment O
with O
constructing O
an O
alternative O
pseudo-labeled O
train O
set O
that O
is O
based O
on O
a O
token-level O
heuristic. O
In O
this O
method O
, O
the O
examples O
are O
chosen O
based O
on O
GloVe-based O
similarity O
to O
the O
class O
names. O
First O
, O
for O
each O
example O
and O
class O
we O
calculate O
a O
" O
GloVe-to-closest-token O
" O
score O
, O
which O
is O
the O
similarity O
between O
the O
class O
and O
the O
closest O
token O
in O
the O
example O
, O
following O
a O
similar O
protocol O
as O
that O
for O
finding O
tokens O
to O
mask O
( O
cf. O
3.4 O
) O
. O
Then O
, O
for O
each O
class O
c O
we O
construct O
a O
list O
of O
size O
n O
of O
the O
top O
candidates O
: O
we O
take O
the O
examples O
where O
the O
" O
GloVe-to-closest-token O
" O
score O
was O
highest O
for O
c O
; O
these O
examples O
are O
sorted O
by O
the O
difference O
between O
the O
" O
GloVe-to-closest-token O
" O
score O
for O
c O
and O
for O
the O
class O
with O
the O
second O
highest O
score O
, O
and O
the O
top O
n O
examples O
are O
selected. O
We O
apply O
masking O
for O
the O
selected O
examples O
using O
the O
same O
protocol O
we O
use O
in O
the O
self-training O
approach. O
Fig. O
5 O
compares O
this O
approach O
to O
a O
single O
iteration O
of O
self-training. O
As O
can O
be O
seen O
, O
for O
some O
datasets O
this O
pseudo-labeling O
approach O
does O
improve O
the O
zero-shot O
classifier O
, O
yet O
, O
the O
results O
are O
not O
consistent O
across O
datasets O
and O
in O
4 O
of O
the O
datasets O
applying O
this O
approach O
results O
in O
a O
lower O
accuracy B-MetricName
compared O
to O
the O
zero-shot O
classifier O
. O

-DOCSTART- O
Fine-tuning O
Pre-trained O
Language O
Models O
for O
Few-shot O
Intent O
Detection O
: O
Supervised O
Pre-training O
and O
Isotropization O

It O
is O
challenging O
to O
train O
a O
good O
intent O
classifier O
for O
a O
task-oriented O
dialogue O
system O
with O
only O
a O
few O
annotations. O
Recent O
studies O
have O
shown O
that O
fine-tuning O
pre-trained O
language O
models O
with O
a O
small O
amount O
of O
labeled O
utterances O
from O
public O
benchmarks O
in O
a O
supervised O
manner O
is O
extremely O
helpful. O
However O
, O
we O
find O
that O
supervised O
pre-training O
yields O
an O
anisotropic O
feature O
space O
, O
which O
may O
suppress O
the O
expressive O
power O
of O
the O
semantic O
representations. O
Inspired O
by O
recent O
research O
in O
isotropization O
, O
we O
propose O
to O
improve O
supervised O
pretraining O
by O
regularizing O
the O
feature O
space O
towards O
isotropy. B-MetricName
We O
propose O
two O
regularizers O
based O
on O
contrastive B-HyperparameterName
learning I-HyperparameterName
and O
correlation B-HyperparameterName
matrix I-HyperparameterName
respectively O
, O
and O
demonstrate O
their O
effectiveness O
through O
extensive O
experiments. O
Our O
main O
finding O
is O
that O
it O
is O
promising O
to O
regularize O
supervised O
pre-training O
with O
isotropization O
to O
further O
improve O
the O
performance O
of O
few-shot O
intent O
detection. O
The O
source O
code O
can O
be O
found O
at O
https O
: O
/ O
/ O
github.com O
/ O
fanolabs O
/ O
isoIntentBert-main O
. O

Introduction B-MethodName
Intent I-MethodName
detection I-MethodName
is O
a O
core O
module O
of O
task-oriented O
dialogue O
systems. O
Training O
a O
well-performing O
intent O
classifier O
with O
only O
a O
few O
annotations O
, O
i.e. O
, O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
is O
of O
great O
practical O
value. O
Recently O
, O
this O
problem O
has O
attracted O
considerable O
attention O
( O
Vulić O
et O
al. O
, O
2021 O
; O
Zhang O
et O
al. O
, O
b O
; O
Dopierre O
et O
al. O
, O
b O
) O
but O
remains O
a O
challenge O
. O

To O
tackle O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
earlier O
works O
employ O
induction O
network O
( O
Geng O
et O
al. O
, O
2019 O
) O
, O
generation-based O
methods O
( O
Xia O
et O
al. O
, O
a O
) O
, O
metric O
learning O
( O
Nguyen O
et O
al. O
, O
2020 O
) O
, O
and O
selftraining O
( O
Dopierre O
et O
al. O
, O
b O
) O
, O
to O
design O
sophisticated O
algorithms. O
Recently O
, O
pre-trained B-MethodName
language I-MethodName
models I-MethodName
( O
PLMs B-MethodName
) O
have O
emerged O
as O
a O
simple O
yet O
promising O
solution O
to O
a O
wide O
spectrum O
of O
natural B-MethodName
language I-MethodName
processing I-MethodName
( O
NLP B-MethodName
) O
tasks O
, O
triggering O
the O
surge O
of O
PLM- B-MethodName
* O
Corresponding O
author O
. O

based O
solutions O
for O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
Zhang O
et O
al. O
, O
a O
, O
b O
; O
Vulić O
et O
al. O
, O
2021 O
; O
Zhang O
et O
al. O
, O
b O
) O
, O
which O
typically O
fine-tune O
PLMs B-MethodName
on O
conversation O
data O
. O

A O
PLM-based B-MethodName
fine-tuning O
method O
( O
Zhang O
et O
al. O
, O
a O
) O
, O
called O
IntentBERT B-MethodName
, O
utilizes O
a O
small O
amount O
of O
labeled O
utterances O
from O
public O
intent O
datasets O
to O
fine-tune O
PLMs B-MethodName
with O
a O
standard O
classification O
task O
, O
which O
is O
referred O
to O
as O
supervised O
pre-training. O
Despite O
its O
simplicity O
, O
supervised O
pre-training O
has O
been O
shown O
extremely O
useful O
for O
few-shot O
intent O
detection O
even O
when O
the O
target O
data O
and O
the O
data O
used O
for O
fine-tuning O
are O
very O
different O
in O
semantics. O
However O
, O
as O
will O
be O
shown O
in O
Section O
3.2 O
, O
IntentBERT B-MethodName
suffers O
from O
severe O
anisotropy O
, O
an O
undesirable O
property O
of O
PLMs B-MethodName
Ethayarajh O
, O
2019 O
; O
Li O
et O
al. O
, O
2020 O
) O
. O

Anisotropy O
is O
a O
geometric O
property O
that O
semantic O
vectors O
fall O
into O
a O
narrow O
cone. O
It O
has O
been O
identified O
as O
a O
crucial O
factor O
for O
the O
sub-optimal O
performance O
of O
PLMs B-MethodName
on O
a O
variety O
of O
downstream O
tasks O
Arora O
et O
al. O
, O
b O
; O
Cai O
et O
al. O
, O
2020 O
; O
Ethayarajh O
, O
2019 O
) O
, O
which O
is O
also O
known O
as O
the O
representation O
degeneration O
problem O
( O
Gao O
et O
al. O
, O
a O
) O
. O
Fortunately O
, O
isotropization O
techniques O
can O
be O
applied O
to O
adjust O
the O
embedding O
space O
and O
yield O
significant O
performance O
improvement O
in O
many O
tasks O
Rajaee O
and O
Pilehvar O
, O
2021a O
) O
. O

Hence O
, O
this O
paper O
aims O
to O
answer O
the O
question O
: O

• O
Can O
we O
improve O
supervised O
pre-training O
via O
isotropization O
for O
few-shot O
intent O
detection O
? O

Many O
isotropization O
techniques O
have O
been O
developed O
based O
on O
transformation O
Huang O
et O
al. O
, O
2021 O
) O
, O
contrastive O
learning O
( O
Gao O
et O
al. O
, O
b O
) O
, O
and O
top O
principal O
components O
elimination O
( O
Mu O
and O
Viswanath O
, O
2018 O
) O
. O
However O
, O
these O
methods O
are O
designed O
for O
off-the-shelf O
PLMs. B-MethodName
When O
applied O
on O
PLMs B-MethodName
that O
have O
been O
fine-tuned O
on O
some O
NLP B-MethodName
task O
such O
as O
semantic B-TaskName
textual I-TaskName
similarity I-TaskName
or O
intent B-TaskName
classification I-TaskName
, O
they O
may O
introduce O
an O
adverse O
effect O
, O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
are O
designed O
to O
regularize O
SPT B-MethodName
and O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
, O
which O
leads O
to O
better O
performance O
on O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
. O

as O
observed O
in O
Rajaee O
and O
Pilehvar O
( O
2021c O
) O
and O
our O
pilot O
experiments. O
In O
this O
work O
, O
we O
propose O
to O
regularize O
supervised O
pre-training O
with O
isotropic O
regularizers. O
As O
shown O
in O
Fig. O
1 O
, O
we O
devise O
two O
regularizers O
, O
a O
contrastive-learning-based B-HyperparameterName
regularizer I-HyperparameterName
( O
CL-Reg B-HyperparameterName
) O
and O
a O
correlation-matrix-based B-HyperparameterName
regularizer I-HyperparameterName
( O
Cor-Reg B-HyperparameterName
) O
, O
each O
of O
which O
can O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
during O
supervised O
training. O
Our O
empirical O
study O
shows O
that O
the O
regularizers O
can O
significantly O
improve O
the O
performance O
of O
standard O
supervised O
training O
, O
and O
better O
performance O
can O
often O
be O
achieved O
when O
they O
are O
combined O
. O

The O
contributions O
of O
this O
work O
are O
three-fold O
: O

• O
We O
present O
the O
first O
study O
on O
the O
isotropy B-MetricName
property O
of O
PLMs B-MethodName
for O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
shedding O
light O
on O
the O
interaction O
of O
supervised O
pre-training O
and O
isotropization O
. O

• O
We O
improve O
supervised O
pre-training O
by O
devising O
two O
simple O
yet O
effective O
regularizers O
to O
increase O
the O
isotropy B-MetricName
of O
the O
feature O
space O
. O

• O
We O
conduct O
a O
comprehensive O
evaluation O
and O
analysis O
to O
validate O
the O
effectiveness O
of O
the O
proposed O
approach O
. O

2 O
Related O
Works O

Few-shot B-TaskName
Intent I-TaskName
Detection I-TaskName
With O
a O
surge O
of O
interest O
in O
few-shot O
learning O
( O
Finn O
et O
al. O
, O
2017 O
; O
Vinyals O
et O
al. O
, O
2016 O
; O
Snell O
et O
al. O
, O
2017 O
) O
, O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
has O
started O
to O
receive O
attention. O
Earlier O
works O
mainly O
focus O
on O
model O
design O
, O
using O
capsule O
network O
( O
Geng O
et O
al. O
, O
2019 O
) O
, O
variational O
autoencoder O
( O
Xia O
et O
al. O
, O
a O
) O
, O
or O
metric O
functions O
( O
Yu O
et O
al. O
, O
2018 O
; O
Nguyen O
et O
al. O
, O
2020 O
) O
. O
Recently O
, O
PLMs-based B-MethodName
methods O
have O
shown O
promising O
performance O
in O
a O
variety O
of O
NLP B-MethodName
tasks O
and O
become O
the O
model O
of O
choice O
for O
few-shot B-TaskName
intent I-TaskName
detection. I-TaskName
Zhang O
et O
al. O
( O
c O
) O
cast O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
into O
a O
natural B-MethodName
language I-MethodName
inference I-MethodName
( O
NLI B-MethodName
) O
problem O
and O
fine-tune O
PLMs B-MethodName
on O
NLI B-MethodName
datasets. O
Zhang O
et O
al. O
( O
b O
) O
propose O
to O
fine-tune O
PLMs B-MethodName
on O
unlabeled O
utterances O
by O
contrastive O
learning. O
Zhang O
et O
al. O
( O
a O
) O
leverage O
a O
small O
set O
of O
public O
annotated O
intent O
detection O
benchmarks O
to O
fine-tune O
PLMs B-MethodName
with O
standard O
supervised O
training O
and O
observe O
promising O
perfor-mance O
on O
cross-domain O
few-shot O
intent O
detection O
. O

Meanwhile O
, O
the O
study O
of O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
has O
been O
extended O
to O
other O
settings O
including O
semisupervised O
learning O
( O
Dopierre O
et O
al. O
, O
b O
, O
a O
) O
, O
generalized O
setting O
( O
Nguyen O
et O
al. O
, O
2020 O
) O
, O
multi-label O
classification O
( O
Hou O
et O
al. O
, O
2021 O
) O
, O
and O
incremental O
learning O
( O
Xia O
et O
al. O
, O
b O
) O
. O
In O
this O
work O
, O
we O
consider O
standard O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
following O
the O
setup O
of O
Zhang O
et O
al. O
( O
a O
) O
and O
aiming O
to O
improve O
supervised O
pre-training O
with O
isotropization O
. O

Further O
Pre-training O
PLMs B-MethodName
with O
Dialogue B-DatasetName
Corpora O
Recent O
works O
have O
shown O
that O
further O
pre-training O
off-the-shelf O
PLMs B-MethodName
using O
dialogue B-DatasetName
corpora O
( O
Henderson O
et O
al. O
, O
b O
; O
Peng O
et O
al. O
, O
2020Peng O
et O
al. O
, O
, O
2021 O
are O
beneficial O
for O
task-oriented O
downstream O
tasks O
such O
as O
intent O
detection. O
Specifically O
, O
TOD-BERT B-MethodName
conducts O
self-supervised O
learning O
on O
diverse O
task-oriented O
dialogue B-DatasetName
corpora. O
ConvBERT B-MethodName
( O
Mehri O
et O
al. O
, O
2020 O
) O
is O
pre-trained O
on O
a O
700 O
million O
open-domain O
dialogue B-DatasetName
corpus. O
Vulić O
et O
al. O
( O
2021 O
) O
propose O
a O
two-stage O
procedure O
: O
adaptive B-TaskName
conversational I-TaskName
fine-tuning I-TaskName
followed O
by O
task-tailored B-TaskName
conversational I-TaskName
fine-tuning. I-TaskName
In O
this O
work O
, O
we O
follow O
Zhang O
et O
al. O
( O
a O
) O
to O
further O
pre-train O
PLMs B-MethodName
using O
a O
small O
amount O
of O
labeled O
utterances O
from O
public O
intent O
detection O
benchmarks O
. O

Measuring O
isotropy B-MetricName
Following O
Mu O
and O
Viswanath O
( O
2018 O
) O
; O
Biś O
et O
al. O
( O
2021 O
) O
, O
we O
adopt O
the O
following O
measurement O
of O
isotropy B-MetricName
: O

I B-MetricName
( I-MetricName
V I-MetricName
) I-MetricName
= O
min O
c O
∈ O
C O
Z O
( O
c O
, O
V O
) O
max O
c O
∈ O
C O
Z O
( O
c O
, O
V O
) O
, O
( O
1 O
) O

where O
V O
∈ O
R O
N O
×d O
is O
the O
matrix O
of O
stacked O
embeddings O
of O
N O
utterances O
( O
note O
that O
the O
embeddings O
have O
zero O
mean O
) O
, O
C O
is O
the O
set O
of O
unit O
eigenvectors O
of O
V O
⊤ O
V O
, O
and O
Z O
( O
c O
, O
V O
) O
is O
the O
partition O
function O
( O
Arora O
et O
al. O
, O
b O
) O
defined O
as O
: O

Z O
( O
c O
, O
V O
) O
= O
N O
i=1 O
exp O
c O
⊤ O
v O
i O
, O
( O
2 O
) O

where O
v O
i O
is O
the O
i O
th O
row O
of O
V. O
I O
( O
V O
) O
∈ O
[ O
0 O
, O
1 O
] O
, O
and O
1 O
indicates O
perfect O
isotropy B-MetricName
. O

Fine-tuning O
Leads O
to O
Anisotropy O
To O
observe O
the O
impact O
of O
fine-tuning O
on O
isotropy B-MetricName
, O
we O
follow O
IntentBERT B-MethodName
( O
Zhang O
et O
al. O
, O
a O
) O
to O
fine-tune O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
with O
standard O
supervised O
training O
on O
a O
small O
set O
of O
an O
intent O
detection O
benchmark O
OOS O
( O
Larson O
et O
al. O
, O
2019 O
) O
( O
details O
are O
given O
in O
Section O
4.1 O
) O
. O
We O
then O
compare O
the O
isotropy B-MetricName
of O
the O
original O
embedding O
space O
( O
BERT B-MethodName
) O
and O
the O
embedding O
space O
after O
fine-tuning O
( O
IntentBERT B-MethodName
) O
on O
target O
datasets. O
As O
shown O
in O
Table O
1 O
, O
after O
finetuning O
, O
the O
isotropy B-MetricName
of O
the O
embedding O
space O
is O
notably O
decreased O
on O
all O
datasets. O
Hence O
, O
it O
can O
be O
seen O
that O
fine-tuning O
may O
render O
the O
feature O
space O
more O
anisotropic O
. O

Isotropization O
after O
Fine-tuning O
May O
Have O
an O
Adverse O
Effect O
To O
examine O
the O
effect O
of O
isotropization O
on O
a O
finetuned O
model O
, O
we O
apply O
two O
strong O
isotropization O
techniques O
to O
IntentBERT B-MethodName
: O
dropout-based O
contrastive O
learning O
( O
Gao O
et O
al. O
, O
b O
) O
and O
whitening O
transformation O
. O
The O
former O
fine-tunes O
PLMs B-MethodName
in O
a O
contrastive O
learning O
manner O
1 O
, O
while O
the O
latter O
transforms O
the O
semantic O
feature O
space O
into O
an O
isotropic O
space O
via O
matrix O
transformation. O
These O
methods O
have O
been O
demonstrated O
highly O
effective O
( O
Gao O
et O
al. O
, O
b O
; O
when O
applied O
to O
off-the-shelf O
PLMs B-MethodName
, O
but O
things O
are O
different O
when O
they O
are O
applied O
to O
fine-tuned O
models. O
As O
shown O
in O
Fig. O
2 O
, O
contrastive O
learning O
improves O
isotropy B-MetricName
, O
but O
it O
significantly O
lowers O
the O
performance O
on O
two O
benchmarks. O
As O
for O
whitening O
transformation O
, O
it O
has O
inconsistent O
effects O
on O
the O
two O
datasets O
, O
as O
shown O
in O
Fig. O
3. O
It O
hurts O
the O
performance O
on O
HWU64 B-DatasetName
( O
Fig. O
3a O
) O
but O
yields O
better O
results O
on O
BANKING77 B-DatasetName
( O
Fig. O
3b O
) O
, O
while O
producing O
nearly O
perfect O
isotropy B-MetricName
on O
both. O
The O
above O
observations O
indicate O
that O
isotropization O
may O
hurt O
fine-tuned O
models O
, O
which O
echoes O
the O
recent O
finding O
of O
Rajaee O
and O
Pilehvar O
. O

Method O
The O
pilot O
experiments O
reveal O
the O
anisotropy O
of O
a O
PLM B-MethodName
fine-tuned O
on O
intent O
detection O
tasks O
and O
the O
1 O
We O
refer O
the O
reader O
to O
the O
original O
paper O
for O
details. O
Whitening O
transformation O
leads O
to O
perfect O
isotropy B-MetricName
but O
has O
inconsistent O
effects O
on O
the O
performance O
. O

challenge O
of O
applying O
isotropization O
techiniques O
on O
the O
fine-tuned O
model. O
In O
this O
section O
, O
we O
propose O
a O
joint O
fine-tuning O
and O
isotropization O
framework. O
Specifically O
, O
we O
propose O
two O
regularizers O
to O
make O
the O
feature O
space O
more O
isotropic O
during O
fine-tuning. O
Before O
presenting O
our O
method O
, O
we O
first O
introduce O
supervised O
pre-training O
. O

Supervised O
Pre-training O
for O
Few-shot B-TaskName
Intent I-TaskName
Detection I-TaskName
Few-shot B-TaskName
intent I-TaskName
detection I-TaskName
targets O
to O
train O
a O
good O
intent O
classifier O
with O
only O
a O
few O
labeled O
data O
D O
target O
= O
{ O
( O
x O
i O
, O
y O
i O
) O
} O
Nt O
, O
where O
N B-HyperparameterName
t O
is O
the O
number B-HyperparameterName
of I-HyperparameterName
labeled I-HyperparameterName
samples I-HyperparameterName
in O
the O
target O
dataset O
, O
x B-HyperparameterName
i I-HyperparameterName
denotes O
the O
i B-HyperparameterName
th I-HyperparameterName
utterance I-HyperparameterName
, O
and O
y O
i O
is O
the O
label. O
x B-HyperparameterName
i I-HyperparameterName
is O
the O
i O
th O
utterance O
in O
a O
batch B-HyperparameterName
of O
size O
3. B-HyperparameterValue
In O
( O
a O
) O
, O
x B-HyperparameterName
i I-HyperparameterName
is O
fed O
to O
the O
PLM B-MethodName
twice O
with O
built-in O
dropout O
to O
produce O
two O
different O
representations O
of O
x B-HyperparameterName
i I-HyperparameterName
: O
h B-HyperparameterName
i I-HyperparameterName
and O
h B-HyperparameterName
+ I-HyperparameterName
i I-HyperparameterName
. O
Positive O
and O
negative O
pairs O
are O
then O
constructed O
for O
each O
x B-MethodName
i I-MethodName
. O
For O
example O
, O
h O
1 O
and O
h O
+ O
1 O
form O
a O
positive O
pair O
for O
x O
1 O
, O
while O
h O
1 O
and O
h O
+ O
2 O
, O
and O
h O
1 O
and O
h O
+ O
3 O
, O
form O
negative O
pairs O
for O
x O
1 O
. O
In O
( O
b O
) O
, O
the O
correlation O
matrix O
is O
estimated O
from O
h O
i O
, O
feature O
vectors O
generated O
by O
the O
PLM B-MethodName
, O
and O
is O
regularized O
towards O
the O
identity O
matrix. O
can O
work O
well O
when O
the O
label O
spaces O
of O
D O
source O
and O
D O
target O
are O
disjoint O
. O

Specifically O
, O
the O
pre-training O
is O
conducted O
by O
attaching O
a O
linear O
layer O
( O
as O
the O
classifier O
) O
on O
top O
of O
the O
utterance O
representation O
generated O
by O
the O
PLM B-MethodName
: O

p O
( O
y|h O
i O
) O
= O
softmax O
( O
Wh O
i O
+ O
b O
) O
∈ O
R O
L O
, O
( O
3 O

) O

where O
h O
i O
∈ O
R O
d O
is O

Regularizing O
Supervised O
Pre-training O
with O
Isotropization O
To O
mitigate O
the O
anisotropy O
of O
the O
PLM B-MethodName
fine-tuned O
by O
supervised O
pre-training O
, O
we O
propose O
a O
joint O
training O
objective O
by O
adding O
a O
regularization O
term O
L B-MetricName
reg I-MetricName
for O
isotropization O
: O

L O
= O
L O
ce O
( O
D O
source O
; O
θ O
) O
+ O
λL B-MetricName
reg I-MetricName
( O
D O
source O
; O
θ O
) O
, O
( O
5 O

) O

where O
λ B-HyperparameterName
is O
a O
weight B-HyperparameterName
parameter. I-HyperparameterName
The O
aim O
is O
to O
learn O
intent O
detection O
skills O
while O
maintaining O
an O
appropriate O
degree O
of O
isotropy. B-MetricName
We O
devise O
two O
different O
regularizers O
introduced O
as O
follows O
. O

Contrastive-learning-based B-TaskName
Regularizer. I-TaskName
Inspired O
by O
the O
recent O
success O
of O
contrastive O
learning O
in O
mitigating O
anisotropy O
( O
Yan O
et O
al. O
, O
2021 O
; O
Gao O
et O
al. O
, O
b O
) O
, O
we O
employ O
the O
dropout-based O
contrastive O
learning O
loss O
used O
in O
Gao O
et O
al. O
( O
b O
) O
as O
the O
regularizer O
: O

L B-MetricName
reg I-MetricName
= O
− O
1 O
N O
b O
N O
b O
i O
log O
e O
sim O
( O
h O
i O
, O
h O
+ O
i O
) O
/ O
τ O
N O
b O
j=1 O
e O
sim O
( O
h O
i O
, O
h O
+ O
j O
) O
/ O
τ O
. O
( O
6 O
) O

In O
particular O
, O
h O
i O
∈ O
R O
d O
and O
h O
+ O
i O
∈ O
R O
d O
are O
two O
different O
representations O
of O
utterance O
x O
i O
generated O
by O
the O
PLM B-MethodName
with O
built-in O
standard O
dropout O
( O
Srivastava O
et O
al. O
, O
2014 O
) O
, O
i.e. O
, O
x B-MetricName
i I-MetricName
is O
passed O
to O
the O
PLM B-MethodName
twice O
with O
different O
dropout O
masks O
to O
produce O
h O
i O
and O
h O
+ O
i O
. O
sim O
( O
h O
1 O
, O
h O
2 O
) O
denotes O
the O
cosine O
similarity O
between O
h O
1 O
and O
h O
2 O
. O
τ O
is O
the O
temperature O
parameter. O
N O
b O
is O
the O
batch O
size. O
Since O
h O
i O
and O
h O
+ O
i O
represent O
the O
same O
utterance O
, O
they O
form O
a O
positive O
pair. O
Similarly O
, O
h O
i O
and O
h O
+ O
j O
form O
a O
negative O
pair O
, O
since O
they O
represent O
different O
utterances. O
An O
example O
is O
given O
in O
Fig. O
4a. O
By O
minimizing O
the O
contrastive O
loss O
, O
positive O
pairs O
are O
pulled O
together O
while O
negative O
pairs O
are O
pushed O
away O
, O
which O
in O
theory O
enforces O
an O
isotropic O
feature O
space O
( O
Gao O
et O
al. O
, O
b O
) O
. O
In O
Gao O
et O
al. O
( O
b O
) O
, O
the O
contrastive O
loss O
is O
used O
as O
the O
single O
objective O
to O
fine-tune O
off-the-shelf O
PLMs O
in O
an O
unsupervised O
manner O
, O
while O
in O
this O
work O
we O
use O
it O
jointly O
with O
supervised O
pre-training O
to O
fine-tune O
PLMs B-MethodName
for O
fewshot O
learning O
. O

Correlation-matrix-based B-TaskName
Regularizer. I-TaskName
The O
above O
regularizer O
enforces O
isotropization O
implicitly O
. O

Here O
, O
we O
propose O
a O
new O
regularizer O
that O
explicitly O
enforces O
isotropization. O
The O
perfect O
isotropy B-MetricName
is O
characterized O
by O
zero O
covariance O
and O
uniform O
variance O
Zhou O
et O
al. O
, O
2021 O
) O
, O
i.e. O
, O
a O
covariance O
matrix O
with O
uniform O
diagonal O
elements O
and O
zero O
non-diagonal O
elements. O
Isotropization O
can O
be O
achieved O
by O
endowing O
the O
feature O
space O
with O
such O
statistical O
property. O
However O
, O
as O
will O
be O
shown O
in O
Section O
5.3 O
, O
it O
is O
difficult O
to O
determine O
the O
appropriate O
scale O
of O
variance. O
Therefore O
, O
we O
base O
the O
regularizer O
on O
correlation O
matrix O
: O

L B-MetricName
reg I-MetricName
= O
∥Σ O
− O
I∥ O
, O
( O
7 O
) O

where O
∥•∥ O
denotes O
Frobenius O
norm O
, O
I O
∈ O
R O
d×d O
is O
the O
identity O
matrix O
, O
Σ O
∈ O
R O
d×d O
is O
the O
correlation O
matrix O
with O
Σ O
ij O
being O
the O
Pearson O
correlation O
coefficient O
between O
the O
i O
th O
dimension O
and O
the O
j O
th O
dimension O
. O

As O
shown O
in O
Fig. O
4b O
, O
Σ O
is O
estimated O
with O
utterances O
in O
the O
current O
batch. O
By O
pushing O
the O
correlation O
matrix O
towards O
the O
identity O
matrix O
during O
training O
, O
we O
can O
learn O
a O
more O
isotropic O
feature O
space. O
Moreover O
, O
the O
proposed O
two O
regularizers O
can O
be O
used O
together O
as O
follows O
: O

L O
= O
L O
ce O
( O
D O
source O
; O
θ O
) O
+ O
λ O
1 O
L O
cl O
( O
D O
source O
; O
θ O
) O
+λ O
2 O
L O
cor O
( O
D O
source O
; O
θ O
) O
, O
( O
8 O
) O

where O
λ B-HyperparameterName
1 I-HyperparameterName
and O
λ B-HyperparameterName
2 I-HyperparameterName
are O
the O
weight B-HyperparameterName
parameters I-HyperparameterName
, O
and O
L O
cl O
and O
L O
cor O
denote O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
, O
respectively. O
Our O
experiments O
show O
that O
better O
performance O
is O
often O
observed O
when O
they O
are O
used O
together O
. O

Experimental O
Setup O
Datasets. O
Gaming O
" O
. O
HWU64 B-DatasetName
( O
Liu O
et O
al. O
, O
2019a O
) O
is O
a O
largescale O
dataset O
containing O
21 O
domains. O
Dataset O
statistics O
are O
summarized O
in O
Table O
3 O
. O

Our O
Method. O
Our O
method O
can O
be O
applied O
to O
fine-tune O
any O
PLM. B-MethodName
We O
conduct O
experiments O
on O
two O
popular O
PLMs B-MethodName
, O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al. O
, O
2019b O
) O
. O
For O
both O
of O
them O
, O
the O
embedding O
of O
[ O
CLS O
] O
is O
used O
as O
the O
utterance O
representation O
in O
Eq. O
3. O
We O
employ O
logistic O
regression O
as O
the O
classifier. O
We O
select O
the O
hyperparameters O
λ B-HyperparameterName
, O
λ B-HyperparameterName
1 I-HyperparameterName
, O
λ B-HyperparameterName
2 I-HyperparameterName
, O
and O
τ B-HyperparameterName
by O
validation. O
The O
best O
hyperparameters O
are O
provided O
in O
Table O
4. O
Baselines. O
We O
compare O
our O
method O
to O
the O
following O
baselines O
. O

First O
, O
for O
BERT-based B-MethodName
methods O
, O
BERT-Freeze B-MethodName
freezes O
BERT B-MethodName
; O
CON-VBERT B-MethodName
( O
Mehri O
et O
al. O
, O
2020 O
) O
, O
TOD-BERT B-MethodName
, O
and O
DNNC-BERT B-MethodName
( O
Zhang O
et O
al. O
, O
c O
) O
further O
pre-train O
BERT B-MethodName
on O
conversational O
corpus O
or O
natural B-MethodName
language I-MethodName
inference I-MethodName
tasks. O
USE-ConveRT B-MethodName
Casanueva O
et O
al. O
, O
2020 O
) O
5 O
: O
5-way O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
using O
BERT. B-MethodName
We O
report O
the O
mean O
and O
standard O
deviation O
of O
our O
methods O
and O
IntentBERT B-MethodName
variants. O
CL-Reg B-HyperparameterName
, O
Cor-Reg B-HyperparameterName
, O
and O
CL-Reg B-HyperparameterName
+ O
CorReg B-HyperparameterName
denote O
supervised O
pre-training O
regularized O
by O
the O
corresponding O
regularizer. O
The O
top O
3 O
results O
are O
highlighted. O
¶ O
denotes O
results O
from O
( O
Zhang O
et O
al. O
, O
a O
Finally O
, O
to O
show O
the O
superiority O
of O
the O
joint O
finetuning O
and O
isotropization O
, O
we O
compare O
our O
method O
against O
whitening O
transformation O
. O
BERT-White B-MethodName
and O
RoBERTa-White B-MethodName
apply O
the O
transformation O
to O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
respectively. O
IntentBERT-White B-MethodName
and O
IntentRoBERTa-White B-MethodName
apply O
the O
transformation O
to O
IntentBERT-ReImp B-MethodName
and O
IntentRoBERTa B-MethodName
, O
respectively. O
All O
baselines O
use O
logistic O
regression O
as O
classifier O
except O
DNNC-BERT B-MethodName
and O
DNNC-RoBERTa B-MethodName
, O
wherein O
we O
follow O
the O
original O
work O
2 O
to O
train O
a O
pairwise O
encoder O
for O
nearest O
neighbor O
classification O
. O

Training O
Details. O
We O
use O
PyTorch O
library O
and O
Python O
to O
build O
our O
model. O
We O
employ O
Hugging O
Face O
implementation O
3 O
of O
bert-base-uncased B-MethodName
and O
roberta-base. O
We O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
as O
the O
optimizer O
with O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e B-HyperparameterValue
− I-HyperparameterValue
05 I-HyperparameterValue
and O
weight B-HyperparameterName
decay I-HyperparameterName
of O
1e B-HyperparameterValue
− I-HyperparameterValue
03. I-HyperparameterValue
The O
model O
is O
trained O
with O
Nvidia O
RTX O
3090 O
GPUs. O
The O
training O
is O
early O
stopped O
if O
no O
improvement O
in O
validation O
accuracy B-MetricName
is O
observed O
for O
100 B-HyperparameterValue
steps. B-HyperparameterName
The O
same O
set O
of O
random O
seeds B-HyperparameterName
, O
{ B-HyperparameterValue
1 I-HyperparameterValue
, I-HyperparameterValue
2 I-HyperparameterValue
, I-HyperparameterValue
3 I-HyperparameterValue
, I-HyperparameterValue
4 I-HyperparameterValue
, I-HyperparameterValue
5 I-HyperparameterValue
} I-HyperparameterValue
, O
is O
used O
for O
IntentBERT-ReImp B-MethodName
, O
IntentRoBERTa B-MethodName
, I-MethodName
and O
our O
method O
. O

Evaluation. O
The O
baselines O
and O
our O
method O
are O
evaluated O
on O
C-way B-MetricName
K-shot B-MetricName
tasks. O
For O
each O
task O
, O
we O
randomly O
sample O
C B-MetricName
classes O
and O
K B-MetricName
examples O
per O
class. O
The O
C B-MetricName
×K B-MetricName
labeled O
examples O
are O
used O
to O
train O
the O
logistic O
regression O
classifier. O
Note O
that O
we O
do O
not O
further O
fine-tune O
the O
PLM B-MethodName
using O
the O
labeled O
data O
of O
the O
task. O
We O
then O
sample O
another O
5 O
examples O
per O
class O
as O
queries. O
Fig. O
1 O
gives O
an O
example O
with O
C B-MetricName
= O
2 B-MetricValue
and O
K B-MetricName
= O
1. B-MetricValue
We O
report O
the O
averaged O
accuracy B-MetricName
of O
500 O
tasks O
randomly O
sampled O
from O
D O
target O
. O

Main O
Results O
The O
main O
results O
are O
provided O
in O
Table O
5 O
( O
BERTbased B-MethodName
) O
and O
Table O
6 O
( O
RoBERTa-based B-MethodName
) O
. O
The O
following O
observations O
can O
be O
made. O
First O
, O
our O
proposed O
regularized O
supervised O
pre-training O
, O
with O
either O
CL-Reg B-HyperparameterName
or O
Cor-Reg B-HyperparameterName
, O
consistently O
outperforms O
all O
the O
baselines O
by O
a O
notable O
margin O
in O
most O
cases O
, O
indicating O
the O
effectiveness O
of O
our O
method. O
Our O
method O
also O
outperforms O
whitening O
transformation O
, O
demonstrating O
the O
superiority O
of O
the O
proposed O
joint O
finetuning O
and O
isotropization O
framework. O
Second O
, O
Cor-Reg B-HyperparameterName
slightly O
outperforms O
CL-Reg B-HyperparameterName
in O
most O
cases O
, O
showing O
the O
advantage O
of O
enforcing O
isotropy O
explicitly O
with O
the O
correlation O
matrix. O
Finally O
, O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
show O
a O
complementary O
effect O
in O
many O
cases O
, O
especially O
on O
BANKING77. B-DatasetName
The O
above O
observations O
are O
consistent O
for O
both O
BERT B-MethodName
and O
RoBERTa. B-MethodName
It O
can O
be O
also O
seen O
that O
higher O
performance O
is O
often O
attained O
with O
RoBERTa. B-MethodName
The O
observed O
improvement O
in O
performance O
comes O
with O
an O
improvement O
in O
isotropy. B-MetricName
We O
report O
the O
change O
in O
isotropy O
by O
the O
proposed O
regularizers O
in O
Table O
7. O
It O
can O
be O
seen O
that O
both O
regularizers O
and O
their O
combination O
make O
the O
feature O
space O
more O
isotropic O
compared O
to O
IntentBERT-ReImp B-MethodName
that O
only O
uses O
supervised O
pre-training. O
In O
addition O
, O
in O
general O
, O
Cor-Reg B-HyperparameterName
can O
achieve O
better O
isotropy B-MetricName
than O
CL-Reg B-HyperparameterName
. O

Ablation O
Study O
and O
Analysis O
Moderate O
isotropy B-MetricName
is O
helpful. O
To O
investigate O
the O
relation O
between O
the O
isotropy B-MetricName
of O
the O
feature O
space O
and O
the O
performance O
of O
few-shot B-TaskName
intent I-TaskName
detection I-TaskName
, O
we O
tune O
the O
weight B-HyperparameterName
parameter I-HyperparameterName
λ B-HyperparameterName
of O
Cor-Reg B-HyperparameterName
to O
increase O
the O
isotropy B-MetricName
and O
examine O
the O
performance. O
As O
shown O
in O
Fig. O
5 O
, O
a O
common O
pattern O
is O
observed O
: O
the O
best O
performance O
is O
achieved O
when O
the O
isotropy B-MetricName
is O
moderate. O
This O
observation O
indicates O
that O
it O
is O
important O
to O
find O
an O
appropriate O
trade-off O
between O
learning O
intent O
detection O
skills O
and O
learning O
an O
insotropic O
feature O
space. O
In O
our O
method O
, O
we O
select O
the O
appropriate O
λ B-HyperparameterName
by O
validation. O
Correlation O
matrix O
is O
better O
than O
covariance O
matrix O
as O
regularizer. O
In O
the O
design O
of O
Cor-Reg B-HyperparameterName
( O
Section O
4.2 O
) O
, O
we O
use O
the O
correlation O
matrix O
, O
rather O
than O
the O
covariance O
matrix O
, O
to O
characterize O
isotropy B-MetricName
, O
although O
the O
latter O
contains O
more O
informationvariance. O
The O
reason O
is O
that O
it O
is O
difficult O
to O
determine O
the O
proper O
scale O
of O
the O
variances. O
Here O
, O
we O
conduct O
experiments O
using O
the O
covariance O
matrix O
, O
by O
pushing O
the O
non-diagonal B-HyperparameterName
elements I-HyperparameterName
( O
covariances B-HyperparameterName
) O
towards O
0 B-HyperparameterValue
and O
the O
diagonal B-HyperparameterName
elements I-HyperparameterName
( O
variances B-HyperparameterName
) O
towards O
1 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
or O
the O
mean O
value O
, O
which O
are O
denoted O
by O
Cov-Reg-1 B-HyperparameterName
, O
Cov-Reg-0.5 B-HyperparameterName
, I-HyperparameterName
and O
Cov-Reg-mean B-HyperparameterName
respectively O
in O
Table O
8. O
It O
can O
be O
seen O
that O
all O
the O
variants O
perform O
worse O
than O
Cor-Reg O
. O

Our O
method O
is O
complementary O
with O
batch O
normalization. O
Batch O
normalization O
( O
Ioffe O
and O
Szegedy O
, O
2015 O
) O
anisotropy O
problem O
via O
normalizing O
each O
dimension O
with O
unit O
variance. O
We O
find O
that O
combining O
our O
method O
with O
batch O
normalization O
yields O
better O
performance O
, O
as O
shown O
in O
Table O
9. O
The O
performance O
gain O
is O
not O
from O
the O
reduction O
in O
model O
variance. O
Regularization O
techniques O
such O
as O
L1 O
regularization O
( O
Tibshirani O
, O
1996 O
) O
andL2 O
regularization O
( O
Hoerl O
andKennard O
, O
1970 O
) O
are O
often O
used O
to O
improve O
model O
performance O
by O
reducing O
model O
variance. O
Here O
, O
we O
show O
that O
the O
performance O
gain O
of O
our O
method O
is O
ascribed O
to O
the O
improved O
isotropy B-MetricName
( O
Table O
7 O
) O
rather O
than O
the O
reduction O
in O
model O
variance. O
To O
this O
end O
, O
we O
compare O
our O
method O
against O
L2 O
regularization O
with O
a O
wide O
range O
of O
weights O
, O
and O
it O
is O
observed O
that O
reducing O
model O
variance O
can O
not O
achieve O
comparable O
performance O
to O
our O
method O
, O
as O
shown O
in O
Fig. O
6. O
The O
computational O
overhead O
is O
small. O
To O
analyze O
the O
computational O
overheads O
incurred O
by O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
, O
we O
decompose O
the O
duration O
of O
one O
epoch O
of O
our O
method O
using O
the O
two O
regularizers O
jointly. O
As O
shown O
in O
Fig. O
7 O
, O
the O
overheads O
of O
CL-Reg B-HyperparameterName
and O
Cor-Reg B-HyperparameterName
are O
small O
, O
only O
taking O
up O
a O
small O
portion O
of O
the O
time O
. O

Acknowledgments O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
comments. O
This O
research O
was O
supported O
by O
the O
grants O
of O
HK O
ITF O
UIM O
/ O
377 O
and O
PolyU O
DaSAIL O
project O
P0030935 O
funded O
by O
RGC O
. O

